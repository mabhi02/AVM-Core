[
  {
    "theorem": "Can every proof by contradiction also be shown without contradiction?",
    "context": "Are there some proofs that can only be shown by contradiction or can everything that can be shown by contradiction also be shown without contradiction? What are the advantages/disadvantages of proving by contradiction?\nAs an aside, how is proving by contradiction viewed in general by 'advanced' mathematicians. Is it a bit of an 'easy way out' when it comes to trying to show something or is it perfectly fine? I ask because one of our tutors said something to that effect and said that he isn't fond of proof by contradiction.\n",
    "proof": "To determine what can and cannot be proved by contradiction, we have to formalize a notion of proof.  As a piece of notation, we let $\\bot$ represent an identically false proposition. Then $\\lnot A$, the negation of $A$, is equivalent to $A \\to \\bot$, and we take the latter to be the definition of the former in terms of $\\bot$. \nThere are two key logical principles that express different parts of what we call \"proof by contradiction\":\n\nThe principle of explosion: for any statement $A$, we can take \"$\\bot$ implies $A$\" as an axiom.  This is also called ex falso quodlibet.  \nThe law of the excluded middle: for any statement $A$, we can take \"$A$ or $\\lnot A$\" as an axiom. \n\nIn proof theory, there are three well known systems:\n\nMinimal logic has neither of the two principles above, but it has basic proof rules for manipulating logical connectives (other than negation) and quantifiers. This system corresponds most closely to \"direct proof\", because it does not let us leverage a negation for any purpose. \nIntuitionistic logic includes minimal logic and the principle of explosion\nClassical logic includes intuitionistic logic and the law of the excluded middle\n\nIt is known that there are statements that are provable in intuitionistic logic but not in minimal logic, and there are statements that are provable in classical logic that are not provable in intuitionistic logic. In this sense, the principle of explosion allows us to prove things that would not be provable without it, and the law of the excluded middle allows us to prove things we could not prove even with the principle of explosion.  So there are statements that are provable by contradiction that are not provable directly. \nThe scheme \"If $A$ implies a contradiction, then $\\lnot A$ must hold\" is true even in intuitionistic logic, because $\\lnot A$ is just an abbreviation for $A \\to \\bot$, and so that scheme just says \"if $A \\to \\bot$ then $A \\to \\bot$\". But in intuitionistic logic, if we prove $\\lnot A \\to \\bot$, this only shows that $\\lnot \\lnot A$ holds. The extra strength in classical logic is that the law of the excluded middle shows that $\\lnot \\lnot A$ implies $A$, which means that in classical logic if we can prove $\\lnot A$ implies a contradiction then we know that $A$ holds. In other words: even in intuitionistic logic, if a statement implies a contradiction then the negation of the statement is true, but in classical logic we also have that if the negation of a statement implies a contradiction then the original statement is true, and the latter is not provable in intuitionistic logic, and in particular is not provable directly. \n",
    "tags": [
      "logic",
      "proof-writing",
      "propositional-calculus",
      "proof-theory"
    ],
    "score": 381,
    "answer_score": 303,
    "is_accepted": true,
    "question_id": 243770,
    "answer_id": 243866
  },
  {
    "theorem": "Why do people use &quot;it is easy to prove&quot;?",
    "context": "Math is not generally what I am doing, but I have to read some literature and articles in dynamic systems and complexity theory. What I noticed is that authors tend to use (quite frequently) the phrase \"it is easy to see/prove/verify/...\" in the manuscripts. But to me, it is usually not easy at all; maybe because I haven't spent much time in the field, maybe not.\nMy question is: why do people use the phrase \"it is easy\" in their proofs?\nP.S. I hope this question is not too subjective, and has some value for the community.\n",
    "proof": "Use of \"it is easy to see that\" is common and traditional in mathematical writing, but it is not exactly a proud tradition: really good mathematical exposition uses this and similar locutions very sparingly.\nTo be more specific, I think it is bad writing to say \"It is easy to see that X is true\" and say no more about how to prove X.  If this occurs in formal mathematical writing and all else is as it should be, then no information is being conveyed.  In other words, what other reason is there to assert that X is true and say no more about the proof except that the author expects the reader to be able to supply the details unassisted?  If you are skipping the proof for any other reason, you had better say something!\n[And of course some of the harm is psychological.  If you're carefully reading a text or paper that asserts X is true and says nothing else about it, you know you need to stop and think about how to prove X.  If \"it is easy to see X\" then after every minute or so part of your brain will quit thinking about how to prove X and think, \"They said it was easy to see this, and I can't see it at all.  What am I, stupid?\"  I definitely remember thinking this way when I started out reading \"serious\" math books.]\nWhen I referee papers I often suggest that authors suppress their \"it is easy to see that\"'s.  As others have said in the comments, as a careful, skeptical reader, you also need to stop and be sure that indeed you can supply the proof yourself, and it is notorious among mathematicians that such phrases are likely places to find gaps in mathematical arguments.  But it is just as easy -- in fact, easier -- to have a gap in the argument where you don't have any text at all, so writing \"it is easy to see that\" is not really the guilty party but rather a possible piece of incriminating evidence.  \nSo if it's not so good to write this, why do people write this way?  And they certainly do: I happened to be editing my commutative algebra notes when I read this question, so out of curiosity I searched for \"easy\" and found about ten instances of \"it is easy to see that\" in 265 pages of notes.  About half of them I simply took out.  The other half I thought were okay because I didn't just say \"it is easy to see that\": I went on to explain why it was easy!  So having caught myself doing what I said not to, I can reflect on some causes:\n1) Tradition/habit.  \nI have read \"it is easy to see that\" thousands of times, so it is in my vocabulary whether I like it or not.  Most mathematicians know that they have funny phrases which appear ubiquitously in mathematical writing but not in the rest of their lives: one of the very first questions I answered on this site was about the meaning and use of \"in the sequel\".  In the year or so since then I have observed it in my own writing: it just fits in there.  You have to really actively dislike some of these standard locutions in order to avoid writing them yourself.  For instance, I have more than a thousand pages of mathematical writings available online and I challenge anyone to find \"by inspection\" anywhere in these.  \"By inspection\" is the deformed cousin of \"it is easy to see that\": whereas at least it is easy to see what \"it is easy to see that\" means, even the meaning of \"by inspection\" is obscure.  \n2) A conflation of formal writing and informal writing / speaking / teaching.\nThe way you speak mathematics to someone else is very different from the way you write it: it is much more temporal.  If you are teaching someone new mathematics then most often they cannot verify / process / understand every single mathematical statement you make, in real time, so they have to make choices about exactly what to think about as you're talking to them.  In spoken conversation it's extremely useful to say \"this is easy\": by saying it, you're cueing the listener that it's safe to direct her attention elsewhere.  Also, because when you talk -- or write informally --  you don't give anywhere near as complete information as you do in formal mathematical writing, commentary on what you're skipping becomes more important.  For instance, in an intermediate level graduate course I may prove approximately 2/3 of the theorems I state in class.  If I'm skipping something, it's probably because it's too easy or too hard.  I had better say which it is!\n3) Immaturity/Laziness.\nCertainly when you're reading your own writing and you find \"it is easy to see that\", you need to stop short and make sure you know exactly what you omitted.  If it's not easy for you to see what you wrote it was easy to see, you may have a serious problem: indeed, you may be papering over a gap in your argument.  To do this intentionally is a sign of great mathematical immaturity -- someone who hides (in plain sight!) what they don't know in this way is not going to make it very far in this profession -- but even doing it unintentionally is something that most mathematicians largely grow out of with experience.  \n",
    "tags": [
      "soft-question",
      "math-history",
      "proof-writing"
    ],
    "score": 175,
    "answer_score": 153,
    "is_accepted": false,
    "question_id": 54431,
    "answer_id": 54440
  },
  {
    "theorem": "Why are mathematical proofs that rely on computers controversial?",
    "context": "There are many theorems in mathematics that have been proved with the assistance of computers, take the famous four color theorem for example. Such proofs are often controversial among some mathematicians. Why is it so?\nI my opinion, shifting from manual proofs to computer-assisted proofs is a giant leap forward for mathematics. Other fields of science rely on it heavily. Physics experiments are simulated in computers. Chemical reactions are simulated in supercomputers. Even evolution can be simulated in an advanced enough computer. All of this can help us understand these phenomena better. \nBut why are mathematicians so reluctant?\n",
    "proof": "What is mathematics? One answer is that mathematics is a collection of definitions, theorems, and proofs of them. But the more realistic answer is that mathematics is what mathematicians do. (And partly, that's a social activity.) Progress in mathematics consists of advancing human understanding of mathematics.\nWhat is a proof for? Often we pretend that the reason for a proof is so that we can be sure that the result is true. But actually what mathematicians are looking for is understanding.\nI encourage everyone to read the article On Proof and Progress in Mathematics by the Fields Medalist William Thurston. He says (on page 2):\n\nThe rapid advance of computers has helped dramatize this point, because computers and people are very different. For instance, when Appel and Haken completed a proof of the 4-color map theorem using a massive automatic computation, it evoked much controversy. I interpret the controversy as having little to do with doubt people had as to the veracity of the theorem or the correctness of the proof. Rather, it reflected a continuing desire for human understanding of a proof, in addition to knowledge that the theorem is true.\nOn a more everyday level, it is common for people first starting to grapple\nwith computers to make large-scale computations of things they might have\ndone on a smaller scale by hand. They might print out a table of the first\n10,000 primes, only to find that their printout isn’t something they really\nwanted after all. They discover by this kind of experience that what they\nreally want is usually not some collection of “answers”—what they want is\nunderstanding.\n\nSome people may claim that there is doubt about a proof when it has been proved by a computer, but I think human proofs have more room for error. The real issue is that (long) computer proofs (as opposed to, something simple like checking a numerical value by calculator) are hard to keep in your head.\nCompare these quotes from Gian-Carlo Rota's Indiscrete Thoughts, where he describes the mathematicians' quest for understanding:\n\n“eventually every mathematical problem is proved trivial. The quest for ultimate triviality is characteristic of the mathematical enterprise.” (p.93)\n“Every mathematical theorem is eventually proved trivial. The mathematician’s ideal of truth is triviality, and the community of mathematicians will not cease its beaver-like work on a newly discovered result until it has shown to everyone’s satisfaction that all difficulties in the early proofs were spurious, and only an analytic triviality is to be found at the end of the road.” (p. 118, in The Phenomenology of Mathematical Truth)\nAre there definitive proofs?\nIt is an article of faith among mathematicians that after a new theorem is discovered, other simpler proofs of it will be given until a definitive one is found. A cursory inspection of the history of mathematics seems to confirm the mathematician’s faith. The first proof of a great many theorems is needlessly complicated. “Nobody blames a mathematician if the first proof of a new theorem is clumsy”, said Paul Erdős. It takes a long time, from a few decades to centuries, before the facts that are hidden in the first proof are understood, as mathematicians informally say. This gradual bringing out of the significance of a new discovery takes the appearance of a succession of proofs, each one simpler than the preceding. New and simpler versions of a theorem will stop appearing when the facts are finally understood. (p.146, in The Phenomenology of Mathematical Proof).\n\nIn my opinion, there is nothing wrong with, or doubtful about, a proof that relies on computer. However, such a proof is in the intermediate stage described above, that has not yet been rendered trivial enough to be held in a mathematician's head, and thus the theorem being proved is to be considered still work in progress.\n",
    "tags": [
      "soft-question",
      "proof-writing"
    ],
    "score": 115,
    "answer_score": 117,
    "is_accepted": true,
    "question_id": 632705,
    "answer_id": 632745
  },
  {
    "theorem": "Does notation ever become &quot;easier&quot;?",
    "context": "I'm in my first semester of college going for a math major and it's pretty great. I'm doing well, however, there seems to be huge gap between how difficult /complex an idea is and how convoluted it is presented. \nLet me make an example: In Analysis we discussed the Bolzano Weierstrass theorem and one of the lemmas showed that every sequence in $\\mathbb{R}$ has a monotone subsequence. The idea behind the proof with the maximum spots ( speaking colloquially here ) is super simple and pretty elegant if you asked me, but I spent a significant amount of time trying to understand the notation of the professor until I went to this site to read a \"proper explanation\" of the proof, which had much simpler notation in it. \nExtracting the idea of the proof took me lots of time because of the strange notation, but once you understand what is going on, it is really easy. Most of the time spent studying lectures is about digging through the formalities. \nDo I just have to spend more time really going through all the formal details of a proof to become accustomed to that formality? Or do more advanced mathematicians also struggle to extract the ideas from the notation?\nI'd assume there will come a point, where the idea itself is the most complex part, so I do not want to get stuck at the notation, when that happens.\n( Proof Verification - Every sequence in $\\Bbb R$ contains a monotone sub-sequence if you are interested )\n",
    "proof": "As others have pointed out, it gets much better if that's your first semester. But in my experience, there is not much relief between, say, years 2 and 4 of your studies. Sure, you get more mature, but the material gets more difficult too. \nTo address your question whether \"more advanced mathematicians also struggle to extract the ideas from the notation\", I'd like to quote V.I. Arnold, since I think it's exactly in the spirit of your frustration.\n\nIt is almost impossible for me to read contemporary mathematicians who, instead of saying \"Petya washed his hands,\" write simply: \"There is a $t_1<0$ such that the image of $t_1$ under the natural mapping $t_1 \\mapsto {\\rm Petya}(t_1)$ belongs to the set of dirty hands, and a $t_2$, $t_1<t_2 \\leq 0$, such that the image of $t_2$ under the above-mentioned mapping belongs to the complement of the set defined in the preceding sentence.''\n\nThe trade-off is clear: without rigor math would've been quite a mess. But if rigor is the only way math gets communicated to someone, this person simply won't have time to get far in math.\n",
    "tags": [
      "notation",
      "proof-writing",
      "self-learning"
    ],
    "score": 100,
    "answer_score": 135,
    "is_accepted": true,
    "question_id": 2022771,
    "answer_id": 2022948
  },
  {
    "theorem": "Use of &quot;without loss of generality&quot;",
    "context": "Why do we use \"without loss of generality\" when writing proofs?\nIs it necessary or convention? What \"synonym\" can be used?\n",
    "proof": "I think this is great question, as the mathematical use of \"without loss of generality\" often varies from its literal meaning.  The literal meaning is when you rephrase a general statement\n\n$P(x)$ is true for all $x \\in S$,\n\nusing another set (which is easier to work with)\n\n$P(z)$ is true for all $z \\in T$,\n\nwhere $P$ is some property of elements in $S$ and $T$, and it can be shown (or is known) that $S=T$.\nFor example:\n\nWe want to show that $P(x)$ is true for all $x \\in \\mathbb{Z}$.  Without loss of generality, we can assume that $x=z+1$ for some $z \\in \\mathbb{Z}$.  [In this case, $S=\\mathbb{Z}$ and $T=\\{z+1:z \\in \\mathbb{Z}\\}$.]\n\nWe want to show that $P(x)$ is true for all $x \\in \\mathbb{Z}$.  Without loss of generality, we can assume that $x=5q+r$ where $q,r \\in \\mathbb{Z}$ and $0 \\leq r < q$.  [In this case, $S=\\mathbb{Z}$ and $T=\\{5q+r:q \\in \\mathbb{Z} \\text{ and } r \\in \\mathbb{Z} \\text{ and } 0 \\leq r < q\\}$.]\n\n\nIn the above instances, indeed no generality has been lost, since in each case we can prove $S=T$ (or, more likely, it would be assumed that the reader can deduce that $S=T$).  I.e., proving that $P(z)$ holds for $z \\in T$ is the same as proving that $P(x)$ holds for $x \\in S$.\nThe above cases are examples of clear-cut legitimate usage of \"without loss of generality\", but there is a widespread second use.  Wikipedia writes:\n\nThe term is used before an assumption in a proof which narrows the premise to some special case; it is implied that the proof for that case can be easily applied to all others (or that all other cases are equivalent). Thus, given a proof of the conclusion in the special case, it is trivial to adapt it to prove the conclusion in all other cases.\n\n[emphasis mine.]\nSo, paradoxically, \"without loss of generality\" is often used to highlight when the author has deliberately lost generality in order to simplify the proof.  Thus, we are rephrasing a general statement:\n\n$P(x)$ is true for all $x \\in S$,\n\nas\n\n$P(z)$ is true for all $z \\in T$, and\nif $x \\in S$, then there exists $z \\in T$ for which $P(x)$ is true if $P(z)$ is true.\n\nFor example:\n\nLet $S$ be a set of groups of order $n$.  We want to show $P(G)$ is true for all $G \\in S$.  Without loss of generality, assume the underlying set of $G$ is $\\{0,1,\\ldots n-1\\}$ for some $n \\geq 1$.  [Here, $T$ is a set of groups with underlying set $\\{0,1,\\ldots n-1\\}$ that are isomorphic to groups in $S$, and the reader is assumed to be able to deduce that property $P$ is preserved by isomorphism.]\n\nMy personal preference is to replace the second case with:\n\n\"It is sufficient to prove $P(z)$ for $z \\in T$, since [[for some reason]] it follows that $P(x)$ is true for all $x \\in S$.\"\n\n",
    "tags": [
      "terminology",
      "proof-writing"
    ],
    "score": 80,
    "answer_score": 57,
    "is_accepted": true,
    "question_id": 129137,
    "answer_id": 129159
  },
  {
    "theorem": "How do I prove that a function is well defined?",
    "context": "How do you in general prove that a function is well-defined?\n$$f:X\\to Y:x\\mapsto f(x)$$\nI learned that I need to prove that every point has exactly one image. Does that mean that I need to prove the following two things:\n\nEvery element in the domain maps to an element in the codomain:\n$$x\\in X \\implies f(x)\\in Y$$\nThe same element in the domain maps to the same element in the codomain:\n$$x=y\\implies f(x)=f(y)$$\n\n\nAt the moment I'm trying to prove this function is well-defined: $$f:(\\Bbb Z/12\\mathbb Z)^∗→(\\Bbb Z/4\\Bbb Z)^∗:[x]_{12}↦[x]_4 ,$$ but I'm more interested in the general procedure.\n",
    "proof": "When we write $f\\colon X\\to Y$ we say three things:\n\n$f\\subseteq X\\times Y$.\nThe domain of $f$ is $X$.\nWhenever $\\langle x,y_1\\rangle,\\langle x,y_2\\rangle\\in f$ then $y_1=y_2$. In this case whenever $\\langle x,y\\rangle\\in f$ we denote $y$ by $f(x)$.\n\nSo to say that something is well-defined is to say that all three things are true. If we know some of these we only need to verify the rest, for example if we know that $f$ has the third property (so it is a function) we need to verify its domain is $X$ and the range is a subset of $Y$. If we know those things we need to verify the third condition.\nBut, and that's important, if we do not know that $f$ satisfies the third condition we cannot write $f(x)$ because that term assumes that there is a unique definition for that element of $Y$.\n",
    "tags": [
      "functions",
      "logic",
      "proof-writing"
    ],
    "score": 79,
    "answer_score": 74,
    "is_accepted": true,
    "question_id": 313169,
    "answer_id": 313182
  },
  {
    "theorem": "Is it bad form to write mysterious proofs without explaining what one intends to do?",
    "context": "Often when doing assignments, I find myself deliberately writing in a \"mysterious\" way. By this I mean that the reader usually will not understand what exactly is going on and what for, until the very end where all the things come together. \nA simple example is if I wish to prove that $S$ is true by showing that it is equivalent to $s$ being true, and then proving that $s$ is true. Often I will find myself doing this by writing something that reads like ... \n\n\"Consider s, this seemingly random object that I present to you out of\n  nowhere. Let us work on this for the next 1-2 pages, don't ask me why\n  ..... [1-2 pages later] .... and that proves that $s$ is true. Now by noticing this and that, we see that this implies $S$ is true.  Surprise! QED\"\n\nIs this bad form? It also seems a bit pretentious to be, because the reason I think I sometimes do this is because these are the proofs I have mostly met in textbooks. Rarely is the proof sketched before it's given, very often new, foreign, confusing objects are introduced without introductions and motivations, and it's usually near the end of the proof that I would get my \"aha\" moment. The problem with this is of course that if one does not know the why behind some of the steps during the first reading, then one will have a harder time remembering how the pieces fit together throughout the proof.\nBut of course my instructors are not students reading (undergaduate) textbooks, and therefore they can perhaps deal with these sorts of mysterious proofs? Maybe they even prefer them, rather than having to waste time on reading me informally writing a few sentences prior to the proof outlining my ideas, giving a proof sketch, etc? I also do not wish to run the risk of sounding patronising or arrogant: \"look at me and my geniusly complicated proof that I will now explain to you step by step\".\n",
    "proof": "The purpose of proofs is communication. If your proof is obscure, then you have failed to communicate.\nStrive to be as clear as possible, including motivation for complicated arguments, if necessary.\n",
    "tags": [
      "proof-writing"
    ],
    "score": 75,
    "answer_score": 119,
    "is_accepted": false,
    "question_id": 2070143,
    "answer_id": 2070160
  },
  {
    "theorem": "Prove: If a sequence converges, then every subsequence converges to the same limit.",
    "context": "I need some help understanding this proof:\nProve: If a sequence converges, then every subsequence converges to the same limit.\nProof:\nLet $s_{n_k}$ denote a subsequence of $s_n$. Note that $n_k \\geq k$ for all $k$. This easy to prove by induction: in fact, $n_1 \\geq 1$ and $n_k \\geq k$ implies $n_{k+1} > n_k \\geq k$ and hence $n_{k+1} \\geq k+1$.\nLet $\\lim s_n = s$ and let $\\epsilon > 0$. There exists $N$ so that $n>N$ implies $|s_n - s| < \\epsilon$. Now $k > N \\implies n_k > N \\implies |s_{n_k} - s| < \\epsilon$. \nTherefore: $\\lim_{k \\to \\infty} s_{n_k} = s$.\n\nWhat is the intuition that each subsequence will converge to the same limit\nI do not understand the induction that claims $n_k \\geq k$\n\n",
    "proof": "\nA sequence converges to a limit $L$ provided that, eventually, the entire tail of the sequence is very close to $L$. If you restrict your view to a subset of that tail, it will also be very close to $L$.\nAn example might help. Suppose your subsequence is to take every other index: $n_1 = 2$, $n_2 = 4$, etc. In general, $n_k = 2k$. Notice $n_k \\geq k$, since each step forward in the sequence makes $n_k$ increase by $2$, but $k$ increases only by $1$. The same will be true for other kinds of subsequences (i.e. $n_k$ increases by at least $1$, while $k$ increases by exactly $1$).\n\n",
    "tags": [
      "real-analysis",
      "limits",
      "proof-writing"
    ],
    "score": 74,
    "answer_score": 38,
    "is_accepted": true,
    "question_id": 213285,
    "answer_id": 213288
  },
  {
    "theorem": "Will assuming the existence of a solution ever lead to a contradiction?",
    "context": "I'm reading Manfredo Do Carmo's differential geometry book. In section 1-7, he discusses the \"Isoperimetric Inequality\" which is related to the question of what 2-dimensional shape maximizes the enclosed area for a closed curve of constant length. He mentions that \n\nA satisfactory proof of the fact that the circle is a solution to the isoperimetric problem took, however, a long time to appear. The main reason seems to be that the earliest proofs assumed that a solution should exist. It was only in 1870 that K. Weierstrass pointed out that many similar questions did not have solutions.\n\nThis line of reasoning would suggest that assuming the existence of a solution might lead to a contradiction (such as an apparent solution that is not in fact valid). Is this actually a problem?\nAre there any problems that produce invalid solutions under the (flawed) assumption that a solution exists at all? If so, what is an example and how does it differ from the statement of the isoperimetric problem?\n",
    "proof": "Just the first thing that came to my mind... assume $A=\\sum_{n=0}^{\\infty}2^n $ exists, it is very easy to find $A $: note $A=1+2\\sum_{n=0}^{\\infty}2^n =1+2A $, so $A=-1$.\nOf course, this is all wrong precisely because $A $ does not exist.\n",
    "tags": [
      "logic",
      "proof-writing"
    ],
    "score": 71,
    "answer_score": 95,
    "is_accepted": true,
    "question_id": 2582488,
    "answer_id": 2582494
  },
  {
    "theorem": "Is it technically incorrect to write proofs forward?",
    "context": "A question on an assignment was similar to prove:\n$$2a^2-7ab+2b^2 \\geq -3ab.$$\nand my proof was:\n$$2a^2-4ab+2b^2\\geq0$$\n$$a^2-2ab+b^2\\geq0$$\n$$(a-b)^2\\geq0$$\nwhich is true.\nHowever, my professor marked this as incorrect and the \"correct\" way to do it was:\nStarting from $$(a-b)^2\\geq0$$ we have:\n$$a^2-2ab+b^2\\geq0$$\n$$2a^2-4ab+2b^2\\geq0$$\n$$2a^2-7ab+2b^2 \\geq -3ab.$$\nHis point was that if we start with a false statement we can also reduce it to a true statement (like $-5 =5$ we can square for $25 = 25$). I argued however that you can go back and forth between my operations (if and only if, which doesn't work for the $-5 =5$ example). He still didn't give me the marks for it. Which leads me to my questions:\n\nIs my proof equally valid?\nDo real mathematicians all write one way or the other when writing in a paper?\n\n",
    "proof": "There are two ways to interpret your argument (which hardly counts as a proof if this is exactly what you have written in your assignment).\n\nFirst Way\nYou actually meant to write, \\begin{align}2a^2-7ab+2b^2 \\geq -3ab&\\implies 2a^2-4ab+2b^2\\geq0\\\\&\\implies a^2-2ab+b^2\\ge 0\\\\&\\implies (a-b)^2\\ge0\\end{align}\nSecond Way\nYou actually meant to write, \\begin{align}2a^2-7ab+2b^2 \\geq -3ab&\\iff 2a^2-4ab+2b^2\\geq0\\\\&\\iff a^2-2ab+b^2\\ge 0\\\\&\\iff (a-b)^2\\ge0\\end{align}\n\nIn the second case your argument is correct but in the first case it is not (as your professor has elaborated via an example).\nNow the answer to your questions,\n\nIf you didn't say explicitly in your assignment in which way your argument is to be interpreted and then if your teacher interprets your argument in the first way, you can't blame him/her for not giving you the corresponding marks of the question simply because it was you who failed to be explicit )and the first way of interpreting your argument indeed shows a misunderstanding of the working of $\\implies$). So, if I were in the position of your professor, I would give you no marks.\n\nI don't know what you mean by \"one way or the other\" here. The best way to answer this question will be to read the papers of some real mathematicians. However, when real mathematicians write a paper it is in general clear what they are assuming to be true and how the steps lead to the conclusion (which is not the case in your argument as I have explained above).\n\n\n",
    "tags": [
      "proof-writing",
      "alternative-proof"
    ],
    "score": 71,
    "answer_score": 108,
    "is_accepted": false,
    "question_id": 2432558,
    "answer_id": 2432598
  },
  {
    "theorem": "How does a non-mathematician go about publishing a proof in a way that ensures it to be up to the mathematical community&#39;s standards?",
    "context": "I'm a computer science student who is a maths hobbyist. I'm convinced that I've proven a major conjecture. The problem lies in that I've never published anything before and am not a mathematician by profession. Knowing full well that my proof may be fallacious, erroneous, or simply lacking mathematical formality, what advice would you give me?\n",
    "proof": "\nI'm convinced that I've proved a major conjecture.\n\nYou are almost certainly mistaken. I say this on purely probabilistic grounds, so don't get upset $-$ even professional mathematicians are sometimes mistaken about their own 'proofs', and amateurs almost always.\nI suggest you tell us what this major conjecture is, and post a link to your proof (or just post it here, if it's short enough). This is enough to establish your priority, if you are worried about somebody stealing your proof. Then the sharks of MSE can devour it.\nPS Your proof will probably be more favourably received if it is nicely formatted, using LaTeX.\n",
    "tags": [
      "soft-question",
      "proof-writing",
      "advice",
      "publishing"
    ],
    "score": 66,
    "answer_score": 71,
    "is_accepted": false,
    "question_id": 275194,
    "answer_id": 275197
  },
  {
    "theorem": "What is a proof?",
    "context": "I am just a high school student, and I haven't seen much in mathematics (calculus and abstract algebra).\nMathematics is a system of axioms which you choose yourself for a set of undefined entities, such that those entities satisfy certain basic rules you laid down in the first place on your own.\nNow using these laid-down rules and a set of other rules for a subject called logic which was established similarly, you define certain quantities and name them using the undefined entities and then go on to prove certain statements called theorems.\nNow what is a proof exactly? Suppose in an exam, I am asked to prove Pythagoras' theorem. Then I prove it using only one certain system of axioms and logic. It isn't proved in all the axiom-systems in which it could possibly hold true, and what stops me from making another set of axioms that have Pythagoras' theorem as an axiom, and then just state in my system/exam \"this is an axiom, hence can't be proven\".\nEDIT : How is the term \"wrong\" defined in mathematics then? You can say that proving Fermat's Last Theorem using the number theory axioms was a difficult task but then it can be taken as an axiom in another set of axioms.\nIs mathematics as rigorous and as thought-through as it is believed and expected to be? It seems to me that there many loopholes in problems as well as the subject in-itself, but there is a false backbone of rigour that seems true until you start questioning the very fundamentals.\n",
    "proof": "There are really two very different kinds of proofs:\n\nInformal proofs are what mathematicians write on a daily basis to convince themselves and other mathematicians that particular statements are correct. These proofs are usually written in prose, although there are also geometrical constructions and \"proofs without words\". \nFormal proofs are mathematical objects that model informal proofs. Formal proofs contain absolutely every logical step, with the result that even simple propositions have amazingly long formal proofs. Because of that, formal proofs are used mostly for theoretical purposes and for computer verification. Only a small percentage of mathematicians would be able to write down any formal proof whatsoever off the top of their head. \n\nWith a little humor, I should say there is a third kind of proof: \n\nHigh-school proofs are arguments that teachers force their students to reproduce in high school mathematics classes. These have to be written according to very specific rules described by the teacher, which are seemingly arbitrary and not shared by actual informal or formal proofs outside high-school mathematics. High-school proofs include the \"two-column proofs\" where the \"steps\" are listed on one side of a vertical line and the \"reasons\" on the other.  The key thing to remember about high-school proofs is that they are only an imitation of \"real\" mathematical proofs.\n\nMost mathematicians learn about mathematical proofs by reading and writing them in classes. Students develop proof skills over the course of many years in the same way that children learn to speak - without learning the rules first. So, as with natural languages, there is no firm definition of \"what is an informal proof\", although there are certainly common patterns. \nIf you want to learn about proofs, the best way is to read some real mathematics written at a level you find comfortable. There are many good sources, so I will point out only two: Mathematics Magazine and Math Horizons both have well-written articles on many areas of mathematics. \n",
    "tags": [
      "logic",
      "proof-writing",
      "big-picture"
    ],
    "score": 64,
    "answer_score": 56,
    "is_accepted": false,
    "question_id": 397972,
    "answer_id": 398133
  },
  {
    "theorem": "Could I be using proof by contradiction too much?",
    "context": "Lately, I've developed a habit of proving almost everything by contradiction. Even for theorems for which direct proofs are the clear choice, I'd just start by writing \"Assume not\" then prove it directly, thereby reaching a \"contradiction.\" Is this a bad habit?  I don't know why, but there's something incredibly satisfying about proof by contradiction.  \n",
    "proof": "One general reason to avoid proof by contradiction is the following. When you prove something by contradiction, all you learn is that the statement you wanted to prove is true. When you prove something directly, you learn every intermediate implication you had to prove along the way.\nMore explicitly, if you want to prove that $p \\Rightarrow q$ by contradiction, you assume $p$ and $\\neg q$ and derive a contradiction. None of the intermediate implications along the way can be reused because your premises were contradictory. \nIf you want to prove that $p \\Rightarrow q$ directly, say by proving that $p \\Rightarrow p_1$ and $p_1 \\Rightarrow p_2$ and so on until $p_n \\Rightarrow q$, then you've also proven that $p_i \\Rightarrow p_{i+1}$ for all of the relevant $i$. Many of these statements might be more useful than the original statement you were trying to prove. \nAnother general reason to avoid a proof by contradiction is that it is often not explicit. For example, if you want to prove that something exists by contradiction, you can show that the assumption that it doesn't exist leads to a contradiction. But this doesn't necessarily give you a method for constructing the actual thing, which you might learn more from trying to do. \nA third reason is that frequently, or so it seems to me, a proof by contradiction is really a proof by contrapositive, where you assume $\\neg q$ and derive $\\neg p$. This feels like a proof by contradiction except that you never make use of the hypothesis $p$ except at the very end, and pretending that these are proofs by contradiction will make you blind to the fact that any intermediate implications you prove in a proof by contrapositive are still valid. \n",
    "tags": [
      "proof-writing"
    ],
    "score": 63,
    "answer_score": 116,
    "is_accepted": true,
    "question_id": 319896,
    "answer_id": 319926
  },
  {
    "theorem": "how to be good at proving?",
    "context": "I'm starting my Discrete Math class, and I was taught proving techniques such as proof by contradiction, contrapositive proof, proof by construction, direct proof, equivalence proof etc.\nI know how the proving system works and I can understand the sample proofs in my text to a sufficient extent. However, whenever I tried proving on my own, I got stuck, with no advancement of ideas in my head. How do you remedy this solution? Should i practise proving as much as possible? \nSo far I've been googling proofs for my homework questions. But the final exam got proving questions (closed-book) so I need to come up with the proofs myself.\nWe mainly focus on proving questions related to number theory. Should I read up on number theory and get acquainted with the properties of integers? I don't know how I should go about becoming proficient in proving. Can you guys share your experience on overcoming such an obstacle? What kind of resources do you use for this purpose?\nThank you!\n",
    "proof": "I do not consider myself \"good\" at proving things. However, I know that I have gotten better. The key to writing a proof is understanding what you are trying to prove, which is harder than it may seem. \nKnow your definitions. Often, I have been hampered or seen students hampered by not really knowing all of the definitions in the problem statement. \nWork with others. Look at what someone else has done in a proof and ask questions. Ask how they came up with the idea, ask that person to explain the proof to you. Also, do the same for them. Explain your proofs to a classmate and have them ask you questions. \nTry everything. Students often get stuck on proofs because they try one idea that does not work and give up. I often go through several bad ideas before getting anywhere on a proof. Another good strategy is to work with specific examples until you understand the problem. Plug in numbers and see why the theorem seems to be true. Also, try to construct a counterexample. The reason counterexamples fail often leads to a way to prove the statement.\n",
    "tags": [
      "proof-writing"
    ],
    "score": 62,
    "answer_score": 42,
    "is_accepted": true,
    "question_id": 190981,
    "answer_id": 190988
  },
  {
    "theorem": "On a long proof",
    "context": "On wikipedia there is a claim that the Abel–Ruffini theorem was nearly proved by Paolo Ruffini, and that his proof spanned over $500$ pages, is this really true? I don't really know much abstract algebra, and I know that the length of a paper will vary due to the size of the font, but what could possibly take $500$ pages to explain? Did he have to introduce a new subject part way through the paper or what? It also says Niels Henrik Abel published a proof that required just six pages, how can someone jump from $500$ pages to $6$?\n",
    "proof": "Not only true, but not unique.  The abc conjecture has a recent (2012) proposed proof by Shinichi Mochizuki that spans over 500 pages, over 4 papers.  The record is the classification of finite simple groups which consists of tens of thousands of pages, over hundreds of papers.  Very few people have read all of them, although the result is important and used frequently.\n",
    "tags": [
      "proof-writing",
      "math-history"
    ],
    "score": 53,
    "answer_score": 21,
    "is_accepted": false,
    "question_id": 367869,
    "answer_id": 367871
  },
  {
    "theorem": "Where is the flaw in this &quot;proof&quot; of the Collatz Conjecture?",
    "context": "Edit\nI've highlighted the area in the proof where the mistake was made, for the benefit of anyone stumbling upon this in the future. It's the same mistake, made in two places:\n\nThis has proven the Collatz Conjecture for all even numbers\n\nThe Collatz Conjecture was shown to hold for $N+1$ when $N+1$ is even -- it was never shown to hold for all even numbers -- just that one, lone even number.\n\n[The Collatz Conjecture holds for] all odd numbers for which $N-1$ is a multiple of $4$\n\nThe same as above: it was shown that the Collatz Conjecture holds for $N+1$ if $N+1$ is of the form $4k+1$. It was never shown to hold for all numbers of this form -- just that one, lone number.\nIn order for my proof to be valid, I would need to prove that the Collatz Conjecture holds for $N+1+4j = 4k+1$ (every fourth number after $N+1$) for, at a minimum, $N+1+4j < 1.5N+2$\nOriginal Post\nI spent about an hour thinking about the Collatz Conjecture and stumbled upon what feels like a proof... but I came to it all too easily to have done everything right.\n\nthe obvious that everyone has already figured out:\n\n\nAssume the Collatz Conjecture holds for all numbers $1...N$\n\nWe can trivially prove the Collatz Conjecture for some base cases of $1,$ $2,$ $3,$ and $4$. This is sufficient to go forward.\n\nYet more obvious:\n\n\n\nIf $N$ is odd, $N+1$ is even\n$(N+1)/2 < N$ for $N > 3$\nBy the induction hypothesis, the Collatz Conjecture holds for $N+1$ when $N+1 = 2k$\n\n\n\nNow the last obvious bit:\n\n\n\nIf $N$ is even, $N+1$ is odd\nIf $N+1$ is odd, the next number in the series is 3(N+1)+1\nSince $(N+1)$ is odd, $3(N+1)+1$ is even\nThe next next number in the series is $(3(N+1)+1)/2$\nThis simplifies to: $(3N + 4)/2 = 1.5N + 2$\n\n\n\nNow the first tricky bit:\n\n\n\nIf $N$ is a multiple of $4$:\n$1.5N$ is a multiple of $6$, and therefore even. $1.5N + 2$ is therefore even\nThe next next next number in the series is therefore $(1.5N+2)/2$\nThis simplifies to $0.75N + 1$\nThis is less than $N$ for $N > 4$\nBy the induction hypothesis, the Collatz Conjecture holds for $N+1$ when $N+1 = 4k + 1$\n\n\nThis has proven the Collatz Conjecture for all even numbers and all odd numbers for which $N-1$ is a multiple of $4$... Now to blow your minds:\n\nBreaking out of formal equations into patterns and such since I didn't know how to formalize this bit with math symbols:\n\n\n\nWe now know that a number $N+1$ can ONLY violate the Collatz Conjecture if $N$ is even and not a multiple of $4$. In other words, the only way a number could potentially violate the Collatz Conjecture is if it's of the form $N+1 = 4k - 1$\nThis limits our numbers to test to 2+1, 6+1, 10+1, 14+1, 18+1, 22+1, etc. (note that I wrote these numbers in \"$N+1$\" format so it'd be simpler to apply the $1.5N+2$ shortcut)\nWe'll apply our $1.5N + 2$ shortcut to a handful of these numbers:\n\n\n2  -> 3+2  = 5  (4 +1) -- 4 is a multiple of 4 (duh)\n6  -> 9+2  = 11 (10+1)\n10 -> 15+2 = 17 (16+1) -- 16 is a multiple of 4\n14 -> 21+2 = 23 (22+1)\n18 -> 27+2 = 29 (28+1) -- 28 is a multiple of 4\n22 -> 33+2 = 35 (34+1)\n26 -> 39+2 = 41 (40+1) -- 40 is a multiple of 4\n30 -> 45+2 = 47 (46+1)\n34 -> 51+2 = 53 (52+1) -- 52 is a multiple of 4\n38 -> 57+2 = 59 (58+1)\n42 -> 63+2 = 65 (64+1) -- 64 is a multiple of 4\n46 -> 69+2 = 71 (70+1)\n\n\n\nEvery other line we automatically know the Collatz Conjecture will hold, because we've hit a number that can be expressed as $4k+1$\nLooking at the \"kept\" rows, we can see that all we need to test now are numbers of the form: N+1 = 8k - 1 (in other words, the rows where N = 8k - 2 -- 6, 14, 22, etc.)\n\n\n\nAnd finally, recurse on this solution by drawing a new table and instead of computing the \"next next\" value, compute the \"next next next next\" value:\n\n\n\"Next next next\" value = 3(1.5N + 2) + 1 = 4.5N + 7\n\"next^4\" value is half of this -- 2.25N + 3.5\n\n6  -> 27 +7 = 34  -> 17  (16 +1) -- 16 is a multiple of 4\n14 -> 63 +7 = 70  -> 35  (34 +1)\n22 -> 99 +7 = 106 -> 53  (52 +1) -- 52 is a multiple of 4\n30 -> 135+7 = 142 -> 71  (70 +1)\n38 -> 171+7 = 178 -> 89  (88 +1) -- 88 is a multiple of 4\n46 -> 207+7 = 214 -> 107 (106+1)\n54 -> 243+7 = 250 -> 125 (124+1) -- 124 is a multiple of 4\n62 -> 279+7 = 286 -> 143 (142+1)\n\n\nEvery other line we automatically know the Collatz Conjecture will hold, because we've hit a number that can be expressed as 4k+1\nWe now know a number can only violate the Collatz Conjecture if it's of the form: N+1 = 16k - 1... Recurse again:\n\"next^5\" value is 3(2.25N + 3.5) + 1 = 6.75N + 11.5\n\"next^6\" value is (6.75N + 11.5)/2 = 3.375N + 5.75\n\n14   -> 53  = 52  + 1 -- 52 is a multiple of 4\n30   -> 107 = 106 + 1\n46   -> 161 = 160 + 1 -- 160 is a multiple of 4\n62   -> 215 = 214 + 1\n78   -> 269 = 268 + 1 -- 268 is a multiple of 4\n94   -> 323 = 322 + 1\n110  -> 377 = 376 + 1 -- 376 is a multiple of 4\n126  -> 431 = 430 + 1\n\n\nWe now know a number can only violate the Collatz Conjecture if it's of the form N+1 = 32k - 1\n\nAt this point, a pattern is quickly emerging:\n\nFirst, a number could only violate the Collatz Conjecture if it was of the form N+1 = 4k - 1\nNext, a number was shown that it could only violate the Collatz Conjecture if it was of the form N+1 = 8k - 1\nNext, a number was shown that it could only violate the Collatz Conjecture if it is of the form N+1 = 16k - 1\nNow, a number has been shown that it can only violate the Collatz Conjecture if it is of the form N+1 = 32k - 1\n\nI've continued this process (recursively building this table and removing rows that I know cannot violate the Collatz Conjecture since they can be expressed as 4k+1) all the way up until 512k - 1 by hand.\nI do not know how to formalize this final process in mathematical notation, but I believe it demonstrates at least a viable method for proving the Collatz Conjecture. For every two steps we take into the Collatz series, we increase the power on our definition of \"only numbers that could possibly violate the conjecture\". Therefore for an arbitrarily large power we know that the conjecture will still hold.\nFor Fun\nTo help me in building these tables, I crafted the following Python script:\n# Increment this variable to recurse one level deeper\ntest = 1\n\n### No need to edit below here, but feel free to read it ###\ndepth = 2 * test\nstep = 2 ** (test + 1)\nstart = step - 1\n\nfor x in range(0, 20):\n    num = start + x * step\n    _num = num\n    _depth = depth\n    while _depth > 0:\n        if _num % 2 == 0:\n            _num = _num / 2\n        else:\n            _num = 3 * _num + 1\n        _depth -= 1\n    text = \"\"\n    if (_num - 1) % 4 == 0:\n        text = \"-- multiple of 4\"\n    print \"%s: %s = %s + 1 %s\" % (num - 1, _num, _num - 1, text)\n\n",
    "proof": "There is a subtle issue with your induction argument: you are assuming that the Collatz conjecture holds for all integers $\\leq n$, and then want to prove it holds for $n+1$ (strong induction). So far, so good.\nYou then prove that for some cases ($n+1$ even, or of the form $4k+1$) that the Collatz conjecture holds by the inductive hypothesis. Fine.\nYou then try to argue that for some numbers of the form $4k+3$, you eventually hit a number of the form $4k+1$, so that the Collatz conjecture holds... not so fast. You haven't proven that the Collatz conjecture holds for all integers of the form $4k+1$. You've proven it's true for $n+1$, if $n+1$ happens to be of that form, and you've assumed it's true for all numbers of that form $\\leq n$ (by the inductive hypothesis) but you haven't shown that Collatz holds for numbers of the form $4k+1$ that are larger than $n+1$.\n",
    "tags": [
      "proof-verification",
      "proof-writing",
      "collatz-conjecture"
    ],
    "score": 51,
    "answer_score": 107,
    "is_accepted": true,
    "question_id": 2497833,
    "answer_id": 2497856
  },
  {
    "theorem": "How would one be able to prove mathematically that $1+1 = 2$?",
    "context": "Is it possible to prove that $1+1 = 2$? Or rather, how would one prove this algebraically or mathematically?\n",
    "proof": "In a very \"raw\" sense the symbol $2$ is just a shorthand for $1+1$. There is really not much to prove there.\nIf we want to talk about proof we need axioms to derive the wanted conclusion from. Let us take the \"usual\" axioms of the natural numbers here, namely Peano Axioms. These axioms give very basic rules which describe the natural numbers, and from them we will derive $1+1=2$.\nIn those axioms the numbers $1$ and $2$ don't exist. We have $0$ and we have $S(n)$, which can be thought of as a \"successor function\" which generates the next number, so to speak. In this system $1$ is a symbol for $S(0)$ and $2$ is a symbol for $S(S(0))$.\nAddition is defined inductively, that is $x+0=x$, $x+S(y)=S(x+y)$. From this we can derive:\n$$1+1 = 1+S(0) = S(1+0) = S(1)$$\nNow replace $1$ with its \"full form\" of $S(0)$ and we have:\n$$S(0)+S(0) = S(S(0)+0) = S(S(0))$$\nWhich is what we wanted.\n\nIn a more general setting, one needs to remember that $0,1,2,3,\\ldots$ are just symbols. They are devoid of meaning until we give them such, and when we write $1$ we often think of the multiplicative identity. However, as I wrote in the first part, this is often dependent on the axioms - our \"ground rules\".\nIf we consider, instead of the natural numbers, the binary digits $0,1$ with addition $\\bmod 2$, then we have that $1+1=0$. Now you can argue that of course that $0\\neq 2$, however in this set of axioms (which I have not expressed explicitly here) we can prove that $0=2$, where $2$ is the shorthand for $1+1$ and $0$ is the additive neutral element.\nActually, just writing $1+1=0$ is a proof of that.\nI can't really stress that enough, because this is a very important part of mathematics. We often use some natural notion, such as the natural numbers, before we define it. Later we define it \"to work as we want it to work\" and only then we have a formal framework to work with.\nThese axioms, these frameworks, those often remain \"in the shadows\" and if you don't know where to look for them then you are less likely to find them.\nThis is why the question \"Why $1+1=2$?\" is nearly meaningless - since you don't have a formal framework, and the interpretation (while assumed to be the natural one) is ill-defined.\nOn that same note, this question is also very important when starting with mathematics. It helps to you understand what there is to prove, and how to do it. Of course this too lack of context because one would have to define what is a proof, and all the other things first.\n",
    "tags": [
      "arithmetic",
      "proof-writing"
    ],
    "score": 50,
    "answer_score": 90,
    "is_accepted": false,
    "question_id": 95069,
    "answer_id": 95070
  },
  {
    "theorem": "Limit of $(1+ x/n)^n$ when $n$ tends to infinity",
    "context": "Does anyone know the exact proof of this limit result?\n$$\\lim_{n\\to\\infty} \\left(1+\\frac{x}{n}\\right)^n = e^x$$\n",
    "proof": "A short proof:\n$\\left(1+\\frac{x}{n}\\right)^n = e^{n\\log\\left(1+\\dfrac{x}{n}\\right)}$\nSince $\\log(1+x) = x + O(x^2)$ when $x \\to 0$, we have $n\\log(1 + \\frac{x}{n}) = x + O(\\frac{x^2}{n})$ when $n\\to +\\infty$\n",
    "tags": [
      "limits",
      "proof-writing"
    ],
    "score": 49,
    "answer_score": 84,
    "is_accepted": true,
    "question_id": 882741,
    "answer_id": 882749
  },
  {
    "theorem": "Systems of linear equations: Why does no one plug back in?",
    "context": "When someone wants to solve a system of linear equations like\n$$\\begin{cases} 2x+y=0 \\\\ 3x+y=4 \\end{cases}\\,,$$\nthey might use this logic: \n$$\\begin{align}\n\\begin{cases} 2x+y=0 \\\\ 3x+y=4 \\end{cases} \n\\iff &\\begin{cases} -2x-y=0 \\\\ 3x+y=4 \\end{cases} \n\\\\ \n\\color{maroon}{\\implies} &\\begin{cases} -2x-y=0\\\\ x=4 \\end{cases} \n\\iff \\begin{cases} -2(4)-y=0\\\\ x=4 \\end{cases} \n\\iff \\begin{cases} y=-8\\\\ x=4 \\end{cases}\n\\,.\\end{align}$$\nThen they conclude that $(x, y) = (4, -8)$ is a solution to the system. This turns out to be correct, but the logic seems flawed to me. As I see it, all this proves is that \n$$\n  \\forall{x,y\\in\\mathbb{R}}\\quad\n  \\bigg(\n  \\begin{cases} 2x+y=0 \\\\ 3x+y=4 \\end{cases} \n  \\color{maroon}{\\implies}\n  \\begin{cases} y=-8\\\\ x=4 \\end{cases}\n  \\bigg)\\,.\n$$\nBut this statement leaves the possibility open that there is no pair $(x, y)$ in $\\mathbb{R}^2$ that satisfies the system of equations.\n$$ \n\\text{What if}\\; \n\\begin{cases} 2x+y=0 \\\\ 3x+y=4 \\end{cases} \n\\;\\text{has no solution?}\n$$\nIt seems to me that to really be sure we've solved the equation, we have to plug back in for $x$ and $y$. I'm not talking about checking our work for simple mistakes. This seems like a matter of logical necessity. But of course, most people don't bother to plug back in, and it never seems to backfire on them. So why does no one plug back in?\nP.S. It would be great if I could understand this for systems of two variables, but I would be deeply thrilled to understand it for systems of $n$ variables. I'm starting to use Gaussian elimination on big systems in my linear algebra class, where intuition is weaker and calculations are more complex, and still no one feels the need to plug back in.\n",
    "proof": "You wrote this step as an implication: \n\n$$\\begin{cases} -2x-y=0 \\\\ 3x+y=4 \\end{cases} \\implies \\begin{cases} -2x-y=0\\\\ x=4 \\end{cases}$$\n\nBut it is in fact an equivalence:\n$$\\begin{cases} -2x-y=0 \\\\ 3x+y=4 \\end{cases} \\iff \\begin{cases} -2x-y=0\\\\ x=4 \\end{cases}$$\nThen you have equivalences end-to-end and, as long as all steps are equivalences, you proved that the initial equations are equivalent to the end solutions, so you don't need to \"plug back\" and verify. Of course, carefulness is required to ensure that every step is in fact reversible.\n",
    "tags": [
      "linear-algebra",
      "algebra-precalculus",
      "proof-writing",
      "systems-of-equations"
    ],
    "score": 48,
    "answer_score": 93,
    "is_accepted": true,
    "question_id": 2125340,
    "answer_id": 2125343
  },
  {
    "theorem": "Level of Rigor in Mathematical Physics",
    "context": "I am a physics/math undergrad and I have recently become familiar with some more rigorous formalisms of mechanics, such as Lagrangian mechanics and Noether's Theorem.  However, I've noticed that the writing on mathematical physics (at a level that I can understand) that I've been able to find is not nearly as rigorous as math writing.  It often relies on heuristic reasoning or assumptions.  I understand that this is sort of how physics is often done (at least this is how it is taught at my uni), but I was wondering if more advanced physics/mathematical physics is as rigorous as pure math?  Or is this lack of rigor something I will just have to accept as I move on in physics?\nMaking assumptions in physics doesn't bother me, but I feel that sometimes I see arguments in mechanics that are very hand-wavy.  I feel I'd get a better understanding if the author would explicitly state whichever assumptions are needed: then the argument could take the form of a proof instead of heuristic reasoning.\nI appreciate any insight into the subject.  Thanks for any help and sorry if this question is too vague.\n",
    "proof": "It varies. A lot. The vast majority of physics you are likely to encounter at undergrad will be in the category of \"things which can be formulated into theorems and rigorously proved\". There are notable exceptions. For example, the foundational assumptions of statistical physics (around mixing and ergodic theory) are used fairly unjustifiably.\nThings can be much more shaky closer to the forefront of physics research. In particular, quantum field theories, string theories and their ilk are treated rather more confidently than their foundations allow. Yet certain results about them are proved rigorously, with and without assumptions that everything is suitably well behaved. If you are a rigorous, meticulous person there are many many areas of research which are very thorough.\nThere is a difference between the above and things like \"assuming that the solution to this equation is continuously differentiable\" in mechanics, or (in some cases) \"assume that this PDE has a differentiable solution\" perhaps (with standard but tedious proofs, or difficult and off-topic proofs) where it's simply that it would be a completely different course to discuss the foundations. (Though this would also be the case for the above anyway.) I agree that providing references or quoting theorems would be nice here. It's not done often because it's considered unnecessary or boring or off-topic...\nThe simple point is that the answers to the above issues are not known but at the same time are almost certainly expected not to be pressing issues because making these handwaving assumptions gives good physics. There are plenty of corner cases and exceptions of course, which is why one major type of physics research amounts to finding exceptions to rules. Even when (to some extent at least) rigorous results are proved, it may not immediately be obvious what the loopholes are (supersymmetric theories come to mind, as related to Coleman-Mandula).\nUltimately, I feel strongly that teachers should make clear the distinction between known but off topic and unknown but physically plausible; but do be prepared to find a community which has to make assumptions because it is grounded in experiment rather than axioms. It would be insane to refuse to listen to anyone in particle physics in the last century simply because axiomatic QFT is hard.\n",
    "tags": [
      "soft-question",
      "proof-writing",
      "mathematical-physics"
    ],
    "score": 48,
    "answer_score": 23,
    "is_accepted": false,
    "question_id": 472331,
    "answer_id": 472355
  },
  {
    "theorem": "The &#39;Factorialth Root&#39;",
    "context": "I was dealing with the following question, given by my friend:\n\nLet $\\xi(x)=\\sqrt{x+\\sqrt{x+\\sqrt{x+\\sqrt{\\cdots}}}}$\nDefine the series $X$ as $\\xi(1),\\xi(2),\\xi(3),\\dots$\nFind $n$ for which $\\xi(n)$ is the 51st Whole Number in the series.\n\nI solved it, of course, [and interestingly $\\xi(1)={{1+\\sqrt5}\\over2}$, the Golden Ratio] but that led us on a competition in which we would try to find out the value of increasingly convoluted expressions.\n\nSome time later, I made an expression, which I called 'The Factorialth Root', written as $\\sqrt[!]{x}$.\n\nFor some $x$, $\\sqrt[!]{x}=\\sqrt{x\\sqrt{(x-1)\\sqrt{(x-2)\\sqrt{\\ddots\\sqrt{2\\sqrt1}}}}}$\n\nMy friend thought that $(x>y)\\to(\\sqrt[!]{x}<\\sqrt[!]{y})$, while I thought the opposite, that $(x>y)\\to(\\sqrt[!]{x}>\\sqrt[!]{y})$.\nI showed by example that mine was correct, but couldn't prove it.\n\nMy attempt:\n\nIf $[(x>y)\\to(\\sqrt[!]{x}>\\sqrt[!]{y})]$ is true, then $\\sqrt[!]{x}>\\sqrt[!]{x-1}$. This is possible only when $x>\\sqrt[!]{x-1}$. It follows that $\\sqrt[!]{2}>\\sqrt[!]{1},\\sqrt[!]{3}>\\sqrt[!]{2}$, and so on.\n\nSo, I thought I could prove it by induction, but can't seem to find any way to apply it.\nCan anyone help?\n",
    "proof": "For $n > 1$, \n$$\\begin{align}\n\\frac{\\sqrt[!]{n}}{\\sqrt[!]{n-1}} &= \\frac{n^{1/2} \\cdot (n-1)^{1/4} \\cdot (n-2)^{1/8} \\cdot\\,\\cdots\\,\\cdot 1^{1/2^{n\\phantom{-1}}}}{\\phantom{n^{1/2}\\cdot}(n-1)^{1/2}\\cdot(n-2)^{1/4}\\cdot\\,\\cdots\\,\\cdot 1^{1/2^{n-1}}} \\\\[4pt]\n&= \\frac{n^{1/2}}{(n-1)^{1/4}\\cdot(n-2)^{1/8}\\cdot\\,\\cdots\\,\\cdot 1^{1/2^n}} \\\\[4pt]\n&= \\frac{n^{1/4+1/8+1/16+\\cdots+1/2^{n}+1/2^{n}}}{(n-1)^{1/4}\\cdot(n-2)^{1/8}\\cdot\\,\\cdots\\,\\cdot 1^{1/2^n}} \\\\[4pt]\n&= \\left(\\frac{n}{n-1}\\right)^{1/4}\\left(\\frac{n}{n-2}\\right)^{1/8}\\left(\\frac{n}{n-3}\\right)^{1/16}\\cdot\\,\\cdots\\,\\cdot \\left(\\frac{n}{1}\\right)^{1/2^{n}}\\cdot n^{1/2^n} \\\\[4pt]\n&> 1 \\cdot 1 \\cdot 1 \\cdot\\,\\cdots\\,\\cdot1 \\cdot 1 \\\\[4pt]\n&= 1\n\\end{align}$$\n",
    "tags": [
      "algebra-precalculus",
      "proof-writing",
      "radicals"
    ],
    "score": 47,
    "answer_score": 51,
    "is_accepted": true,
    "question_id": 2550386,
    "answer_id": 2550439
  },
  {
    "theorem": "What really is mathematical rigor? How can I be more rigorous?",
    "context": "I'm an undergraduate mathematics student who has received some constructive feedback from two instructors at the end of my exams. Namely, that I am a bit hand-wavey and not always very rigorous. While I greatly appreciate this feedback since I intend to apply to graduate school, I worry because when I hand in assignment, I generally feel the proofs are rigorous already, and I'm not sure what I'm missing. Obviously I will be engaging in a dialogue with my professors about what I can do to be more rigorous, but I'm hoping the experts on this forum can provide some examples or distinctions between rigorous arguments and arguments that are close, but maybe gloss over important details so I can more clearly see the difference.\nIt is worth noting that I have taken a course in mathematical reasoning and logic and did very well in it. Something has happened in the last year or so to reduce the quality of my arguments, or rather, the expectation has gone up and my level of rigor has not matched it.\nI think another aspect of what makes this a troublesome problem for me is that i typically do really well on homework problem sets. 90% consistently, sometimes with little mistakes. I feel like I am getting mixed signals from some of my classes when I am consistently told I am doing well, but am given this advice. All of this is said without spite or malice, I just sometimes feel confused about my own level of understanding.\nSo if anyone has any general advice, or useful examples of arguments that look rigorous but need to be patched up, that would be greatly appreciated. I want to get a sense for what a really solid proof looks like compared to a less polished argument that looks passable to someone still with naivete in them.\nEdit: Here is an example of one such question in which I struggled for a long while to produce the proof posted, and even then I was missing something to be fully rigorous. Homology groups of orientable surfaces.\n",
    "proof": "You should be able to delineate the precise mathematical theorems that allow you to make each step in a proof. For example, if you have $(x,y) \\in \\mathbb{R}^2$ and you write: let $r,\\theta$ satisfy $x = r\\cos \\theta,y=r\\sin \\theta$ with $r\\geq 0$ and $2\\pi > \\theta \\geq 0$, you are using a theorem that says that:\n\nProposition. For all $x,y \\in \\mathbb{R}$, there exists $r \\in \\mathbb{R}_{\\geq 0}$ and $\\theta \\in \\mathbb{R}$ such that $x=r\\cos \\theta$ and $y = r \\sin \\theta$ and $2\\pi > \\theta \\geq 0$.\n\nMake sure you can explicitly write out the theorems that allow you to make the steps you are making.\nThe other thing is that you need to develop an intuition for what the instructor (reader, etc.) expects you to take for granted. You may need to write some follow-up lemmas if there are steps in your proof which invoke theorems that you really shouldn't be taking for granted.\n",
    "tags": [
      "proof-writing"
    ],
    "score": 47,
    "answer_score": 22,
    "is_accepted": true,
    "question_id": 1283974,
    "answer_id": 1283989
  },
  {
    "theorem": "When working proof exercises from a textbook with no solutions manual, how do you know when your proof is sound/acceptable?",
    "context": "\nWhen working proof exercises from a textbook with no solutions manual, how do you know when your proof is sound/acceptable?\n\nOften times I \"feel\" as if I can write a proof to an exercise but most of those times I do not feel confident that the proof that I am thinking of is good enough or even correct at all.  I can sort of think a proof in my head, but am not confident this is a correct proof.\nAny input would be appreciated. Thanks.\n",
    "proof": "Ask a more experienced person.  IMHO that's really the only option, and one of the reasons for this is that it is very important for a proof to communicate a result and its justification to another person.  If the proof is good enough to convince yourself, that's a start, but the real test is whether you can express it in such a way as to convince someone else.\nAnd BTW... the same applies if the textbook does have a solutions manual.  Your proof is inevitably going to be different from the one in the book, and it takes a lot of experience and mathematical understanding to decide whether the differences are important or not.\n",
    "tags": [
      "soft-question",
      "proof-writing",
      "self-learning",
      "advice"
    ],
    "score": 47,
    "answer_score": 32,
    "is_accepted": true,
    "question_id": 802276,
    "answer_id": 802280
  },
  {
    "theorem": "Why do we write proofs &quot;forward?&quot;",
    "context": "I am aware that this might turn into a discussion, but I have a feeling this might have an answer (maybe something historical?) instead. I'm hoping that those with speculations keep it in the comments.\nI have started to work on formal proof writing this quarter, and I discovered that the key to getting to some of them is to think of the problem \"backwards.\" But, alas, when I wrote my proof starting with this, my professor said I shouldn't do it. But why not? It gives the reader a sense of what motivated this type of proof and allows for more understanding, doesn't it?\nMods: Feel free to close, if this turns out to be too much of a discussion. I will be in chat for those willing to discuss this.\n",
    "proof": "One main problem with writing an argument backwards, especially for a student beginning to learn about proofs, is that it would be much more difficult to keep track of what is an assumption and what is a goal. In a proof that $A\\implies B$, we should never along the way assume that $B$ is true, otherwise we are being circular; but if the statement of $B$ is written down on your paper already, you might get confused and think you'd already demonstrated it to be true. I'm not saying this will always happen, just that it is a greater risk.\nWhile it's true that \"thinking backwards\" can sometimes be a useful strategy for attacking a problem, and explaining your strategy to the reader can be a good addition to a formal proof, it is not a substitute; one should always be able to explain the argument starting from your given information and axioms, and proceeding to the desired statement completely \"forwards\". It is essential to get sufficient practice with phrasing your argument this way. \n",
    "tags": [
      "proof-writing"
    ],
    "score": 46,
    "answer_score": 48,
    "is_accepted": true,
    "question_id": 95461,
    "answer_id": 95464
  },
  {
    "theorem": "What is the proof that covariance matrices are always semi-definite?",
    "context": "Suppose that we have two different discreet signal vectors of $N^\\text{th}$ dimension, namely $\\mathbf{x}[i]$ and $\\mathbf{y}[i]$, each one having a total of $M$ set of samples/vectors.\n$\\mathbf{x}[m] = [x_{m,1} \\,\\,\\,\\,\\, x_{m,2} \\,\\,\\,\\,\\, x_{m,3} \\,\\,\\,\\,\\, ... \\,\\,\\,\\,\\, x_{m,N}]^\\text{T}; \\,\\,\\,\\,\\,\\,\\, 1 \\leq m \\leq M$\n$\\mathbf{y}[m] = [y_{m,1} \\,\\,\\,\\,\\, y_{m,2} \\,\\,\\,\\,\\, y_{m,3} \\,\\,\\,\\,\\, ... \\,\\,\\,\\,\\, y_{m,N}]^\\text{T}; \\,\\,\\,\\,\\,\\,\\,\\,\\, 1 \\leq m \\leq M$\nAnd, I build up a covariance matrix in-between these signals.\n$\\{C\\}_{ij} = E\\left\\{(\\mathbf{x}[i] - \\bar{\\mathbf{x}}[i])^\\text{T}(\\mathbf{y}[j] - \\bar{\\mathbf{y}}[j])\\right\\}; \\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\, 1 \\leq i,j \\leq M $\nWhere, $E\\{\\}$ is the \"expected value\" operator.\nWhat is the proof that, for all arbitrary values of $\\mathbf{x}$ and $\\mathbf{y}$ vector sets, the covariance matrix $C$ is always semi-definite ($C \\succeq0$) (i.e.; not negative definte; all of its eigenvalues are non-negative)?\n",
    "proof": "A symmetric matrix $C$ of size $n\\times n$ is semi-definite if and only if $u^tCu\\geqslant0$ for every $n\\times1$ (column) vector $u$, where $u^t$ is the $1\\times n$ transposed (line) vector. If $C$ is a covariance matrix in the sense that $C=\\mathrm E(XX^t)$ for some $n\\times 1$ random vector $X$, then the linearity of the expectation yields that $u^tCu=\\mathrm E(Z_u^2)$, where $Z_u=u^tX$ is a real valued random variable, in particular $u^tCu\\geqslant0$ for every $u$. \nIf $C=\\mathrm E(XY^t)$ for two centered random vectors $X$ and $Y$, then $u^tCu=\\mathrm E(Z_uT_u)$ where $Z_u=u^tX$ and $T_u=u^tY$ are two real valued centered random variables. Thus, there is no reason to expect that $u^tCu\\geqslant0$ for every $u$ (and, indeed, $Y=-X$ provides a counterexample).\n",
    "tags": [
      "probability",
      "matrices",
      "vector-spaces",
      "proof-writing",
      "positive-semidefinite"
    ],
    "score": 44,
    "answer_score": 59,
    "is_accepted": true,
    "question_id": 114072,
    "answer_id": 114077
  },
  {
    "theorem": "Proof for triangle inequality for vectors",
    "context": "Generally,the length of the sum of two vectors is not equal to the sum of their lengths. To see this consider the vectors $u$ and $v$ as shown below.\nBy considering $u$ and $v$ as two sides of a triangle, we can see that the lengths of the third side is $\\| u + v \\|$ and we have $\\| u + v \\| \\leq \\|u\\| + \\|v\\|$.  Under what circumstance equality occurs and how can one prove that? \n",
    "proof": "I have noticed that the answer has been written down in the comments. Just to have an answer I am writing this one down.\nConsider $\\|u+v\\|^2=(u+v) \\cdot (u+v)$ where $u \\cdot v$ represents the standard inner product/scalar product.Therefore $$\\|u+v\\|^2=\\|u\\|^2+2 (u \\cdot v) + \\|v\\|^2 .$$\nBy the Cauchy-Schwarz Inequality we have $$u \\cdot v \\leq \\|u\\| \\cdot \\|v\\|.$$\nSo, $$\\|u+v \\|^2= \\|u\\|^2+2(u \\cdot v)+ \\|v \\|^2 \\leq \\|u\\|^2+ 2 \\|u\\| \\cdot \\|v\\| + \\|v\\|^2=(\\|u\\|+ \\|v\\|)^2 ,$$\ni.e., $$\\|u+v\\|^2 \\leq (\\|u\\|+ \\|v\\|)^2 \\implies \\|u+v\\| \\leq \\|u \\|+ \\|v\\| .$$\nThe Cauchy-Schwarz Inequality holds for any inner Product, so the triangle inequality holds irrespective of how you define the norm of the vector to be, i.e., the way you define scalar product in that vector space. \nIn this case, the equality holds when vectors are parallel i.e, $u=kv$, $k \\in \\mathbb{R}^+$ because $u \\cdot v= \\|u \\| \\cdot \\|v\\| \\cos \\theta$ when $\\cos \\theta=1$, the equality of the Cauchy-Schwarz inequality holds. \n",
    "tags": [
      "inequality",
      "vector-spaces",
      "proof-writing",
      "inner-products"
    ],
    "score": 44,
    "answer_score": 69,
    "is_accepted": true,
    "question_id": 91181,
    "answer_id": 91194
  },
  {
    "theorem": "Why do proof authors use natural language sentences to write proofs?",
    "context": "I haven't read very many proofs. The majority of the ones that I've read, I've read in my first-year proofs textbook. Nevertheless, its first chapter expatiates on the proper use of English in mathematical proofs, so I suspect that most proof authors do use both English (or another natural language) and formal proof systems for writing their proofs. However, I can't imagine many circumstances where mathematical notation and a formal proof system wouldn't suffice to convey the author's pertinent thoughts. \nI suspect that proof authors have good reasons for making use of both English and formal expressions in their proofs, but I can't think of what those reasons are. Why do proof authors use English and formal expressions to write proofs? Are there stylistic rules (or guidelines) that state how, and in what parts of the proof, an author should, and shouldn't, use English? \n",
    "proof": "An ordinary mathematical proof1 is a piece of expository prose. Its purpose is to convince the reader that the theorem is true. Obviously it should be mathematically correct and logically sound, but it should also be clear and easy to follow. Writing in (paragraphs of) sentences, and using words for the connective tissue that gives a road map for the logic of the argument, generally makes the prose more readable. By all means use symbols when they’re appropriate: the quadratic formula is much easier to follow when expressed symbolically than when written out in words! But don’t fall into the trap of thinking that the more symbolism you use, the more professional your argument looks.\nFor an actual (and fairly simple) example you might look at this question and my answer to it.\n1 The word proof is also used for a formal object in mathematical logic. Trying to follow a formal proof is rather like trying to figure out what a computer program does by looking at the machine language.\n",
    "tags": [
      "proof-writing",
      "article-writing"
    ],
    "score": 43,
    "answer_score": 68,
    "is_accepted": true,
    "question_id": 1027694,
    "answer_id": 1027720
  },
  {
    "theorem": "Explain proof that any positive definite matrix is invertible",
    "context": "\nIf an $n \\times n$ symmetric A is positive definite, then all of its eigenvalues are positive, so $0$ is not an eigenvalue of $A$. Therefore, the system of equations $A\\mathbf{x}=\\mathbf{0}$ has no non-trivial solution, and so A is invertible.\n\nI don't get how knowing that $0$ is not an eigenvalue of $A$ enables us to conclude that $A\\mathbf{x}=\\mathbf{0}$ has the trivial solution only. In other words, how do we exclude the possibility that for all $\\mathbf{x}$ that is not an eigenvector of $A$, $A\\mathbf{x}=\\mathbf{0}$ will not have a non-trivial solution?\n",
    "proof": "Note that if $Ax=0=0\\cdot x$ for some $x\\ne 0$ then by definition of eigenvalues, $x$ is an eigenvector with eigenvalue $\\lambda = 0$, contradicting that $0$ is not an eigenvalue of $A$.\n$$Ax=\\lambda x$$\n",
    "tags": [
      "linear-algebra",
      "proof-writing",
      "eigenvalues-eigenvectors"
    ],
    "score": 43,
    "answer_score": 44,
    "is_accepted": true,
    "question_id": 1059566,
    "answer_id": 1059576
  },
  {
    "theorem": "Why don&#39;t Venn diagrams count as formal proofs?",
    "context": "Just curious. If the purpose of a proof is to inform and persuade, why don't Venn diagrams count? Is it just convention or is there a more, umm, formal reason haha. Thanks!\n",
    "proof": "I don't know exactly what you have written, but I would venture to say that anything you \"prove\" with Venn diagrams probably has an extremely direct translation into set theory, which would certainly be an acceptable form of proof.\nThe strongest reason to not let you just use a Venn diagram alone is that your teacher probably wants you to verbalize your explanation. This is a key part of mathematics. Drawing a picture can really help illustrate the idea involved, but it does not always explain the connection to the logic you are working within.\nThere is also a huge drawback to proving things by Venn diagram: your visual preconceptions may fool you into making a mistake. This cannot happen (or happens to a much smaller degree) when you work in the language of set theory.\n",
    "tags": [
      "elementary-set-theory",
      "proof-writing"
    ],
    "score": 42,
    "answer_score": 49,
    "is_accepted": true,
    "question_id": 304173,
    "answer_id": 304186
  },
  {
    "theorem": "Should a mathematical proof be &#39;convincing&#39;?",
    "context": "I just read a description of what is a mathematical proof in my mathematical logic textbook, and I'm a bit puzzled by it.  It goes like this:\nA mathematical proof is a finite sequence of mathematical assertions which forms a valid and convincing argument for the desired conclusion from stated assumptions.\nWhy \"convincing\"? What does this mean?  Sadly, the text doesn't comment much on this.  It only says that \"convincing\" is a psychological notion, and so from the point of view of formal proofs is unsatisfactory.\nThis of course makes sense, but what bothers me is the notion that mathematical proofs should be convincing.  This seems to suggest that a logically valid mathematical proof can somehow be rejected because it was \"unconvincing\"?\nEdit\nAdded the full page for context here:\nhttps://image.ibb.co/gjso1U/proofsss.png\n\nA related question:\nIf there were only one single mathematician in the world, would s/he be able to produce a mathematical proof? \n",
    "proof": "The primary reason to write down a proof is in order to communicate with other mathematicians. If mathematicians acquainted with the relevant literature cannot understand your argument (i.e: do not find it convincing) then I think the author has failed to write down a proof.\n",
    "tags": [
      "proof-writing",
      "philosophy",
      "formal-proofs"
    ],
    "score": 42,
    "answer_score": 38,
    "is_accepted": false,
    "question_id": 2879811,
    "answer_id": 2879812
  },
  {
    "theorem": "How do I make proofs with long formulae more readable without sacrificing clarity?",
    "context": "Question\nA lot of things I'm trying to prove just now are turning into \"notational hell\", which I think makes them very hard to read. I've tried to cut down on this by assuming my reader will understand what definitions are in play, modularising my proofs and skipping explanation of steps that I hope are obvious. I've also tried relabeling formulae with short names (i.e., $\\def\\val#1{V_\\pli(#1)}\\def\\p{\\phi}\\def\\q{\\psi}\\def\\s{\\vDash_{\\tiny\\text{PL}}}\\def\\ns{\\nvDash_{\\tiny\\text{PL}}}\\def\\pli{\\mathscr{I}}\\def\\aa{\\p_1,\\,\\p_2,\\,\\ldots,\\,\\p_n\\s\\q}\\def\\ab{\\p_1,\\,\\p_2,\\,\\ldots,\\,\\p_n\\ns\\q}\\def\\ba{\\s\\p_n\\to(\\p_{n-1}\\to(\\cdots\\to(\\p_1\\to\\q)\\cdots))}\\def\\bb{\\ns\\p_n\\to(\\p_{n-1}\\to(\\cdots\\to(\\p_1\\to\\q)\\cdots))}\\s\\p_n\\to(\\p_{n-1}\\to(\\cdots\\to(\\p_1\\to\\q)\\cdots)):=\\,\\s Q),$ but for proofs of any length it seems to be more confusing than helpful. How do I make proofs more readable without sacrificing clarity?\n\nExample Proof\nLet $\\p$ and $\\q$ be wffs and $n\\in\\mathbb{N}$ (please note that $0\\not\\in\\mathbb{N}$). We want to show $\\def\\p{\\phi}\\def\\q{\\psi}\\def\\s{\\vDash_{\\tiny\\text{PL}}}\\def\\ns{\\nvDash_{\\tiny\\text{PL}}}\\p_1,\\,\\p_2,\\,\\ldots,\\,\\p_n\\s\\q$ iff $\\s\\p_n\\to(\\p_{n-1}\\to(\\cdots\\to(\\p_1\\to\\q)\\cdots))$.\n\nIn (L1) I prove both directions of the biconditional, which I don't think I need to do because we're dealing with \"=\" - is this correct? I also think that (L1) is so basic that \"by inspection\" is appropriate - is that fair?\n\nLemma 1 (L1)\nWe want to show by induction that for some PL-interpretation, $\\pli,$ $\\val{\\p_n\\to(\\p_{n-1}\\to(\\cdots\\to(\\p_1\\to\\q)\\cdots))}=0$ iff $\\val{\\p_n}=\\val{\\p_{n-1}}=\\cdots=\\val{\\p_1}=1$ and $\\val{\\q}=0$.\nBase Case\n\nIf $\\val{\\p\\to\\q}=0$ then, by definition, $\\val{\\p}=1$ and $\\val{\\q}=0$. If $\\val{\\p}=1$ and $\\val{\\q}=0$, then, by definition, $\\val{\\p\\to\\q}=0$\n\nInduction Hypothesis (IH)\n\nAssume for some arbitrary $k\\in\\mathbb{N}$ that $\\val{p_k\\to(\\p_{k-1}\\to(\\cdots\\to(\\p_1\\to\\q)\\cdots))}=0$ and $\\val{\\p_k}=\\val{\\p_{k-1}}=\\cdots=\\val{\\p_1}=1$ and $\\val{\\q}=0$\n\nInduction Step\n\nIf $\\val{\\p_{k+1}\\to(p_k\\to(\\p_{k-1}\\to(\\cdots\\to(\\p_1\\to\\q)\\cdots)))}=0,$ then, as we know $\\val{p_k\\to(\\p_{k-1}\\to(\\cdots\\to(\\p_1\\to\\q)\\cdots))}=0$ from the (IH), $\\val{\\p_{k+1}}=1$. From the (IH) $\\val{\\p_k}=\\val{\\p_{k-1}}=\\cdots=\\val{\\p_1}=1$ and $\\val{\\q}=0$, thus $\\val{\\p_{k+1}}=\\val{\\p_k}=\\val{\\p_{k-1}}=\\cdots=\\val{\\p_1}=1$ and $\\val{\\q}=0$\n\nLet $\\val{\\p_{k+1}}=1$. From the (IH) $\\val{\\p_k}=\\val{\\p_{k-1}}=\\cdots=\\val{\\p_1}=1,$ $\\val{\\q}=0,$ and $\\val{p_k\\to(\\p_{k-1}\\to(\\cdots\\to(\\p_1\\to\\q)\\cdots))}=0,$ thus $\\val{\\p_{k+1}\\to(p_k\\to(\\p_{k-1}\\to(\\cdots\\to(\\p_1\\to\\q)\\cdots)))}=0$\n\n\nProof of First Direction (P1)\n\nFor reductio, suppose it is not the case that $\\aa\\implies\\ba$\n\nIt follows from (1) that there exists an $\\def\\pli{\\mathscr{I}}\\pli$ such that $\\def\\aa{\\p_1,\\,\\p_2,\\,\\ldots,\\,\\p_n\\s\\q}\\def\\ab{\\p_1,\\,\\p_2,\\,\\ldots,\\,\\p_n\\ns\\q}\\def\\ba{\\s\\p_n\\to(\\p_{n-1}\\to(\\cdots\\to(\\p_1\\to\\q)\\cdots))}\\def\\bb{\\ns\\p_n\\to(\\p_{n-1}\\to(\\cdots\\to(\\p_1\\to\\q)\\cdots))}\\aa$ and $\\bb$\n\nIt follows from (2) that $\\def\\val#1{V_\\pli(#1)}\\val{\\p_n\\to(\\p_{n-1}\\to(\\cdots\\to(\\p_1\\to\\q)\\cdots))}=0$\n\nFrom (L1), the valuation on (3) can only occur when $\\val{\\p_n}=\\val{\\p_{n-1}}=\\cdots=\\val{\\p_1}=1$ and $\\val{\\q}=0$\n\nIt follows from (4) that $\\ab$, which contradicts (2) and proves our first direction\n\n\nProof of Second Direction (P2)\n\nFor reductio, suppose it's not the case that $\\ba\\implies\\aa$\n\nIt follows from (1) that there exists an $\\pli$ such that $\\ba\\text{ and }\\ab$\n\nFrom (2) we have $\\ab$, thus, $\\val{\\p_n}=\\val{\\p_{n-1}}=\\cdots=\\val{\\p_1}=1$ and $\\val{\\q}=0$\n\nIt follows from (3) and (L1) that $\\bb\\text{,}$ which contradicts (2) and proves our second direction\n\n\n(P1) and (P2) prove both directions of the biconditional, hence $\\def\\p{\\phi}\\def\\q{\\psi}\\def\\s{\\vDash_{\\tiny\\text{PL}}}\\def\\ns{\\nvDash_{\\tiny\\text{PL}}}\\p_1,\\,\\p_2,\\,\\ldots,\\,\\p_n\\s\\q$ iff $\\s\\p_n\\to(\\p_{n-1}\\to(\\cdots\\to(\\p_1\\to\\q)\\cdots))\\,\\square.$\n",
    "proof": "Main suggestion\nInstead of\n\nWe want to show $\\def\\p{\\phi}\\def\\q{\\psi}\\def\\s{\\vDash_{\\tiny\\text{PL}}}\\def\\ns{\\nvDash_{\\tiny\\text{PL}}}\\p_1,\\,\\p_2,\\,\\ldots,\\,\\p_n\\s\\q$ iff $\\s\\p_n\\to(\\p_{n-1}\\to(\\cdots\\to(\\p_1\\to\\q)\\cdots))$\n\ntry it like this:\n\nWe want to show that $$\\def\\p{\\phi}\\def\\q{\\psi}\\def\\s{\\vDash_{\\tiny\\text{PL}}}\\def\\ns{\\nvDash_{\\tiny\\text{PL}}}\\p_1,\\,\\p_2,\\,\\ldots,\\,\\p_n\\s\\q \\tag{1}$$ if and only if\n$$\\s\\p_n\\to(\\p_{n-1}\\to(\\cdots\\to(\\p_1\\to\\q)\\cdots)).\\tag{2}$$\n\nAfter that, instead of repeating the long formulas every time, just call them $(1)$ and $(2)$:\n\nFor reductio, suppose it's not the case that $(2)\\implies (1)$.  Then there must exist an $\\mathscr I$ such that $(2)$ holds but $(1)$ does not.\n\nLesser suggestions\n\nAbbreviate $$\\phi_n\\to(\\phi_{n-1}\\to(\\cdots\\to(\\phi_1\\to\\q)\\cdots))$$ as $$\\Phi_n.$$  (Don't use $Q$.  Why would you use $Q$?)\nInstead of $$V_\\mathscr{I}({p_k\\to(\\p_{k-1}\\to(\\cdots\\to(\\p_1\\to\\q)\\cdots))})=0$$ you can now write $$V_\\mathscr{I}({p_k\\to\\Phi_{k-1}})=0$$ and the reader won't miss that the first variable is a  $p$ and not a $\\phi$.\nYou said abbreviating seems “more confusing than helpful”.  It's not.\n\nAbbreviate $\\phi_1,\\phi_2,\\ldots,\\phi_n$ as $\\vec\\phi$.\n\nAbbreviate $$V_\\mathscr{I}(\\phi_n)=V_\\mathscr{I}(\\phi_{n-1})=\\cdots=V_\\mathscr{I}(\\phi_1)=1$$ as $$V_\\mathscr{I}(\\phi_i) = 1\\quad (i=1\\ldots n)$$ or perhaps $$V_\\mathscr{I}(\\phi_{1\\ldots n}) = 1.$$\n\nYou're abbreviating the wrong things.  You don't need to abbreviate “if and only if” as “iff”, or “Lemma 1” as “L1”. The goal here is not to remove all the normal English from your proof.  These abbreviations are more confusing than helpful.\n\n\nDon't make the reader compare two long formulas to make sure they are the same, or to wonder why they are not.  Design your notation to highlight the differences between similar formulas.\nNotation, like language, is flexible. There are no rules; you are allowed to make things up.  $\\vec\\phi$ is not really a vector.  It doesn't matter. You can explain it briefly: “We will abbreviate $\\phi_1,\\phi_2,\\ldots,\\phi_n$ as $\\vec\\phi$.”  Nobody will be confused or forget what it means.  My suggestion $V_\\mathscr{I}(\\phi_{1\\ldots n})$ is not standard.  It doesn't matter; the meaning is clear.\nOrthogonal suggestions\n\nYou're not using TeX correctly.  You don't need to keep repeating \\defs.  Once you \\def a new control sequence, the definition remains in force until the end of the group, or the document.  Define the important macros once, at the beginning of the file, or in an \\included file.\n\nDefine better macros.  The structure of the macros should follow the syntactic structure of your formulas.  Instead of typing out\n\\def\\aa{\\p_1,\\,\\p_2,\\,\\ldots,\\,\\p_n\\s\\q}\n\\def\\ab{\\p_1,\\,\\p_2,\\,\\ldots,\\,\\p_n\\ns\\q}\n\\def\\ba{\\s\\p_n\\to(\\p_{n-1}\\to(\\cdots\\to(\\p_1\\to\\q)\\cdots))}  \n\\def\\bb{\\ns\\p_n\\to(\\p_{n-1}\\to(\\cdots\\to(\\p_1\\to\\q)\\cdots))\n\ntry it this way:\n\\def\\ps{\\p_1,\\,\\p_2,\\,\\ldots,\\,\\p_n}\n\\def\\aa{\\ps\\s\\psi}\n\\def\\ab{\\ps\\ns\\psi}\n\\def\\pformn{\\p_n\\to(\\p_{n-1}\\to(\\cdots\\to(\\p_1\\to\\psi)\\cdots))}\n% now you don't need \\ba or \\bb, just use \\s\\pformn and \\ns\\pformn\n\n\n\n",
    "tags": [
      "logic",
      "proof-writing",
      "notation",
      "propositional-calculus"
    ],
    "score": 42,
    "answer_score": 68,
    "is_accepted": true,
    "question_id": 4256057,
    "answer_id": 4256085
  },
  {
    "theorem": "How to attack &quot;if true, prove it; if not true, give a counterexample&quot; question?",
    "context": "I am taking a basic analysis course. This is a general question that I often encounter in weekly homework. How should we start to attack this type of question: if the statement is true, prove it; if not true, give a counterexample? Yesterday evening I spent four hours looking for a counterexample, and finally figured that the statement was indeed true. I tried so hard to find a counterexample because I was not able to prove the statement at first. Are there any steps to follow in terms of this type of questions? In particular, sometimes a statement does not look like it is true. In this case, how can I quickly figure out whether I should try counterexamples or keep the efforts on proving the statement is true? The core problem is that this type of question is very time-consuming according to my homework experience. I desperately doubt I could figure out a correct answer during a timed exam next week. \n",
    "proof": "One strategy is to first try to prove by contradiction that the statement is true. Such an effort will identify necessary conditions for a counterexample. If through such an analysis you realise you can also give sufficient conditions for a counterexample, and you can work out how to satisfy them, you'll have a counterexample. With any luck, in the event the statement is true you'll find a valid proof soon enough. (Once you do, it's worth checking whether it can be rewritten to not use contradiction; students new to proof-writing sometimes unnecessarily add a contradiction \"wrapper\" around a direct proof.)\nNote that if a counterexample exists in a textbook exercise, there will be a simple one that slightly complicates a situation that illustrates a weaker true claim. For example, if I asked you to prove or refute-by-counterexample the claim that all finite groups are Abelian, the hope is you'd quickly find this counterexample. It's slightly more complicated, though only slightly, than the case of a finite group with a single generator, which of course would be Abelian. So the hope is you'd think, \"let's try to make a group with two generators; that might do it\".\n",
    "tags": [
      "proof-writing",
      "soft-question",
      "problem-solving"
    ],
    "score": 40,
    "answer_score": 47,
    "is_accepted": true,
    "question_id": 3111482,
    "answer_id": 3111492
  },
  {
    "theorem": "Are There Any Symbols for Contradictions?",
    "context": "Perhaps, this question has been answered already but I am not aware of any existing answer. Is there any international icon or symbol for showing Contradiction or reaching a contradiction in Mathematical contexts? The same story can be seen for showing that someone reached to the end of the proof of a theorem (i.e. as shown the tombstone symbol ∎, Halmos).\n",
    "proof": "Different sources use different symbols (if they use symbols at all). I've seen $\\Rightarrow\\Leftarrow$ most often. For some others, see \"Symbolic Representation\" here.\n",
    "tags": [
      "proof-writing",
      "notation"
    ],
    "score": 40,
    "answer_score": 34,
    "is_accepted": true,
    "question_id": 160039,
    "answer_id": 160040
  },
  {
    "theorem": "A practical way to check if a matrix is positive-definite",
    "context": "Let $A$ be a symmetric $n\\times n$ matrix.\nI found a method on the web to check if $A$ is positive definite:\n\n$A$ is positive-definite if  all the diagonal entries are positive, and\n  each diagonal entry is greater than the sum of the absolute values of all other entries in the corresponding row/column.\n\nI couldn't find a proof for this statement. I also couldn't find a reference in my linear algebra books.\nI've a few questions.\n\nHow do we prove the above statement?\nIs the following slightly weaker statement true? \n\n\nA symmetric matrix $A$ is positive-definite if all the diagonal entries are positive, each diagonal entry is greater than or equal to the sum of the absolute values of all other entries in the corresponding row/column, and there exists one diagonal entry which is strictly greater than the sum of the absolute values of all other entries in the corresponding row/column.\n\n",
    "proof": "These matrices are called (strictly) diagonally dominant. The standard way to show they are positive definite is with the Gershgorin Circle Theorem. Your weaker condition does not give positive definiteness; a counterexample is $\r\n\\left[ \\begin{matrix} 1 & 0 & 0 \\\\ 0 & 1 & 1 \\\\ 0 & 1 & 1 \\end{matrix} \\right]\r\n$.\n",
    "tags": [
      "linear-algebra",
      "matrices",
      "numerical-methods",
      "proof-writing",
      "positive-definite"
    ],
    "score": 40,
    "answer_score": 24,
    "is_accepted": true,
    "question_id": 87528,
    "answer_id": 87539
  },
  {
    "theorem": "The art of proof summarizing. Are there known rules, or is it a purely common sense matter?",
    "context": "When a proof is long and difficult, it can be really nice vis-à-vis the reader to give a summary or an outline of the deduction before beginning hard work. \nAre there known rules to give a good proof summary? \nAre there known rules to find which point to emphasize in a proof summary? I mean rules to find the \"nervus probandi\". \n",
    "proof": "I agree with the commentors that this is a difficult question to answer, and what I write here may be only a partial answer.  I've not encountered any set of guidelines for summarising a mathematical proof, but in general the same as are used for summarising any large or complex topic can be applied.  (If you are interested in mathematical proof in particular you might want to look at https://www.math.wustl.edu/~sk/eolss.pdf which is a history of mathematical proof writing by Steven Krantz.)\nA good summary tells the reader what to expect from the story(proof) that follows, should highlight any requisite knowledge, and should motivate the reader into making the effort to follow the argument.  If you look at the Bourbaki style of proof, for example, there is often none of this -- a theorem is given and is proved and it is up to the reader to contextualise it, link it to previous work and knowledge and find a reason for remembering it.  However if you look at some of Steven Krantz's books you will find that he actually spends the majority of a chapter motivating and explaining the ideas, and relegates the actual mathematical proof to the very end of the chapter -- the complete antithesis of a Bourbaki proof.\nTo write a good summary the author must understand the material thoroughly: in fact, being able to summarise a proof well is a good indication that the author has properly understood it.  If at any point the author finds themselves waving their hands, or glossing over a detail, the chances are that there's something there they themselves don't have clear in their own mind.\nAs an example then: consider the stalwart of calculus lectures, the Intermediate Value Theorem.  This says that if we have a continuous function defined on a contiguous set (a 'closed interval') of points, and one endpoint is smaller than zero while the other is greater than zero, then there is a point inside that interval where the function value is zero.  This is nicely summarised by saying \"the graph of a continuous function has no breaks in it\".  This immediately suggests a way to start thinking about it (draw a graph), it connects it to other things the reader already knows about (how to graph a function), and highlights that the reader should know what a continuous function is before proceeding.\nEDIT: ruakh points out in the comments that I've summarised the theorem and not the proof, for which I apologise.  To summarise the proof then:\nSince $f$ is negative at some points and positive at others, and is continuous, we can show that the supremum of the set of negative points is both $0$ and is achieved by $f$.  $0$ plays a key role here, so we can expect that we can find points where $f(x)=c$ by considering $f(x)-c$, and that we can find roots of polynomials by finding points $a$ and $b$ with $f(a)<0$ and $f(b)>0$ (which might lead us to the bisection method).\nThis achieves our goals in summarising: we know what to expect coming up (the study of the set $\\{x: f(x)<0 \\}$); we know we need to know what a supremum is, and we can see how we might use this in future.\n",
    "tags": [
      "logic",
      "proof-writing",
      "soft-question"
    ],
    "score": 40,
    "answer_score": 23,
    "is_accepted": true,
    "question_id": 3200583,
    "answer_id": 3200612
  },
  {
    "theorem": "How can I answer this Putnam question more rigorously?",
    "context": "\nGiven real numbers $a_0, a_1, ..., a_n$ such that $\\dfrac {a_0}{1} + \\dfrac {a_1}{2} + \\cdots + \\dfrac {a_n}{n+1}=0,$ prove that $a_0 + a_1 x + a_2 x^2 + \\cdots + a_n x^n=0$ has at least one real solution.\n\nMy solution:\n\nLet $$f(x) = a_0 + a_1 x + a_2 x^2 + \\cdots + a_n x^n$$\n$$\\int f(x) = \\dfrac {a_0}{1} x + \\dfrac {a_1}{2}x^2 + \\cdots + \\dfrac {a_n}{n+1} x^{n+1} + C$$\n$$\\int_0^1 f(x) = \\left[ \\dfrac {a_0}{1} + \\dfrac {a_1}{2} + \\cdots + \\dfrac {a_n}{n+1} \\right]-0$$\n$$\\int_0^1 f(x) = 0$$\nSince $f$ is continuous, by the area interpretation of integration, it must have at least one zero.\n\nMy question is, is this rigorous enough? Do I need to prove the last statement, perhaps by contradiction using Riemann sums? Is this a theorem I can/should quote?\n",
    "proof": "Why not write it the other way round?\nThe polynomial function \n$$F(x)=\\sum_{k=0}^n\\frac{a_k}{k+1}x^{k+1} $$\nis a differentiable function $\\Bbb R\\to\\Bbb R$ with derivative $$F'(x)=\\sum_{k=0}^na_kx^k.$$\nWe are given that $F(1)=0$, and clearly $F(0)=0$. Hence by Rolle's theorem, there exists $x\\in(0,1)$ such that $F'(x)=0$, as was to be shown.\n",
    "tags": [
      "calculus",
      "proof-writing",
      "contest-math"
    ],
    "score": 39,
    "answer_score": 72,
    "is_accepted": true,
    "question_id": 1874159,
    "answer_id": 1874164
  },
  {
    "theorem": "English words in written mathematics",
    "context": "I recently marked over $100$ assignments for a multivariable calculus course. One question which a lot of people did poorly was proving a given set was open. Aside from issues relating to rigour and logic (or lack thereof), I noticed an issue that I wasn't as aware of before this experience. A lot of students used English words incorrectly (from a mathematical point of view) when writing sentences within their proof.\nSome of the words/phrases I am referring to are:\n\nAs\nAssume\nLet\nSuppose\nHence\nTherefore\nSuch that\nThere exists\n\n\nIs anyone aware of a reference which explains how to use these words and phrases (and others that frequently occur) in a mathematical context?\n\n",
    "proof": "Charles Wells, The Handbook of Mathematical Discourse; it’s available as a PDF here. His site abstractmath.org may also be useful.\n",
    "tags": [
      "reference-request",
      "soft-question",
      "proof-writing",
      "education"
    ],
    "score": 38,
    "answer_score": 22,
    "is_accepted": true,
    "question_id": 365280,
    "answer_id": 365289
  },
  {
    "theorem": "prove $\\sqrt{a_n b_n}$ and $\\frac{1}{2}(a_n+b_n)$ have same limit",
    "context": "I am given this problem:\nlet $a\\ge0$,$b\\ge0$, and the sequences $a_n$ and $b_n$ are defined in this way: $a_0:=a$, $b_0:=b$ and $a_{n+1}:= \\sqrt{a_nb_n}$ and $b_{n+1}:=\\frac{1}{2}(a_n+b_n)$ for all $n\\in\\Bbb{N}$  \nTo prove is that both sequences converge and that they have the same limit. I don't know how to show this. I have spent 2 hours on this, no sign of success\n",
    "proof": "The easy way to proceed, is to show that $a_n, b_n \\geq 0$, and then $ b_{n+1} - a_{n+1} = \\frac {1}{2} (\\sqrt{b_{n}} -\\sqrt{ a_{n}})^2$, so $ b_{n} \\geq a_{n} \\forall n \\geq 2$.\nThen, $a_{n+1} = \\sqrt{a_n b_n} \\geq a_n$ is an monotonically increasing sequence (after $n=2$).\n$b_{n+1} = \\frac {1}{2} (a_n + b_n) \\leq b_n$ is a monotomically decreasing sequnece (after $n=2$).\nFinally, $$ b_{n+1} - a_{n+1} = \\frac {1}{2} (\\sqrt{b_{n}} -\\sqrt{ a_{n}})^2 \\leq \\frac {1}{2} (\\sqrt{b_n} - \\sqrt{a_n} ) ( \\sqrt{b_n} + \\sqrt{a_n} ) = \\frac {1}{2} ( b_n - a_n) \\leq \\frac {1}{2^n} (b_1-a_1),$$ so the difference between the sequences go to 0. Hence, these sequences converge to the same limit.\n\nNote: Of course we could show that since $a_i \\leq b_2$, the limit of $a_i$ exists (since monotonic+bounded). But I think it's more fun to jump directly to the conclusion with the final step.\n",
    "tags": [
      "sequences-and-series",
      "limits",
      "recurrence-relations",
      "proof-writing",
      "problem-solving"
    ],
    "score": 37,
    "answer_score": 41,
    "is_accepted": true,
    "question_id": 267489,
    "answer_id": 267499
  },
  {
    "theorem": "Teacher claims this proof for $\\frac{\\csc\\theta-1}{\\cot\\theta}=\\frac{\\cot\\theta}{\\csc\\theta+1}$ is wrong. Why?",
    "context": "My son's high school teacher says his solution to this proof is wrong because it is not \"the right way\" and that you have to \"start with one side of the equation and prove it is equal to the other\". After reviewing it, I disagree. I believe his solution is correct, even if not \"the right way\", whatever that means. I asked my son how he did it: he cross-multiplied the given identity, simplified it to a known/obvious equality, and then reversed the steps for the proof. This was a graded exam, and the teacher gave him a zero for this problem.\nWhat do you think about my son's solution? Thanks!\nProblem: prove the following trigonometric identity\n\\begin{align*}\n\\frac{\\csc(\\theta)-1}{\\cot(\\theta)}&=\\frac{\\cot(\\theta)}{\\csc(\\theta)+1}\\ .\\\\\n\\end{align*}\nSolution:  for all real $\\theta$ not equal to an integer multiple of $\\pi/2$, we have\n\\begin{align*}\n\\cot^2(\\theta)&=\\cot^2(\\theta)\\\\[8pt]\n\\frac{\\cos^2(\\theta)}{\\sin^2(\\theta)}&=\\cot^2(\\theta)\\\\[8pt]\n\\frac{1-\\sin^2(\\theta)}{\\sin^2(\\theta)}&=\\cot^2(\\theta)\\\\[8pt]\n\\csc^2(\\theta)-1&=\\cot^2(\\theta)\\\\[8pt]\n\\frac{\\csc^2(\\theta)-1}{\\cot(\\theta)}&=\\cot(\\theta)\\\\[8pt]\n\\frac{\\csc(\\theta)-1}{\\cot(\\theta)}&=\\frac{\\cot(\\theta)}{\\csc(\\theta)+1}\n\\end{align*}\n",
    "proof": "I would at least have skipped the first line and started with $\\dfrac{\\cos^2\\theta}{\\sin^2\\theta} = \\cot^2\\theta.$\nOne reason why an instructor might have qualms about this argument is the frequency with which students do this in the opposite direction, i.e. they write\n$$\n\\require{cancel}\n\\xcancel{\n\\begin{align}\n\\frac{\\csc(\\theta)-1}{\\cot(\\theta)}&=\\frac{\\cot(\\theta)}{\\csc(\\theta)+1} \\\\[8pt]\n\\frac{\\csc^2(\\theta)-1}{\\cot(\\theta)}&=\\cot(\\theta)\\\\[8pt]\n\\csc^2(\\theta)-1&=\\cot^2(\\theta)\\\\[8pt]\n\\frac{1-\\sin^2(\\theta)}{\\sin^2(\\theta)}&=\\cot^2(\\theta)\\\\[8pt]\n\\frac{\\cos^2(\\theta)}{\\sin^2(\\theta)}&=\\cot^2(\\theta)\\\\[8pt]\n\\cot^2(\\theta)&=\\cot^2(\\theta) \n\\end{align}\n}\n$$\nThis would be logically correct if one wrote \"This equality is true if the one below it is true.\" on each line. But one must be clear about the correct direction of \"If ... then ...\". Without those explicit words, the sequence of equalities can be taken to mean \"If ... then ...\" in just the opposite order.\nOften what instructors want is something like this:\n$$\n\\frac{\\csc\\theta-1}{\\cot\\theta} = \\cdots\\cdots\\cdots\\cdots = \\frac{\\cot\\theta}{\\csc\\theta+1}.\n$$\nin which each \"equals\" sign asserts the equality of things already known to be equal.\nI would give full credit to the answer that your son wrote if each line had some brief explanation of how it is deduced from the line before it. For example, he could write this:\n\n$$ \\cot^2\\theta = \\frac{\\cos^2\\theta}{\\sin^2\\theta} = \\frac{1-\\sin^2\\theta}{\\sin^2\\theta} = \\csc^2\\theta-1. $$ Then, dividing both sides of the equality $\\cot^2\\theta = \\csc^2\\theta-1$ by $\\cot\\theta,$ we get $$ \\cot\\theta = \\frac{\\csc^2\\theta-1}{\\cot\\theta}. $$ Finally, dividing both sides of that by $\\csc\\theta+1$ we get $$ \\frac{\\csc\\theta-1}{\\cot\\theta} =  \\frac{\\cot\\theta}{\\csc\\theta+1}. $$\n\nNote that I wrote words above, not just mathematical notation. Well written answers do that, except perhaps in fairly simple cases.\n",
    "tags": [
      "algebra-precalculus",
      "trigonometry",
      "proof-writing",
      "solution-verification"
    ],
    "score": 37,
    "answer_score": 14,
    "is_accepted": false,
    "question_id": 4043453,
    "answer_id": 4043511
  },
  {
    "theorem": "Why does drawing $\\square$ mean the end of a proof?",
    "context": "To end a proof, I often write \"as was to be shown\" or \"q.e.d\". Both of these terms make sense to me as a reader. On the other hand, I feel a little strange to put down $\\square$ although I  saw it many times here and there. In fact, I learned $\\square$ notation here. I wonder if anyone could give me a brief explanation of $\\square$ notation in mathematics. Where does it come from? More importantly, how does it logically mean \"end\" of a proof? Thank you.\n",
    "proof": "It just means the same thing as q.e.d. Its introduction is usually attributed to Paul Halmos:\n\n\"The symbol is definitely not my invention — it appeared in popular magazines (not mathematical ones) before I adopted it, but, once again, I seem to have introduced it into mathematics. It is the symbol that sometimes looks like ▯, and is used to indicate an end, usually the end of a proof. It is most frequently called the 'tombstone', but at least one generous author referred to it as the 'halmos'.\", Paul R. Halmos, I Want to Be a Mathematician: An Automathography, 1985, p. 403.\n\n(This is quoted in Wikipedia)\n",
    "tags": [
      "proof-writing",
      "notation",
      "article-writing"
    ],
    "score": 36,
    "answer_score": 39,
    "is_accepted": true,
    "question_id": 56606,
    "answer_id": 56607
  },
  {
    "theorem": "Does one accepted false statement allow proving anything?",
    "context": "When I was studying, the mathematical analysis professor said something interesting when he was explaining the implication (logical operator), namely $(False \\implies True) = True$. He said something like (from my memory):\n\nYou can derive any truth from a falsehood. If we accept a single falsehood as truth, then we can prove any single theorem we want.\n\nIf I understand it correctly, it means that if we assume that $2+2=5$, then we can provide proof that $\\pi = 3$, or that $1 = 2$ or that $\\sin^2 x + \\cos^2 x \\neq 1$.\nIs the bold statement true? Is it possible to canonically prove it even though it assumes (temporarily) accepting falsehood as truth?\n\nJust to clarify a little, my question is primarily about the professor’s statement and not about the principles of implication. I’m just curious, what is the scale of destructiveness (in terms of drawing otherwise logical conclusions) of accepting something false as a truth.\nIt is also interesting to me whether this observation can be generalized, in terms of if we, e.g., falsely assume that dogs and cats are exactly the same animal, can we prove (with a series of otherwise logical conclusions) that, e.g., the Statue of Liberty is actually placed underwater or that the Moon is heavily populated by chipmunks (but that’s a bonus question, because it expands outside the field of mathematics, I guess.)\n",
    "proof": "Strangely, this is pretty much correct.  It is difficult to come up with a reasonable logic that doesn't permit this.\nYour arithmetic examples are easy to deal with.  If $$2+2=5$$ then we can proceed:\n$$\\begin{align}\n2+2 &= 5 \\\\\n2 & = 3 &\\text{subtract $2$}\\\\\n0 & = 1 &\\text{subtract $2$ again}\\\\\n0 & = \\pi - 3 &\\text{multiply by $\\pi-3$} \\\\\n3 & = \\pi     &\\text{add $3$}\n\\end{align}$$\nas you requested.  (It's similarly easy to get $1=2$ or your other formulas.)\nBut the problem goes deeper than arithmetic.  The logical principle here is called the principle of explosion or sometimes “EFQ” for short, and it is very difficult to avoid.\nSuppose our logical system has an “or” operation $\\lor$ where $$X\\lor Y$$ means $$X \\text{ is true, or } Y \\text{ is true, or both.}$$  Then these rules both make perfect sense:\n\nIf we have proved $X$, we can conclude $X\\lor Y$ for any $Y$ at all.\nIf we have proved $X\\lor Y$ and we know that $X$ is false, we can conclude $Y$.\n\nIf you accept these, then you should also accept EFQ.  This is why: Suppose we have proved $X$, which is false.  By (1)  we can conclude $X\\lor Y$, and since $X$ is false, by (2) we can conclude $Y$.  So after proving $X$, which is false, we can conclude $Y$ for any $Y$ at all.\nIf you don't like this result, you need to say which of (1) and (2) you will reject.\n(I asked about this previously. It is hard to get rid of.)\n",
    "tags": [
      "logic",
      "proof-writing"
    ],
    "score": 35,
    "answer_score": 65,
    "is_accepted": true,
    "question_id": 4820890,
    "answer_id": 4820898
  },
  {
    "theorem": "When has one sufficiently mastered an area of mathematics?",
    "context": "This is a rather soft question regarding the mastery of various mathematical subjects, such as undergraduate subjects. \nIn particular, say, when has one mastered undergraduate analysis? Is it realistic to expect some individual to be able to prove every theorem and do every exercise in Baby Rudin? What about analysis not covered in Baby Rudin? Say, should one also be able to prove compactness is equivalent to sequential compactness, and sequential compactness is equivalent to closed and boundedness in $\\mathbb{R}^n$? Should one be able to prove every major statement about completeness? Continuity? Integration?\nOr is it enough to be able to read and understand any proof in a particular area?\nWhen can one feel satisfied with what they know about a particular area of basic mathematics? Can the seasoned mathematician prove these theorems by heart without much thought? If so, is this due to brute memorization, or simply mathematical maturity?\nIn short, what should one reasonably expect an undergraduate to know about any particular subject after completing a course in said subject? I ask specifically about undergraduate subjects, since these are often the areas expected to be mastered by the time one begins graduate school and beyond. This question applies to all areas of mathematics, graduate and undergraduate.  \nInterested to hear different interpretations of the words sufficiently and mastered.\n",
    "proof": "Richard Feynman made some interesting comments about this in chapter 14 of volume 1 of the Feynman Lectures on Physics:\n\nIn learning any subject of a technical nature where mathematics plays\na role, one is confronted with the task of understanding and storing\naway in the memory a huge body of facts and ideas, held together by\ncertain relationships which can be “proved” or “shown” to exist\nbetween them. It is easy to confuse the proof itself with the\nrelationship which it establishes. Clearly, the important thing to\nlearn and to remember is the relationship, not the proof. In any\nparticular circumstance we can either say “it can be shown that” such\nand such is true, or we can show it. In almost all cases, the\nparticular proof that is used is concocted, first of all, in such form\nthat it can be written quickly and easily on the chalkboard or on\npaper, and so that it will be as smooth-looking as possible.\nConsequently, the proof may look deceptively simple, when in fact, the\nauthor might have worked for hours trying different ways of\ncalculating the same thing until he has found the neatest way, so as\nto be able to show that it can be shown in the shortest amount of\ntime! The thing to be remembered, when seeing a proof, is not the\nproof itself, but rather that it can be shown that such and such is\ntrue. Of course, if the proof involves some mathematical procedures or\n“tricks” that one has not seen before, attention should be given not\nto the trick exactly, but to the mathematical idea involved.\nIt is\ncertain that in all the demonstrations that are made in a course such\nas this, not one has been remembered from the time when the author\nstudied freshman physics. Quite the contrary: he merely remembers that\nsuch and such is true, and to explain how it can be shown he invents a\ndemonstration at the moment it is needed. Anyone who has really\nlearned a subject should be able to follow a similar procedure, but it\nis no use remembering the proofs. That is why, in this chapter, we\nshall avoid the proofs of the various statements made previously, and\nmerely summarize the results.\n\n(The bold is mine.)  I think this quote is open to some interpretation / debate, but it's definitely interesting.\n",
    "tags": [
      "real-analysis",
      "analysis",
      "soft-question",
      "proof-writing"
    ],
    "score": 34,
    "answer_score": 33,
    "is_accepted": false,
    "question_id": 1387519,
    "answer_id": 1387534
  },
  {
    "theorem": "Show that the eigenvalues of a unitary matrix have modulus $1$",
    "context": "\nShow that the eigenvalues of a unitary matrix have modulus $1$.\n\nI know that a unitary matrix can be defined as a square complex matrix $A$, such that\n$$AA^*=A^*A=I$$\nwhere $A^*$ is the conjugate transpose of $A$, and $I$ is the identity matrix. Furthermore, for a square matrix $A$, the eigenvalue equation is expressed by $$Av=\\lambda v$$\nIf I use the relationship $(u v)^*=v^*{u^*}$ and take the conjugate transpose of this equation then\n$$v^*A^*=\\lambda^*v^*$$\nBut now I got stuck. Can someone help?\n",
    "proof": "You multiply your two relations to obtain \n\\begin{align}\nv^*A^*Av &=\\lambda^* v^*\\lambda v \\\\\nv^*Iv &=\\left(\\lambda^*\\lambda\\right) v^*v \\\\\nv^*v &=\\left(\\lambda^*\\lambda\\right) v^*v \\\\\n||v||^2 &= |\\lambda|^2 ||v||^2 \\\\\n\\sqrt{1} &=|\\lambda| \\\\\n1 &=|\\lambda|\n\\end{align}\n\nRecall that the modulus of a complex number $\\lambda = a + bi$, also called the \"complex norm\", is denoted $|\\lambda|$ and defined by $|\\lambda| = |a + bi| = \\sqrt{a^2 + b^2}$ and $\\lambda^*\\lambda = (a -bi)(a + bi) = a^2 + b^2$. Hence $\\lambda^*\\lambda = |\\lambda|^2.$\n",
    "tags": [
      "linear-algebra",
      "matrices",
      "proof-writing",
      "eigenvalues-eigenvectors",
      "unitary-matrices"
    ],
    "score": 34,
    "answer_score": 42,
    "is_accepted": true,
    "question_id": 1717713,
    "answer_id": 1717735
  },
  {
    "theorem": "Prove that a set $E$ is closed iff it&#39;s complement $E^{c}$ is open",
    "context": "I was wondering if this proof was right.\n$\\Leftarrow$ Suppose $E$ is closed. Then choose $x\\in E^{c}$, then $x\\notin E$, and so $x$ is not a limit point of $E$. \nHence there exists a neighborhood $N$ of $x$ such that $E \\cap N$ is empty, such that $N \\subset E^{c}$. Thus $x$ is an interior point of $E^{c}$ and $E^{c}$ is open\n$\\Rightarrow$ Then suppose $E^{c}$ is open. Let $x$ be a limit point of $E$, then every neighborhood of $x$ contains a point of $E$, so that $x$ is not an interior point of $E^{c}$. Since $E^{c}$ is open, this means that $ x \\in E$, Therefore $E$ is closed. \n",
    "proof": "Looks good. All true statements and they make logical sense. However, it kinda depends on where you start. Is this meant to be a proof for metric spaces? Then it's absolutely fine. Or is it a proof for topological spaces? For topological spaces, the definition of being a closed set is usually that the complement is open. You could use the same definition for metric spaces, but that's usually not done. So it depends on the definitions from which you start.\n",
    "tags": [
      "real-analysis",
      "general-topology",
      "proof-writing"
    ],
    "score": 34,
    "answer_score": 7,
    "is_accepted": false,
    "question_id": 392212,
    "answer_id": 392226
  },
  {
    "theorem": "Is it okay to reverse engineer proofs in homework questions?",
    "context": "In a linear algebra text book, one homework question I received was:\n\nProve that $\\mathbf{a \\cdot b} = \\frac{1}{4}(\\|\\mathbf{a + b}\\|^2 -\n \\|\\mathbf{a - b}\\|^2)$.\n\nWhere $\\mathbf{a}$ and $\\mathbf{b}$ are vectors in $\\Bbb{R}^n$.\nThis is trivial to prove if we start from $\\frac{1}{4}(\\|\\mathbf{a + b}\\|^2 -\n \\|\\mathbf{a - b}\\|^2)$ and reverse engineer it in $\\Bbb{R}^2$:\n$$\n\\|\\mathbf{a + b}\\|^2 = a_1^2 + 2a_1b_1 + b_1^2 + a_2^2 + 2a_2b_2 + b_2^2 \\\\\n\\|\\mathbf{a - b}\\|^2 = a_1^2 - 2a_1b_1 + b_1^2 + a_2^2 - 2a_2b_2 + b_2^2 \\\\\n\\|\\mathbf{a + b}\\|^2 - \\|\\mathbf{a - b}\\|^2 = 4a_1b_1 + 4a_2b_2 \\\\\n\\frac{1}{4}(\\|\\mathbf{a + b}\\|^2 - \\|\\mathbf{a - b}\\|^2) = \\frac{4}{4}(a_1b_1 + a_2b_2) \\\\\n= a_1b_1 + a_2b_2 = \\mathbf{a \\cdot b}\n$$\nBut I'm worried about whether or not proofs like this are \"legal\", if that makes any sense. There was no wording in the question stating that I couldn't start from the right side of the identity, but I still have this strange feeling of guilt that I should've tried solving the identity starting from the left side and working in the \"normal\" direction. \nFor questions like these, is it okay to start from the right side of the identity? Would what I get out of doing the question in reverse be the same as if I did it normally?\n",
    "proof": "It's absolutely fine to reverse engineer proofs! As long as the proof works in the end, it doesn't matter how you got to it - in fact, reverse engineering proofs is a fairly standard technique.\nHowever you need to be careful in this approach: you need to make sure that each step is reversible ($p$ implies $q$ does not mean $q$ implies $p$). With your example here though, we can easily reverse each step because everything is just equality. \nIt is worth noting that although your proof method is fine, the proof itself doesn't quite get what you want because you have assumed that $a,b\\in\\mathbb{R^2}$ when you wish to prove for any two vectors $a,b\\in\\mathbb{R^n}$. This problem, though, is easily dealt with (can you see how?).\n",
    "tags": [
      "proof-writing"
    ],
    "score": 33,
    "answer_score": 34,
    "is_accepted": true,
    "question_id": 851830,
    "answer_id": 851836
  },
  {
    "theorem": "How can I simply prove that the pearson correlation coefficient is between -1 and 1?",
    "context": "For building a recommendation system, I also use the Pearson correlation coefficient. This is the definition:\n$r(x, y)=\\frac{\\sum_{i=1}^n (x_i-\\bar{x})(y_i-\\bar{y})}{\\sqrt{\\sum_{i=1}^n (x_i-\\bar{x})^2 \\cdot \\sum_{i=1}^n (y_i-\\bar{y})^2}}$\n$x$ and $y$ are part of $\\mathbb{R}$.\nNow for coding, it is important to take care of all potential outcomes. For example, if the denominator is zero, you will have to filter that or throw an exception.\nI came up with some arguments, one of them being that if all values of $x_i$ and/or $y_i$ were equal to the average of $x$ and/or $y$, then the denominator would be zero.\nBut how can I prove that the coefficient is either undefined (zero denominator) or in between -1 and 1? What is the best approach?\n",
    "proof": "First of all Pearson's correlation coefficient is bounded between -1 and 1, not 0 and one. It's absolute value is bounded between 0 and 1, and that useful later.\nPearson's correlation coefficient is simply this ratio:\n$$\\rho = \\frac{Cov(X,Y)}{\\sqrt{Var(X)Var(Y)}}$$\nBoth of the variances are non-negative by definition, so the denominator is $\\ge 0$. The only way a singularity can occur is if one of the variables has 0 variance.\nIf two random variables are perfectly uncorrelated, (i.e. independent) then their covariance is 0. So 0 is a valid lower bound for the absolute value of the expression.\nThis can be shown like so:\n$$Cov(X,Y) = E[(X-\\bar{X})(Y-\\bar{Y})] = E[XY] - E[X]E[Y]$$\nif two random variables are independent, then $E[XY]=E[X]E[Y]$, and\n$$Cov(X,Y) = E[XY] - E[X]E[Y] = E[X]E[Y] - E[X]E[Y] = 0.$$\nNow for the upper bound. Here we apply the Cauchy-Schwarz inequality.\n$$|Cov(X,Y)|^2 \\le Var(X)Var(Y)$$\n$$\\therefore |Cov(X,Y)| \\le \\sqrt{Var(X)Var(Y)}$$\nplug this result from the Cauchy-Schwarz inequality into the formula for $\\rho$, and we get:\n$$|\\rho| = \\left|\\frac{Cov(X,Y)}{\\sqrt{Var(X)Var(Y)}}\\right| \\le \\frac{\\sqrt{Var(X)Var(Y)}}{\\sqrt{Var(X)Var(Y)}} = 1$$\nThus we have the absolute value of the correlation is bounded below by 0 and above by 1.\n",
    "tags": [
      "proof-writing",
      "correlation"
    ],
    "score": 33,
    "answer_score": 43,
    "is_accepted": true,
    "question_id": 564751,
    "answer_id": 564843
  },
  {
    "theorem": "Transpose of block matrix",
    "context": "I'm attempting to prove that\n$$\n\\left[ \\begin{array}{c c}\nA & B \\\\\nC & D \\\\\n\\end{array} \\right]^\\top =\n\\left[ \\begin{array}{c c}\nA^\\top & C^\\top \\\\\nB^\\top & D^\\top \\\\\n\\end{array} \\right].\n$$\nIntuitively, I can see that it's true. However, when I try to formally prove it, I quickly get lost in the indices. What tricks can I use to keep things straight?\nSource: Exercise 2.6.16, P116, Intro to Linear Algebra, 4th Ed by Strang\n",
    "proof": "Most people would just claim this is obvious and omit the proof, but if you don't want to do that then perhaps you could first prove that \n\\begin{equation}\n\\begin{bmatrix} M & N \\end{bmatrix}^T \n= \\begin{bmatrix} M^T \\\\ N^T \\end{bmatrix}\n\\end{equation}\nand\n\\begin{equation}\n\\begin{bmatrix} M \\\\ N \\end{bmatrix}^T \n= \\begin{bmatrix} M^T & N^T \\end{bmatrix}.\n\\end{equation}\nThen\n\\begin{align}\n\\begin{bmatrix} A & B \\\\ C & D \\end{bmatrix}^T \n&= \\begin{bmatrix} \n\\begin{bmatrix} A \\\\ C \\end{bmatrix}^T \\\\\n\\begin{bmatrix} B \\\\ D \\end{bmatrix}^T\n\\end{bmatrix} \\\\\n&= \\begin{bmatrix} A^T & C^T \\\\ B^T & D^T \\end{bmatrix}.\n\\end{align}\n",
    "tags": [
      "linear-algebra",
      "matrices",
      "proof-writing",
      "transpose"
    ],
    "score": 33,
    "answer_score": 50,
    "is_accepted": true,
    "question_id": 246289,
    "answer_id": 246305
  },
  {
    "theorem": "A subset of a compact set is compact?",
    "context": "Claim:Let $S\\subset T\\subset X$ where $X$ is a metric space. If $T$ is compact in $X$ then $S$ is also compact in $X$.\nProof:Given that $T$ is compact in $X$ then any open cover of T, there is a finite open subcover, denote it as $\\left \\{V_i  \\right \\}_{i=1}^{N}$. Since $S\\subset T\\subset \\left \\{V_i  \\right \\}_{i=1}^{N}$ so $\\left \\{V_i  \\right \\}_{i=1}^{N}$ also covers $S$ and hence $S$ is compact in X\nEdited: I see why this is false but in general, why every closed subset of a compact set is compact?\n",
    "proof": "If $S\\subseteq T$ and $T$ is compact and $S$ is closed then $S$ is compact.\nWhy? Let $\\cal U$ be an open cover of $S$. Every open set in $\\cal U$ is of the form $U\\cap S$ for some open set $U$ (open in $T$). Let $\\mathcal V=\\{U\\subseteq T\\mid U\\text{ is open, and }\\exists U'\\in\\mathcal U:U\\cap S=U'\\}$. Then $\\mathcal V$ is an open cover of $S$ as well, since $S$ is closed we have that $T\\setminus S$ is open so $\\mathcal V\\cup\\{T\\setminus S\\}$ is an open cover of $T$.\nBy compactness of $T$ we have a finite subcover, from which we can produce a finite subcover of $\\cal U$.\n\nWe have shown that every open cover of $S$ has a finite subcover, and therefore $S$ is compact. We have used the fact that $S$ is closed to make sure that $T\\setminus S$ is open. If $S$ is not closed we cannot use this to produce an open cover of $T$ and we cannot continue and find an open subcover for $\\cal U$.\n",
    "tags": [
      "general-topology",
      "proof-writing",
      "compactness"
    ],
    "score": 32,
    "answer_score": 50,
    "is_accepted": false,
    "question_id": 212181,
    "answer_id": 212191
  },
  {
    "theorem": "Disjoint compact sets in a Hausdorff space can be separated",
    "context": "I want to show that any two disjoint compact sets in a Hausdorff space $X$ can be separated by disjoint open sets. Can you please let me know if the following is correct? Not for homework, just studying for a midterm. I'm trying to improve my writing too.\nMy work:\nLet $C$,$D$ be disjoint compact sets in a Hausdorff space $X$. Now fix $y \\in D$ and for each $x \\in C$ we can find (using Hausdorffness) disjoint open sets $U_{x}(y)$ and $V_{x}(y)$ such that $x \\in U_{x}(y)$ and $y \\in V_{x}(y)$. Now the collection $\\{U_{x}: x \\in C\\}$ covers $C$ so by compactness we can find some natural k such that\n$C \\subseteq \\bigcup_{i=1}^{k} U_{x_{i}}(y)$ \nNow for simplicity let $U = \\bigcup_{i=1}^{k} U_{x_{i}}(y)$, then $C \\subseteq U$ and let $W(y) = \\bigcap_{i=1}^{k} V_{x_{i}}(y)$. Then $W(y)$ is a neighborhood of $y$ and disjoint from $U$.\nNow consider the collection $\\{W(y): y \\in D\\}$, this covers D so by compactness we can find some natural q such that $D \\subseteq \\bigcup_{j=1}^{q} W_{y_{j}}$.\nFinally set $V = \\bigcup_{j=1}^{q} W_{y_{j}}$, then $U$ and $V$ are disjoint open sets containing $C$ and $D$ respectively. \nWhat do you think?\n",
    "proof": "This is a very good start, but there is a slight problem with your argument: as you change $y$, your $U$ changes as well (since $U$ is constructed in terms of $y$); you should really call it $U(y)$. \nYour construction gives you an open neighborhood $W(y)$ of $y$ for each $y$; $W(y)$ is disjoint from $U(y)$. But for all you know, $W(y)$ may fail to be disjoint from $U(y')$ with $y'\\neq y$.\nSo you really still have a bit more to go before you are done. \n",
    "tags": [
      "general-topology",
      "solution-verification",
      "proof-writing",
      "compactness"
    ],
    "score": 32,
    "answer_score": 14,
    "is_accepted": true,
    "question_id": 18192,
    "answer_id": 18193
  },
  {
    "theorem": "Can you use both sides of an equation to prove equality?",
    "context": "For example:\n\n$\\color{red}{\\text{Show that}}$$$\\color{red}{\\frac{4\\cos(2x)}{1+\\cos(2x)}=4-2\\sec^2(x)}$$\n\nIn high school my maths teacher told me\n\nTo prove equality of an equation; you start on one side and manipulate it algebraically until it is equal to the other side.\n\nSo starting from the LHS: $$\\frac{4\\cos(2x)}{1+\\cos(2x)}=\\frac{4(2\\cos^2(x)-1)}{2\\cos^2(x)}=\\frac{2(2\\cos^2(x)-1)}{\\cos^2(x)}=\\frac{4\\cos^2(x)-2}{\\cos^2(x)}=4-2\\sec^2(x)$$ $\\large\\fbox{}$\nAt University, my Maths Analysis teacher tells me\n\nTo prove a statement is true, you must not use what you are trying to prove.\n\nSo using the same example as before:\nLHS = $$\\frac{4\\cos(2x)}{1+\\cos(2x)}=\\frac{4(2\\cos^2(x)-1)}{2\\cos^2(x)}=\\frac{2(2\\cos^2(x)-1)}{\\cos^2(x)}=\\frac{2\\Big(2\\cos^2(x)-\\left[\\sin^2(x)+\\cos^2(x)\\right]\\Big)}{\\cos^2(x)}=\\frac{2(\\cos^2(x)-\\sin^2(x))}{\\cos^2(x)}=\\bbox[yellow]{2-2\\tan^2(x)}$$\nRHS =$$4-2\\sec^2(x)=4-2(1+\\tan^2(x))=\\bbox[yellow]{2-2\\tan^2(x)}$$\nSo I have shown that the two sides of the equality in $\\color{red}{\\rm{red}}$ are equal to the same highlighted expression. But is this a sufficient proof?\nSince I used both sides of the equality (which is effectively; using what I was trying to prove) to show that $$\\color{red}{\\frac{4\\cos(2x)}{1+\\cos(2x)}=4-2\\sec^2(x)}$$\nOne of the reasons why I am asking this question is because I have a bounty question which is suffering from the exact same issue that this post is about.\n\nEDIT:\nComments and answers below seem to indicate that you can use both sides to prove equality. So does this mean that my high school maths teacher was wrong?\n$$\\bbox[#AFF]{\\text{Suppose we have an identity instead of an equality:}}$$ $$\\bbox[#AFF]{\\text{Is it possible to manipulate both sides of an identity to prove that the identity holds?}}$$\nThank you.\n",
    "proof": "There's no conflict between your high school teacher's advice\n\nTo prove equality of an equation; you start on one side and manipulate it algebraically until it is equal to the other side.\n\nand your professor's\n\nTo prove a statement is true, you must not use what you are trying to prove.\n\nAs in Siddarth Venu's answer, if you prove $a = c$ and $b = c$ (\"working from both sides\"), then $a = c = b$ by transitivity of equality. This conforms to both your teacher's and professor's advice.\n\nBoth your high school teacher and university professor are steering you away from \"two-column proofs\" of the type:\n\\begin{align*}\n-1 &= 1 &&\\text{To be shown;} \\\\\n(-1)^{2} &= (1)^{2} && \\text{Square both sides;} \\\\\n1 &= 1 && \\text{True statement. Therefore $-1 = 1$.}\n\\end{align*}\nHere, you assume what you want to prove, deduce a true statement, and assert that the original assumption was true. This is bad logic for at least two glaring reasons:\n\nIf you assume $-1 = 1$, there's no need to prove $-1 = 1$.\nLogically, if $P$ denotes the statement \"$-1 = 1$\" and $Q$ denotes \"$1 = 1$\", the preceding argument shows \"$P$ implies $Q$ and $Q$ is true\", which does not eliminate the possibility \"$P$ is false\".\n\nWhat you can do logically is start (\"provisionally\", on scratch paper) with the statement $P$ you're trying to prove and perform logically reversible operations on both sides until you reach a true statement $Q$. A proof can then be constructed by starting from $Q$ and working backward until you reach $P$. Often times, the backward argument can be formulated as a sequence of equalities, conforming to your teacher's advice. (Note that in the initial phase of seeking a proof, you aren't bound by anything: You can make inspired guesses, additional assumptions, and the like. Only when you write up a final proof must you be careful to assume no more than is given, and to make logically-valid deductions.)\n",
    "tags": [
      "soft-question",
      "proof-writing"
    ],
    "score": 31,
    "answer_score": 47,
    "is_accepted": true,
    "question_id": 1763978,
    "answer_id": 1764070
  },
  {
    "theorem": "Intuition of Addition Formula for Sine and Cosine",
    "context": "The proof of two angles for sine function is derived using $$\\sin(A+B)=\\sin A\\cos B+\\sin B\\cos A$$ and $$\\cos(A+B)=\\cos A\\cos B-\\sin A\\sin B$$ for cosine function. I know how to derive both of the proofs using acute angles which can be seen here http://en.wikibooks.org/wiki/Trigonometry/Addition_Formula_for_Cosines but pretty sure those who have taken trig know what I'm talking about. So I know how to derive and prove both of the two-angle functions using the acute angles, but what I am completely confused about is where those triangles came from. So for proving the two-angle cosine function, we look at two acute angles, $A$ and $B$, where $A+B<90$ and keep on expanding. So my question is, where did those two triangles come from and what is the intuition behind having two acute triangles on top of each other? \n",
    "proof": "This is a great question, because I'm convinced that all the cool trig identities have simple explanatory diagrams.\nIn a \"unit-hypotenuse\" right triangle $\\triangle ABC$ (with right angle at $C$), the leg opposite $A$ has length $\\sin A$ and the side adjacent to $A$ has length $\\cos A$. By proportionality, if the hypotenuse has length $c$, then the leg opposite $A$ has length $c\\;\\sin A$ and the leg adjacent to $A$ has length $c\\;\\cos A$. But you know this.\nSo, for diagramming trig identities,\n\nWherever you see anything like \"this $\\cdot \\sin(\\text{that})$\", you look to represent that term by the leg of a right triangle whose hypotenuse is \"this\" and whose angle opposite the leg is \"that\". Likewise, represent \"this $\\cdot \\cos(\\text{that})$\" with a segment adjacent to angle \"that\" in a triangle with hypotenuse \"this\".\n\n... and exploit theorems about parallel lines and congruent angles wherever possible.\nI'll talk through an example that's slightly simpler than (but strongly related to) the one in your question.\n\nConsider the identity\n$$p \\sin\\theta + q \\cos\\theta = r \\sin\\left(\\theta+\\phi\\right)$$\nwhere $p^2+q^2=r^2$ and $\\tan\\phi = q/p$.\nFor the left-hand side, we'll need a right triangle with hypotenuse $p$ and one with hypotenuse $q$, each with an acute angle $\\theta$. We want to arrange those triangles so that the leg opposite $\\theta$ in the $p$-triangle and the leg adjacent to $\\theta$ in the $q$-triangle make a straight segment that represents the sum of their lengths. Like so (with $\\theta$ represented by a black dot, since I'm recycling an old image):\n\nLooking to the right hand side's $r$, the Pythagorean relation $p^2+q^2=r^2$ suggests we need to show $r$ as the hypotenuse of a right triangle with legs $p$ and $q$. By interesting coincidence, the angle between the hypotenuses of our $p$-triangle and $q$-triangle is itself a right angle! (Why?) So, joining the other ends of these segments gives us $r$. \n\nIn fact, we now also have $\\phi$ (the white dot), since $q/p$ is clearly the \"opposite-over-adjacent\" ratio for the marked angle.\nTo complete the diagram, we need a right triangle with hypotenuse $r$ and acute angle $\\theta+\\phi$ (strategically placed so that the left-hand and right-hand sides of the identity are clearly equal). Well, we have a right triangle with hypotenuse $r$, but that's not good enough; the angles are wrong. Where can we see $\\theta+\\phi$ (the sum of the black dot and white dot)? Hmmm ... Look at the top-left corner of the diagram: we have an angle $\\theta$ next to and angle $\\phi$ ... together they make $\\theta+\\phi$; and, hey! They're right next to that segment $r$! If only we could make $r$ the hypotenuse of a right triangle with angle $\\theta+\\phi$.\nThat's easy: draw a perpendicular!\n\nThen, then new (purple) segment opposite $\\theta+\\phi$ in a triangle with hypotenuse $r$; it must have length $r\\;\\sin(\\theta+\\phi)$.\n\nSince the new segment is clearly congruent to the bottom segment (they're opposite sides of a rectangle), we've demonstrated that the parts of the trig identity are equal. Mission accomplished!\nIn point of fact, we can do a little bit better. It's not cool that the purple segment is obscuring other parts of the diagram. But how can we move it? We drew it because we needed a triangle with a $\\theta+\\phi$ in it; to move it, we'd need another angle like that ... oh, wait a minute ... Notice how the sides of the diagram are parallel (being perpendicular to a common segment)? and how that $r$ segment is their transversal? Hmmmm ...\n\nNow we can draw another convenient ---and non-obscuring--- perpendicular to represent $r\\;\\sin(\\theta+\\phi)$.\n\nAnd there you have a diagram of the identity $p\\;\\sin\\theta + q\\;\\cos\\theta = r\\;\\sin(\\theta+\\phi)$.\n\nFor the angle-sum formulas,\n$$\\begin{align}\n\\sin(\\alpha + \\beta) &= \\sin \\alpha\\;\\cos\\beta + \\cos\\alpha\\;\\sin\\beta \\\\\n\\cos(\\alpha + \\beta) &= \\cos \\alpha\\;\\cos\\beta - \\sin\\alpha\\;\\sin\\beta\n\\end{align}$$\nit takes a slight leap to know how to represent the component products. Take for instance, $\\sin\\alpha\\;\\cos\\beta$. Is it a \"$\\text{this}\\cdot\\sin(\\text{that})$\" situation (with \"this\"$=\\cos\\beta$ and \"that\"$=\\alpha$), or a \"$\\text{this}\\cdot\\cos(\\text{that})$\" situation? It could be either, but let's assume the former; that means we'd want our diagram to feature a right triangle with hypotenuse $\\cos\\beta$ and acute angle $\\alpha$. But, to create our hypotenuse of length $\\cos\\beta$, we'll need it to be a leg adjacent to $\\beta$ in a triangle with hypotenuse $1$: we must stack one triangle on top of another!\n\nThe good news is that the \"other\" leg in the (blue) triangle with hypotenuse $\\cos\\beta$ will have length $\\cos\\alpha\\;\\cos\\beta$, which is convenient, because we'll need one of those. Oh, and hey ... The other leg in the (pink) triangle of hypotenuse $1$ has length $\\sin\\beta$, which could be the hypotenuse of a convenient new triangle with legs $\\cos\\alpha\\;\\sin\\beta$ and $\\sin\\alpha\\;\\sin\\beta$; we need those, too!\n\nConveniently, we find that we've created (at the bottom-left) the angle $\\alpha+\\beta$. As before, we can drop a simple perpendicular to bound that angle inside a triangle with hypotenuse $1$; but, as before, we can do a little better:\n\nThis gives us a (white) triangle with hypotenuse $1$ and legs $\\sin(\\alpha+\\beta)$ and $\\cos(\\alpha+\\beta)$, positioned in such a way that the sine segment is clearly the sum of the two vertical segments we constructed, and that the cosine segment is clearly difference of the horizontal segments.\nWith a very slight adjustment, we turn the diagram into an illustration of the angle-difference identities\n$$\\begin{align}\n\\sin(\\alpha-\\beta) &= \\sin\\alpha\\;\\cos\\beta - \\cos\\alpha\\;\\sin\\beta \\\\\n\\cos(\\alpha-\\beta) &= \\cos\\alpha\\;\\cos\\beta + \\sin\\alpha\\;\\sin\\beta\n\\end{align}$$  \n\n\nI encourage you to seek-out more diagrams of identities, both as a simple exercise in understanding your trig, but also as a quest to determine what the identities are really trying to tell you. (Search the web for \"trig proof without words\", and you'll see how helpful these things can be.)\nFor instance, here's a simple picture-proof of the Law of Cosines, created using the diagramming strategies described here:\n\nOnce you start thinking about Calculus, you might ponder diagrams for the \"power series\" of sine and cosine and secant and tangent. (If you come up with the diagrams for cosecant and cotangent, let me know!)\n",
    "tags": [
      "trigonometry",
      "proof-writing",
      "intuition"
    ],
    "score": 31,
    "answer_score": 38,
    "is_accepted": true,
    "question_id": 402487,
    "answer_id": 402561
  },
  {
    "theorem": "Starting sentences with mathematical symbols.",
    "context": "I apologise if this is a duplicate in any way or is too opinion-based.\n\nTo what extent is it best not to start a sentence with a mathematical symbol?\n\nI find that when trying to solve a problem or prove something it's an unnecessary distraction to care too much about forming proper sentences and so forth, but when writing things up, I just can't bring myself to start a sentence with a symbol. It doesn't look right.\nI remember in my first year as an undergraduate I was told by a PhD student that it's \"bad form\" to do so and I somewhat agree, but I've seen it frequently in lectures, seminars, and even in papers.\nConsider the following example I cooked up:\n\nDefinition: Let $L$ be a Lie algebra. $\\color{red}L$ is solvable if there exists an $n\\in\\mathbb{N}$ such that $L^{(n)}=\\{0\\}$ in the derived series of $L$.\n\nThis, to me, is a word away from how it \"should\" be written; just stick \"Then\" at the start of the second sentence.\nNow, I am aware that mathematical concepts are difficult enough to write about without worrying over such things. (I agree with Stephen Fry when it comes to language.) But what's the convention? Does it matter?\n",
    "proof": "You might be interested in the essay Halmos, How to write mathematics and the lecture Serre, How to write mathematics badly. Both suggest to avoid starting a sentence with a symbol. In my personal experience from reading mathematical texts and having my own texts corrected by professors, they all agree with this convention. It's just not pleasing to read a sentence that starts off with a symbol.\nUpdate: I removed the noise from the recording of Serre's lecture and changed the link to the improved version.\n",
    "tags": [
      "soft-question",
      "proof-writing",
      "convention",
      "article-writing",
      "publishing"
    ],
    "score": 31,
    "answer_score": 20,
    "is_accepted": true,
    "question_id": 694968,
    "answer_id": 695042
  },
  {
    "theorem": "Definition: Theorem, Lemma, Proposition, Conjecture and Principle etc.",
    "context": "\nDefinition: Theorem, Lemma, Proposition, Corollary, Postulate, Statement, Fact, Observation, Expression, Fact, Property, Conjecture and Principle\n\nMost of the time a mathematical statement is classified with one the words listed above.\nHowever, I can't seem to find definitions of them all online, so I will request your aid in describe/define them.\nAlso, when is a mathematical statement a theorem versus a lemma ? I've read that a theorem is important while a lemma is not so important and used to prove a theorem. However a theorem is sometimes used to prove some other theorem. This implies that some theorems are also lemmas ?\nIs it subjective with respect to the author, which statements become a theorem, lemma, etc. ?\n",
    "proof": "I have taken this excerpt out from How to think like a Mathematician\n\nDefinition: an explanation of the mathematical meaning of a word.\nTheorem: a very important true statement that is provable in terms of definitions and axioms.\nProposition: a statement of fact that is true and interesting in a given context.\nLemma: a true statement used in proving other true statements.\nCorollary: a true statement that is a simple deduction from a theorem or proposition.\nProof: the explanation of why a statement is true.\nConjecture: a statement believed to be true, but for which we have no proof.\nAxiom: a basic assumption about a mathematical situation (model) which requires no proof.\n\nI think it does a great job of describing what those words mean in a sentence. Later in the chapter, the author goes on to describe how we have some conjectures which have been called \"Theorems\" even though they weren't proven. For example, Fermat's Last Theorem was referred to as a Theorem even though it hadn't been proven. If you haven't read the book then I highly recommend it if you are a undergraduate in your first two years of math.\n",
    "tags": [
      "soft-question",
      "terminology",
      "proof-writing"
    ],
    "score": 30,
    "answer_score": 31,
    "is_accepted": true,
    "question_id": 644996,
    "answer_id": 645062
  },
  {
    "theorem": "Find a simple proof that π is irrational",
    "context": "I know there are many questions on the site about finding a proof that π is irrational, but I'm posting the question separately to discuss a particular proof further\nWe know that the Wallis Product is :\n$$\\frac{π}{2}=(\\frac{2}{1}\\cdot\\frac{2}{3})(\\frac{4}{3}\\cdot\\frac{4}{5})(\\frac{6}{5}\\cdot\\frac{6}{7})(\\frac{8}{7}\\cdot\\frac{8}{9})\\cdots$$\nThis means that if $\\pi$ is a rational number, its numerator will be an even number and its denominator will be an odd number\nAfter that, all we have to do is find a formula for the number $\\pi$ that gives a \"reversed\" fraction whose numerator is an odd number and whose denominator is an even number. Thus, we obtain a proof similar to the classical proof that $\\sqrt{2}$ is irrational. Indeed, after some research, I found formula of this model that are attributed to Leonard  Euler:\nAssuming that $p_n$ is a notation that refers to the prime number $n$, the formula we want is :\n$$\\frac{π}{4}=\\prod_{n=1}^∞ (\\frac{p_n}{p_n+(-1)^{\\frac{p_n+1}{2}}})=\\frac{3}{3+1}\\cdot\\frac{5}{5-1}\\cdot\\frac{7}{7+1}\\cdot\\frac{11}{11+1}\\cdot\\frac{13}{13-1}\\cdots$$\nIt represents an odd number divided by an even number as required. Thus, we obtain a contradiction showing that $\\pi$ is irrational.\nMy question is : Is my proof valid or is there an error in this proof?\nI don't know if I can deduce this from an infinite ratio or if it is invalid. If this is not true please give an example of a rational number that has two representations as an infinite product of the two opposite forms.\n",
    "proof": "Here is an explicit example to show the proof doesn't work. Consider the infinite product\n$$\\frac12\\prod_{n=0}^\\infty\\frac{2^{2^n}+1}{2^{2^n}}=\\frac12\\cdot\\frac32\\cdot\\frac54\\cdot\\frac{17}{16}\\cdots$$\nwhere all numerators are odd and all denominators even. We can check by induction that the partial products are\n$$\\frac12\\prod_{n=0}^N\\frac{2^{2^n}+1}{2^{2^n}}=\\frac{2^{2^{N+1}}-1}{2^{2^{N+1}}},$$\nso the infinite product evaluates to $1$.\nThen taking the reciprocals of all terms gives another representation of $1$ with opposite parities.\n",
    "tags": [
      "solution-verification",
      "proof-writing",
      "irrational-numbers",
      "pi",
      "infinite-product"
    ],
    "score": 29,
    "answer_score": 36,
    "is_accepted": true,
    "question_id": 4819924,
    "answer_id": 4820271
  },
  {
    "theorem": "Are there statements so self-evident that writing a proof for them is meaningless? Is this an example of one?",
    "context": "Context: I know nothing about proofs and only a small amount about formal logic used in proofs. I'm trying to learn the basics of how to write a proof.\nFor example, suppose I wanted to prove that \"all integers divisible by 4 are also divisible by 2\".\nMy first reaction to this would be: \"It's self-evident, because 4 is just 2*2\". But this is obviously not a formal proof.\nAnother attempt: \"Any integer n that is divisible by 4 must have 2*2 as part of its prime factorisation, therefore it is divisible by 2.\" But this seems to be explaining something simple by means of something more complex.\nIt seems that you are just supposed to know that \"all integers divisible by 4 are also divisible by 2\" and take it for granted. Or you are supposed to know, more generally, that all integers divisible by N are also divisible by P if N is divisible by P. Do you have to know this beforehand? Do mathematicians sometimes just say \"it is true because it's obvious\"?.\n",
    "proof": "Great question, and well done for thinking carefully about the basics of proof and mathematics. Let's get into it!\n\n\nIt seems that you are just supposed to know that \"all integers divisible by 4 are also divisible by 2\" and take it for granted.\n\nThis is not true! We can and must prove it.\nIn order to prove a statement, we must know what it means. In this case, we must define \"divisible\".\nDefinition. Let $a$ and $b$ be integers. We say that \"$a$ is divisible by $b$\" if there exists an integer $c$ such that $a = bc$.\nNow that we know what the statement means, we can prove it!\nProposition. Let $a$ be an integer. If $a$ is divisible by $4$, then $a$ is divisible by $2$.\nProof. Suppose $a$ is divisible by $4$. This means that there is an integer $c$ such that $a = 4c$. Equivalently, $a = 2(2c)$. Since $c$ is an integer, $2c$ is an integer. Thus, $a$ is divisible by $2$. $\\square$\nOf course it would be annoying to reprove thousands of different statements like this for every instance of a basic divisibility fact we need to use. So, it's much better to prove general statements like the one you suggested. Can you try to prove the following more general statement?\nProposition. Let $a$, $b$, and $c$ be integers. If $a$ is divisible by $b$ and $b$ is divisible by $c$, then $a$ is divisible by $c$.\nProof. Fill me in!\n\nEverything in mathematics is this way. When we want to know that something is true, we must first make careful definitions to specify exactly what it is that we want to prove. Once we have those definitions in place, we can try to prove whatever we want!\n",
    "tags": [
      "proof-writing",
      "formal-proofs"
    ],
    "score": 29,
    "answer_score": 50,
    "is_accepted": true,
    "question_id": 4930916,
    "answer_id": 4930921
  },
  {
    "theorem": "False proof that $ρe^{iθ} = ρ$ and so complex numbers do not exist?",
    "context": "My professor showed the following false proof, which showed that complex numbers do not exist. We were told to find the point where an incorrect step was taken, but I could not find it. Here is the proof: (Complex numbers are of the form $\\rho e^{i\\theta}$, so the proof begins there) $$\\large\\rho e^{i\\theta} = \\rho e^{\\frac{2 \\pi i \\theta}{2\\pi}} = \\rho (e^{2\\pi i})^{\\frac{\\theta}{2\\pi}} = \\rho (1)^{\\frac{\\theta}{2\\pi}} = \\rho$$\n$$Note: e^{i\\pi} = -1, e^{2\\pi i} = (-1)^2 = 1$$\nSince we started with the general form of a complex number and simplified it to a real number (namely, $\\rho$), the proof can claim that only real numbers exist and complex numbers do not. My suspicion is that the error occurs in step $4$ to $5$ , but I am not sure if that really is the case.\n",
    "proof": "The error lies in assuming that $(\\forall a,b\\in\\mathbb{C}):e^{ab}=(e^a)^b$. \nIt's worse than wrong; it doesn't make sense. The reason why it doesn't make sense is because $e^a$ can be an arbitrary complex number (except that it can't be $0$). And what is $z^w$, where $z,w\\in\\mathbb C$? A reasonable definition is that it means $e^{w\\log z}$, where $\\log z$ is a logarithm of $z$. Problem: every non-zero complex number has infinitely many logarithms: if a number $\\omega$ is a logarithm, then every number of the form $\\omega+2k\\pi i$ ($k\\in\\mathbb Z$) is also a logarithm.\n",
    "tags": [
      "complex-numbers",
      "proof-writing",
      "solution-verification",
      "exponentiation",
      "fake-proofs"
    ],
    "score": 29,
    "answer_score": 65,
    "is_accepted": true,
    "question_id": 2582046,
    "answer_id": 2582053
  },
  {
    "theorem": "When do I use &quot;arbitrary&quot; and/or &quot;fixed&quot; in a proof?",
    "context": "In many proofs I see that some variable is \"fixed\" and/or \"arbitrary\". Sometimes I see only one of them and I miss a clear guideline for it. Could somebody point me to a reliable source (best a well-known standard book) which explains, when and how to use both in proofs?\nEDIT: A little add-on to the question: Take a usual induction on natural numbers and assume that you are teaching it to students in their first semester. How do you explain \"fixed\" and \"arbitrary\" in this scenario?\n",
    "proof": "Both \"arbitrary\" and \"fixed\" are just shorthand for a universal quantifier. When I say something like \"fix $\\epsilon > 0$\" it means I am about to prove a statement that is true for all $\\epsilon > 0$ (and thereby prove that some function is continuous, for example) but I don't want to actually write out \"for all $\\epsilon > 0$\" in front of every sentence I'm about to write. That's really all there is to it. \n",
    "tags": [
      "proof-writing",
      "terminology"
    ],
    "score": 29,
    "answer_score": 38,
    "is_accepted": true,
    "question_id": 46726,
    "answer_id": 46728
  },
  {
    "theorem": "Differentiability implies continuity - A question about the proof",
    "context": "I have a question, to aid my understanding, about the proof that differentiability implies continutity.$\\mathstrut$ \nDifferentiability Definition\nWhen we say a function is differentiable at $x_0$, we mean that the limit:\n$$‎f^{\\prime} ‎(x) = \\lim_{x\\to x_0} \\frac{f(x) - f(x_0)}{x-x_0}$$ exists.\nContinuity Definition\nWhen we say a function is continuous at $x_0$, we mean that:\n$$\\lim_{x\\to x_0} f(x) - f(x_0) = 0$$\nTheorem: Differentiability implies Continuity: If $f$ is a differentiable function at $x_0$, then it is continuous at $x_0$.\nProof:\nLet us suppose that $f$ is differentiable at $x_0$. Then\n$$ \\lim_{x\\to x_0} \\frac{f(x) - f(x_0)}{x-x_0} =  ‎f^{\\prime} ‎(x) $$\nand hence\n$$ \\lim_{x\\to x_0} f(x) - f(x_0) = \\lim_{x\\to x_0} \\left[ \\frac{f(x) - f(x_0)}{x-x_0} \\right] \\cdot \\lim_{x\\to x_0} (x-x_0) = 0$$\nWe have therefore shown that, using the definition of continuous, if the function is differentiable at $x_0$, it must also be continuous.\nMy Question\nThe proof seems to execute the following steps:\n\nAssume the function is continuous at $x_0$\nShow that, with little algebra, we can change this into an equivalent question about differentiability at $x_0$. With this little bit of algebra, we can show that if a function is differentiable at $x_0$ it is also continuous.\n\nWhat I am slightly unsure about is the apparent circularity. In my mind it seems to say, if a function is continuous, we can show that if it is also differentiable, then it is continuous. Rather than what I was expecting, namely, if a function is differentiable, we can show it must be continuous.\nHopefully my confusion is clear. Any help will be greatly appreciated.\n",
    "proof": "Technically, there is an implicit issue of existence of limits which is being swept under the rug in the presentation you have given. The assumption of differentiability at $x_0$ says that the limit\n$$\\lim_{x \\to x_0} \\frac{f(x) - f(x_0)}{x-x_0}$$\nexists as a finite number. The limit $\\lim_{x \\to x_0} x-x_0$ exists and is zero regardless of our assumptions. Then the product rule for limits tells us both that $\\lim_{x \\to x_0} f(x)-f(x_0)$ exists, and that it is the product of the two limits above, which means it must be zero. Because the product rule also tells us that the limit exists, we do not have to assume continuity first.\n",
    "tags": [
      "real-analysis",
      "derivatives",
      "proof-writing",
      "continuity"
    ],
    "score": 29,
    "answer_score": 36,
    "is_accepted": true,
    "question_id": 1314630,
    "answer_id": 1314637
  },
  {
    "theorem": "Prove that the multiplicative groups $\\mathbb{R} - \\{0\\}$ and $\\mathbb{C} - \\{0\\}$ are not isomorphic.",
    "context": "Is my proof correct? I have made use of the fact isomorphism preserves order of elements, which I proved couple of exercises back. I am also interested in other ways of proving it. Is there a more explicit way or is this explicit enough?\n\nProblem Prove that the multiplicative groups $\\mathbb{R} - \\{0\\}$ and $\\mathbb{C} - \\{0\\}$ are not isomorphic.\nSolution Recall that isomorphism preserves order of elements and hence if there exists an isomorphism from $\\phi: \\mathbb{C}-\\{0\\} \\mapsto \\mathbb{R}-\\{0\\}$, then $x \\in \\mathbb{C} - \\{0\\}$, and $\\phi(x) \\in \\mathbb{R} - \\{0\\}$, then $\\vert x \\vert = \\vert \\phi(x) \\vert$. Now note that the element $i \\in \\mathbb{C} - \\{0\\}$ has order $4$. However, no element in $\\mathbb{R}-\\{0\\}$ has order $4$. Hence, no isomorphism can exist. Hence, the multiplicative groups $\\mathbb{R} - \\{0\\}$ and $\\mathbb{C} - \\{0\\}$ are not isomorphic.\n\nThanks\n",
    "proof": "Another way would be. \nLet's take $i$ and see what happen:\na) $f(i^2)=f(-1)=-1$, using basic properties of a morphism.\nb) on the other side, we have $f(i)^2=x^2$ that can't produce -1.\n",
    "tags": [
      "abstract-algebra",
      "group-theory",
      "proof-writing",
      "proof-verification"
    ],
    "score": 29,
    "answer_score": 17,
    "is_accepted": false,
    "question_id": 620495,
    "answer_id": 765105
  },
  {
    "theorem": "How to prove that $\\sqrt{2+\\sqrt3}-\\sqrt{2-\\sqrt3}=\\sqrt2$ without squaring both sides",
    "context": "I have been asked to prove:\n\n$$\\sqrt{2+\\sqrt3}-\\sqrt{2-\\sqrt3}=\\sqrt2$$\n\nWhich I can easily do by converting the LHS to index form, then squaring it and simplifying it down to get 2, which is equal to the RHS squared, hence proved.\nHowever I know you can't square a side during proof because it generates an extraneous solution. So: how do you go about this proof without squaring both sides? Or can my method be made valid if I do this:\n$$\\sqrt{2+\\sqrt3}-\\sqrt{2-\\sqrt3}=\\sqrt2$$\n$$...=...$$\n$$2=2$$\n$$\\lvert\\sqrt2\\rvert=\\lvert\\sqrt2\\rvert$$\n$$\\sqrt2=\\sqrt2\\text{ hence proved.}$$\nCheers in advance :)\n",
    "proof": "\\begin{eqnarray}\\sqrt{2+\\sqrt3}-\\sqrt{2-\\sqrt3} &=& \\sqrt{4+2\\sqrt3 \\over 2}-\\sqrt{4-2\\sqrt3 \\over 2}\\\\ &=&\\sqrt{(\\sqrt{3} +1)^2 \\over 2}-\\sqrt{(\\sqrt{3} -1)^2\\over 2}\\\\ \n &=&{\\sqrt{3} +1 \\over \\sqrt{2}}-{\\sqrt{3} -1\\over \\sqrt{2}}\\\\ \n&=&\\sqrt2\n\\end{eqnarray}\n",
    "tags": [
      "algebra-precalculus",
      "proof-verification",
      "proof-writing",
      "proof-explanation",
      "radicals"
    ],
    "score": 28,
    "answer_score": 51,
    "is_accepted": true,
    "question_id": 2570656,
    "answer_id": 2570660
  },
  {
    "theorem": "How do I stop overcomplicating proofs?",
    "context": "I'm a third year student majoring in Math.\nWhenever I sit down and try to prove something, I just don't know what and where to start with. The first proofs course I took was graded very strictly so missing a very tiny detail made me lose a lot of marks (which does make sense since it is an introductory class to proofs and the \"little details\" could have been not \"little\").\nBut after that, I just get way too anxious when I do proofs because I don't know what kind of detail I would be missing. I end up completing the proofs by getting a lot of hints on where to start, and it takes way too much time for me to do a single proof (almost 2-3 days per one theorem). And because I don't want to get the proofs wrong, I keep searching up resources to do the proofs; so I kind of end up not doing the proofs myself. But when I see the \"solutions\" to the proofs, I realize they were very simple and I have been over-complicating it a lot.\nI really love math and I want to be able to really understand courses like Real Analysis, and how scared I am with proofs definitely is an issue that I want to overcome. So my question is\n(i) If you have gone through this stage, how did you overcome?\n(ii) Are there any general tips on starting proofs?\nThanks.\n",
    "proof": "Are you sure that you are understanding the proofs you are working with? In my experience, there isn't that much scope for genuine overcomplication in undergraduate mathematics. Instead, some students employ a scattergun approach where they mention a lot of things they consider true and potentially relevant, and hope that all steps of an actual proof are somewhere in there.\nFor the proofs you are writing yourself, make sure that you have clear picture of the purpose of each part. You may even consider drawing diagrams here: For each point that you make, mark both its prequisites and where you use it in the end. Do the same for the sample solutions that you find. Any part that doesn't actually contribute to the desired conclusion shouldn't really be there.\nA more advanced exercise with a similar goal is to try and attack your proofs. If you can make yourself doubt certain parts, those are the parts that may need more detail to clarify why those doubts are not actually merited. A structured way to get there is to make small modifications to the statement you are trying to prove that yield something false. Which part of your proof breaks down?\n",
    "tags": [
      "real-analysis",
      "proof-writing",
      "soft-question"
    ],
    "score": 28,
    "answer_score": 8,
    "is_accepted": true,
    "question_id": 4546324,
    "answer_id": 4546711
  },
  {
    "theorem": "In proofs, are &quot;for each&quot; and &quot;for any&quot; synonyms?",
    "context": "In proofs, are \"for each\" and \"for any\" synonyms?  Or some context is usually required to determine this?  \n",
    "proof": "Compare\nA1. If there’s a simple solution for each of the problems, the test is too easy.\nA2. If there’s a simple solution for any of the problems, the test is too easy.\nThese are not equivalent. The first might be true without the second being true.\nCompare also the plainly different\nB1. There isn’t a simple solution for each of the problems.\nB2. There isn’t a simple solution for any of the problems.\nSo it is important when formally regimenting English into the language of logic to note that, while many standalone or wide scope uses of ‘for any’ and ‘for each’ are equivalent, they do embed differently inside other logical operators.\n[And before you rush to making synonymy claims at least for unembedded uses, it is worth remarking that there remain complicated differences between 'any' and 'each' even here. For example, 'any' can take plurals as well as singulars, so we can have both 'for any man' and 'for any men' (and these are different -- a table may be too heavy for any man to lift, but not too heavy for any men to lift); but we can't have 'for each men'. And 'any' can take mass nouns, while 'each' can't  (so compare, ‘for any ice that doesn’t shift, try salt’ vs the ungrammatical ‘for each ice that doesn’t shift, try salt’). And so it goes. There's a good reason why we introduce formal quantifiers to avoid the vagaries of English usage.]\n",
    "tags": [
      "proof-writing",
      "terminology",
      "quantifiers"
    ],
    "score": 27,
    "answer_score": 45,
    "is_accepted": true,
    "question_id": 2696959,
    "answer_id": 2697653
  },
  {
    "theorem": "How do I make a student understand contradiction?",
    "context": "We were trying to prove that if $3p^2=q^2$ for nonnegative integers $p$ and $q$, then $3$ divides both $p$ and $q$. I finished writing the solution (using Euclid's lemma) when a student asked me \n\n\"How can you assume $3p^2=q^2$ when that implies $\\sqrt 3$ is rational which we know is false?\"\n\nI told him that question at hand is used as a lemma in proving that $\\sqrt 3$ is irrational. But he gave another argument using fundamental theorem of arithmetic to independently prove that $\\sqrt 3$ is irrational. So I could not convince him that one can give a chain of arguments starting from a hypothesis without actually knowing the truth value of the hypothesis itself. Later I came back and tried to think more about this a bit fundamentally to understand what it means to prove the implication $\"A \\to B\"$ without worrying about truth of $A$ and how different is it from proving $B$ when we know $A$ to be true (or false, as in this case, using some other method). But I am also confused. Another student made this remark, \"you are giving me one false statement, and asking me to prove another false statement, I don't understand\". Can someone please resolve this?\n",
    "proof": "Step 0. Use truth tables to convince him that $A \\rightarrow \\bot$ is equivalent to $\\neg A$. Deduce that to prove $\\neg P$, we can assume $P$ and attempt to derive $\\bot$, since this allows us to infer $P \\rightarrow \\bot$, which is the same as $\\neg P$.\nStep 1. Use truth tables to convince him that $A \\rightarrow B$ is equivalent to $\\neg(A \\wedge \\neg B).$ Conclude that to prove $A \\rightarrow B$, we may assume $A \\wedge \\neg B$ and attempt to deduce $\\bot$.\n\nEdit. My original answer (above) focuses on the logic of proof by contradiction, but there is also a pedagogical issue here that deserves to be addressed. The following material is taken from the user Keen and from Steve Jessop's excellent answer.\n\nUnfortunately, assume is one of those words where the mathematical\n  definition differs from its popular English definition. In English,\n  assume S means believe S. In mathematics, it means only imagine if S were true. So when we say, \"assume $3p^2=q^2$\", we are not asserting that there exists an ordered pair $(p,q)$ with this property. Rather, we are considering the hypothetical properties that such a pair would have, if it did exist.\n\n",
    "tags": [
      "elementary-number-theory",
      "logic",
      "proof-writing",
      "education"
    ],
    "score": 27,
    "answer_score": 9,
    "is_accepted": true,
    "question_id": 1435950,
    "answer_id": 1436013
  },
  {
    "theorem": "The inverse of a lower triangular matrix is lower triangular",
    "context": "\nThe inverse of a non-singular lower triangular matrix is lower triangular.\nConstruct a proof of this fact as follows. Suppose that $L$ is a non-singular lower triangular matrix. If $b \\in \\mathbb{R^n}$ is such that $b_i = 0$ for $i = 1, . . . , k \\leq n$, and $y$ solves $Ly = b$, then $y_i = 0$ for\n$i = 1, . . . , k \\leq n$.\nHint: partition $L$ by the first $k$ rows and columns.\n\nCan someone tell me what exactly we are showing here and why it will prove that the inverse of any non-singular lower triangular matrix is lower triangular?\n",
    "proof": "Let's write $$L^{-1}=[y_1\\:\\cdots\\:y_n],$$ where each $y_k$ is an $n\\times 1$ matrix.\nNow, by definition, $$LL^{-1}=I=[e_1\\:\\cdots\\:e_n],$$ where $e_k$ is the $n\\times 1$ matrix with a $1$ in the $k$th row and $0$s everywhere else. Observe, though, that $$LL^{-1}=L[y_1\\:\\cdots\\:y_n]=[Ly_1\\:\\cdots\\: Ly_n],$$ so $$Ly_k=e_k\\qquad(1\\leq k\\leq n)$$\nBy the proposition, since $e_k$ has only $0$s above the $k$th row and $L$ is lower triangular and $Ly_k=e_k$, then $y_k$ has only $0$s above the $k$th row. This is true for all $1\\leq k\\leq n$, so since $$L^{-1}=[y_1\\:\\cdots\\:y_n],$$ then $L^{-1}$ is lower triangular, too.\n$$********$$\nHere's an alternative (but related) approach.\nObserve that a lower triangular matrix is nonsingular if and only if it has all nonzero entries on the diagonal. Let's proceed by induction on $n$. The base case ($n=1$) is simple, as all scalars are trivially \"lower triangular\". Now, let's suppose that all nonsingular $n\\times n$ lower triangular matrices have lower triangular inverses, and let $A$ be any nonsingular $(n+1)\\times(n+1)$ lower triangular matrix. In block form, then, we have $$A=\\left[\\begin{array}{c|c}L & 0_n\\\\\\hline x^T & \\alpha\\end{array}\\right],$$ where $L$ is a nonsingular $n\\times n$ lower triangular matrix, $0_n$ is the $n\\times 1$ matrix of $0$s, $x$ is some $n\\times 1$ matrix, and $\\alpha$ is some nonzero scalar. (Can you see why this is true?) Now, in compatible block form, we have $$A^{-1}=\\left[\\begin{array}{c|c}M & b\\\\\\hline y^T & \\beta\\end{array}\\right],$$ where $M$ is an $n\\times n$ matrix, $b,y$ are $n\\times 1$ matrices, and $\\beta$ some scalar. Letting $I_n$ and $I_{n+1}$ denote the $n\\times n$ and $(n+1)\\times(n+1)$ identity matrices, respectively, we have $$I_{n+1}=\\left[\\begin{array}{c|c}I_n & 0_n\\\\\\hline 0_n^T & 1\\end{array}\\right].$$ Hence, $$\\left[\\begin{array}{c|c}I_n & 0_n\\\\\\hline 0_n^T & 1\\end{array}\\right]=I_{n+1}=A^{-1}A=\\left[\\begin{array}{c|c}ML+bx^T & M0_n+b\\alpha\\\\\\hline y^TL+\\alpha x^T & y^T0_n+\\beta\\alpha\\end{array}\\right]=\\left[\\begin{array}{c|c}ML+bx^T & \\alpha b\\\\\\hline y^TL+\\alpha x^T & \\beta\\alpha\\end{array}\\right].$$ Since $\\alpha$ is a nonzero scalar and $\\alpha b=0_n$, then we must have $b=0_n$. Thus, $$A^{-1}=\\left[\\begin{array}{c|c}M & 0_n\\\\\\hline y^T & \\beta\\end{array}\\right],$$ and $$\\left[\\begin{array}{c|c}I_n & 0_n\\\\\\hline 0_n^T & 1\\end{array}\\right]=\\left[\\begin{array}{c|c}ML & 0_n\\\\\\hline y^TL+\\alpha x^T & \\beta\\alpha\\end{array}\\right].$$ Since $ML=I_n$, then $M=L^{-1}$, and by inductive hypothesis, we have that $M$ is then lower triangular. Therefore, $$A^{-1}=\\left[\\begin{array}{c|c}M & 0_n\\\\\\hline y^T & \\beta\\end{array}\\right]$$ is lower triangular, too, as desired.\n",
    "tags": [
      "linear-algebra",
      "matrices",
      "proof-writing",
      "inverse"
    ],
    "score": 27,
    "answer_score": 32,
    "is_accepted": true,
    "question_id": 245871,
    "answer_id": 245895
  },
  {
    "theorem": "Does this prove that no sequential squares have a ratio of 2?",
    "context": "The goal: \nProve that there is no integer $k$ such that ${(k+1)^2\\over{k^2}}=2$.\nMy proof:\nIf ${(k+1)^2\\over{k^2}}=2$, then ${{k+1}\\over{k}}=\\sqrt2$, and if $k$ is an integer, $k+1$ is also an integer. This implies that $\\sqrt2$ is a rational number, which is also provably false.\n",
    "proof": "How about solving the equation: $(k+1)^2=2k^2$ for $k\\ne 0$?\n$$(k+1)^2=2k^2$$\n$$k^2+2k+1=2k^2$$\n$$k^2-2k-1=0$$\n$$k^2-2k+1=2$$\n$$(k-1)^2-(\\sqrt{2})^2=0$$\n$$(k-1-\\sqrt{2})(k-1+\\sqrt{2})=0$$\nThen $k=1+\\sqrt{2}$ or $k=1-\\sqrt{2}$, neither of them are integers, because you can prove that $\\sqrt{2}$ is an irrational number (but knowing that it is not an integer is enough here).\nFor your method, the only minor mistake you made is:\n$${(k+1)^2\\over{k^2}}=2\\Leftrightarrow {{k+1}\\over{k}}=\\pm \\sqrt2.$$\n",
    "tags": [
      "proof-verification",
      "proof-writing",
      "irrational-numbers"
    ],
    "score": 26,
    "answer_score": 27,
    "is_accepted": true,
    "question_id": 2744850,
    "answer_id": 2744859
  },
  {
    "theorem": "How to prove that a very large number is not prime",
    "context": "I'm solving few math problems for an upcoming math contest .\nI am stuck with a short problem, where I have to prove that $A$ is not prime .\n$$A = 100\\ 000\\ 000\\ 000\\ 000\\ 000\\ 001$$\n$A$ is not a binary number. It's a decimal one.\nI've tried to rewrite like this:\n$$ A = 1 \\times 10^{20} + 1 $$\nBut what can I do with that . I can't use GCD since it would a very long time to finish it and Obviously this isn't the point from this problem .\n",
    "proof": "We have the number $10^{20}+1$. Whenever we have something in this kind of form, we need to find an odd factor of the exponent. In this case $5 \\mid 20$, so we can use $5$ as the factor.\nNow, we can say $10^{20}+1=(10^4)^5+1$. How does this help us? Well, if we say that $x=10^4$, we have the polynomial $x^5+1$. This polynomial has $-1$ as a zero, meaning $(x+1) \\mid (x^5+1)$. Substituting $x=10^4$ back into this statement, we get $(10^4+1) \\mid ((10^4)^5+1)=(10^{20}+1)$. Thus, $10^4+1$ is a factor of $10^{20}+1$, so the number is composite.\nNotice how the factor had to be odd. Otherwise, if we have an even factor $n$, then $x^n+1$ would not have had $-1$ as a zero. This is a very common technique in math competitions that I have used several times before, so it will come in handy.\n",
    "tags": [
      "prime-numbers",
      "proof-writing",
      "contest-math"
    ],
    "score": 26,
    "answer_score": 57,
    "is_accepted": true,
    "question_id": 1773192,
    "answer_id": 1773219
  },
  {
    "theorem": "A continuous function $f$ from a closed bounded interval $[a, b]$ into $\\mathbb{R}$ is uniformly continuous",
    "context": "I am reading (as a supplement) the book Basic Real Analysis, by Anthony Knapp. Before I proceed into reading a proof, I want to be sure that the result seems obvious. Yet, I am having trouble seeing through this one. It annoys me too much in order to disregard it:\nTheorem.\nA continuous function $f$ from a closed bounded interval $[a, b]$ into $\\mathbb{R}$ is uniformly continuous.\nWhat gives? Why can't we provide the counterexample $f(x)=x^2$ and $[a,b] \\subset [1,+\\infty)$, for $b < +\\infty$ sufficiently large and show that the theorem is incorrect? \nDoesn't it seem to be an insufficient statement? \nI'm having trouble picturing it, is all.\nHints are fine.\n",
    "proof": "I can provide you with a proof. We use the lemma that $[a,b]$ is compact. The general statement is that if $f:X\\to Y$ is continuous and $X$ is a compact metric space, then i$f$ is uniformly continuous. This is usually known as the Heine Cantor theorem, while the fact that $[a,b]$is compact might be found as Borel's Lemma, if memory serves. So\nTHEOREM (Spivak) Let $f:[a,b]\\to \\Bbb R$ be continuous. Then it is uniformly continuous.\nWe first prove the \nLEMMA Let $f$ be a continuous function defined on $[a,c]$. If, given $\\epsilon >0$, there exists $\\delta_1>0$ such that, for each pair $$x,y\\in[a,b]\\text{ ; } |x-y|<\\delta_1 \\implies |f(x)-f(y)|<\\epsilon$$  and $\\delta_2>0$ such that for each\n$$x,y\\in[b,c]\\text{ ; } |x-y|<\\delta_2 \\implies |f(x)-f(y)|<\\epsilon$$\nThen there exists $\\delta $ such that for each \n$$x,y\\in[a,c]\\text{ ; } |x-y|<\\delta  \\implies |f(x)-f(y)|<\\epsilon$$\nP\nSince $f$ is continuous at $x=b$, there exists a $\\delta_3$ such that for every $x$ with $|b-x|<\\delta_3$, we have $|f(b)-f(x)|<\\frac{\\epsilon}2$.\nThus, whenever $|x-b|<\\delta_3$ and $|y-b|<\\delta_3$ we will certainly have $$|f(x)-f(y)|<\\epsilon$$ \nWe take $\\delta=\\min\\{\\delta_1,\\delta_2,\\delta_3\\}$. Then $\\delta$ works: indeed, consider any pair $x,y\\in[a,c]$. If $x,y\\in[a,b]$ or $x,y\\in[b,c]$, we're done. If $x<b<y$ or $y<b<x$. In any case, since $|x-y|<\\delta$, we must have $|x-b|,|y-b|<\\delta$, so that $|f(x)-f(y)|<\\epsilon$, as claimed.\nPROOF1 Fix $\\epsilon >0$. Let's agree to call $f$ $\\epsilon$-good on an interval $[a,b]$ if for this $\\epsilon$ there exists a $\\delta$ such that for any $x,y\\in[a,b]$, $|x-y|<\\delta\\implies |f(x)-f(y)|<\\epsilon$. We thus want to prove that $f$ is $\\epsilon$-good on $[a,b]$ for any $\\epsilon >0$. Let $\\epsilon >0$ be given, and consider the set $$A(\\epsilon)=\\{x\\in[a,b]:f \\text{ is } \\epsilon \\text{-good on}: [a,x]\\}$$ Then $A\\neq \\varnothing$ for $a\\in A(\\epsilon)$, and $A(\\epsilon)$ is bounded above by $b$. Thus $\\sup A=\\alpha $ exists. Suppose that $\\alpha <b$. Since $f$ is continuous at $\\alpha$ there exists a $\\delta'$ such that $|y-\\alpha|<\\delta'$ implies $|f(y)-f(\\alpha)|<\\epsilon/2$. Thus, if $|y-\\alpha|,|x-\\alpha|<\\delta'$, we'll have  $|f(y)-f(x)|<\\epsilon$. Thus $f$ is $\\epsilon$-good on $[\\alpha-\\delta,\\alpha+\\delta]$. Since $\\alpha=\\sup A(\\epsilon)$, it is clear $f$ is $\\epsilon$-good on $[a,\\alpha+\\delta]$, which is absurd. Thus $\\alpha\\geq b$, which means $\\alpha =b$. It suffices to show that $b$ is also an element of $A(\\epsilon)$. But since $f$ is continuous on $b$, there exists a $\\delta_0$ such that $|b-y|<\\delta_0$ implies $|f(b)-f(y)|<\\epsilon/2$. Thus, $f$ is $\\epsilon$-good on $[b-\\delta_0,b]$. The lemma implies $f$ is $\\epsilon$-good on $[a,b]$. Since $\\epsilon$ was arbitrary, the result follows. $\\blacktriangle$ \n\nPROOF2 Let $\\epsilon >0$ be given. Assign, to each $x\\in [a,b]$ a $\\delta_x>0$ such that for each $y\\in(x-2\\delta_x,x+2\\delta_x)$, we have $|f(x)-f(y)|< \\epsilon/2$, to obtain a open cover of $[a,b]$, namely the set $\\mathcal O$ of intervals $(x-\\delta_x,x+\\delta_x)$. This is possible since $f$ is continuous at each $x$. Since $[a,b]$ is compact, there is a finite number of $x_i\\in [a,b]$ such that $$\\bigcup_{i=1}^n (x_i-\\delta_{x_i},x_i+\\delta_{x_i})\\supset [a,b]$$\nChoose now $\\delta =\\min{\\delta_{x_i}}$, and let $x,y\\in [a,b]$ with $ |y-x|<\\delta$. Since $\\mathcal O$ is a cover, for some $x_i$ we have that $|x-x_i|<\\delta_{x_i}$. Then, we'll have $$|y-x_i|\\leq |y-x|+|x-x_i|<\\delta+\\delta_i\\leq 2\\delta_i$$ It follows that $$|f(x_i)-f(x)|<\\epsilon/2$$\n$$|f(y)-f(x)|<\\epsilon/2$$\nwhich means by the triangle inequality that $$|f(x)-f(y)|<\\epsilon$$\nThen for any $x,y\\in[a,b]$,  $|x-y|<\\delta$ will imply $|f(x)-f(y)|<\\epsilon$; and $f$ is uniformly continuous. $\\blacktriangle$ \n\nThere is yet another way of proving this. \nLEMMA (Mendelson) Let $X$ be a metric space such that every infinite subset of $X$ has an accumulation point in $X$. Then for each covering $\\mathcal O=\\{O_\\beta\\}_{\\beta\\in I}$ there exists a positive $\\epsilon$ such that each ball $B(x;\\epsilon)$ is contained in an element $O_\\beta$ of this covering. \nPROOF  If it wasn't the case, we'd obtain for each $n$ an point $x_n$ and an open ball $B(x_n;1/n)\\not\\subseteq O_\\beta$ for each $\\beta \\in I$. Let $A=\\{x_1,\\dots\\}$. If $A$ is finite, $x_n=x$ infinitely often for some $x\\in X$. Since $\\mathcal O$ is a cover, $x\\in O_\\alpha$ for some $\\alpha$. Since the cover is open, there is a $\\delta >0$ for which $B(x;\\delta)\\subseteq O_\\alpha$. We can take $n$ such that $1/n<\\delta$ and $x_n=x$, in whichcase we get a contradiction $$B\\left(x;\\frac 1n \\right)\\subseteq B\\left(x;\\delta\\right)\\subseteq O_\\alpha$$ If $A$ is infinite, there is an accumulation point $x\\in X$. Thus $x\\in O_\\beta$ for some index, and there are infinitely many points of $A$ in $B(x:\\delta /2)\\subseteq O_\\beta$. We can take $n$ such that $1/n<\\delta /2$ and we'd have $B(x_n;1/n)\\subseteq B(x;\\delta)\\subseteq O_\\beta$, a contradiction. \nAfter having proven that for metric spaces, the existence of accumulation points for infinite subsets is equivalent to compactness. \nPROOF3 Let $f:X\\to Y$ be a continuous function from a compactum $X$ to a metric space $Y$. Then $f$ is uniformly continuous.\nPROOF Given $\\epsilon >0$, for each $x\\in X$ there is a $\\delta_x>0$ such that if $y\\in B(x:\\delta_x)$, $f(y)\\in B\\left(f(x);\\epsilon /2\\right)$. These balls are an open cover for $X$, thus there exists such a number $\\delta_L$ as in the previous lemma (usually called a Lebesgue number). Choose $\\delta$ to be positive yet smaller than $\\delta_L$. If $z,z'\\in X$ and $d(z,z')<\\delta$ (so that $z,z'$ are in a ball of radius less than $\\delta$), we have $z,z'\\in B(x,\\delta_x)$ for some $x\\in X$. In that case $f(z),f(z')\\in B(f(x),\\epsilon/2)$ so $d'(f(z),f(z'))<\\epsilon$ by the triangle inequality. $\\blacktriangle$.\n",
    "tags": [
      "analysis",
      "proof-writing"
    ],
    "score": 26,
    "answer_score": 51,
    "is_accepted": false,
    "question_id": 333125,
    "answer_id": 333129
  },
  {
    "theorem": "Can&#39;t find mistake in an easy proof.",
    "context": "Consider the following theorem.\n$\\textbf{Theorem:}$ for any sets $A, B, C, D$, if $A \\times B \\subseteq C \\times D$ then $A \\subseteq C$ and $B \\subseteq D$.\nThen the following proof is given.\n$\\textbf{Proof:}$ Suppose $A \\times B \\subseteq C \\times D$. Let $a$ be an arbitrary element of $A$ and let $b$ be an arbitrary element of $B$. Then $(a,b) \\in A\\times B$. so since $A\\times B \\subseteq C \\times D$, $(a,b) \\in C \\times D$. Therefore $a \\in C$ and $b \\in D$. Since $a$ and $b$ were arbitrary elements of $A$ and $B$, respectively, this shows that $A \\subseteq C$ and $B \\subseteq D$. QED.\nI know that the theorem is not correct, because there is a counterexample $A = \\left\\lbrace 1 \\right\\rbrace$, $B = C = D = \\emptyset$. Notice that $A \\times B \\subseteq C \\times D \\sim \\emptyset \\subseteq \\emptyset$ but $A \\nsubseteq C$.\nSo, clearly, this is an invalid proof, but I cannot figure out which step is wrong.\n",
    "proof": "They write \"let $b$ be an arbitrary element of $B$\" without checking whether $B$ actually has elements.\nAddendum: The proof is otherwise essentially OK assuming that we require $A,B\\neq\\emptyset$, but I would write it a bit differently: Let $a$ be an arbitrary element of $A$.  As $B$ is not empty, we can find a $b\\in B$ and thus $(a,b)\\in A\\times B$.  This implies $(a,b)\\in C\\times D$ and thereby $a\\in C$.  That proves $A \\subseteq C$ and we can likewise prove $B \\subseteq D$.\n",
    "tags": [
      "proof-verification",
      "proof-writing"
    ],
    "score": 26,
    "answer_score": 35,
    "is_accepted": false,
    "question_id": 924931,
    "answer_id": 924934
  },
  {
    "theorem": "Is there more to explain why a hypothesis doesn&#39;t hold, rather than that it arrives at a contradiction?",
    "context": "Yesterday, I had the pleasure of teaching some maths to a high-school student. She wondered why the following doesn't work:\n$\\sqrt{a+b}=\\sqrt{a}+\\sqrt{b}$.\nI explained it as follows (slightly less formal)\n\nFor your hypothesis to hold, it should hold given an arbitrary set of operations performed on your equation.\nFor example, it should hold if we square the equation, and after that take the square root, i.e. (note that I applied her logic in the second line; I know it's not OK to do maths like that)\n$\\sqrt{a+b}=\\sqrt{a}+\\sqrt{b}\\\\\na+b=a+b+2\\sqrt{ab}\\\\\n\\sqrt{a+b}=\\sqrt{a}+\\sqrt{b}+\\sqrt{2\\sqrt{ab}}$\nWe now arrive at a contradiction, which means that your hypothesis is false.\n\nHowever, she then went on to ask 'But why then is it false? You only proved that it's false!'. As far as I'm concerned, my little proof is a perfect why explanation as far as mathematicians are concerned, but I had a hard time convincing her - the only thing I could think of is to say that the square root operator is not a linear operator, but I don't really think that adds much (besides, I really don't want to be explaining and proving linearity to a high school student).\nSo, my question: is there anything 'more' as to why the above doesn't work, or was I justified in trying to convince her that this is really all there is to it?\n",
    "proof": "I was one of those students that was fanatical about the \"why,\" so I can give my own viewpoint on this.  When I asked \"why\" on something like this, its because I was looking for a general principle I could apply.  I wasn't just concerned with the problem at hand, but rather the situation at hand raised a red flag because my intuition wasn't lining up with what I was told.  I was concerned with the legion of potential misconceptions I might be holding onto because there was some underlying thing I was missing.  I needed to understand, in my own terms, what the \"real deal\" was.\nThat \"real deal\" is different for each individual.  Several other answers have already pointed out that there won't be a one-size-fits-all answer for this, because each student thinks about math in a different way.  Some are visual learners, needing pictures.  Personally, I viewed a great deal of this from the perspective of semantics vs syntax:  What meanings are true versus what syntactic manipulations are valid.  So naturally when I view the issue your student had, I view it from my own perspective.\nFrom my perspective, the issue is that it looks like there is a valid syntactic manipulation, $\\sqrt{a+b}=\\sqrt a + \\sqrt b$.  If I am told that that manipulation is invalid, I want to understand why.  The words I would now use to describe why is that what I am trying to do here is \"distribute\" the square root over the addition operation, and you're not allowed to do that.  If I were to remove the square root, and replace it with a more generic function, $f(a+b) = f(a) + f(b)$, it starts to become more clear that such an assumption is not always valid.  In fact, it starts to look like a truly special case (especially when you consider that the above assumption doesn't even hold water for simple cases like $f(x) = x + 1$).  The ability to distribute one function over another is not the norm, its the special case that occurs in a few situations (like distributing multiplication over addition).\nThis ends up reframing the problem away from \"why can't I distribute a square root over addition\" to \"what special properties are needed to allow the distributed property?\"  It moves the specialness away from square roots, and puts specialness on things like addition and multiplication.  It starts to lead one to appreciate why addition and multiplication are so useful: they have so many nice convenient properties other functions don't have!\n",
    "tags": [
      "proof-writing",
      "linear-transformations"
    ],
    "score": 25,
    "answer_score": 40,
    "is_accepted": true,
    "question_id": 1439406,
    "answer_id": 1440051
  },
  {
    "theorem": "Showing that $1^k+2^k + \\dots + n^k$ is divisible by $n(n+1)\\over 2$",
    "context": "\nFor any odd positive integer $k\\geq1$, the sum $1^k+2^k + \\dots + n^k$ is divisible by $n(n+1)\\over 2$.\n\nI used induction principle for the solution but cannot prove it.\nI took $P(k) = 1^k+2^k+\\dots+n^k$.\nFor $P(1)$ it is true.\nFor $P(n)$ let it be true.\nBut for $P(n+1)$ I cannot solve it.\n",
    "proof": "For odd $k$ we have that\n$$\na^k+b^k=(a+b)(a^{k-1}-a^{k-2}b+a^{k-3}b^2-\\dots+b^{k-1})\n$$\nThus, each column of\n$$\n\\begin{align}\n&0^k+\\hphantom{(n-\\ )}1^k+\\hphantom{(n-\\,\\,)}2^k+\\dots+n^k\\\\\n&n^k+(n-1)^k+(n-2)^k+\\cdots+0^k\n\\end{align}\n$$\nis divisible by $n$ and each column of\n$$\n\\begin{align}\n&1^k+\\hphantom{(n-\\ )}2^k+\\hphantom{(n-\\,\\,)}3^k+\\dots+n^k\\\\\n&n^k+(n-1)^k+(n-2)^k+\\cdots+1^k\n\\end{align}\n$$\nis divisible by $n+1$. Since $(n,n+1)=1$ we get that\n$$\nn(n+1)\\,|\\,2(1^k+2^k+3^k+\\dots+n^k)\n$$\nand therefore, since $n(n+1)$ is even,\n$$\n\\left.\\frac{n(n+1)}{2}\\middle|1^k+2^k+3^k+\\dots+n^k\\right.\n$$\n",
    "tags": [
      "elementary-number-theory",
      "proof-writing"
    ],
    "score": 25,
    "answer_score": 42,
    "is_accepted": false,
    "question_id": 427744,
    "answer_id": 427902
  },
  {
    "theorem": "When stating a theorem in textbook, use the word &quot;For all&quot; or &quot;Let&quot;?",
    "context": "(Some report that my question is similar to another post. However, that post is talking about writing the \"proof\", rather than \"stating\" the theorem. \"Proving\" a theorem is NOT of the same structure and situation as \"stating\" a theorem. So this question is not duplicated to the other! Do not let it to be closed! And by the way, I'm also the OP of that question...)\nIn writing a textbook, when we need to state a theorem that is a universal quantification, we can use the word\n\n\"for all ...\"(or equivalently \"for every\", \"for any\",\n\"for arbitrary\", \"for each\")\n\nor\n\n\"let ...\",\n\nWhich of these ways is more ideal? Why?\nAlthough I think writing as \"for all\" is the more natural way to reflect the logical structure, that is a universal quantifier $\\forall$, the popular style I have seen tends to use \"let\".\nAny theoretical aspect or experience is welcome.\nExample set 1.\n\n\nFor all natural numbers $n$, if $n$ is even, then $n$ squared is even.\n\nLet $n$ be a natural number. If $n$ is even, then $n$ squared is even.\n\n\n\nExample set 2.\n\n\nLet $A,B$ be two sets. If for all $x\\in A$, $x\\in B$, then we say $A$ is a subset of $B$.\n\nFor all pairs $A,B$ of sets, if for all $x\\in A$, $x\\in B$, then we say $A$ is a subset of $B$.\n\n\n\nExample set 3.\n\n\nLet $Y$ be a subspace of $X$. Then $Y$ is compact if and only if every covering of $Y$ by sets open in $X$ contains a finite subcollection covering $Y$. (Munkres Topology Lemma 26.1)\n\nFor all subspaces $Y$ of $X$, $Y$ is compact if and only if every covering of $Y$ by sets open in $X$ contains a finite subcollection covering $Y$.\n\n\n\nExample set 4.\n\n\nFor every $f:X\\to Y$ being a bijective continuous function, if $X$ is compact and $Y$ is Hausdorff, then $f$ is a homeomorphism. (adapted by me, maybe ill-grammared?)\nFor every bijective continuous function $f:X\\to Y$, if $X$ is compact and $Y$ is Hausdorff, then $f$ is a homeomorphism. (adapted by me.)\nLet $f:X\\to Y$ be a bijective continuous function. If $X$ is compact and $Y$ is Hausdorff, then $f$ is a homeomorphism. (Munkres Topology Theorem 26.6)\n\n\n\nNew added example set 5\n(I skipped the quantification on $E,f:E\\to\\mathbb{R},L,c$, just focus on the key part here.)\n\n\nIf \"$\\forall\\varepsilon>0,\\exists\\delta>0,\\forall x\\in E,0<|x-c|<\\delta\\rightarrow |f(x)-L|<\\varepsilon$\", then we say $f(x)$ converges to $L$ when $x$ approaches $c$.\nIf, for all $\\varepsilon>0$, there exists $\\delta>0$ such that for all $x\\in E$, if $0<|x-c|<\\delta$ then $|f(x)-L|<\\varepsilon$\", then we say $f(x)$ converges to $L$ when $x$ approaches $c$. (Using \"for all\")\nIf let $\\varepsilon>0$, there is $\\delta>0$, such that let $x\\in E$, if $0<|x-c|<\\delta$ then $|f(x)-L|<\\varepsilon$\", then we call $f(x)$ converges to $L$ when $x$ approaches $c$. (Using \"let\". I think this type is not natural. But I can't tell why.)\n\n\n",
    "proof": "If you're doing informal mathematics, there's really no difference. I guess from a type-theoretic perspective, it's kind of the difference between $$x:\\mathbb{R} \\vdash P(x) \\qquad \\mbox{and} \\qquad \\vdash (\\forall x:\\mathbb{R})\\,P(x).$$\nThe former is arguably better, because it doesn't presuppose that we're working in a category that interprets universal quantification. So \"let\" is preferable to \"for all\" for this reason. But, again, unless you're doing highly formal mathematics, it's not really worth worrying about. (I say that, but a part of me finds the question fascinating, and I've just gone and favourited it.)\n",
    "tags": [
      "proof-writing"
    ],
    "score": 25,
    "answer_score": 20,
    "is_accepted": true,
    "question_id": 1961938,
    "answer_id": 1961949
  },
  {
    "theorem": "Proof writing: how to write a clear induction proof?",
    "context": "What is an effective way to write induction proofs? \nEssentially, are there any good examples or templates of induction proofs that may be helpful (for beginners, non-English-native students, etc.)? \nTo guide readers, please state whether your answer handles:\n\nCase 1: a simple induction $(P_n \\implies P_{n+1}$), or\nCase 2: a strong induction ($P_1,\\ldots,P_n \\implies P_{n+1}$), or\nCase 3: a more exotic induction (e.g. over $\\Bbb Q$ on $|p|+q$).\n\nPS: I have seen many induction related questions, and very often the problem lies with the OP's lack of a proper methodology (or style) in writing the proof whereas the answers focus on the particular case of the OP's question. The matter of style is obviously subjective, but it seems to me that the \"craft\" of writing good proofs is almost as important as understanding underlying concepts, so advice on proper proof-writing practices should fall within the scope of MSE (under the proof-writing tag).\n",
    "proof": "Initial comments: This is an excellent question in my opinion and is just what the proof-writing tag is for. Unfortunately, there are often many problems plaguing beginners when it comes to induction proofs:\n\nWhy induction is a valid proof technique should be understood at the outset, and this is rarely the case.\nLess relevant in high school or undergrad, but certainly important on MSE--it is expected for one to be able to typeset their math correctly. This often means taking the time to learn some MathJax, ${\\rm\\LaTeX}$, or a hybrid of these, something that takes a good bit of time.\nIn order to be able to communicate a proof effectively, one must necessarily understand how to write well, a talent often under appreciated and underdeveloped amongst mathematicians.\n\nThe list could go on and on, but those are some of the more salient points. That being said, I will provide a template for writing up nice, polished induction proofs, and then I will detail the rationale for this template, both adopted from David Gunderson's marvelous book Handbook of Mathematical Induction. More specifically, what has been adapted is from the chapter \"The written MI proof\" (yes, there's an entire chapter devoted just to how to effectively write induction proofs--pgs 109-119, specifically). Finally, I will conclude by showing how one might use this template to prove the statement $\\prod_{i=2}^n\\left(1-\\frac{1}{i}\\right)=\\frac{1}{n}$ for $n\\geq 2$.\n\nTemplate\n\nNote: In the above template, if the proof is by strong induction, then the induction hypothesis should be replaced with \"assume that for each $j, 3\\leq j\\leq k$,\n$$\n\\text{$S(j) : $ (write out what $S(j)$ says)}\n$$\nholds. Also, in the sequence of equations, at the point where the induction hypothesis is invoked, either write \"by IH\" or mention which statements of the IH are used (e.g., by $S(4)$ and $S(k)$). \n\nRationale for Template\nSuppose that a particular statement regarding $n$ is to be proved for $n\\geq 3$. \n\nDefine the statement that needs to be proved. For example: \"For each $n\\geq 3$, let $S(n)$ be the statement $\\ldots$\". If there is more than one variable, be careful of quantification; for example, the expression\n$$\n\\text{For each $n\\geq 3$ let $S(n)$ be the statement that for all $m\\leq n$ $\\ldots$}\n$$\nis different from\n$$\n\\text{For each $n\\geq 3$ and all $m\\leq n$, let $S(n)$ be the statement that $\\ldots$}\n$$\nIn the second expression, the lower bound for $m$ is not stated, and it is not clear whether or not $S(n)$ depends on the particular value of $m$, so perhaps something like\n$$\n\\text{For each $n\\geq 3$ and each $m$ satisfying $1\\leq m\\leq n$, let $S(m,n)$ be the statement $\\ldots$}\n$$\nis better. It might help to also identify in advance for which variables a particular sentence even makes sense, later restricting the variable to the cases that are being proved.\nState the range of $n$ for which the statement is to be proved. For example: \"To be proved is that for each integer $n\\geq 3$, the statement $S(n)$ is true.\" \nBase step: Write the words \"Base step\" and verify that the base case is true (giving reasons if it is not trivial). For example:\nBase step: $S(3)$ says $\\ldots$ which is true. \nInductive step: Write out the words \"Inductive step:\"\nState the inductive hypothesis. For simple mathematical induction, this will read like: For some fixed $k\\geq 3$, assume that $S(k)$ is true. [Writing out precisely what $S(k)$ says is usually an excellent idea.] For strong induction, this will read something like: \"For some fixed $k\\geq 3$, assume that $S(3), S(4), \\ldots, S(k)$ are all true,\" or \"For some fixed $k\\geq 3$, assume that for $3\\leq j\\leq k, S(j)$ is true.\" Labelling the inductive hypothesis with the words \"inductive hypothesis\" (or \"IH\") is often a useful practice for the novice.\nState what needs to be proved, namely $S(k+1)$. It is highly recommended that one writes out $S(k+1)$ specifically so that one sees the required form of the conclusion in the inductive step.\nProve $S(k+1)$. If $S(n)$ is an equality (or inequality), it is best to start with one side of $S(k+1)$, and via a sequence of equalities (or inequalities), derive the other side. At the point where the inductive hypothesis is used, this should be mentioned either as a side comment \"by $S(k)$\", \"by induction hypothesis\", or even by putting the initials \"IH\" over the relevant equal sign. \nMention when the inductive step is done. For example, one might write \"$\\ldots$ completing the inductive step $S(k)\\to S(k+1)$.\", or simply \"This completes the inductive step.\" \nState the conclusion: \"Therefore by mathematical induction, for all $n\\geq 3, S(n)$ is true. $\\Box$\", using the symbol \"$\\Box$\" to denote that the entire proof is complete (this symbol is even more shorthand for QED; more information about this may be found here). Some mathematicians prefer to quantify variables before they are used, as in \"$\\ldots$ for all $n\\geq 3, S(n)$ is true.\" This is a good practice, as it reads more logically; however, remember to insert a comma (because \"$n\\geq 3\\, S(n)$\" might be meaningless) or an extra phrase like \"$\\ldots$ for $n\\geq 3$, the statement $S(n)$ holds.\" \n\nSomewhat surprisingly, in an attempt to simply memorize the format of an inductive proof, students often discover what was wrong with their previous formats. The novelty of the template is that it forces students, more or less, to understand their own inductive proofs. \n\nUsing the Template\nProblem: Prove that for all $n\\geq 2, \\prod_{i=2}^n\\left(1-\\frac{1}{i}\\right)=\\frac{1}{n}$.\nSolution: For any integer $n\\geq 2$, let $S(n)$ denote the statement\n$$\nS(n) : \\prod_{i=2}^n\\left(1-\\frac{1}{i}\\right)=\\frac{1}{n}.\n$$\nBase step ($n=2$): $S(2)$ says $\\prod_{i=2}^2\\left(1-\\frac{1}{i}\\right)=\\frac{1}{2}$, and this is true because $$\\prod_{i=2}^2\\left(1-\\frac{1}{i}\\right)=1-\\frac{1}{2}=\\frac{1}{2}.$$ \nInductive step $S(k)\\to S(k+1)$: Fix some $k\\geq 2$. Assume that\n$$\nS(k) : \\prod_{i=2}^k\\left(1-\\frac{1}{i}\\right)=\\frac{1}{k}\n$$\nholds. To be proved is that\n$$\nS(k+1) : \\prod_{i=2}^{k+1}\\left(1-\\frac{1}{i}\\right)=\\frac{1}{k+1}\n$$\nfollows. Beginning with the left side of $S(k+1)$,\n\\begin{align}\n\\prod_{i=2}^{k+1}\\left(1-\\frac{1}{i}\\right) &= \\left[\\prod_{i=2}^k\\left(1-\\frac{1}{i}\\right)\\right]\\left(1-\\frac{1}{k+1}\\right)\\tag{by defn. of $\\Pi$}\\\\[1em]\n&= \\frac{1}{k}\\left(1-\\frac{1}{k+1}\\right)\\tag{by $S(k)$, the ind. hyp.}\\\\[1em]\n&= \\frac{1}{k}\\left(\\frac{k+1-1}{k+1}\\right)\\tag{common denom.}\\\\[1em]\n&= \\frac{1}{k}\\cdot\\frac{k}{k+1}\\tag{simplify}\\\\[1em]\n&= \\frac{1}{k+1},\\tag{simplify further}\n\\end{align}\none arrives at the right side of $S(k+1)$, thereby showing $S(k+1)$ is also true, completing the inductive step. \nConclusion: By mathematical induction, it is proved that for all $n\\geq 2$, the statement $S(n)$ is true. $\\Box$\n",
    "tags": [
      "soft-question",
      "proof-writing",
      "induction",
      "big-list"
    ],
    "score": 25,
    "answer_score": 32,
    "is_accepted": false,
    "question_id": 1253956,
    "answer_id": 1255268
  },
  {
    "theorem": "Prove the inequality $n! \\geq 2^n$ by induction",
    "context": "I'm having difficulty solving an exercise in my course.\nThe question is: \n\nProve that $n!\\geq 2^n$.\n\nWe have to do this by induction. I started like this:\n\nThe lowest natural number where the assumption is correct is $4$ as: $4! \\geq 2^4 \\iff 24 \\ge 16$.\nThe assumption is: $n! \\ge 2^n$.\n\nNow proof for $(n+1)$ which brings me to: $(n+1)! \\ge 2^{(n+1)}$\nI think I can rewrite it somehow like this:\n$$ {(n+1)} \\times {n!}  \\stackrel{\\text{(definition of factorial)}}{\\ge} 2^n \\times 2 $$\n$$ (n+1) \\times 2^n \\ge 2^n \\times 2 $$\nThen I think I can eliminate the $2^n$ and have something like this: $n+1 \\ge 2$, or $n \\ge 1$.\nBut I think I'm wrong here some where and was hoping somebody has some advice on this. How can I prove the above assumption?\nAny help would be appreciated, kind regards. \n",
    "proof": "In the induction step you want to show that if $k!\\ge 2^k$ for some $k\\ge 4$, then $(k+1)!\\ge 2^{k+1}$. Since you already know that $4!\\ge 2^4$, the principle of mathematical induction will then allow you to conclude that $n!\\ge 2^n$ for all $n\\ge 4$. You have all of the necessary pieces; you just need to put them together properly. Specifically, you can argue as follows.\nSuppose that $k!\\ge 2^k$, where $k\\ge 4$; this is your induction hypothesis. Then $$\\begin{align*}\r\n(k+1)! &= (k+1)k!\\text{ (by the definition of factorial)}\\\\\r\n&\\ge (k+1)2^k\\text{ (by the induction hypothesis)}\\\\\r\n&> 2\\cdot2^k\\text{ (since }k\\ge 4\\text{)}\\\\\r\n&= 2^{k+1}.\r\n\\end{align*}$$ This completes the induction step: it shows that if $k\\ge 4$, then $$k!\\ge 2^k \\implies (k+1)!\\ge 2^{k+1}.$$\n",
    "tags": [
      "inequality",
      "induction",
      "proof-writing",
      "factorial"
    ],
    "score": 25,
    "answer_score": 22,
    "is_accepted": true,
    "question_id": 76946,
    "answer_id": 76953
  },
  {
    "theorem": "Is a brute force method considered a proof?",
    "context": "Say we have some finite set, and some theory about a set, say \"All elements of the finite set $X$ satisfy condition $Y$\".\nIf we let a computer check every single member of $X$ and conclude that the condition $Y$ holds for all of them, can we call this a proof? Or is it possibly something else?\n",
    "proof": "Yes, you can. This method is known as proof by exhaustion.\nAlso, see computer-assisted proof.\nEdit: As others have noted, this of course works only for finite sets.\n",
    "tags": [
      "proof-writing",
      "computer-science",
      "proof-verification"
    ],
    "score": 24,
    "answer_score": 43,
    "is_accepted": true,
    "question_id": 717467,
    "answer_id": 717472
  },
  {
    "theorem": "$f: \\mathbf{R} \\rightarrow \\mathbf{R}$ monotone increasing $\\Rightarrow$ $f$ is measurable",
    "context": "Problem. Let $f: \\mathbf{R} \\rightarrow \\mathbf{R}$ be a monotone increasing function. Show that $f$ is measurable. \nSolution.\nWe know that the set of discontinuites of any monotone increasing function $f$ is measure zero (since it is at most countable). We define a continuous function $g$ such that $g(x)=f(x)$ except the discontinuous points of $g$. Then $g(x)=f(x)$ almost everywhere. Note that any continuous function $g: \\mathbf{R}\\rightarrow \\mathbf{R}$ is measurable. Also note that if $g$ is measurable and $f=g$ almost everywhere, then $f$ is measurable. Hence we conclude that $f$ is measurable.\n\nIs my solution correct? Thanks.\n",
    "proof": "It is true that $f$ is continuous almost everywhere, but it is not true that there exists a continuous function $g:\\mathbb R\\to\\mathbb R$ such that $f=g$ almost everywhere, unless $f$ is already continuous, as Jacob said in a comment.  Note that the left-hand and right-hand limits of $f$ exist everywhere, and if they are not equal for $f$, then they cannot be equal for any function equal to $f$ almost everywhere.  E.g., the characteristic function of $[0,\\infty)$ is monotone and not equal almost everywhere to a continuous function.  \n(The characteristic function of the rationals is equal almost everywhere to a continuous function, but is continuous nowhere.  This shows from the other direction why \"continuous almost everywhere\" and \"equal almost everywhere to a continuous function\" are very different.)\nModifying your work to something correct takes more effort than the simpler and clearer solution given by Cass.  Let $D$ be the set of discontinuities of $f$.  Then $D$ is countable, hence of measure $0$.  The restriction $f|_D$ is measurable on $D$ because every subset of $D$ is measurable, and the restriction $f|_{\\mathbf R\\setminus D}$ is measurable on $\\mathbf R\\setminus D$ because it is continuous.  You can show this implies that $f$ is measurable.\n",
    "tags": [
      "real-analysis",
      "measure-theory",
      "proof-writing"
    ],
    "score": 24,
    "answer_score": 19,
    "is_accepted": true,
    "question_id": 229497,
    "answer_id": 229525
  },
  {
    "theorem": "What are some common proof strategies in  mathematics?",
    "context": "I want to start out by saying that I am new at proof based mathematics. I am used to seeing patterns and using them to solve similar problems. However, I have found this is not a very good way to study for courses in abstract mathematics! I have noticed that similar themes in mathematics keep coming up over and over again(for example, homomorphisims of objects). And partially because of this, we also have recurring proof methods. Many times I initially struggle with proofs because I don't think, \"What are the proof methods available to solve this sort of problem?\".\nI have found that stopping to think about proof strategies is the best way to solve a proof based problem, but many times, because of inexperience I lack knowledge about what tools are available to me in this regard. For example, to prove $A=B$, a way to attack this problem is to try to show that $A\\leq B$, and also that $A\\geq B$. This proof strategy came up today when I was trying to prove $G_{b}=gG_{a}g^{-1}$ where $G_{x}$ is the stabilizer of the element $x$ under appropriate conditions. Anyway, I showed that $G_{b}\\subseteq gG_{a}g^{-1}$ and also that $gG_{a}g^{-1}\\subseteq G_{b}$. Once I realized how to attack this problem, the problem sort of solved itself. I have noticed that many problems sort of tell you how to solve them. \nWhat are your favorite proof strategies, or ones that you have found extremely useful? \n",
    "proof": "There are many interesting problem-solving techniques listed at the Tricki.\nhttp://www.tricki.org/\n",
    "tags": [
      "reference-request",
      "soft-question",
      "big-list",
      "proof-writing"
    ],
    "score": 24,
    "answer_score": 24,
    "is_accepted": false,
    "question_id": 73776,
    "answer_id": 73784
  },
  {
    "theorem": "Prove if $56x = 65y$ then $x + y$ is divisible by $11$",
    "context": "If $x$ and $y$ are natural numbers, and $56x = 65y$, prove that $x + y$ is divisible by $11$.\nI tried  taking the $\\gcd(56x,65y)$ using the Euclidean algorithm, but I got nowhere with it and do not know where to head. \n",
    "proof": "Why not just $x+y=(56x-55x)+(66y-65y)=11(6y-5x)+(56x-65y)=11(6y-5x)$ (since $56x-65y=0$).   \n",
    "tags": [
      "proof-writing",
      "divisibility"
    ],
    "score": 23,
    "answer_score": 63,
    "is_accepted": true,
    "question_id": 571215,
    "answer_id": 571269
  },
  {
    "theorem": "Infinite set always has a countably infinite subset",
    "context": "I'm trying to show that one infinite always has a countably infinite subset, but I'm confused with something on the proof. Let $S$ be one infinite set. In that case, to show it has one countably infinite subset means to find a subset $S_0\\subset S$ such that there is a bijection between $S_0$ and $\\mathbb{N}$.\nThe procedure I considered is: pick one arbitrary $a_0\\in S$ and consider $S\\ \\setminus \\{a_0\\}$. This new set will also be infinite, so we can pick $a_1\\in S\\setminus \\{a_0\\}$ and consider $S\\setminus \\{a_0,a_1\\}$. This new set is again infinite. We can repeat this procedure $n$ times, for every $n\\in \\mathbb{N}$ and thus construct a set $\\{a_0,a_1,\\dots\\}$ which will clearly be in bijection to $\\mathbb{N}$.\nNow there is a point I'm quite unsure. This idea that: we pick $a_0$ from $S$ then we pick $a_1$ from $S\\setminus \\{a_0\\}$ and so forth. It seems to me that in the way I wrote, this is not fully rigorous.\nIs that the case? If so, how can this idea be made rigorous?\n",
    "proof": "Welcome to the wormhole.\nA set which has a countably infinite subset is called Dedekind Infinite.  It is not provable within Zermelo-Fraenkel set theory (ZF) that every infinite set is Dedekind infinite.  This was demonstrated by Paul Cohen, who exhibited a model of ZF in which there is an infinite, Dedekind-finite set of real numbers.  See Asaf's answer to my question for implications of this statement in analysis.\nYou do not need any form of the axiom of choice to prove that infinite equals Dedekind infinite, in the sense that one can construct models of ZF in which all infinite sets have countably infinite subsets but no form of the axiom of choice holds.  However, if we do have some form of the axiom of choice available, even a weak form, then we can prove your statement.\nLet me explain.  The axiom of choice states that, given any collection $(A_i\\;\\colon\\; i\\in I)$ of nonempty sets, indexed by some set $I$, there exists a choice function picking out, for each $i\\in I$, an element $a_i\\in A_i$.  In other words, we have a function $f\\colon I\\to\\bigcup_iA_i$ such that $f(i)\\in A_i$ for each $i$.\nA well-known (but not easy to prove) consequence of the axiom of choice is the well-ordering theorem: given any set $A$, we can order the elements of $A$ in such a way that any nonempty subset of $A$ has a least element under the ordering.  In fact, the well-ordering theorem is equivalent to the axiom of choice, since, given $(A_i\\;\\colon\\;i\\in I)$, we can define $f(i)$ to be the least element of $A_i$, under this ordering, giving us a choice function.\nWith this form of the axiom of choice, one can prove your statement as follows: given your set $S$, order it according to the well-ordering theorem.  Then let $a_0$ be the least element of $S$, let $a_1$ be the least element of $S\\setminus\\{a_0\\}$ and so on.  This process will never terminate; otherwise, $S$ would be equal to $\\{a_0,\\dots,a_n\\}$ for some $n$ and therefore would be finite.\nThat's all well and good, but the well-ordering theorem is a non-obvious theorem in set theory, and certainly isn't implicit in your argument.  We shouldn't need to use it to prove this statement.\nThankfully, there is another way, and it actually involves a strictly weaker form of the axiom of choice.  The axiom of countable choice is the same as the axiom of choice, except that we require the set $I$ to be countable.  There are models of set theory in which the axiom of choice fails but still holds whenever the index set is countable, so countable choice is strictly weaker than full choice.\n\nHere's how we prove your statement using only countable choice.  Let $S$ be an infinite set.  Then, for each $n=0,1,2,\\dots$, let $S_n$ be the set of subsets of $S$ of size $2^n$.\nWe can show that each $S_n$ is non-empty using your argument (note that the axiom of choice is not required for a finite collection of sets, since we can proceed by induction): inductively pick elements $a_1,\\dots,a_{2^n}$ from $S$; since $S$ is infinite, this process will not terminate and since we are only choosing finitely many elements, there are no dodgy set-theory issues.\nNow, there are countably many $S_n$, so the axiom of countable choice tells us that there is a sequence $A_0,A_1,A_2,\\dots$ of subsets of $S$ such that $A_n$ has size $2^n$ for each $n$.\nNow, for each $n$, let $B_n$ be the set of elements of $A_n$ that are not contained in $A_0\\cup\\dots\\cup A_{n-1}$.  Since $A_n$ has size $2^n$, and $A_0,\\dots,A_{n-1}$ have at most $1+2+4+\\dots+2^{n-1}=2^n-1$ distinct elements between them, $B_n$ must be non-empty.  By the axiom of countable choice again, we can choose a sequence $s_0,s_1,\\dots$ with each $s_n\\in B_n$.  In particular, the $s_n$ are distinct.  We have found a countably infinite subset of $S$.\n",
    "tags": [
      "proof-writing",
      "set-theory"
    ],
    "score": 23,
    "answer_score": 25,
    "is_accepted": false,
    "question_id": 1403416,
    "answer_id": 1413728
  },
  {
    "theorem": "Tips for writing proofs",
    "context": "When writing formal proofs in abstract and linear algebra, what kind of jargon is useful for conveying solutions effectively to others? In general, how should one go about structuring a formal proof so that it is both clear and succinct? What are some strategies for approaching problems that you will need to write formal proofs for?\n",
    "proof": "That is a mouthful of questions.  There are no definitive answers, but here are some ideas:\nBeing clear and succinct:\nFirst, when you write your proof, make sure you define your terms, and your symbols.  Don't make your readers scramble to look up terms.  And if someone doesn't know what $\\phi (a)$ is, how is he supposed to find out?  \nOf course I'm not suggesting you define things like \"a group of order 15\".  This really is standard.  If your reader doesn't know what a group is, he should read elsewhere.  But I've had recent encounters with words like \"unicity\" (does he mean uniqueness?).  What does the symbol dL/dT.L mean?  Maybe it's standard somewhere, but I've never seen it.  If there is any doubt, define what you mean.\nNext, settle on a clear scheme for your notation.  If you have two vector spaces V and W, maybe your V vectors should be {$v_1, v_2, ... v_n$} and use w's for the W space.  Why use a's and b's?  Yet some people do.  Worse, they change the notation in midstream -- a guaranteed way to confuse people. Then there are the writers who reuse symbols and assign them different meanings later in the proof.\nI would add that personally I am not a fan a Greek letters, if only because they are a pain to type; but they can be hard to read also.  However, if you have an \"A\" and \"a\" that are related and want to introduce an $\"\\alpha\"$ that is also connected, that makes sense.  \nOnce you've figured out your proof, think of writing it as if you were trying to explain it to a bright high school algebra student.  Write down each step and at least on the first draft don't leave out any steps.  Make sure you explain properly how you get from each step to the next.\nAs with defining your terms, this requires some judgement.  If you write \"x + 3 = 6 so that x = 3\" you don't have to explain.  If you write \"a group of order 15 must have subgroups of order 5 and 3\" whether you explain depends on who your audience will be.  If it is a conference of group theorists, you can skip the explanation.  If it is a 1st course in group theory, you probably should include it.\nBegin by erring on the side of including more rather than less explanation. Note the word \"begin\".  Clear, succinct proofs to not spontaneously spring into life.  They are usually the result of several careful, attentive drafts. If I'm writing seriously I usually plan on 4 drafts.  This is true even for material that has nothing to do with math, which people manage to misunderstand anyway. \nDo not skimp on drafts, but put each one away for awhile before starting on the next.  Then reread what you did with a newly critical eye. What would your supposed high school student think?\nFinally, if you have a lot of explanation that is impeding the flow of the proof, move some of it to footnotes.  That way people can follow your argument without getting tangled up in the details.  If they really want the details, they'll read the notes.\nHow to approach:  Many books have been written on this endless subject. I can't write a book but here are some ideas:\nA.  Make sure you understand what is being asked.  Do you understand the definitions?  How are you going to prove something about independent vectors if you don't know what \"independent\" means. \nB.  Start with some simple examples.  You have a problem in $R^n$?  Can you solve it for $R^2$?  Many such solutions do not really depend on the 2 and will generalize immediately.  You have a problem about groups?  Can you solve it for a cyclic group? For an Abelian group? For $S_3$? \nSomeone once accused me of thinking all matrices are diagonal.  I don't really think that, but if I can solve it for a diagonal matrix, maybe I can solve it for a diagonalizable matrix.\nOnce you see how things are working in a simple case, you may get some insight into what is going on. Or maybe you can piggyback on your special case -- show that the difference between that and the general case doesn't affect things much.  \nAnd starting with a special case is time-honored.  Many important papers have proved only a special case of what is really desired.\nC. Develop a big bag of techniques. There are things that are used over and over,  homomorphisms, isomorphisms, linear operators, basis, adjoint etc. etc. Start with the common techniques.  Work a lot of problems involving these techniques.  Someone said that if you have a hammer all problems look like a nail --well not everything can be solved with an isomorphism (I wish it could). How is that for a mixed metaphor?  Avoid them in your papers.\nThe more problems you work, the more techniques you will know.  You can never know too many.  \nThere is no short cut to this. \n",
    "tags": [
      "linear-algebra",
      "abstract-algebra",
      "soft-question",
      "proof-writing"
    ],
    "score": 23,
    "answer_score": 19,
    "is_accepted": true,
    "question_id": 612472,
    "answer_id": 615276
  },
  {
    "theorem": "Prove that the min and max of 2 continuous function are continuous",
    "context": "Prove that if $f$ and $g$ are continuous functions the so are $\\min⁡\\{f(x),g(x)\\}$ and $\\max⁡\\{f(x),g(x)\\}$\nI know this is true when $f$ and $g$ are not intersect each other, then I can compare them. However, I don't know how to prove it's true when they are intersect.\n",
    "proof": "Let $h(x) = \\min\\{f(x),g(x)\\}$. Suppose $x_0$ is such that $f(x_0) = g(x_0)$. We want to show $h$ is continuous at $x_0$. \nTake $\\epsilon > 0$, then there is a $\\delta_f$ so that $|f(x) - f(x_0)| < \\epsilon$ for $|x-x_0| < \\delta_f$, and similarly for $g$ and some $\\delta_g$ (with the same $\\epsilon$). \nUse this, and the fact that $h(x_0) = f(x_0) = g(x_0)$ to show that $|h(x) - h(x_0)| < \\epsilon$ whether $h(x) = f(x)$ or $h(x) = g(x)$ as long as $|x-x_0| < \\delta$ for some $\\delta$.\n",
    "tags": [
      "calculus",
      "proof-writing"
    ],
    "score": 23,
    "answer_score": 24,
    "is_accepted": true,
    "question_id": 530135,
    "answer_id": 530144
  },
  {
    "theorem": "Why are proofs written in first person plural? Were they ever written differently?",
    "context": "It's probably a silly question but it interests me when was the convention of writing proofs in first person plural introduced?\nAre here any historical examples of a different POV for proof writing?\n",
    "proof": "From what I could gather from some google searches (1, 2, 3-pdf, and Krantz has a good book on writing mathematical prose) the most common reasons for using we instead of I are:\n\nTo emphasize participation by the reader and ensure that he or she is included.\nTo not sound egotistical. As in, to stress the mathematics and reduce the role of the author in particular.\nTo keep the writing in active voice. \n\nI couldn't find when a the shift in writing style occurred, but I do recall reading papers by Euler in which he uses I. Though, I cannot read Latin and these were translated.\n",
    "tags": [
      "proof-writing",
      "math-history"
    ],
    "score": 23,
    "answer_score": 16,
    "is_accepted": true,
    "question_id": 604277,
    "answer_id": 604331
  },
  {
    "theorem": "Critiques on proof showing $\\sqrt{12}$ is irrational.",
    "context": "My only exposure to proofs was in a math logic class I took in University. I was wondering if my attempt at proving that $\\sqrt{12}$ is irrational is OK.\n$$\\Big(\\frac{m}{n}\\Big)^2 = 12$$\n$$\\Big(\\frac{m}{2n}\\Big)^2 = 3$$\n$$m^2=3*(2n)^2$$\nThis implies $m$ is even and so $n$ must be odd.\nThe problem can be reduced to:\n$$\\Big(\\frac{p}{n}\\Big)^2 = 3$$ \nBecause $n$ is odd, $p^2$ is odd, so $p$ is odd.\nThis implies:\n$$4a+1 = 3(4b+1)$$\n$$4a - 12b = 2$$\n$$2a - 6b = 1$$\nI'm kind of stuck at this point. I know that this can't be true but I don't know how to state it. Any critiques or suggestions? Thanks!\n",
    "proof": "You made it too complicated in my opinion.\nFirst we show that a rational number (different from $0$) times an irrational is irrational.\nProof by contradiction:\nLet $x\\in \\mathbb{R}\\setminus\\mathbb{Q}$, $a,c\\in\\mathbb{Z}\\setminus\\{0\\}$, $b,d\\in \\mathbb{N}, d \\neq 0$.\n$$x\\cdot \\frac{a}{b}=\\frac{c}{d} \\iff x=\\frac{bc}{ad},$$ so $x$ would be rational.\nUse\n$$\\sqrt{12}=\\sqrt{4\\cdot 3} = \\sqrt{4} \\cdot \\sqrt{3} = 2 \\cdot \\sqrt{3}$$\nSo $\\sqrt{12}$ irrational $\\iff \\sqrt{3}$ is irrational.\nNow, we show  $3|p^2 \\implies 3|p$: We know 3 is prime so with Euclid's lemma we have\n$$3|p^2 \\implies 3|p \\lor 3|p \\implies 3|p$$\nTo prove that $\\sqrt{3}$ is irrational, you derive a contradiction:\n$$\\sqrt{3}=\\frac{p}{q}\\iff 3q^2=p^2 \\implies \\exists k \\in \\mathbb{N}: 3k =p$$\n$$q^2=3k^2$$\nSo $q$ and $p$ both have the divisor $3$. Now there are two different ways to use this information. The first proceeds by contradiction, assuming that $p$ and $q$ don't have a common divisor, but as you can show they always have the divisor $3$, you can't write $\\sqrt{3}$ as a fraction of numbers without common divisors. This is the more elegant way in my opinion.\nThe other way is to show that both $p$ and $q$ can't be finite, because\nby repeating this argument we see $3^n|p$ for all $n \\in \\mathbb{N}$ and similarly for $q$.\nBut because of $p,q\\in \\mathbb{N}$, $3^n> 1+2n$ and the Archimedean principle we get a contradiction.\n",
    "tags": [
      "proof-writing",
      "irrational-numbers",
      "rationality-testing"
    ],
    "score": 22,
    "answer_score": 38,
    "is_accepted": true,
    "question_id": 305559,
    "answer_id": 305562
  },
  {
    "theorem": "The &quot;assumption&quot; in proof by induction",
    "context": "The second step in proof by induction is to:\n\nProve that if the statement is true for some integer $n=k$, where\n  $k\\ge n_0$ then it is also true for the next larger integer, $n=k+1$\n\nMy question is about the \"if\"-statement. Can we just assume that indeed the statement is true? If we assume it, then the proof works... but isn't that similar to the following \"proof\":\n\nLet $N$ be the largest positive integer.\n  Since $1$ is a positive integer, we must have $N\\ge1$.\n  Since $N^2$ is a positive integer, it cannot exceed the largest positive integer.\n  Therefore, $N^2\\le N$ and so $N^2-N\\le0$.\n  Thus, $N(N-1)\\le0$ and we must have $N-1\\le0$.\n  Therefore, $N\\le1$. Since also $N\\ge1$, we have $N=1$.\n  Therefore, $1$ is the largest positive integer.\n\nThe only thing that is wrong with this \"proof\" is that we falsely assume there actually exists a largest positive integer.\nSo both in the above case and in proof by induction we do an assumption. In the second case the assumption leads to a false conclusion. What is the difference with proof by induction? Why is doing the assumption that the hypothesis is actually true valid here and why doesn't it lead to a similar contradiction?\nEDIT: the \"proof\" above is not mine, it is taken from Calculus a Complete Course 8th edition as an example of why existence proofs are important.\n",
    "proof": "Indeed, a fallacious conclusion may be reached by valid deduction from an unjustifiable premise.\nThis is why the first step of induction is to prove that the predicate is justified for the base case; to ensure that we do not do that.\nIf $\\mathcal P(0)$ is proven and for all natural numbers $n$ we can show that $\\mathcal P(n)\\to\\mathcal P(n+1)$ is true, then we may successively prove $\\mathcal P(1)$, $\\mathcal P(2)$, $\\mathcal P(3)$, and so forth, by iterative applications of modus ponens.\n$${\\text{We may soundly prove }\\mathcal P(1)\\text{ from having proven }\\mathcal P(0)\\textit{ and }\\mathcal P(0)\\to\\mathcal P(1).\\\\\\text{We may soundly prove }\\mathcal P(2)\\text{ from having proven }\\mathcal P(1)\\textit{ and }\\mathcal P(1)\\to\\mathcal P(2).\\\\\\text{We may soundly prove }\\mathcal P(3)\\text{ from having proven }\\mathcal P(2)\\textit{ and }\\mathcal P(2)\\to\\mathcal P(3).\\\\\\text{And so forth, and so on,}\\textit{ et cetera,}\\ldots}$$\n",
    "tags": [
      "proof-writing",
      "induction",
      "proof-explanation",
      "fake-proofs"
    ],
    "score": 22,
    "answer_score": 34,
    "is_accepted": true,
    "question_id": 2738115,
    "answer_id": 2738211
  },
  {
    "theorem": "How many faces of a solid can one &quot;see&quot;?",
    "context": "What is the maximum number of faces of totally convex solid that one can \"see\" from a point? \n...and, more importantly, how can I ask this question better? (I'm a college student with little experience in asking well formed questions, much less answering them.) \nBy \"see\" I mean something like this: you point a camera from a point at the solid, and look at the picture. How many of the faces of the solid look like faces and not just lines? Let's assume that the lens is just a point in space (no lenses wider than the solid itself) and that the camera is a finite distance from the solid. I know this is a crude definition... if you have any ideas for a more rigorous definition, this would be awesome, then maybe there's ways to prove the answer to my question mathematically. \nFor example, in the picture of this cube, you can see 3 faces. This is the maximum you can see for a cube. How can that be proved? \nWhat methods might you use to prove this for a convex solid of any size and shape? Are there ways to do so using only relatively basic math (Multivariable calc, linear algebra, high school geometry)? \n",
    "proof": "You can always find a place from which to see at least half the faces.\nTo see why, start by considering a polyhedron with central symmetry. Imagine \na viewpoint from which you don't see any lines as points or faces as lines (i.e. general position) and far enough away so that you can see all the faces whose normal points into your side of the half plane perpendicular to your line of sight. Then think about what you see from far enough away in the opposite direction. You can see all the faces from one side or the other and no face from both sides, so the symmetry says you see half each time.\nFour of the five regular polyhedra have a center of symmetry. The tetrahedron does not: there's no place to put the origin that allows invariance under the map $x \\to -x$.\nEven without central symmetry, you see all the faces from one side or the other, so you see at least half from at least one side. Pyramids represent an extreme case. You can see all but one face from one direction and just one from the other, as @almagest points out in a comment.\nSince the polyhedron has only finitely many faces, \"far enough away\" in the preceding proof does not have to be at infinity (though it may be pretty far). As @JohhHughes comments, if you put your camera close enough to any face that's the only face you'll see.\nNote: the arguments work in all dimensions. They are particularly easy to visualize in the plane. (On the line they're trivial.)\n",
    "tags": [
      "geometry",
      "proof-writing",
      "platonic-solids"
    ],
    "score": 22,
    "answer_score": 8,
    "is_accepted": true,
    "question_id": 1749730,
    "answer_id": 1749751
  },
  {
    "theorem": "Is there such a thing as a mathematical thesaurus?",
    "context": "I want this for two reasons:\n\nWhen writing proofs, I am constantly in need of synonyms of basic words like thus, there exists, for all, such as, contains, etc.\nA lot of mathematical concepts have multiple names, e.g. abelianization vs derived quotient, vanishes vs goes to $0$, and so on.\n\nA big list of either of these types of synonyms would help improve the flow of my mathematical writing.  Also, the latter case would familiarize me with alternative terminology I may run into later.\nDoes anything like this exist?\n",
    "proof": "There is a very comprehensive (at least very long!) \"Handbook of Mathematical Discourse\" which you can find here.\nUnder the various topics they list some synonyms, but I can't vouch for its completeness or accuracy. Also, it is more of a dictionary or encyclopedia than a pure thesaurus.\nEDIT: Another question on Math.SE asks a similar question and one of the answers points to this online encyclopedia. It seems, however, that this one might be of limited use as a thesaurus. After a few tries it looks like synonyms are rarely included. \n",
    "tags": [
      "reference-request",
      "terminology",
      "proof-writing"
    ],
    "score": 22,
    "answer_score": 10,
    "is_accepted": false,
    "question_id": 354586,
    "answer_id": 354591
  },
  {
    "theorem": "Question from Putnam &#39;89: Primes of the form $101\\ldots01$",
    "context": "I'm not a math major, but would like to compete in the Putnam.  As suggested in other questions here, I'm working some old contest problems.  I'd like some input on this attempted proof--general input is gladly welcomed, but I do have some specific questions (see bottom of post.)\n\nPrompt:\nHow many primes among the positive integers, written as usual in the base 10, are such that their digits are alternating 1's and 0's, beginning and ending with 1?\nSource: http://www.math.ksu.edu//events/ksucomp/putnam/examquestions.htm\n\nAnswer: There is only one such prime, namely, $101$.\nProof:\nConsider the numbers of the form $101\\ldots01$.  Let $a_n$ be the number of this form with $n$ ones in its base-10 representation.  That is:\n$$a_n = \\sum_{k=0}^{n-1}10^{2k} \\tag{1}$$\nIt follows:\n$$\\begin{align}\na_n &= \\sum_{k=0}^{n-1}\\left(10^2\\right)^k \\\\\n&= \\frac{\\left(10^2\\right)^n-1}{10^2-1} \\\\\n&= \\frac{(10^n)^2-1}{99} \\\\\n&= \\frac{(10^n-1)(10^n+1)}{9\\cdot 11}\n\\end{align}$$\nWe now note that $10^n-1=9\\sum_{k=0}^{n-1}10^k$.  Thus:\n$$a_n = \\frac{\\left(\\sum_{k=0}^{n-1}10^k\\right)(10^n+1)}{11} \\tag{2}$$\nLet $b_n$ be the numerator in $(2)$; that is:\n$$b_n = \\left(\\sum_{k=0}^{n-1}10^k\\right)(10^n+1)$$\nFrom formula $(1)$, we can see that $a_n$ is the sum of integers, thus, $a_n$ is clearly an integer.  For $a_n$ to be a prime, $b_n$ must satisfy the following: \n$$b_n = 11\\cdot p\\text{, where $p$ is prime.}$$\nTherefore, one of either $\\left(\\sum_{k=0}^{n-1}10^k\\right)$ or $(10^n+1)$ must be $11$, and the other is prime.  For $n=1$, we see:\n$$\\left(\\sum_{k=0}^{n-1}10^k\\right) = 1;\\qquad(10^n+1)=11$$\nFor $n=2$, we see:\n$$\\left(\\sum_{k=0}^{n-1}10^k\\right) = 11;\\qquad(10^n+1)=101$$\nAs both terms are monotonically increasing, these two are the only ways to have either one equal to $11$.  Of these two, only one pair contains a prime (namely, $101$).\n\nMy questions:\nFirst and foremost, I want to know if my proof is valid/on the right track. :)  After that, I'd like to know if I'm covering the proof in enough detail.  Specifically:\n\nI assert that $10^n-1=9\\sum_{k=0}^{n-1}10^k$.  Do I need to prove this, or is it obvious enough?  (Subtracting $1$ from a power of $10$ leaves a number with as many $9$s as there are $0$'s in the power of $10$.)\nDo I need to show further why $b_n$ must be $11$ times a prime?  Or is it clear that all other cases yield composite $a_n$?\nIs my reasoning that there are only these two solutions (i.e. monotonically increasing terms) legitimate?  \n\nAnd, finally, is there some well-known theorem that I have totally overlooked that would make solving this problem easy?\n",
    "proof": "You can actually cut your proof short a fair bit earlier — once you have $a_n=(10^n+1)(10^n-1)/99$, you can note that for $n\\gt 2$, both of these factors are larger than $2\\times 99$ and so there must be a factorization of $a_n$ into two numbers each larger than $2$.\nMore formally, you can split into the case of even $n$ and odd $n$; for even $n$ then $99|10^n-1$ so you have $a_n=(10^n+1)\\times \\left((10^n-1)/99\\right)$ with each factor $\\gt 1$, while for odd $n$ $11| 10^n+1$ and $9|10^n-1$ so $a_n=\\left((10^n+1)/11\\right)\\times\\left((10^n-1)/9\\right)$ with each factor $\\gt 1$.  (These divisibility properties are immediate from $10\\equiv 1\\pmod 9$ and $10\\equiv -1\\pmod{11}$).  This also explains why the proof breaks down at $n=2$; there, you still have $a_n=(10^n+1)\\times\\left((10^n-1)/99\\right)$, but it's no longer the case that the right factor is $\\gt 1$.\n",
    "tags": [
      "elementary-number-theory",
      "prime-numbers",
      "proof-writing",
      "contest-math",
      "proof-verification"
    ],
    "score": 22,
    "answer_score": 6,
    "is_accepted": true,
    "question_id": 479500,
    "answer_id": 479506
  },
  {
    "theorem": "What is wrong with my proof by contradiction?",
    "context": "\nThere exist no integers $a$ and $b$ for which $18a+6b=1$.\n\nProof: Assume that $18a+6b=1$. We find that\n$$6(3a + b)=1$$\nwhich leads to\n$$3a+b=\\frac16$$\nWe know that the sum of two integers can't produce a non-integer result, therefore a contradiction arises, as the proof demonstrates that two integers can produce a non-integer value. $\\blacksquare$\nMy professor said that if one ends up with fractions in a proof, there is likely a problem. Can someone explain why this is the case?\n",
    "proof": "I'm assuming you're in a number theory class or abstract algebra. At this level of such a course, we haven't formally reintroduced $\\Bbb Q $ so fractions don't formally exist yet.\nWe have multiplication and addition. And we have the integers.\nThe better proof is to show that $\\gcd(18,6)=6$ and hence that the smallest positive linear combination of $18$ and $6$ we can make is $6$.\n",
    "tags": [
      "elementary-number-theory",
      "proof-verification",
      "proof-writing"
    ],
    "score": 21,
    "answer_score": 28,
    "is_accepted": false,
    "question_id": 1962510,
    "answer_id": 1962530
  },
  {
    "theorem": "prove $x \\mapsto x^2$ is continuous",
    "context": "I am to show the continuity of this function with the help of $\\epsilon$-$\\delta$ argument. \nThe function is: $g: \\Bbb{R} \\rightarrow \\Bbb{R}$, $x \\mapsto x^2$. \nGiven the $\\epsilon$-$\\delta$ definition of limit, I tried as follows: \nWe must have: $|f(x)-f(x_0)|<\\epsilon$, so then $|x^2-x_0^2|=|(x-x_0)(x+x_0)|=|(x-x_0)||(x+x_0)|$, so $|x-x_0|<\\frac{\\epsilon}{|x+x_0|}$. So now I will choose $\\delta=\\frac{\\epsilon}{|x+x_0|}$. \nIs it enough? I am writing this sentences like a machine, but I am not understanding intuitively.\n",
    "proof": "No. $\\delta$ must only depend on $x_0,\\epsilon$ and never on $x$. Here is what we will do:\nThe problematic term is $\\left|x+x_0\\right|$. We have that \n\\begin{equation} \\left|x+x_0\\right|= \\left|x-x_0+2x_0\\right|\\le \\left|x-x_0\\right|+\\left|2x_0\\right|<\\delta+2\\left|x_0\\right|\\end{equation}\nThus, \n\\begin{equation}\\left|x-x_0\\right|<\\delta\\implies \\left|f(x)-f(x_0)\\right|=\\left|x+x_0\\right|\\left|x-x_0\\right|<(\\delta+2\\left|x_0\\right|)\\delta\\end{equation}\nWe must choose a $\\delta$ so that\n$$(\\delta+2\\left|x_0\\right|)\\delta<\\epsilon$$\nChoosing a $\\delta$ by the above might be complicating. But because $\\delta$ is ours to choose we can simplify things a bit by demanding $\\delta<1$. Then,\n$$(\\delta+2\\left|x_0\\right|)\\delta<(1+2\\left|x_0\\right|)\\delta$$\nand so it suffices to choose $0<\\delta<1$ so that\n$$(1+2\\left|x_0\\right|)\\delta<\\epsilon$$\nThings should be straighforward now.\nFor the sake of completion we have \n$$(1+2\\left|x_0\\right|)\\delta<\\epsilon\\iff \\delta<\\frac{\\epsilon}{1+2\\left|x_0\\right|}$$\nBut because $\\delta<1$ we must choose $\\delta>0$ so that\n$$\\delta<\\min\\left\\{1,\\frac{\\epsilon}{1+2\\left|x_0\\right|}\\right\\}$$\nTaking \n$$\\delta=\\frac12\\min\\left\\{1,\\frac{\\epsilon}{1+2\\left|x_0\\right|}\\right\\}$$\ncompletes the proof\n",
    "tags": [
      "functions",
      "proof-writing",
      "continuity"
    ],
    "score": 21,
    "answer_score": 59,
    "is_accepted": true,
    "question_id": 264081,
    "answer_id": 264083
  },
  {
    "theorem": "How to make sure a proof is correct",
    "context": "If you come up with a proof of a mathematical proposition, how do you verify the proof is correct?\nPut it another way, how do you avoid a wrong proof?\nI guess there is no definitive answer to this.\nHowever, I believe this question is important.\nAny idea, suggestion or method to help make sure a proof is correct would be appreciated.\nEDIT\nThanks for all your suggestions.\nI'd like to add my idea which is perhaps similar to Leslie Lamport's (I haven't read his paper yet).\nMy idea is basically \"divide and conquer\".\nDivide your proof to small propositions or lemmas.\nThe smaller, the better.\nIdeally each of these small proposition should be trivial.\nTo do this, first divide the main theorem into several propositions.\nThe main theorem should be almost trivial assuming each proposition is correct.\nThen divide each proposition into several propositions.\nRepeat this process until each proposition cannot be divided or trivial enough.\nThen apply some or all of your ideas to each proposition.\nGeneralizing your theorem can help make this process easier(there is an adage(?): if your problem is difficult, generalize it). \nI got this idea from the Grothendieck's approach in his EGA.\nSee the article \"The rising sea; Grothendieck on simplicity and generality\" by Colin McLarty.\n",
    "proof": "Leslie Lamport's paper on \"How to Write a Proof\" (PDF) offers some useful advice.\nLamport suggests structuring the proof formally, with each statement accompanied by its own sub-proof in terms of simpler claims. How far should one continue this refinement?  Lamport says: \"My own rule of thumb is to expand the proof until the lowest level statements are obvious, and then continue for one more level. This takes discipline.\"\n",
    "tags": [
      "soft-question",
      "proof-writing"
    ],
    "score": 21,
    "answer_score": 9,
    "is_accepted": false,
    "question_id": 154116,
    "answer_id": 154235
  },
  {
    "theorem": "Proofs from the &quot;Ugly Book&quot;",
    "context": "There is a famous saying in mathematics from Paul Erdős: \"You don't have to believe in God, but you should believe in The Book.\" \"The Book\" is an imaginary book in which God had written down the best and most elegant proofs for mathematical theorems.\nIf there is a book written down by God, why not a book from the Devil? I mean, a book with the most ugly proofs, but yet the best ones we have as accepted proofs. I don't mean to make a horrible proof on purpose, but sometimes ugly proofs is all you have.\nI wish if you could share some theorem from the Ugly Book, some theorem proved by a real ugly proof (and yet the only one that there is). I'm asking this not for fun only, but I'm curious about how ugly proofs can be.\n",
    "proof": "Beauty is quite subjective. One may prove something in 2 lines using the newly developed supersymmetric coffee spaces, and that may be cool to some. Suppose you have another proof of the same result that uses only some primitive set of axioms. This proof may potentially be 1000 pages long, but it will be more beautiful to some (for example me), as it is a demonstration of the fact that all that mind blogging complexity is actually the result of addition, multiplication, etc, and some first order logic.\n",
    "tags": [
      "proof-writing",
      "philosophy"
    ],
    "score": 21,
    "answer_score": 5,
    "is_accepted": true,
    "question_id": 433251,
    "answer_id": 433260
  },
  {
    "theorem": "Why universal G-bundles are contractible?",
    "context": "Let $G$ be a nice topological group and $E\\to B$ a universal $G$-bundle. I'm interested in a proof of contractibility of $E$ using only the universal property of it. I also know that if there is a contractible $G$-bundle, then all of others are also contractible, but does there exist a direct proof not using a special construction of contractible universal $G$-bundles?\n",
    "proof": "Let $p:E\\to B$ be any $G$-bundle, and let's assume for simplicity that $E, B$ and $G$ are path-connected (this simplifies the use of the long exact sequence of homotopy groups). Let $\\text{Bun}_X(G)$ denote the set of isomorphism classes of $G$-bundles over $X$. Fix basepoints $b_0\\in B, e_0\\in p^{-1}(b_0), x_0\\in S^n, d_0\\in \\partial D^n$.\nRecall the connecting homomorphisms from the long exact sequence of homotopy groups for the bundle $E$:\n$$\\partial_n:\\pi_n(B,b_0)\\to \\pi_{n-1}(G)$$\nWe have also have a map\n$$\\beta_n:\\pi_n(B,b_0)\\to \\text{Bun}_{S^n}(G)$$\nwhich takes the homotopy class of a map $f:(S^n,x_0)\\to (B,b_0)$ to the isomorphism class of $f^\\ast E$. Next, we will define a bijection\n$$\\gamma_n:\\text{Bun}_{S^n}(G)\\to \\pi_{n-1}(G)$$\nand show that\n$$\\partial_n=\\gamma_n\\beta_n.$$\nTo see how this helps, suppose $E$ were a universal $G$-bundle. Hence $\\beta_n$ is a bijection (see edit for clarification), and consequently $\\partial_n$ is also a bijection (since $\\partial_n=\\gamma_n\\beta_n$). The long exact sequence of homotopy groups for the bundle $E$ then implies that $E$ is weakly contractible (and hence contractible in the case that it is a CW complex). Furthermore, this line of reasoning together with the fact that contractibility implies universality (see theorem 2.5 in the note linked to by @ah--) also shows that a $G$-bundle which is a CW complex is universal if and only if it is universal with respect to the spheres.\nNow we construct $\\gamma_n$. Let $(E',p')$ be a $G$-bundle over $S^n$ and fix a basepoint $e'_0\\in p'^{-1}(x_0)$. Let $q:(D^n,\\partial D^n)\\to (S^n,x_0)$ be a quotient map. The pullback $q^\\ast E'$ is trivial (since $D^n$ is contractible), so we have a section $s:D^n\\to q^\\ast E'$. Let $t:\\partial D^n\\to G$ be such that\n$$s(d)=e'_0\\cdot t(d)\\,\\,\\forall\\,d\\in\\partial D^n$$\nWe now define $\\gamma_n$ on the isomorphism class of $(E',p')$ to be the homoptopy class of $t$ in $\\pi_{n-1}(G)$. It is a standard result that $\\gamma_n$ is well-defined. To show that $\\gamma_n$ is a bijection, we reverse the above construction. Given $t:\\partial D^n\\to G$, construct a $G$-bundle over $S^n$ by identifying $(d,g)\\in \\partial D^n\\times G$ with $t(d)g\\in G$ in the space $(D^n\\times G)\\sqcup G$. The $G$-action is the obvious one and a projection onto $S^n$ is constructed using $q$.\nTo see that $\\partial_n=\\gamma_n\\beta_n$, let $f:(S^n,x_0)\\to (B,b_0)$. Let $E'=f^\\ast E\\subset S^n\\times E$ in the above construction and take $e_0'=(x_0,e_0)$. Let $\\tilde{f}:(D^n,\\partial D^n,d_0)\\to (E,p^{-1}(b_0),e_0)$ be a lift of $fq$ and set $\\partial\\tilde{f}=\\tilde{f}|_{\\partial D^n}$. Hence $\\partial_n[f]=[\\partial\\tilde{f}]$ by definition of $\\partial_n$ (here we identify $p^{-1}(b_0)$ with $G$ as $e_0\\cdot g\\sim g$). The section $s$ of $q^\\ast E'\\subset D^n\\times E'$ required in the construction of $\\gamma_n(E')$ can be taken to be\n$$d\\mapsto \\left(d,q(d),\\tilde{f}(d)\\right)\\in D^n\\times S^n\\times E.$$\nIt can now be seen that the map $t$ obtained from the construction of $\\gamma_n$ is precisely $\\partial\\tilde{f}$, which completes the proof.\nP.S. Sorry for reviving an (very) old question.\nEdit: When $E$ is universal, we know that the pullback construction gives a bijection\n$$[S^n, B]\\to \\text{Bun}_{S^n}(G)$$\nHowever, this doesn't immediately imply that $\\beta_n$ is a bijection since the map\n$$\\pi_n(B,b_0)\\to [S^n,B]$$\nis a bijection if and only if $\\pi_1(B,b_0)$ acts trivially on $\\pi_n(B,b_0)$. However, in our case we can deduce that $\\pi_1(B,b_0)$ is trivial (and hence $\\beta_n$ is a bijection) as follows. $\\text{Bun}_{S^1}(G)$ is trivial (since $G$ is path-connected and $\\gamma_1$ is a bijection), so $[S^1,B]$ is trivial (since $E$ is universal). Hence $\\pi_1(B,b_0)$ is also trivial.\n",
    "tags": [
      "algebraic-topology",
      "proof-writing"
    ],
    "score": 21,
    "answer_score": 1,
    "is_accepted": false,
    "question_id": 1301061,
    "answer_id": 4944221
  },
  {
    "theorem": "How to prove convergence of polynomials in $e$ (Euler&#39;s number)",
    "context": "These polynomials in $e$ converge to 2$$f(i)=e^i - i \\sum_{k=1}^{i-1}\\frac{(i-k)^{k-1}{e^{i-k}}{(-1)^{k+1}}}{k!}, \\text{ where } i>1$$ \nThis function goes to 2. I've calculated this with sage math tool.\n$$f(\\infty) = 2$$\nfor example,\n$$f(2)=e^2-2e=1.95249244... $$\n$$f(3)=e^3-3e^2+\\frac{3}2e=1.99579136... $$\n$$f(4)=e^4-4e^3+4e^2-\\frac{2}3e=2.000038... $$\n$$...$$\n$$f(10)=\\newcommand{\\Bold}[1]{\\mathbf{#1}}-\\frac{1}{36288} \\, e + \\frac{2}{63} \\, e^{2} - \\frac{81}{56} \\, e^{3} + \\frac{128}{9} \\, e^{4} - \\frac{625}{12} \\, e^{5} + 90 \\, e^{6} - \\frac{245}{3} \\, e^{7} + 40 \\, e^{8} - 10 \\, e^{9} + e^{10} = 2.00000000...$$\n\nIsn't this interesting?\nThese polynomials in e (2.71828182845904523536...) converge to 2.\nHowever, I have no idea how to mathematically prove this.  I guess this would have been already proved, but I have no idea where I can find the proof.\nI would greatly appreciate it if you can give me some tips or the proof of this convergence.\n\nFor more information, this function $f(i)$ is from a different function $h_i(x),\\text{ when } x = 1$ of an original problem\n$$ f(i) = h_i(1)$$\nso proving the convergence of the above polynominals will be the same as proving $h_\\infty(1) = 2$\nI have recently found that the general form of $h_i(x)$ function \n$$h_i(x) = (-1)^{i+1} e^x \\left[\\frac{1}{(i-1)!}{x}^{i-1} - \\sum_{k=1}^{i-1}\\left(α(k)\\frac{{x}^{i-1-k}(-1)^{k+1}}{(i-1-k)!}\\right)\\right] - α(i-1),$$\nwhere \n$$α(j) = \\sum_{k=0}^{j-1}\\frac{(j-k)^k}{k!}e^{j-k}(-1)^k$$\nupdate list \n\nI have found this: $ f(i)=α(i)−α(i−1) $\nI have found a new property of $α(i)$, when $i > 1$, \n$$ α(i) = \\sum_{k=0}^{i-2} \\left( e \\space α(k+1)\\space\\frac{(-1)^{i+k+2}}{(i-k-2)!} \\right) + \\frac{(-1)^{i+1} \\space e}{(i-1)!},$$\nwhere $α(1) = e$.\n\n",
    "proof": "The convergence to $2$ is explained by my answer to the original question.\n",
    "tags": [
      "sequences-and-series",
      "convergence-divergence",
      "power-series",
      "proof-writing"
    ],
    "score": 21,
    "answer_score": 1,
    "is_accepted": true,
    "question_id": 170749,
    "answer_id": 177204
  },
  {
    "theorem": "How are proofs formatted when the answer is a counterexample?",
    "context": "Suppose it is asked:\n\nProve or find a counterexample: the sum of two integers is odd\n\nThe fact that 1 + 1 = 2 is a counterexample that disproves that statement. What is the proper format in which to write this? I will provide my attempt.\n\nTheorem: the sum of two integers is odd\nProof:\n1 + 1 = 2\n=> the sum of two integers may be even\n\nI understand the math, but I'm not sure how to properly present a counterexample relative to an initial theorem. Was this an acceptable presentation of a proof?\n",
    "proof": "I would say something along the lines of: \nThe proposed result is false.  Here is a counterexample...\n",
    "tags": [
      "proof-writing",
      "examples-counterexamples"
    ],
    "score": 20,
    "answer_score": 31,
    "is_accepted": true,
    "question_id": 1852612,
    "answer_id": 1852615
  },
  {
    "theorem": "Beautiful, simple proofs worthy of writing on this beautiful glass door",
    "context": "What are some of the more beautiful proofs you know? I am measuring beauty in two dimensions -- first, how conceptually elegant is it and second, how aesthetically pleasing is it. \nContext: \nI work at a econ consulting firm. We're mostly math majors or very quantitative econ majors. A buddy and I are trying to decide what to write on the glass door to the office we share. Currently it has a graph of quality of Brad Pitt's movies against how frequently he was shirtless in that movie. Time to upgrade that... \n",
    "proof": "Barak beat me to my #1 choice. This would be second:\n\n",
    "tags": [
      "soft-question",
      "proof-writing",
      "recreational-mathematics"
    ],
    "score": 20,
    "answer_score": 15,
    "is_accepted": true,
    "question_id": 911058,
    "answer_id": 911077
  },
  {
    "theorem": "Can an algorithm be part of a proof?",
    "context": "I am an undergraduate student. I have read several papers in graph theory and found something may be strange: an algorithm is part of a proof. In the paper, except the last two sentences, other sentences describe a labeling algorithm.\nCan an algorithm be part of a proof? I do not understand why it can be part of a proof. I asked my supervisor but he did not explain it.\n\nLEMMA$\\,\\,\\,\\bf 1.$ If $B(G)-b$, then $B(G+e)<2b$.  Proof:\n  Let $f$ be an optimal numbering of $G$, and let $V(G)-\\{v_1,\\ldots,v_n\\}$, numbered so that $f(v_i)-i$. Let $v_lv_m$ be the added edge. We define a new numbering $f'$ such that $|f'(x)-f'(y)|<2|f(x)-f(y)|$, and also $|f'(v_l)-f'(v_m)|-1$. Let $r-\\lfloor(l+m)/2\\rfloor$, and set $f'(v_r)-1$ and $f'(v_{r+1})-2$. For every other $v_i$ such that $|i-r|<\\min\\{r-1,n-r\\}$, let $f'(v_i)-f'(v_{i+1})+2$ if $i<r$ and $f'(v_i)-f'(v_{i-1})+2$ if $i>r$. This defines $f'$ for all vertices except a set of the form $v_i,\\ldots,v_k$ or $v_{n+1-k},\\ldots,v_n$, depending on the sign of $r-\\lfloor(n+1)/2\\rfloor$. In the first case, we assign $f'(v_i)-n+1-i$ for $i<k$; in the second, we assign $f'(v_i)-i$ for $i>n-k$. The renumbering $f'$ numbers the vertices outward from $v_r$ to achieve $|f'(x)-f'(y)|<2|f(x)-f(y)|$. Since we begin midway between $v_l$ and $v_m$, we also have $|f'(v_l)-f'(v_m)|-1$.\n\n",
    "proof": "Yep! Proofs sometimes involve elaborate calculations: presenting an algorithm to do the calculation is a good way to show that the calculation can be done, and to analyze the results of the calculation.\nIn some fields -- e.g. graph theory -- algorithms are used very frequently. Often, the easiest way to prove that something can be done is to actually write down an algorithm that does it, and prove the algorithm works.\nYou might think of this approach as a systematic and convenient way to develop and organize complicated inductive proofs. But I would actually think of it the other way: induction is just a rather simplistic kind of algorithm for generating a proof. i.e. the algorithm is \"use the argument I called the \"base case\" to prove it for one $1$, then iteratively apply the argument I called the \"inductive step\" to prove it for each successive positive integer\".\n",
    "tags": [
      "graph-theory",
      "proof-writing"
    ],
    "score": 20,
    "answer_score": 26,
    "is_accepted": true,
    "question_id": 587650,
    "answer_id": 587658
  },
  {
    "theorem": "Problems understanding proof of if $x + y = x + z$ then $y = z$ (Baby Rudin, Chapter 1, Proposition 1.14)",
    "context": "I'm having trouble with whether Rudin actually proves what he's tried to prove.\nProposition 1.14; (page 6)\nThe axioms of addition imply the following statements:\na) if $x + y = x + z$ then $y = z$\nThe author's proof is as follows:\n$ y = (0 + y) = (x + -x) + y = -x + (x + \\textbf{y})$\n$$ = -x + (x + \\textbf{z}) = (-x + x) + z = (0 + z) = z $$\nI emphased the section which troubles me. \nHow does Rudin prove that $ y = z $ if he substituted $y = z$?\n",
    "proof": "He didn't substitute $z$ for $y$; rather, he substituted $x+z$ for $x+y$. This is legitimate based on the assumption that $x+y = x+z$.\n",
    "tags": [
      "real-analysis",
      "proof-writing",
      "proof-explanation"
    ],
    "score": 20,
    "answer_score": 53,
    "is_accepted": true,
    "question_id": 2131633,
    "answer_id": 2131635
  },
  {
    "theorem": "Proof variance of Geometric Distribution",
    "context": "I have a Geometric Distribution, where the stochastic variable $X$ represents the number of failures before the first success.\nThe distribution function is $P(X=x) = q^x p$ for $x=0,1,2,\\ldots$ and $q = 1-p$.\nNow, I know the definition of the expected value is: $E[X] = \\sum_{i}{x_i p_i}$\nSo, I proved the expected value of the Geometric Distribution like this:\n$E[X]=\\sum _{ i=0 }^{ \\infty  }{ iP(X=i) } = \\sum _{i=0}^{\\infty}{i q^i p} = p\\sum _{i=0}^{\\infty}{i q^i} = pq \\sum _{i=0}^{\\infty}{iq^{i-1}}$\n$\\qquad = pq \\sum _{i=0}^{\\infty}{\\frac{d}{dq}q^i} = pq \\frac{d}{dq}(\\sum _{i=0}^{\\infty}{q^i}) = pq \\frac{d}{dq}(\\frac{1}{1-q})$\n$\\qquad = pq \\frac{1}{(1-q)^2} = \\frac{pq}{p^2} = \\frac{q}{p}$\nSo now, I would like to prove that $Var[X] = \\frac{q}{p^2}$. I know I have to use a simular trick as above (with the derivation).\n$Var[X] = E[X^2] - E[X]^2 = \\sum _{i=0}^{\\infty}{i^2 q^i p} - (\\frac{q}{p})^2 = p \\sum _{i=0}^{\\infty}{i^2 q^i} - (\\frac{q}{p})^2 = pq \\sum _{i=0}^{\\infty}{i^2 q^{i-1}} - (\\frac{q}{p})^2$\n$\\qquad = pq \\sum _{i=0}^{\\infty}{\\frac{d}{dq}i q^i} - (\\frac{q}{p})^2 = pq \\frac{d}{dq} \\sum _{i=0}^{\\infty}{iq^i}-(\\frac{q}{p})^2$\nThen I'm stuck. How can I get another $q$ out of the sum? Won't it mess up the first derivation?\n",
    "proof": "My proof is similar to @Math1000's but is useful if you're not familiar with generating functions.\nHowever, I'm using the variant of the geometric distribution where $X$ is the number of trials until success. Therefore $E[X]=\\frac{1}{p}$. (Both variants have the same variance).\nWe know $E[X]=\\frac{1}{p}$. Then the variance is:\n$$\nE[X^2]-(E[X])^2=\\boxed{E[X(X-1)]} + E[X] -(E[X])^2 = \\boxed{E[X(X-1)]} + \\frac{1}{p} - \\frac{1}{p^2}\n$$\nSplit $E[X^2]$ into $E[X(X-1)]+E[X]$, which is easier to determine. To determine $\\boxed{E[X(X-1)]}$ we have to determine the value of the following series for $p\\in(0,1)$:\n$$\n\\sum_{k=1}^\\infty k(k-1)p(1-p)^{k-1}\n$$\nHere's how it can be done (as an alternative to Math1000's approach):\n$$\n\\begin{align}\n\\sum_{k=1}^\\infty k(k-1)p(1-p)^{k-1}\n&=\np\\sum_{k=1}^\\infty k(k-1)(1-p)^{k-1}\n\\qquad\\text{Subst. }q:=(1-p)\\\\\\\\\n&=\np\\sum_{k=1}^\\infty (k-1)kq^{k-1}\n\\\\\\\\\n&=\np\\frac{d}{dq}\\left(\\sum_{k=1}^\\infty (k-1)q^k\\right)\n\\\\\\\\\n&=\np\\frac{d}{dq}\\left(q^2\\sum_{k=1}^\\infty (k-1)q^{k-2}\\right)\n\\\\\\\\\n&=\np\\frac{d}{dq}\\left(q^2\\sum_{k=2}^\\infty (k-1)q^{k-2}\\right)\n\\\\\\\\\n&=\np\\frac{d}{dq}\\left(q^2\\frac{d}{dq}\\left(\\sum_{k=2}^\\infty q^{k-1}\\right)\\right)\n\\\\\\\\\n&=\np\\frac{d}{dq}\\left(q^2\\frac{d}{dq}\\left(\\sum_{k=1}^\\infty q^{k}\\right)\\right)\n\\\\\\\\\n&=\np\\frac{d}{dq}\\left(q^2\\frac{d}{dq}\\left(\\frac{1}{1-q}-1\\right)\\right)\n\\\\\\\\\n&=\np\\frac{d}{dq}\\left(\\frac{q^2}{(1-q)^2}\\right)\n\\\\\\\\\n&=\np\\left(\\frac{-2q}{(q-1)^3}\\right)\\qquad\\text{Backsub. }q=(1-p)\n\\\\\\\\\n&=\np\\left(\\frac{-2(1-p)}{((1-p)-1)^3}\\right)\n=\np\\left(\\frac{-2+2p}{-p^3}\\right)\n\\\\\\\\\n&=\n\\frac{-2+2p}{-p^2}\n=\\frac{2(p-1)}{-p^2}\n=\n\\frac{2(1-p)}{p^2}.\n\\\\\\\\\n\\end{align}\n$$\nNow putting the result back into the equation for $Var[X]$ gives us:\n$$\nVar[X]=\\boxed{E[X(X-1)]} + E[X] -(E[X])^2 =\\frac{2(1-p)}{p^2} + \\frac{1}{p} - \\frac{1}{p^2} = \\frac{2-2p+p-1}{p^2} = \\frac{1-p}{p^2}.\n$$\n",
    "tags": [
      "statistics",
      "proof-writing"
    ],
    "score": 20,
    "answer_score": 31,
    "is_accepted": false,
    "question_id": 1299465,
    "answer_id": 1616749
  },
  {
    "theorem": "Proving a theorem, what is meant by sufficiency and necessity?",
    "context": "I am looking at the proof of a theorem and the proof begins by saying \n\n...is the proof of the sufficiency part of this theorem so we just need to establish the necessity of the condition.\n\nWhat is the sufficiency part and the necessity part of the theorem?\n",
    "proof": "It is essentially a biconditional, also known as an if and only if.\nAn \"if and only if\" statement goes both ways. That is, $p\\iff q$ means \"if $p$ is true then $q$ is true\" and \"if $q$ is true then $p$ is true.\"\nThe statement \"$p$ is sufficient for $q$\" means \"if $p$ is true, then $q$ is true.\"\nThe statement \"$p$ is necessary for $q$\" means that if we don't have $p$, then we don't have $q$. Therefore, if we have $q$, we certainly have $p$. In other words, \"$q$ implies $p$.\"\nWhen we put the two together, a necessary and sufficient condition is the same as an if and only if.\n",
    "tags": [
      "proof-writing",
      "terminology"
    ],
    "score": 20,
    "answer_score": 20,
    "is_accepted": true,
    "question_id": 1856166,
    "answer_id": 1856172
  },
  {
    "theorem": "proving that the area of a 2016 sided polygon is an even integer",
    "context": "\nLet $P$ be a $2016$ sided polygon with all its adjacent sides perpendicular to each other, i.e., all its internal angles are either $90$°or $270$°. If the lengths of its sides are odd integers, prove that its area is an even integer.\n\nI think visualising what this polygon might look like would more or less be the key to getting started on the problem, but I'm having quite a hard time doing so. Since the internal angles are all $90$° or $270$°, I think the shape would look like a rectangle or a square with smaller squares or rectangles protruding out from each side — but mini rectangles protruding from sides would not be possible because degrees of $180$° are not allowed. And to prove that the area is an even integer, if there are mini squares protruding from the sides there will have to be even number of those squares in total. And then I also need to make sure that at least the width or length of 'bigger' square or rectangle is even. But I seem to get stuck trying to draw a possible diagram...\n",
    "proof": "Not a complete proof.\nHowever, maybe you can visualize the polygon like this:\n\nAll the internal angles are either $90^\\circ$ or $270^\\circ$.\n\nAnother way of visualizing:\n\nObserve that you have a $12$-sided polygon here, comprised of $5$ squares. The next acceptable polygon would be this:\n\nNotice that $2$ sides are lost and $6$ more sides are added, thus we have a $16$-sided polygon. Notice too that there are $3$ more additional squares.\nTherefore, for a polygon in this form, the number sides can be expressed as:\n$$12+4x \\Rightarrow 12+4x=2016\\iff x=501$$\nUsing $x$, the number of squares that compose this polygon will be:\n$$5+3x\\Rightarrow 12+3\\cdot501=1508$$\nTherefore, we have an even number of squares therefore the area is an even integer.\n",
    "tags": [
      "geometry",
      "proof-writing",
      "integers",
      "polygons",
      "angle"
    ],
    "score": 20,
    "answer_score": 5,
    "is_accepted": true,
    "question_id": 2824459,
    "answer_id": 2824472
  },
  {
    "theorem": "Show that if $g \\circ f$ is injective, then so is $f$.",
    "context": "The Problem:\nLet $X, Y, Z$ be sets and $f: X \\to Y, g:Y \\to Z$ be functions.\n(a) Show that if $g \\circ f$ is injective, then so is $f$.\n(b) If $g \\circ f$ is surjective, must $g$ be surjective?\nWhere I Am:\nSo, I really have trouble with these, for some reason. I can draw pictures and make sense of the problems, but writing down proofs is very difficult for me. \nBasically, for (a), I ended up with some complicated statement involving an implication implying another implication and then tried to derive a contradiction. It just got so convoluted that I couldn't make sense of it anymore, and I know there's a quick, elegant way to show it.\nFor (b), I know that $g$ need not be surjective. Once again, though, proving it directly from definitions has given me a bit of a headache.\nAny help here would be appreciated. Thanks in advance.\nThe Proofs!\nOk, I did it. Thanks for the help, everyone! Let me know if there's anything wrong with these proofs, or if they could bet any better.\n(a) Suppose $f$ is not injective. Then\n$$ f(x_1)=f(x_2) \\implies x_1 \\ne x_2 \\text{        }(*).$$\nLet $f(x_1)=y_0=f(x_2)$ and let $g(y_0)=z_0$. Then $$ (g \\circ f)(x_1) = (g \\circ f)(x_2) = g(y_0) = z_0. $$\nSince $g \\circ f$ is injective,\n$$ (g \\circ f)(x_1) = (g \\circ f_2) \\implies x_1 = x_2. $$\nHowever, this contradicts $(*)$. Therefore, $f$ must be injective.\n(b) Suppose $g$ is not surjective. Then\n$$ \\forall y \\in Y, \\exists z \\in Z \\text{ such that } g(y) \\ne z \\text{         }(**).$$\nSince, $g \\circ f$ is surjective,\n$$ \\forall z \\in Z, \\exists x \\in X \\text{ such that } g(f(x)) = z \\text{       } (***). $$\nLet $f(x) = y$. Then,\n$$ g(f(x)) = g(y) = z. $$\nBecause of $(***)$, this is true for all $z \\in Z$, which contradicts $(**)$. Therefore, $g$ must be surjective.\n",
    "proof": "(a) Let $f(x_1)=f(x_2)$. Then, $g(f(x_1))=g(f(x_2))$, but since $g\\circ f$ is injective...\n(b) $g(Y)\\supseteq (g\\circ f)(X)=g(f(X))$. Hence, if $(g\\circ f)(X)=Z$...\n",
    "tags": [
      "functions",
      "discrete-mathematics",
      "proof-writing"
    ],
    "score": 20,
    "answer_score": 17,
    "is_accepted": false,
    "question_id": 1274914,
    "answer_id": 1274919
  },
  {
    "theorem": "Proving a theorem using its converse",
    "context": "Converse of Pythagoras' theorem: If the lengths of the sides of a triangle $T$ are $a$, $b$, and $c$, and if $a^2+b^2=c^2$, then the triangle is a right triangle and the side opposite to the right angle is the one whose length is $c$.\nProof: Construct a line segment $XY$ whose length is $a$. Then construct a line segment $YZ$ whose length is $b$ which is perpendicular to $XY$. By construction, the triangle $XYZ$ is a right triangle, and therefore, by Pythagoras' theorem and because we are assuming that $a^2+b^2=c^2$, the length of $XZ$ is equal to $c$. So, the triangle $XYZ$ is similar to the original triangle $T$. Since the triangle $XYZ$ is a right triangle, then so is $T$.What I find peculiar about this proof is the fact that it uses Pythagoras' theorem in order to prove its converse.\nIt is not the only situation that I am aware of in which this occurs. For instance, there is a proof of the converse of Ceva's theorem  which uses that theorem. But I am not aware of any example outside Euclidean Geometry.\n\nCan anyone provide an example of a theorem of the type $A\\implies B$ outside Geometry with a proof which uses the fact that $B\\implies A$?\n\n",
    "proof": "Say natural number $n$ is good if it can be writen as a sum of two squares.\nTheorem: $n$ is good iff $2n$ is good.\nProof: If $n=a^2+b^2$ then $2n = (a+b)^2+(a-b)^2$ and we are done.\nNow the converse. Say $2n$ is good. Then, by already proven part, also $4n$ is good, so $$4n = x^2+y^2$$ Since $x,y$ must be both even, we can write $a=x/2$ and $b=y/2$ and we are done.\n",
    "tags": [
      "proof-writing",
      "soft-question",
      "examples-counterexamples",
      "big-list"
    ],
    "score": 20,
    "answer_score": 15,
    "is_accepted": false,
    "question_id": 3626912,
    "answer_id": 3928017
  },
  {
    "theorem": "Proving rigorously the supremum of a set",
    "context": "Suppose $\\emptyset \\neq A \\subset \\mathbb{R} $.  Let $A = [\\,0,2).\\,\\,$   Prove that $\\sup A = 2$\nThis is my attempt:\n\n$A$ is the half open interval $[\\,0,2)$ and so all the $x_i \\in A$ look like $0 \\leq x_i < 2$ so clearly $2$ is an upper bound.\nTo show it is the ${\\it least}$ upper bound, suppose that $2 \\neq \\sup A$, that is there exists a number $M < 2$ for some real $M$ qualifying as $\\sup A$.  Certainly this $M \\in [0,2) $ so $ M > 0 \\Rightarrow 2 -M > 0$.\nBy the Archimedean Principle, for all real numbers $r > 0\\,\\, \\exists\\,\\, n \\in \\mathbb{N}$ such that $0 < \\frac{1}{n} < r $. By the Approximation Property of Suprema, there exists $a \\in [0,2)$ such that $\\sup A - \\epsilon < a \\leq \\sup A$, where $\\epsilon > 0$.\nSuppose $\\sup A = M < 2$.  Then the above gives $M - \\epsilon < a < 2\\,\\,\\,\\,\\forall \\epsilon > 0$.  Also, by Archimedean, we have $0 < \\frac{1}{n} < 2-M$, so choose $\\epsilon = 2-M$.  Then $M - (2-M) < a < 2 \\,\\Rightarrow 2(M-1) < a < 2$\nWe can assume $M - 1>0$ and so $2(M-1) > 2$  This results in a contradiction in the previous inequality.  Hence $M < 2$ cannot be the supremum.\n\nI realise there is probably a simpler way, but is what I have written all good?\n",
    "proof": "I think there's a way more simple and intuitive proof. \nFirst, as you observed, it is obvious that 2 is an upper bound. Now, to prove it is the supremum. Assume that $M$ is the supremum and $M<2$. \nOf course, $M>2$ is trivially impossible since $2$ is an upper bound as well and thus any $M$ bigger than $2$ cannot be a supremum.\nNow, let $x=\\frac{2-M}{2}+M$.\nAs you can obviously see $M<x$, so all that is left is to show that $x<2$.\nBut now, assume it is not, that is $x\\geq2$. \nThen, $\\frac{2-M}{2}+M\\geq2$. Multiplying both sides by $2$, we get\n$2-M+2M\\geq4$, that is $2+M\\geq4$. But $M<2$, so $2+M<4$. Thus, by reductio ad absurdum, $x<2$. This shows that $M<2$ cannot be the supremum. \nQED.\nNow, as for the simplicity of this proof, I have written a lot for clarity and in case you are a beginner on this subject. This can be summarized in $2$ lines, but this is for clarity. I hope this helps, and you must soon learn to find the shortest and more intuitive way. Good luck.\n",
    "tags": [
      "real-analysis",
      "proof-verification",
      "proof-writing",
      "supremum-and-infimum"
    ],
    "score": 20,
    "answer_score": 16,
    "is_accepted": true,
    "question_id": 386758,
    "answer_id": 1234705
  },
  {
    "theorem": "How many words (i.e. not &quot;math&quot; symbols&quot;) should I use in my proofs? ${}{}$",
    "context": "I must once again resort to the advice of this great community.\nAs I was reading about the pigeonhole principle something about its proof struck me as odd. Allow me to explain:\nAfter reading the \"The Foundations: Logic and Proofs\" chapter in Rosen's \"Discrete mathematics and its applications\" book I was left with the feeling/notion that I can (and I ought to) describe all my proof's statements in symbols.\nYet, as you can see in the pigeonhole principle's proof:\n\nWe use a proof by contraposition. Suppose  none of the k boxes has\n  more than one object. Then the total number of objects would be at\n  most k. This contradicts the statement that we have k + 1 objects.\n\nWithout a doubt it has more English words than symbols. Yet the proof is actually without flaws. I struggle with the fact that I can't convert it into the A -> B format (I hope you understand what I'm trying to convey).\nHowever, is there a way to symbolically represent what it states (as I'm trying to put it in my mind)? \nOr the only way to argue for this proof, is by using words?\nAnd if so, is there any guide or principle that should tell us when to use words or symbols in our proofs?\n",
    "proof": "Your plan is exactly backwards.\nAll proofs should be readable as English prose, i.e. sentences arranged into paragraphs.  Symbols may be used as needed, but they need to be human-readable.  If you've defined enough symbols, you can write parts of the proof entirely in symbols, provided that they can be parsed back into English.  \nFor example, $$\\forall\\ x\\in\\mathbb{R}, \\exists\\ y\\in \\mathbb{Z}: x\\ge y$$\nreads as \"For all real numbers $x$, there is an integer $y$, such that $x$ is greater than or equal to $y$.\"\n",
    "tags": [
      "logic",
      "proof-writing"
    ],
    "score": 19,
    "answer_score": 44,
    "is_accepted": true,
    "question_id": 2701182,
    "answer_id": 2701186
  },
  {
    "theorem": "Why does proof by contrapositive make intuitive sense?",
    "context": "If you have two statements P and Q, and we say that P implies Q, that suggests that P contains Q. So if we have P, we must have Q because it is contained within P. This is my intuitive understanding of the implication. \nOn the other hand, if we do not have Q, by my example above it would not imply that we do not have P, since Q is only one of the things contained within P. So why would showing that when we don't have Q we don't have P prove the implication? \nIn short, what understanding of the material implication is needed for proof by contrapositive to make intuitive sense? I understand the truth tables are the same, but that does not provide intuition in my opinion. \n",
    "proof": "Since it seems you are thinking in terms of subsets,  It is like saying that if $P\\supseteq Q$  then $Q^c\\supseteq P^c$ (in the sense that if $P$ is the set of all things we know to be true as our hypothesis, then the entirety of $Q$ is among those things we know as a result to be true.  On the other hand if $Q^c$ is the set of all things we know to be true,  then the entirety of $P^c$ is among the things we know to be true)\nIn the following image, $P$ contains $Q$ as a subset.  The lighter shade indicates that it is used by both $P$ and $Q$.\n\nOn the other hand, looking at the complements, the area outside of $Q$ (part, not all, of which is red) contains the area outside of $P$ (in pink) as a subset.  Again, the lighter shade is the area used in both.\n\n",
    "tags": [
      "proof-writing",
      "proof-explanation"
    ],
    "score": 19,
    "answer_score": 12,
    "is_accepted": true,
    "question_id": 1870171,
    "answer_id": 1870180
  },
  {
    "theorem": "Prove that $x^{2} \\equiv -1 \\pmod p$ has no solutions if prime $p \\equiv 3 \\pmod 4$.",
    "context": "Assume: $p$ is a prime that satisfies $p \\equiv 3 \\pmod 4$\nShow: $x^{2} \\equiv -1 \\pmod p$ has no solutions $\\forall x \\in \\mathbb{Z}$.\nI know this problem has something to do with Fermat's Little Theorem, that $a^{p-1} \\equiv 1\\pmod p$. I tried to do a proof by contradiction, assuming the conclusion and showing some contradiction but just ran into a wall. Any help would be greatly appreciated.\n",
    "proof": "Suppose $x^2\\equiv -1\\pmod{p}$. Then $x^4\\equiv 1\\pmod{p}$. Since $p = 4k+3$, we have\n$$x^{p-1} = x^{4k+2} = x^2x^{4k} \\equiv -1(x^4)^k\\equiv -1\\pmod{p},$$\nwhich contradicts Fermat's Little Theorem.\n",
    "tags": [
      "elementary-number-theory",
      "modular-arithmetic",
      "proof-writing"
    ],
    "score": 19,
    "answer_score": 43,
    "is_accepted": true,
    "question_id": 142007,
    "answer_id": 142014
  },
  {
    "theorem": "Can every true theorem that has a proof be proven by contradiction?",
    "context": "After reading and being inspired by, Can every proof by contradiction also be shown without contradiction? and after some thought, I still don't have an answer to this.\nDoes every theorem with a true proof have a proof by contradiction? \n",
    "proof": "In classical logic, the answer is yes. Take any theorem $T$ and any proof $P$ for $T$. Now write the following proof:\n\nIf $\\neg T$:\n  [Write $P$ here.]\n  Thus $T$.\n  Thus a contradiction.\nTherefore $\\neg \\neg T$, by negation introduction.\nThus $T$, by double negation elimination.\n\nOne may object that this proof is essentially the same as $P$, and is just wrapped up. That is true, but it is a perfectly legitimate proof of $T$, even if it is longer than $P$, and it is indeed of the form of a proof by contradiction. A natural question that arises is whether the shortest proof of $T$ is a proof by contradiction. That is a much harder question to answer in general, but there are some easy examples, at least for any reasonable natural deduction system.\nFor instance, the shortest proof of \"$A \\to A$\" for any given statement $A$ is definitely not a proof by contradiction but rather just:\n\nIf $A$:\n  $A$.\nTherefore $A \\to A$, by implication introduction.\n\nOn the other hand, the shortest proof of \"$\\neg ( A \\land \\neg A )$\" for any given statement $A$ is definitely a proof by contradiction:\n\nIf $A \\land \\neg A$:\n  $A$, by conjunction elimination.\n  $\\neg A$, by conjunction elimination.\n  Thus a contradiction.\nTherefore $\\neg( A \\land \\neg A )$.\n\nThe first part of this post shows that the shortest proof by contradiction is at most a few lines longer than the shortest proof, but nothing much else interesting can be said about the shortest proof unless...\n\nWell what if we do not allow the use of double negation elimination? If you have only the other usual rules (the first-order logic rules here but excluding ¬¬elim and including ex falso), then the resulting logic is intuitionistic logic, which is strictly weaker than classical logic, and cannot even prove the law of excluded middle, namely \"$A \\lor \\neg A$\" for any statement $A$. So if you instead ask the more interesting question of whether every true theorem can be proven in intuitionistic logic, then the answer is no.\nNote that intuitionistic logic plus the rule \"$\\neg A \\to \\bot \\vdash A$\" gives back classical logic, and one could say that this rule embodies the 'true principle' of proof by contradiction, in which case one can say that some true theorems require the use of a proof by contradiction somewhere.\n",
    "tags": [
      "logic",
      "proof-writing",
      "proof-theory"
    ],
    "score": 19,
    "answer_score": 23,
    "is_accepted": true,
    "question_id": 2081396,
    "answer_id": 2081513
  },
  {
    "theorem": "help me understand a line in an “$A^TA$ is positive, semi-definite” proof",
    "context": "I am looking at a proof for why $A^TA$ is positive semi-definite when $A$ is $n\\times n$ and it has this line.\n$$\nv^TAA^Tv = A^Tv \\cdot A^Tv ≥ 0.\n$$\nI understand what $v^TAA^Tv$ means and the purpose of proving that it's nonnegative, etc... My problem is that I am a linear algebra novice and do not necessarily understand how the first part $v^TAA^Tv$ is equivalent to $A^Tv \\cdot A^Tv$. I know that $a^Tb = a \\cdot b$, but something else is going on, no? Appreciate any help!\n",
    "proof": "For any column vector $v$, we have $v^tA^tAv=(Av)^t(Av)=(Av)\\cdot (Av)\\geq 0$, therefore $A^tA$ is positive semi-definite. In particular, if $A$ is a nonsingular matrix, then $A^tA$ is positive definite.\n",
    "tags": [
      "linear-algebra",
      "proof-writing"
    ],
    "score": 19,
    "answer_score": 27,
    "is_accepted": false,
    "question_id": 133350,
    "answer_id": 133355
  },
  {
    "theorem": "What are some good introductory books on mathematical proofs?",
    "context": "There was a time when I avoided math proofs, but now I am starting to enjoy them. I am taking Intro to Linear Algebra and am falling in love with proofs. Are there any introduction to mathematical proofs books that blow the others out of the water?\n",
    "proof": "George Polya's How to Solve It immediately comes to mind. I know many now fantastic pre-mathematicians who learned calculus and the basics of analysis from Spivak's Calculus and even if you know the material to go back and do it again in a formal way is very healthy. In addition Proofs from THE BOOK was mentioned above and was recommended to me by Ngo Bao Chao when I asked about books to study problem-solving techniques from. I don't mean to come off as name-dropping but I feel that (as he is a fields medalist) his advice is worth heeding. I, personally, really liked it.  \nHowever I have to make note that I think if you'd phrased your question as \"should I read a book about proofs to learn proofs\" my response would be an emphatic no. In my experience if you don't see proofs by doing some fun mathematics you will not get much better about doing them yourself. Just reading about how to prove things can only get you so far before you're sort of stumped as to how to proceed. I would say the better approach is to find a rigorous treatment of a subject that you're very interested in, and read that, following along with the proofs of the theorems in the book and eventually trying to do them yourself without looking at the proofs given.\n",
    "tags": [
      "reference-request",
      "proof-writing",
      "soft-question"
    ],
    "score": 19,
    "answer_score": 18,
    "is_accepted": true,
    "question_id": 655443,
    "answer_id": 655465
  },
  {
    "theorem": "Proof by induction of Bernoulli&#39;s inequality $ (1+x)^n \\ge 1+nx$",
    "context": "I am working on getting the hang of proofs by induction, and I was hoping the community could give me feedback on how to format a proof of this nature:\nLet $x > -1$ and $n$ be a positive integer. Prove Bernoulli's inequality:\n$$ (1+x)^n \\ge 1+nx$$\nProof: \nBase Case: For $n=1$, $1+x = 1+x$ so the inequality holds.\nInduction Assumption: Assume that for some integer $k\\ge1$, $(1+x)^k \\ge 1+kx$. \nInductive Step: We must show that $(1+x)^{k+1} \\ge 1+(k+1)x$\nProof of Inductive Step: \n$$\\begin{align*}\n(1+x)^k &\\ge 1+kx \\\\\n(1+x)(1+x)^k &\\ge (1+x)(1+kx)\\\\\n(1+x)^{k+1} &\\ge  1 + (k+1)x + kx^2 \\\\\n 1 + (k+1)x + kx^2 &> 1+(k+1)x \\quad (kx^2 >0) \\\\\n\\Rightarrow (1+x)^{k+1} &\\ge 1 + (k+1)x \\qquad \\qquad \\qquad \\square\n\\end{align*}$$ \n",
    "proof": "What you have is perfectly acceptable. The calculations could be organized a little more neatly:\n$$\\begin{align*}\n(1+x)^{k+1}&=(1+x)(1+x)^k\\\\\n&\\ge(1+x)(1+kx)\\\\\n&=1+(k+1)x+kx^2\\\\\n&\\ge1+(k+1)x\\;,\n\\end{align*}$$\nsince $kx^2\\ge 0$. This completes the induction step.\n",
    "tags": [
      "inequality",
      "proof-verification",
      "proof-writing",
      "induction"
    ],
    "score": 19,
    "answer_score": 15,
    "is_accepted": true,
    "question_id": 181702,
    "answer_id": 181706
  },
  {
    "theorem": "Proof by induction that $n^3 + (n + 1)^3 + (n + 2)^3$ is a multiple of $9$. Please mark/grade.",
    "context": "What do you think about my first induction proof? Please mark/grade.\n\nTheorem\nThe sum of the cubes of three consecutive natural numbers is a multiple of 9.\nProof\nFirst, introducing a predicate $P$ over $\\mathbb{N}$, we rephrase the theorem as follows.\n        $$\\forall n \\in \\mathbb{N}, P(n)\n\t\t\\quad \\text{where} \\quad\n\t\tP(n) \\, := \\, n^3 + (n + 1)^3 + (n + 2)^3 \\text{ is a multiple of 9}$$\n        We prove the theorem by induction on $n$.\nBasis\nBelow, we show that we have $P(n)$ for $n = 0$.\n            $$0^3 + 1^3 + 2^3 = 0 + 1  +  8 =  9 = 9 \\cdot  1$$\nInductive step\nBelow, we show that for all $n \\in \\mathbb{N}$, $P(n) \\Rightarrow P(n + 1)$.\nLet $k \\in \\mathbb{N}$. We assume that $P(k)$ holds.\n            In the following, we use this assumption to show that $P(k + 1)$ holds.\nBy the assumption,\n            there is a $i \\in \\mathbb{N}$ such that\n            $i \\cdot 9 = k^3 + (k + 1)^3 + (k + 2)^3$.\n            We use this fact in the following equivalent transformation.\n            The transformation turns the sum of cubes in the first line,\n            for which we need to show that it is a multiple of 9,\n            into a product of 9 and another natural number.\n$(k + 1)^3 + (k + 2)^3 + (k + 3)^3 \\\\\n\t\t\t= (k + 1)^3 + (k + 2)^3 + k^3 + 9k^2 + 27k + 27 \\\\\n\t\t\t= k^3 + (k + 1)^3 + (k + 2)^3 + 9k^2 + 27k + 27 \\quad | \\text{ using the induction hypothesis} \\\\\n\t\t\t= 9i + 9k^2 + 27k + 27 \\\\\n\t\t\t= 9 \\cdot i + 9 \\cdot k^2 + 9 \\cdot 3k + 9 \\cdot 3 \\\\\n\t\t\t= 9 \\cdot (i + k^2 + 3k + 3)$\nWe see that the above product has precisely two factors: 9 and another natural number.\n            Thus the product is a multiple of 9.\n            This completes the induction.\n",
    "proof": "I do not think that this is a real question but if you were my student i would give you an A.\nIt is all fine to me.\n",
    "tags": [
      "elementary-number-theory",
      "proof-writing",
      "induction",
      "proof-verification",
      "divisibility"
    ],
    "score": 19,
    "answer_score": 7,
    "is_accepted": false,
    "question_id": 732445,
    "answer_id": 732453
  },
  {
    "theorem": "Topology: Show restriction of continuous function is continuous, and restriction of a homeomorphism is a homeomorphism",
    "context": "I need to prove two trivial results but I don't know how to work with restricted function and its inverse\nConsider the topological spaces $(X, \\mathcal{T}), (Y, \\mathcal{J})$\n\nClaim 1: Let $f:X \\to Y$ be continuous function, $A \\subset X$ equipped with subspace topology, then\n  $f|_{A}:A \\to Y$ is continuous\n\nProof: Take some $V \\in \\mathcal{J}$, then $f^{-1}|_A(V) = f^{-1}(V) \\cap A$, where $f^{-1}(V)$ is open, therefore $f^{-1}(V) \\cap A$ is open in the subspace topology.\n\nClaim 2: Let $f:X \\to Y$ be homeomorphism, $A \\subset X$ equipped with subspace topology, then $f(A)$ is a subspace of $Y$ and $f|_{A}:A \\to f(A)$ is continuous\n\nProof: We proceed by showing $f|_A$ is continuous and open. \nFirst show $f|_A$ is continuous, take some open set $W$ in the subspace topology on $f(A)$, $W = f(A) \\cap V, V \\in \\mathcal{J}$  then $f^{-1}|_A(V \\cap f(A) ) = f^{-1}|_A(V) \\cap f^{-1}|_A(f(A)) = $$(f^{-1}(V) \\cap A) \\cap  (f^{-1}(f(A)) \\cap A) = f^{-1}(V) \\cap A$ is open. \nNext show $f|_A$ is open. Take some open set $M$ in the subspace topology on $A$, then $M = A \\cap U, U \\in \\mathcal{T}$. Then $f|_A(A \\cap U) = f|_A(A) \\cap f|_A(U) = f(A) \\cap f|_A(U) = f(A) \\cap f(U \\cap A) = $$ f(A) \\cap f(U) \\cap f(A) = f(A) \\cap f(U)$. Note $f(U)$ is open since $f$ is open, therefore $f(A) \\cap f(U)$ is open in the subspace topology of $f(A)$\nThis shows all homeomorphisms are local homeomorphism\n\nCan someone check the two proofs? The second one is a bit messy.\n\n",
    "proof": "Your proof of Claim 1 is correct, and your proof of Claim 2 is correct but unclear.\nFirst, the last line of your proof ``This shows all homeomorphisms...'' can be omitted. The fact that all homeomorphisms are local homeomorphisms has nothing to do with your claim that $f|_A$ is a homeomorphism.\nSecond, some of your reasoning in the third paragraph of Claim 2 depends on the fact that the restricted mapping $f|_A$ is a bijection. You could start out by noting that $f|_A$ is a bijection since $f$ is a homeomorphism. Then it suffices to prove that $f|_A$ is continuous and open (since being open in this case is the same thing as the inverse map being continuous).\nNow consider the second line in your third paragraph, which starts with $M = A\\cap U \\ldots$ I don't understand the presence of $Y$ in this line. $f|_A(A) = f(A),$ not $Y$. So you can replace $Y$ with $f(A)$ and then delete the extraneous references to $Y$. Finally, as I alluded to above, your assertion that the image of the intersection of two sets is the intersection of the images is only true because $f|_A$ is a bijection-- this fact isn't true in general. So it might be good to mention that in your proof.\n",
    "tags": [
      "general-topology",
      "functions",
      "proof-verification",
      "proof-writing",
      "proof-explanation"
    ],
    "score": 19,
    "answer_score": 6,
    "is_accepted": true,
    "question_id": 1826827,
    "answer_id": 1826856
  },
  {
    "theorem": "Papers with unorthodox writing style",
    "context": "I'm not sure if this is the right forum for this question, in any case probably CW is appropriate?\nI've been looking around the mathblogosphere for the past few weeks and ran into mathgen. It's pretty amusing, to be sure, but point 6 in the Why? section has set me to serious thinking. For those opposed to clicking links, mathgen is a random generator of math papers, and the creator give several justifications for the creation, the relevant one being:\n\nI think this project says something about the very small and stylized subset of English used in mathematical writing. This program only knows a handful of sentence templates, and yet I think its writing style is [typical.] I think we could stand to pay more attention to our writing styles, instead of unthinkingly relying on stock phrases.\n\nWith this in mind, have any of you encountered reputable, \"research-tier\" papers that have a writing style dramatically or at least distinctly different from the one that seems to dominate so much of this kind of mathematical writing? I'm not really looking for expository writings, although I imagine that what I am looking for will have a similar feel to it. So I think what I'm going to mean by research-tier (for now) is simply that it proves something new and at least mildly significant.\nLinks, especially free ones, are appreciated.\n",
    "proof": "One piece that comes to mind is Gromov's \"Metric structures for Riemannian and Non-riemannian spaces.\"  Everything from the numbering - a Gromov hallmark - to some of the colorful yet sometimes remarkably illuminating language - a wonderful example is 1.25.1/2, \"If one feels disgusted by the spineless flexibility of arc-wise isometric maps, ...\" - is rather unique in the literature.  (And still, Gromov is far more digestible in print than when speaking.)\n",
    "tags": [
      "proof-writing",
      "examples-counterexamples",
      "big-list",
      "article-writing"
    ],
    "score": 19,
    "answer_score": 3,
    "is_accepted": false,
    "question_id": 515122,
    "answer_id": 1753758
  },
  {
    "theorem": "Can a proof be just words?",
    "context": "I suppose this is a question about mathematical convention. In a problem in Introduction to Probability by Bertsekas and Tsitsiklis, they ask the reader to prove an identity. But then their proof is mostly words:\n\nProblem 3.* Prove the identity $$A \\cup \\Bigg( \\bigcap_{n=1}^\\infty B_n \\Bigg) = \\bigcap_{n=1}^\\infty\\big(A \\cup B_n\\big).$$\nSolution. If $x$ belongs to the set on the left, there are two possibilities. Either $x \\in A$, in which case $x$ belongs to all of\n  the sets $A \\cup B_n$, and therefore belongs to the set on the right.\n  Alternatively, $x$ belongs to all of the sets $B_n$ in which case, it\n  belongs to all of the sets $A \\cup B_n$, and therefore again belongs\n  to the set on the right.\nConversely, if $x$ belongs to the set on the right, then it belongs to\n  $A \\cup B_n$ for all $n$. If $x$ belongs to $A$, then it belongs to\n  the set on the left. Otherwise, $x$ must belong to every set $B_n$ and\n  again belongs to the set on the left.\n\nIn mathematics, why is this allowed? Can you say that this is more correct a proof that is, \"Oh, it's obvious!\" or \"Just keep distributing $A$ over and over ad nauseum and you get the term on the right\"?\nI'm not trolling. I'm genuinely curious as to how thorough one must be when using words as proof.\n",
    "proof": "Exactly as thorough as you would have to be using any other kinds of symbols. It's just that vast messes of symbols are hellish for humans to read, but sentences aren't. Adding symbols to something doesn't make it more rigorous, less likely to be wrong, or really anything else. Symbols are useful for abbreviating in situations where this adds clarity, and making complex arguments easier to follow, but shouldn't be used where they do not help in this regard. \n",
    "tags": [
      "proof-writing"
    ],
    "score": 18,
    "answer_score": 59,
    "is_accepted": true,
    "question_id": 3079966,
    "answer_id": 3079970
  },
  {
    "theorem": "What does it mean when proof by contradiction doesn&#39;t lead to a contradiction?",
    "context": "I started writing a proof using the method of proof by contradiction and encountered a situation which was true. More specifically, the hypothesis that I set out to prove was:\nIf the first 10 positive integer is placed around a circle, in any order, there exists 3 integer in consecutive locations around the circle that have a sum greater than or equal to 17. (From Discrete Mathematics and its Applications - K. Rosen)\nThis is how I proceeded:\nLet $a_i$ denote the $i^{th}$ integer on the boundary of the circle. To proceed with proof by contradiction, we assume that $\\forall i$\n$a_i + a_{i+1} + a_{i+2} < 17$\nThen, \n$a_1 + a_2 + a_3 < 17$\n$a_2 + a_3 + a_4 < 17$\n$\\vdots$\n$a_{10} + a_1 + a_2 < 17$\n$\\therefore\\ 3 \\cdot (a_1 + a_2 + \\dots + a_{10}) < 17 \\cdot10$\n$\\Rightarrow\\ 3 \\cdot 55 < 170$  \n$\\Rightarrow\\ 165 < 170$ \nwhich is true. What does this mean? \nP.S. I am not looking for the solution to this problem. I am aware of how to prove the claim. I am just curious about what it means to arrive at a truth after assuming the negation of the hypothesis. \n",
    "proof": "Your goal is to show that $p$ is false. \nIf $p \\implies q$ and $q$ is true. \nWe can't conclude if $p$ is true or false. Hence, we get an inconclusive situation. \n",
    "tags": [
      "proof-verification",
      "proof-writing",
      "arithmetic"
    ],
    "score": 18,
    "answer_score": 47,
    "is_accepted": true,
    "question_id": 2915786,
    "answer_id": 2915793
  },
  {
    "theorem": "Having hard time understanding proofs by contradiction.",
    "context": "I am reading an introductory book on mathematical proofs and I don't seem to understand the mechanics of proof by contradiction. Consider the following example. \n$\\textbf{Theorem:}$ If $P \\rightarrow Q$ and $R \\rightarrow \\neg Q$, then $P \\rightarrow \\neg R$.\n$\\textbf{Proof:}$ (by contradiction) \nAssume $P$, then it follows that $Q$.\nNow, assume $R$, then it follows that $\\neg Q$. Contradiction, we have $Q$ and $\\neg Q$ at the same time. Hence, $\\neg R$.\nTherefore, if $P \\rightarrow Q$ and $R \\rightarrow \\neg Q$, then $P \\rightarrow \\neg R$, as desired.\nWhat I don't understand in this proof, is that why having arrived at contradiction, we decide that our assumption that $R$ is necessarily false? It also could have been that our first assumption, namely, $P$, was false. Or both of them could be false.\nSo my question is: in general, when proving by contradiction, how do we know which assumption exactly is false? And how do we know that exactly one assumption must be wrong in order to proceed with the proof?\n",
    "proof": "In a proof with multiple assumptions you have to choose one of them to be \"blamed\" for the contradiction.\nThink to your example in terms of assumptions; you start with a couple of them (they can be two Lemmas already proved, or two hypotheses) :\n\n$P→Q$ and $R→¬Q$.\n\nThen we proceed \"formally\" as follows (I'll use the Natural Deduction proof system; for a good explanation of the rules to be used, see : Ian Chiswell & Wilfrid Hodges, Mathematical Logic (2007), Ch.2 : Informal natural deduction, page 5-on) :\n1) $P$ --- assumed\n2) $Q$ --- from 1) and $P→Q$ by $\\rightarrow$-elim (modus ponens)\n3) $R$ --- assumed\n4) $\\lnot Q$ --- from 3) and $R→¬Q$ by $\\rightarrow$-elim (modus ponens)\n5) $\\bot$ --- from 2) and 4) by $\\lnot$-elim [i.e. using the rule : \"from $\\varphi$ and $\\lnot \\varphi$, infer $\\bot$]\n6) $\\lnot R$ --- from 3) and 5) by $\\lnot$-intro [i.e. using the rule : \"if from $\\varphi$ we have derived $\\bot$, then infer $\\lnot \\varphi$], \"discharging\" temporary assumption 3)\n7) $P \\rightarrow \\lnot R$ --- from 1) and 6) by $\\rightarrow$-intro, \"discharging\" temporary assumption 1).\nThus we have proved :\n\n$P→Q, R→¬Q \\vdash P \\rightarrow \\lnot R$.\n\n\nAs per the above answer, we can apply contraposition : $\\varphi \\rightarrow \\lnot \\psi \\vdash \\psi \\rightarrow \\lnot \\varphi$ to conclude also :\n\n$P→Q, R→¬Q \\vdash R \\rightarrow \\lnot P$.\n\nIn the previous proof, we have chosen the assumptiom $R$ to be \"blamed\" for the contradiction. We can as well choose $P$.\nIf you rewrite it introducing $\\lnot P$ in step 6) above, you will end exactly with : $R \\rightarrow \\lnot P$.\n\nComment\nIn order to \"have a feeling\" with the above application of logical rules, modify the above proof using a single assumption $P \\land R$.\nDue to the fact that :\n$P \\land R \\vdash P$ and $P \\land R \\vdash R$ [by : $\\land$-elim]\nwe can repeat the same steps until 5) : $\\bot$.\nIn this case, we have only one assumption to be \"blamed\" : $P \\land R$ and we conclude with :\n\n$\\lnot (P \\land R)$.\n\nThis means that, in the presence of the two Lemmas or hypotheses : $P \\rightarrow Q$ and $R \\rightarrow \\lnot Q$, we cannot \"jointly assert\" $P$ and $R$.\nThus, one of them must be \"removed\". Which one ? it's up to us ...\n",
    "tags": [
      "logic",
      "proof-writing"
    ],
    "score": 18,
    "answer_score": 15,
    "is_accepted": true,
    "question_id": 916963,
    "answer_id": 916989
  },
  {
    "theorem": "$f\\geq 0$, continuous and $\\int_a^b f=0$ implies $f=0$ everywhere on $[a,b]$",
    "context": "This is problem 6.2 from the 3rd edition of Principles of Mathematical Analysis. \n\nProblem 6.2: Suppose $f\\geq 0$, f is continuous on $[a, b]$, and $\\int_a^b f(x) \\, dx = 0$. Prove that $f(x)=0$ for all $x \\in [a, b]$.\n\nI'm looking for a critique of my proof. It's a pretty easy problem, but I am always wary of making too bold of assumptions, especially on these low level/fundamental proofs. I'll be using Rudin's notation and refer to theorems from the text (If I should include the text of each theorem, feel free to leave a comment... I'm lazy but could probably use the TeX practice :p)\nProof: Assume, for contradiction, that $f>0$. Then, for any partition $P$ we have the Lower Riemann Sum: $L(P, f)=\\sum_{i=1}^n m_i \\, \\Delta x_i$. At least one $\\Delta x_i$ must be positive, since $a < b$, and each $m_i$ must be positive since we have $f>0$ by assumption, so certainly $\\sup f > 0$. That means $L(P, f)>0$. Thus, we have:\n$$0 = 0(b-a) < L(P,f)\\leq \\sup L(P, f) = \\inf U(P,f) =L$$\nwhere the last string of equalities holds because our function is continuous on a compact interval, so is integrable by theorems 6.8 and 6.6. So our integral has value $L>0$. This is in contradiction to our given assumption that $\\int_a^b f(x) \\, dx = 0$, so we must have that $f=0$ on $[a, b]$. $\\Box$\nSo, I am wondering if my proof is correct (and is presented well). Also if someone could enlighten me as to what Rudin means when he say \"Compare this with exercise 1,\" I'd be appreciative. Is it a hint or is there something else he expects you to notice? there are a lot of things I could compare :)...\nExercise 6.1: Suppose $\\alpha$ increases on $[a, b]$, $a \\leq x_0 \\leq b$, $\\alpha$ is continuous at $x_0$, $f(x_0)=1$, and $f(x)=0$ if $x\\neq x_0$. Prove that $f$ is Riemann-Stieltjes Integrable and that $\\int f \\, d\\alpha = 0$\n",
    "proof": "A proof: If $f\\ge0$ everywhere and $f(x_0)>0$ and $f$ is continuous, we could do a little $\\varepsilon$-$\\delta$ argument like this: Let $\\varepsilon=f(x_0)/2$.  Let $\\delta>0$ be small enough so that if $x$ is within distance $\\delta$ of $x_0$, then $f(x)$ is within $\\varepsilon$ of $f(x_0)$.  So $f\\ge f(x_0)/2$ on the interval whose endpoints are $x_0 \\pm \\delta$, and so\n$$\r\n\\int_a^b f(x) \\; dx \\ge (2\\delta) (f(x_0)/2) = \\delta f(x_0) > 0.\r\n$$\nTo allow for $x_0$ being near an endpoint, you could just integrate over half that interval.\nSome comments on the posted proof:  The assumption in a proof by contradiction should not be stated as \"$f>0$\".  Rather it should be stated as saying there is at least one point $x_0$ such that $f(x_0)>0$.  Whenever the conclusion says \"All A are B\", then the assumption in a proof by contradiction should be \"At least one A is not B\".\nYour argument to the conclusion that $\\sup f>0$ is too complicated.  If you've assumed $f$ is not everywhere $0$ and you have $f\\ge 0$ everywhere, then as soon as you've assumed there is one point $x_0$ where $f>0$, you've already got $\\sup f\\ge f(x_0)$.\nSince you're working with Riemann integrals defined by Riemann sums, you might make the partition $\\{a, x_0-\\delta,x_0+\\delta, b\\}$ and then you have the lower Riemann sum $\\ge (2\\delta) (f(x_0)/2)$.  If the lower Riemann sum for just one partition is positive, then the integral is positive.\nSaying \"Then for any partition...\" seems at best a needless complication.  Just one partition, as noted above, is enough if you do the right things with it.\n",
    "tags": [
      "real-analysis",
      "integration",
      "proof-writing"
    ],
    "score": 18,
    "answer_score": 21,
    "is_accepted": true,
    "question_id": 102476,
    "answer_id": 102501
  },
  {
    "theorem": "Prove that $\\int_0^\\pi\\frac{\\cos x \\cos 4x}{(2-\\cos x)^2}dx=\\frac{\\pi}{9} (2160 - 1247\\sqrt{3})$",
    "context": "Prove that\n$$\\int_0^\\pi\\frac{\\cos x \\cos 4x}{(2-\\cos x)^2}dx=\\frac{\\pi}{9} (2160 - 1247\\sqrt{3})$$\nI tried to use Weierstrass substitution but the term $\\cos 4x$ made horrible algebraic-forms since $\\cos 4x = \\sin^4 x + \\cos^4 x - 6\\sin^2 x \\cos^2 x$. My friend suggests me use a contour integration method but I am not familiar with that method. Any idea? Any help would be appreciated. Thanks in advance.\n",
    "proof": "Proposition : \n\n\\begin{equation}\\int_0^\\pi\\frac{\\cos mx}{p-q\\cos x}\\, dx=\\frac{\\pi}{\\sqrt{p^2-q^2}}\\left(\\frac{p-\\sqrt{p^2-q^2}}{q}\\right)^m\\qquad\\hbox{for}\\qquad |q|<p\n\\end{equation}\n\n\nProof :\nWe have\n\\begin{equation}\n\\int_0^\\pi\\frac{\\cos mx}{a^2-2ab\\cos x+b^2}\\, dx=\\frac{\\pi}{a^2-b^2}\\left(\\frac{b}{a}\\right)^m\\qquad\\hbox{for}\\qquad |b|<a\\tag1\n\\end{equation}\nThe complete proof is given by Prof. Omran Kouba and can be seen here.\nNow, let $p=a^2+b^2$ and $q=2ab$, then $p+q=\\sqrt{p+q}$ and $p-q=\\sqrt{p-q}$. Therefore\n\\begin{align}\n2a&=\\sqrt{p+q}+\\sqrt{p-q}\\\\[10pt]\n2b&=\\sqrt{p+q}-\\sqrt{p-q}\\\\[10pt]\na^2-b^2&=\\sqrt{p^2-q^2}\\tag2\\\\[10pt]\n\\frac{b}{a}&=\\frac{p-\\sqrt{p^2-q^2}}{q}\\tag3\n\\end{align}\nthen plugging in $(2)$ and $(3)$ to $(1)$ we prove our proposition. $\\quad\\square$\nSet $m=4$ and $p=2$ then differentiate the proposition w.r.t. $q$ and take the limit for $q\\to1$, we obtain\n\\begin{align}\n\\lim_{q\\to1}\\int_0^\\pi\\partial_q\\left(\\frac{\\cos 4x}{2-q\\cos x}\\right)\\, dx&=\\lim_{q\\to1}\\partial_q\\left(\\frac{\\pi}{\\sqrt{4-q^2}}\\left(\\frac{2-\\sqrt{4-q^2}}{q}\\right)^4\\right)\\\\[10pt]\n\\int_0^\\pi\\frac{\\cos x \\cos 4x}{(2-\\cos x)^2}dx&=\\frac{\\pi}{9} \\left(\\,2160 - 1247\\sqrt{3}\\,\\right)\n\\end{align}\nThe last step is confirmed by Wolfram Alpha.\nI think differentiating is easier than using contour integration or partial fraction decomposition. (>‿◠)✌\n",
    "tags": [
      "calculus",
      "integration",
      "trigonometry",
      "definite-integrals",
      "proof-writing"
    ],
    "score": 18,
    "answer_score": 25,
    "is_accepted": true,
    "question_id": 952371,
    "answer_id": 955655
  },
  {
    "theorem": "When is a proof or definition formal?",
    "context": "\nWhen is a proof or definition formal?\n\nI've been searching for an explanation of when or proof or definition is formal. \nSometimes, authors call their proofs or definitions informal without further explanation.\nI've an idea that a formal proof is derived directly from the axioms (with references to already proven statements), or are the requirements less strict?\nWhen is a definition formal? May words whose meaning follow only from the context be used in a formal definition, or should each use of a word be explicitly defined?\n",
    "proof": "It's not a very clear-cut line. There's also a complication, because mathematicians often use certain styles of language in a formal proof, making the proof sound more formal-in-the-colloquial-English-sense. They do this to signal \"this is a formal-in-your-sense proof\". The converse is also true.\nHere's an informal intensional definition:\n\nA formal proof is basically one where nothing is hidden.\n\n(The definition of \"nothing\" is flexible, depending on the level of the proof. A formal proof aimed at world-class mathematicians may miss out more details than a first-year undergraduate formal proof, which is not allowed to miss out many details at all.)\nOne may use words whose meaning follows only from context, as long as it's clear what the meaning is; that's not hiding anything. A proof can skip out details and still be formal, as long as it tells you how to complete the details: that's hiding stuff but making it clear that the stuff is a) hidden, and b) easy to recover.\nHere's an informal extensional definition.\nA proof which leaves large chunks to the reader is informal: it hides large amounts of detail. A definition which misses out some annoying special cases (perhaps to make the statement more slick) is informal. A proof which pedantically goes over every statement is formal. A proof in which every statement clearly follows from earlier statements or from axioms/hypotheses is formal.\n",
    "tags": [
      "proof-writing",
      "definition"
    ],
    "score": 18,
    "answer_score": 15,
    "is_accepted": false,
    "question_id": 2220437,
    "answer_id": 2220447
  },
  {
    "theorem": "Is there a proof that performing an operation on both sides of an equation preserves equality?",
    "context": "so I was learning some abstract algebra and group theory, when they went over the proof of the cancellation law\n$$\nab = ac\\implies a^{-1}(ab) = a^{-1}(ac)\\implies (a^{-1}a)b = (a^{-1}a)c\\implies eb=ec \\implies b=c\n$$\nBut the first step in which you add the additional term seems jarring to me, especially since I felt we were proving ever trivial thing from the ground up. Obviously I'm familiar with middle school pre-algebra, so I know that it is true that if we perform an operation on both sides it preserves equality, but I didn't know how we know this is the case always. Is it an axiom or is it proven?\nHere is my attempt at a proof, let me know if I am going in the correct direction. \nAssume via axiom that $x=x$ and if $a=b$ and $b=c$ then $a=c$, and prove that $a=b\\implies ka=kb$\nWe know that $a=b$, we define $x\\mid x=a \\implies a=x$. Since $a=b$ and $a=x$, then $b=x$ which we can rewrite as $x=x$. Now we perform the operation on both sides $kx=kx$, which is true via our axiom. Then we re-substitute $x=a$ and $x=b$ to get $ka=kb$. Q.E.D\nThat was my original idea but I don't know if that's watertight. Thank you!\n",
    "proof": "In first-order logic, we have the formal substitution principle:\n\nLet $\\phi$ be a propositional formula with a free variable $v$, and let $\\Gamma$ be a context.  Also, let $x, y$ be two terms representing values.  Then:\n  \\begin{align*} \\Gamma & \\vdash x = y \\\\\n\\Gamma & \\vdash \\phi[v := x] \\\\\n\\hline\n\\Gamma & \\vdash \\phi[v := y]. \\end{align*}\n\nInformally, what this says is: if you can prove in some context that $x=y$, and you can also prove some statement is true for $x$, then you can conclude the same statement is true for $y$.  (The notation $\\phi[v := x]$ just means the result of substituting $x$ in for $v$ in the proposition formula $\\phi$.)\nNow, if we are working in a group, let us apply this to the formula $\\phi := (a^{-1} (ab) = a^{-1} v)$.  Then in the context of the proof, we are assuming $ab = ac$.  Also, $\\phi[v := ab]$ results in the proposition $a^{-1}(ab) = a^{-1}(ab)$, which is true by the first-order axiom (or in some formulations, the formal proof rule) of reflexivity of equality: $t = t$ for any term $t$.  Therefore, the substitution principle allows us to conclude that $\\phi[v := ac]$ is true, which results in $a^{-1} (ab) = a^{-1} (ac)$.\nTo give another application which is implicitly used in the proof, let us see how to use the substitution principle to prove the transitivity of equality: if we have $x=y$ and $y=z$ then $x=z$.  For this proof, we will use $\\phi := (x = v)$.  Then we are assuming $y=z$.  We also have that $\\phi[v := y]$ is true since it reduces to the assumption $x = y$.  Therefore, we can conclude $\\phi[v := z]$ which is just $x = z$.\n",
    "tags": [
      "abstract-algebra",
      "group-theory",
      "proof-writing",
      "axioms"
    ],
    "score": 18,
    "answer_score": 19,
    "is_accepted": true,
    "question_id": 2882730,
    "answer_id": 2882761
  },
  {
    "theorem": "Counterexamples to proofs of correct statements",
    "context": "This question is in part inspired by a quote I saw in an answer to another question: \n\nThe problem with incorrect proofs to correct statements is that it is hard to come up with a counterexample.\n\nA little while ago, I attended a graph theory course in which there was a chapter on graph colourings.  The most famous problem in this area is the four-colour theorem, which states that the vertices of any planar graph can be coloured using at most four colours in such a way that no two adjacent vertices have the same colour.  This is easily shown to be equivalent to the more famous statement of the theorem concerning colourings of countries on a map.  Our lecturer showed us the rather simple proofs of the six-colour and five-colour theorems (i.e., the four-colour theorem but with 'five' and 'six' substituted for 'four') and then showed us a slightly more complicated proof of the four-colour theorem.  \nAt this point, whispers started going round the room.  It is well known that the only known proof of the four-colour theorem, due to Appel and Haken, makes essential use of a computer to check thousands of cases and can certainly not be written down on a blackboard in under an hour.  Our leccturer explained that the proof was incorrect, and left it to us as an exercise to find out why.  \nI worked at it a bit, and eventually found a problem with the proof.  I looked online to find out if was right, but was unable to do so.  However, I did see one thing that intrigued me.  The proof that our lecturer had shown us was originally formulated by Alfred Kempe, and stood unchallenged for eleven years until Percy Heawood found the problem with it.  What I found intriguing was the following: \nPercy Heawood found a graph that was a counterexample to the proof!\nSo here's my question.  Given that the four-colour theorem is correct, how is it possible to find a counterexample to an incorrect proof?  My guess is what Kempe proved was slightly stronger than the four colour theorem, and the Heawood graph is a counterexample to that slight strengthening.  But I'd be interested to find out if there's more that can be said.   \nI'd be especially interested if you have any other examples of counterexamples to incorrect proofs to correct statements.  \n",
    "proof": "Kempe's \"proof\" used induction on the number of vertices in the graph G. Given a graph with n vertices, he removed a vertex, colored the remaining graph in four colors using the inductive hypothesis, and then (and this is the hard part) re-inserted the missing vertex, which possibly resulted in having to \"fix up\" the coloring of its adjacent vertices - this is where the problem occurred.\nThe Heawood counterexample showed very clearly why the algorithm that Kempe used was invalid - when applied to Heawood's graph, it did not have the intended result. Thus it was this algorithmic portion of the proof that had a logical error that was exposed by the Heawood counterexample.\nThere are quite a few expositions of this entire affair; one chosen more or less at random is at\nhttp://web.stonehill.edu/compsci/LC/Four-Color/Four-color.htm\n",
    "tags": [
      "graph-theory",
      "proof-writing"
    ],
    "score": 18,
    "answer_score": 14,
    "is_accepted": true,
    "question_id": 123930,
    "answer_id": 123970
  },
  {
    "theorem": "Does there exist a continuous surjection from $\\Bbb R^3-S^2$ to $\\Bbb R^2-\\{(0,0)\\}$?",
    "context": "\nProve or disprove : There exists a continuous surjection from $\\mathbb{R}^3- S^2$ to $\\mathbb{R}^2-\\{(0,0)\\}$ (here $S^2\\subset\n\\mathbb{R}^3$ denotes the unit sphere defined by the equation\n  $x^2+y^2+z^2=1$),.\n\n\nThis question had appeared in TIFR GS-2018 exam for PhD admissions. How should I think about such a map?\n",
    "proof": "There is such a map :\n\nproject $\\mathbb{R}^3 \\setminus \\mathbb{S}_2$ onto $\\mathbb{R}^2$ (projection on the first two coordinates, for instance);\napply the exponential map from $\\mathbb{R}^2 \\simeq \\mathbb{C}$ onto $\\mathbb{R}^2  \\setminus \\{(0,0)\\} \\simeq \\mathbb{C}^*$.\n\nBoth are continuous surjections, so their composition still is.\n\nGiven the bounty, I think this deserves at least some additional material (or: how may someone find this answer). Let $X$ and $Y$ be two topological spaces. If there is no continuous surjective map $f$ from $X$ to $Y$, then $X$ must have some property which is preserved by continuous maps, and which $Y$ doesn't have. It turns out that there are not many such properties. On top of my head, the only general ones I can see are :\n\nCardinality : If Card(X) < Card(Y), then there is no such $f$. Example : $X = \\{0\\}$, $Y = \\{0,1\\}$.\nCompactness : If $X$ is compact and $Y$ is separable but not compact, then there is no such $f$. Example : $X = [0,1]$, $Y = \\mathbb{R}$.\nConnectedness : If $X$ is connected and $Y$ isn't, then there is no such $f$. More generally, if $X$ has less (in the sense of cardinality) connected components than $Y$, then there is no such $f$. Example : $X = (0,1)$ and $Y = \\mathbb{Z}$.\n\nAlthough, I am sure, one may find examples with other, less obvious, obstructions. Outside of these, things may get wild. For instance, in all the following cases, there exists a continuous surjective map from $X$ to $Y$:\n\n$X = C$, any Cantor set, and $Y$ is any compact metric space.\n$X=[0,1]$ and $Y = [0,1]^2$ : Peano curve.\nMore generally, $X = \\mathbb{R}$ and $Y = \\mathbb{R}^n$, with $n \\geq 1$, using variants of the Peano curve.\n$X = \\mathbb{R}^k$ and $Y = \\mathbb{R}^n$, with $k \\geq n$, using projections. Combining with he previous example, we get all $k \\geq 1$ and $n \\geq 0$.\n$X = \\mathbb{R}^k$ and $Y$ is a (separable) connected, $n$-dimensional topological manifold, with $k \\geq 1$. This is not easy to formalize and I have no reference at hand, but the basic idea is to take $k=n$, and make $\\mathbb{R}^n$ a thin tape and wrap it around the manifold (see e.g. this related discussion). I also think that one may replace $X$ by any non-compact $k$-dimensional manifold.\n\nWith that, we have the tools to answer the question. First, since $X$ ha dimension $3$ and $Y$ has dimension $2$, we reduce the dimension by projecting. We get the plane $\\mathbb{R}^2$. Then we wrap the plane around the origin in $\\mathbb{R}^2$ ; and we are lucky, since this can be done very explicitly (thanks to the exponential map).\n",
    "tags": [
      "general-topology",
      "continuity",
      "proof-writing",
      "contest-math"
    ],
    "score": 18,
    "answer_score": 23,
    "is_accepted": true,
    "question_id": 2562233,
    "answer_id": 2566284
  },
  {
    "theorem": "Inductive Proof for Vandermonde&#39;s Identity?",
    "context": "I am reading up on Vandermonde's Identity, and so far I have found proofs for the identity using combinatorics, sets, and other methods. However, I am trying to find a proof that utilizes mathematical induction. Does anyone know of such a proof?\nFor those who don't know Vandermonde's Identity, here it is:\nFor every $m \\ge 0$, and every $0 \\le r \\le m$, if $r \\le n$, then\n$$ \\binom{m+n}r = \\sum_{k=0}^r \\binom mk \\binom n{r-k} $$\n",
    "proof": "We have using the recursion formula for binomial coefficients the following for the induction step\n\\begin{align*}\n  \\binom{m + (n+1)}r &= \\binom{m+n}r + \\binom{m+n}{r-1}\\\\\n       &= \\sum_{k=0}^r \\binom mk\\binom n{r-k} + \\sum_{k=0}^{r-1} \\binom mk\\binom{n}{r-1-k}\\\\\n       &= \\binom mr + \\sum_{k=0}^{r-1} \\binom mk\\biggl(\\binom n{r-k} + \\binom n{r-1-k}\\biggr)\\\\\n       &= \\binom mr\\binom{n+1}0 + \\sum_{k=0}^{r-1} \\binom mk\\binom{n+1}{r-k}\\\\\n       &= \\sum_{k=0}^r \\binom mk \\binom{n+1}{r-k}\n\\end{align*}\n",
    "tags": [
      "combinatorics",
      "summation",
      "proof-writing",
      "binomial-coefficients"
    ],
    "score": 18,
    "answer_score": 18,
    "is_accepted": true,
    "question_id": 219928,
    "answer_id": 219938
  },
  {
    "theorem": "Proof there is a rational between any two reals",
    "context": "This is a problem from Rudin, but I wanted to add my own intuition to it. It uses Rudin's definition of Archimedean property. I'd just like to know if my version holds\n\nIf $x \\in \\mathbb R$, $y\\in \\mathbb R$ and $x<y$, then $\\exists p \\in \\mathbb Q$ such that $x < p < y$\n\nSince $x < y$, then $y-x>0$. Applying Archimedean property, we see $\\exists n \\in \\mathbb Z^+$ such that $n(y-x)>1$.\nAllow $A=\\{i\\in \\mathbb Z\\mid i > nx\\}$. Obviously $A$ is bounded by $nx$, so take $m = \\inf A$. So we have $m > nx$ and $m-1 \\leq nx$ by set and $\\inf$ definition.\nThis gives us $m-1 \\leq nx < m$. Combining and rearranging with $n(y-x) > 1$ gives us $nx < m \\leq nx +1 < ny \\implies nx < m < ny$.\nThus $x < \\frac{m}{n} < y$\n",
    "proof": "This proof works, mostly.\nFor completeness:\n\nYou should mention why $A$ is non-empty. The proof of this uses the Archimedean property.\nYou have shown that $A$ has an infimum, but you haven't shown that the infimum is in $A.$ For lots of sets of real numbers, the infimum is not in the set, so why is $A$ different?\nWorth being explicit with $nx+1<nx+n(y-x)=ny$.\nYou could re-iterate that $n>0$ which is why you can can divide by $n$ in the last step.\n\nBut the second thing is the big problem. Why is $m\\in A?$\nThe key is the result:\n\nIf $A\\subset \\mathbb Z$ is non-empty with an integer lower bound, then $A$ has a minimum element. That is, $\\inf A \\in A.$\n\nThis follows from the well-ordering principal of the positive integers:\n\nIf $A\\subset \\mathbb Z^+$ is non-empty, then it has a minimal element.\n\nWe know that our $A$ has a real lower bound, $xi$. So we need to show that there is an integer $k$ such that $k<xi,$ which is then an integer lower bound for $A.$\nThe integer $k$ can be shown to exist again by the Archimedean property. Find positive integer $K$ so that $-xi<K\\cdot 1$ and then $k=-K.$\n\nSo what this proof is showing is how much our intuitions about the real numbers and integers are related to the Archimedean property. It is used here to:\n\nShow that $n$ exists.\nShow that $A$ is non-empty.\nShow that $A$ has a least element.\n\n",
    "tags": [
      "proof-writing",
      "solution-verification",
      "real-numbers"
    ],
    "score": 18,
    "answer_score": 14,
    "is_accepted": true,
    "question_id": 2602418,
    "answer_id": 2602442
  },
  {
    "theorem": "How to translate mathematical intuition into a rigorous proof?",
    "context": "I've seen a lot of questions about how to develop mathematical intuition, but often I have the opposite problem.\nSeveral times I have run into a situation where I want to solve a math problem, and I play around with it until my intuition reaches the point where I have a good idea of how the complete proof might be structured. But then when it comes to actually write out the proof, I have trouble translating this intuition into a rigorous proof. I want to give the following example, from this PDF of Putnam training problems.\n\n1.13. Prove that for every $n\\ge 2$, the expansion of $(1+x+x^2)^n$ contains at least one even coefficient.\n\nWhen first thinking about this problem, I wanted to look at the polynomials $\\pmod{2}$ so that \"even coefficient\" is simplified to \"zero coefficient.\"\nThen from doing a few computations, I notice a pattern.\n\\begin{align*}\n(1+x+x^2)^2&=(1+x+x^2)+x(1+x+x^2)+x^2(1+x+x^2)\\\\\n&=1+x+x^2\\\\\n&+x+x^2+x^3\\\\\n&+x^2+x^3+x^4\\\\\n&=1+x^2+x^4\n\\end{align*}\nI notice that multiplying a polynomial by $(1+x+x^2)$ is like adding that polynomial with itself three times, each shifted over. The $x$ shifts it over by one place and the $x^2$ shifts it over by two places.\nContinuing this pattern, we can get the coefficients of $(1+x+x^2)^3$ as follows:\n\\begin{array}{ccccccc}\n1&0&1&0&1&&\\\\\n&1&0&1&0&1&\\\\\n&&1&0&1&0&1\\\\\n\\hline\n1&1&0&1&0&1&1\n\\end{array}\nSo $(1+x+x^2)^3\\equiv 1+x+x^3+x^5+x^6 \\pmod{2}$\nLet's make a triangle of a few more results:\n\\begin{array}{cccccccccccc}\n0:&&&&&&1&&&&&\\\\\n1:&&&&&1&1&1&&&&\\\\\n2:&&&&1&0&1&0&1&&&\\\\\n3:&&&1&1&0&1&0&1&1&&\\\\\n4:&&1&0&0&0&1&0&0&0&1&\\\\\n5:&1&1&1&0&1&1&1&0&1&1&1\\\\\n\\end{array}\nWe see that the structure of our problem is essentially the same as an elementary cellular automaton (specifically, rule 150). I will come back to this later.\nI notice another peculiar pattern with the $1$st, $2$nd, and $4$th rows: There are only $1$s on the ends and in the center. Perhaps this pattern continues for all powers of $2$. And it does, which is not hard to prove with induction.\nClaim: $\\forall n\\ge 0,\\ (1+x+x^2)^{2^n}\\equiv 1+x^{2^n}+x^{2^{n+1}}\\pmod{2}$\nBase case: $(1+x+x^2)^{2^0}\\equiv 1+x+x^2\\equiv 1+x^{2^0}+x^{2^1}\\pmod{2}$\nInductive Step:\n\\begin{align*}\n(1+x+x^2)^{2^n}&\\equiv ((1+x+x^2)^{2^{n-1}})^2\\\\\n&\\equiv (1+x^{2^{n-1}}+x^{2^n})^2\\\\\n&\\equiv 1+x^{2^n}+x^{2^{n+1}}\\pmod{2}\n\\end{align*}\nThis is where the intuition comes in that is hard for me to express rigorously. On one of the $2^n$th rows, look at the $0$ equidistant from the leftmost $1$ and the $1$ in the center. If the rightmost $1$ wasn't there, then that $0$ would stay a $0$ forever because of symmetry: any effect from the left is cancelled by an effect from the right.\nHowever, the rightmost $1$ does exist, so it will eventually change that $0$. But since effects are local in this automaton, it will require over $2^n$ more rows until the rightmost $1$ affects the $0$ we are looking at (there are over $2^n$ spaces between them). In other words, we have shown that the $0$ will stay a $0$ for all rows up to the $2^{n+1}$th row. Applying this logic on each power of two row, we show that there is always a zero coefficient from row $2$ onwards.\nIs there a more rigorous way I can express those last two paragraphs? And in general, what advice do you have for translating intuition to a rigorous proof?\n",
    "proof": "You have proved that these binary strings are symmetric so it suffices to prove the result for only the \"half-strings.\" Imagine cutting your triangle down the middle. Let $x_n$ be the $n^{th}$ bit string, which is length $n+1$.  \nWe have $x_0 = 1$, $x_1 = 11$, $x_2 = 101$, etc.\nNow you can address the $j^{th}$ bit inside $x_n$ as $x_{n,j}$.\nThis addressing system will make it easier to talk more concretely about the action of the automaton. \nYou can pin down a definition of \"locality of action\" by saying that there is some function $f(a, b, c)$ such that $\\forall n, j: x_{n,j} = f(x_{n-1, j-1}, x_{n-1,j}, x_{n-1,j+1})$ (with some additional nuisance conditions about the boundary). This means that the $j^{th}$ bit in the current string can be computed by neary-by bits in the previous string. Here $f$ represents the action of this rule 150 you mention.\nNow use $f$ again to show $ x_{n,j} = f(f(x_{n-2, j-2}, x_{n-2,j-1}, x_{n-2, j}), f(x_{n-2, j-1}, x_{n-2,j}, x_{n-2, j+1}), f(x_{n-2, j}, x_{n-2,j+1}, x_{n-2, j+2}))$ \nNow  you can see when we repeat this procedure $k$ times we will end up with a trinary abstract syntax tree depth $k$. To avoid having to talk about the exact structure of the expression at the $k^{th}$ level, we can reason about this substitution process purely formally as a process operating on the (countably infinite) set of symbols $\\{ \\underline{f}, \\underline{(}, \\underline{)} \\} \\cup \\{ \\underline{x_{i,j}}  \\}_{(i,j) \\in \\mathbb{N}^2}$ (here the commas are not meant to be part of the symbol set, they are just there to separate symbols from each other).  You can prove by induction, that for any $k$, that $x_{n,j}$ is computed by an expression using the symbols $\\{ \\underline{f}, \\underline{(}, \\underline{)} \\} \\cup  \\{\\underline{ x_{n-k, j-k}},\\underline{ x_{n-k, j-k+1}}, …,\\underline{ x_{n-k, j+k-1}},\\underline{ x_{n-k, j+k} }\\}$ (the $2k+1$ bits in the $(n-k)^{th}$ string centered on the $j^{th}$ bit) intermixed with function applications of $f$.\nFix $n$ and let $k_n$ be the distance to the previous power of $2$. (So if $n=13$, $k_n=5$ since the previous power of $2$ is $8$). And define $j_n \\equiv (n-k_n)/2$. This is the index of the central bit of the $(n-k_n)^{th}$ bit string.\nYou have demonstrated that those bits at indices $\\{ (n-k_n, m) \\}_{1 \\le m \\lt n }$  are all zero since $n-k_n$ is a power of $2$. So any well-formed expression using only the symbols $\\{ \\underline {f}, \\underline{(}, \\underline{)}\\} \\cup \\{ \\underline{x_{n-k_n, j_n-k_n}}, \\underline{x_{n-k_n, j_n-k_n+1}}, …,\\underline{x_{n-k_n, j_n+k_n-1}}, \\underline{x_{n-k_n, j_n+k_n}}\\}$ will evaluate to zero by $f(0,0,0) = 0$, and in particular the expression representing $x_{n,j_n}$ (created from the $k_n$ iterations of the formal symbol substitution procedure) evaluates to $0$. \nSo as $n$ increments, $x_{n,j_n}$ traces out the path of the central bits you were referring to, all of which are zero.\nGeneral thoughts about formalization:\nThere are certain technical devices which are work horses of the formalization process. Two of them I have used in this proof:\n\nindexing time and space, with a numeric coordinate system \nencoding a time evolving system as function application\n\nA third device which I used is not as common, and comes from computer science and logic:\n\nreasoning about computation by reasoning about string substitutions\n\nSomething that I have been realizing is that these devices are not obvious (at least to me). Humanity was around for a long time before the Cartesian coordinate system was discovered and used to formalize geometrical intuition. It took even longer to develop the idea that almost every mathematical object could be encoded as an intricate tower of sets (ZFC).\nI guess for me it's a matter of building up a library of these devices and binding them to the correct intuitions by repeated use.\nIn certain areas like computer programming, there are very few tools for converting intuition into proof. There were a couple of very good devices created like \n\nloop invariants\nHoare logic\n\nbut multithreaded and heap-allocating programs are still an active area of research.\n",
    "tags": [
      "polynomials",
      "proof-writing",
      "intuition",
      "cellular-automata"
    ],
    "score": 18,
    "answer_score": 3,
    "is_accepted": true,
    "question_id": 2830788,
    "answer_id": 2831036
  },
  {
    "theorem": "Proving the set of the strictly increasing sequences of natural numbers is not enumerable.",
    "context": "How would one proceed to prove this statement?\n\nThe set of the strictly increasing sequences of natural numbers is not enumerable.\n\nI've been trying to solve this for quite a while, however I don't even know where to start.\n",
    "proof": "As other answers note, there are lots of fancy ways to prove this. But we can always go back to the basics. A straightforward diagonalization proof-by-contradiction suffices. Suppose there is such an enumeration. Maybe this is it:\n1 --> 1, 2, 3, 5, ...\n2 --> 4, 5, 7, 100, ...\n3 --> 1, 2, 3, 8, ...\n4 --> 2, 4, 5, 6, ...\n\nNow take the first number of sequence one, and add one to it.  That's our first number: 2.\nNow take the second number of sequence two - 5 - and the number from the previous step - 2. Take the larger and add one: 6.\nNow take the third number of sequence three - 3 - and the number from the previous step - 6. Take the larger and add one: 7.\nNow take the fourth number of sequence four - 6 - and the number from the previous step - 7. Take the larger and add one: 8.\nKeep doing that and construct the sequence of monotone increasing naturals:\n2, 6, 7, 8, ...\n\nBy assumption, this sequence is in our enumeration, but where can it be? It cannot be at spot n for any n because by its construction the nth element of this sequence is larger than the element at spot n of the nth sequence.\nThat's a contradiction, and therefore there cannot be any such enumeration.\n",
    "tags": [
      "real-analysis",
      "sequences-and-series",
      "proof-writing",
      "set-theory"
    ],
    "score": 17,
    "answer_score": 54,
    "is_accepted": true,
    "question_id": 2439340,
    "answer_id": 2439666
  },
  {
    "theorem": "Sum of $k {n \\choose k}$ is $n2^{n-1}$",
    "context": "Proof that $\\suṃ̣_{k=1}^{n}k {n \\choose k}$ for $n \\in \\mathbb N$ is equal to $n2^{n-1}$.\nAs a hint I got that $k {n \\choose k} = n {n-1\\choose k-1} $.\nI tried solving this by induction but, in the inductive step I'm not arriving to the correct result.\n",
    "proof": "Just change the index $s=k-1$\n$$\\sum_{k=1}^{n}k {n \\choose k} = n\\sum_{k=1}^{n} {n-1\\choose k-1}= n\\sum_{s=0}^{n-1} {n-1\\choose s} =n2^{n-1}$$\n",
    "tags": [
      "combinatorics",
      "summation",
      "proof-writing",
      "binomial-coefficients"
    ],
    "score": 17,
    "answer_score": 19,
    "is_accepted": true,
    "question_id": 683733,
    "answer_id": 683740
  },
  {
    "theorem": "Are there any generic thinking approaches for providing mathematical proofs to a given theorem",
    "context": "To produce mathematical proofs for theorems we should have the required knowledge in that area. But even having adequate knowledge, people like me struggle a lot for writing down the proofs for any given theorem. \nIs there any way I can improve these skills? Are there any generic thinking approaches for providing mathematical proofs to a given theorem?\n",
    "proof": "In the absence of more information I will give a generic answer.\nSome general resources on the topic:\n\nhttp://www.tricki.org (wiki-style collection of proof techniques)\nPólya: How to solve it (this book is a classic on the topic)\nEngel: Problem-Solving Strategies (another classic with lots of problems)\nSolow: How to Read and Do Proofs (also highly recommended)\nHouston: \"How to Think like a Mathematician\" (introductory and nice companion to Engel's)\n\nSome general strategies for attacking a problem that is better understood with examples:\n\nTry small (to medium sized) cases\nGeneralize\nSpecialize\nCombine Generalize-Specialize by adding assumptions and weakening conclusions freely during your first proof attempts, then try to remove or weaken assumptions and to strengthens the conclusions\nDraw conclusions from the result\nPut it away and look at it again later\n\nStrategies to improve your skills:\n\nTry to solve lots of problems\nRead solutions after spending time trying\nDiscuss your thoughts with others\nBecome teaching assistant or tutor, as teaching/explaining a topic to others helps your understanding a lot\nRewrite your proofs until they have textbook quality\nWhen reading proofs in textbooks, first check that the proof is correct and complete by verifying each step, then lean back to get the full picture by extracting the key ideas and finally look again at the details to learn the techniques how to write down precisely a maybe vague idea\nReflect why you did not find a particular solution\nIf your strategy of proving something fails, ask yourself whether there is any reason that your strategy had no hope of working (e.g. because the same strategy would have proven a stronger result, which is in fact false).\n\n(This post has been wikified in case someone wants to add.)\n",
    "tags": [
      "proof-writing",
      "intuition",
      "advice"
    ],
    "score": 17,
    "answer_score": 22,
    "is_accepted": true,
    "question_id": 38760,
    "answer_id": 38764
  },
  {
    "theorem": "Proving the limit of a function of a sequence is equal to the function of the limit of that sequence",
    "context": "Suppose $f$ is a continuous function at $x = c$ in $[a,b]$.  Prove that for any sequence ${x_n}$ in $[a,b]$ converging to $c$, the sequence $\\{f(x_n)\\}$ converges to $f(c)$.  That is, $$ \\lim_{n\\to\\infty}f(x_n)= f\\left(\\lim_{n\\to\\infty}x_n\\right)$$\n\nThis proof seems simple but there are a few things that I need to know first. If $\\{x_n\\}$ converges to $c$, is it sufficient to substitute $c$ in for $\\lim_{n\\to\\infty}x_n$?  Also needing some guidance on the structure of this proof.  Thanks!\n",
    "proof": "Just write down the definitions:\n\n$x_n$ converges to $c$ if and only if $\\forall \\delta > 0$ there exists $N = N(\\delta)$ such that $\\forall n \\ge N$ we have $|x_n - c| < \\delta$\n$f$ is continuous if and only if $\\forall \\eta > 0$ there exists $\\gamma = \\gamma(\\eta)$ such that if $|x - y| < \\gamma$ then $|f(x) - f(y)| < \\eta$.\n\nNow we want to prove the following claim\n\n$\\forall \\epsilon > 0$ there exists $M = M(\\epsilon)$ such that if $n \\ge M$ then we have $|f(x_n) - f(c)| < \\epsilon$.\n\nHint: if $x_n \\to c$ then you can make $|x_n - c|$ small enough to use the continuity of $f$ (say, for example, smaller than $\\gamma(\\epsilon)$).\n",
    "tags": [
      "real-analysis",
      "proof-writing"
    ],
    "score": 17,
    "answer_score": 8,
    "is_accepted": true,
    "question_id": 554910,
    "answer_id": 554927
  },
  {
    "theorem": "Do powers of 256 all end by 6 and if so, how to prove it?",
    "context": "I computed the 10 first powers of 256 and I noticed that they all end by 6.\n256^1 = 256\n256^2 = 65536\n256^3 = 16777216\n256^4 = 4294967296\n256^5 = 1099511627776\n256^6 = 281474976710656\n256^7 = 72057594037927936\n256^8 = 18446744073709551616\n256^9 = 4722366482869645213696\n256^10 = 1208925819614629174706176\n\nMy intuition is that it is the same for all powers of 256 but I can't figure out how to prove it, any suggestion?\nMy attempt was to show that $\\forall n$ there is a $k$ such that $2^{8n} = k*10 + 6$, $k$ and $n$ being non null integers. I tried to decompose the right member in powers of $2$ but that leaves me stuck.\n",
    "proof": "$$(10x+6)(10y+6)=100xy+60 (x+y)+3\\color {red}6. $$\n",
    "tags": [
      "proof-writing",
      "modular-arithmetic",
      "exponentiation"
    ],
    "score": 16,
    "answer_score": 50,
    "is_accepted": true,
    "question_id": 3107353,
    "answer_id": 3107358
  },
  {
    "theorem": "Has Euler&#39;s Constant $\\gamma$ been proven to be irrational?",
    "context": "I found a paper by Kaida Shi called \"A Proof: Euler’s Constant γ is an Irrational Number\" which claims to have proven the irrationality of $\\gamma$.\nI know people have been trying to prove that $\\gamma$ is irrational or not for hundreds of years.  I could not find very much on this paper online.  Does this paper in fact have a conclusive proof that $\\gamma$ is irrational?  \n",
    "proof": "No, the paper is incorrect, and in many places. A quick way to check would be to see if it were published anywhere (it isn't). This isn't conclusive, but such a result would almost certainly be sent to some peer-reviewed journal.\nI should also note some of the incredible things Kaida Shi has 'proven' in addition to Hilbert's 7th problem:\n\nHere's a link to his combined proofs of the Goldbach Conjecture, the Twin Primes Conjecture, and some parts of the vastly stronger Schinzel's hypothesis (which he doesn't mention in the abstract, but it nonetheless there).\nHere's a link to a geometric proof of the Riemann Hypothesis. But don't get too excited, because...\nHere's a link to a geometric proof of the generalized Riemann Hypothesis.\n\n",
    "tags": [
      "proof-writing",
      "irrational-numbers",
      "euler-mascheroni-constant"
    ],
    "score": 16,
    "answer_score": 22,
    "is_accepted": true,
    "question_id": 147505,
    "answer_id": 147512
  },
  {
    "theorem": "Intersection of two open dense sets is dense",
    "context": "Let $X$ be a topological space and suppose that $H$ and $G$ are open dense subsets of $X$.Then show that $G \\bigcap H$ is also an open dense subset of $X$.\nMy attempt :\nWell since the finite intersection of open sets is open therefore, $G \\bigcap H$ is also open.\nI was trying to prove the other part by the method of contradiction :\nSuppose on the contrary that $G \\bigcap H$ is not dense. Then that implies that $Int(Cl(G \\bigcap H)) = \\phi$ .\nNow, $Cl(G \\bigcap H) \\subset Cl(G) \\bigcap Cl(H)$\nso, $Int(Cl(G \\bigcap H)) \\subset Int(Cl(G) \\bigcap Cl(H)) = Int(Cl(G)) \\bigcap Int(Cl(H)) = X $.\nHow do i proceed further to get a contradiction ?\n",
    "proof": "You can't proceed further from that point since the inclusion you get at the end doesn't give you anything. I suggest a direct approach. To show that $G\\cap H$ is dense in $X$, it suffices to show that $G\\cap H$ intersects every nonempty open subset of $X$. To this end, let $V$ be a nonempty open subset of $X$. Since $G$ is open and dense in $X$, $G \\cap V$ is a nonempty open set. Then since $H$ is dense in $X$, $H\\cap G \\cap V$ is nonempty, as desired.\n",
    "tags": [
      "general-topology",
      "proof-writing"
    ],
    "score": 16,
    "answer_score": 25,
    "is_accepted": true,
    "question_id": 1143211,
    "answer_id": 1143218
  },
  {
    "theorem": "Difficulty in Mathematical Writing",
    "context": "Lots of people (including myself) face lot of problems in tackling Mathematical Problems, which appear as if we can solve it, but then writing out a solution becomes difficult.\nLet us consider some examples:\n\nI was asked this question, some time back in an exam. Give an example of a continuous function on $(a,b)$ which is not uniformly continuous. Well, one's obvious choice is $$f(x) = \\frac{1}{x-a} \\quad \\text{or} \\ \\frac{1}{x-b}$$ I knew this as soon as I saw the problem, and started proving it. One actually has to make an observation that as $x \\to a$ then, $\\frac{1}{x-a}$ will be larger. But I found that I couldn't actually formally prove it.\n\nSimilarly, to prove that $f(x)=x^{2}$ is not uniformly continuous on $\\mathbb{R}$, one again has to play with the quantifiers, to get the contradiction part.\n\n\nSo, these are two instances, where I have found the problem, which to me appeared that I could solve it, but writing out a formal solution became difficult.\nHow can students improve upon this? Are there any instances, which happened to you like this!\n",
    "proof": "I have a (joke) template file for writing papers, which contains in it\n\nLemma (Main technical lemma)\nLet $D$ be a domain in (INSERT SPACE HERE) such that the following properties hold:\n\nTechnical condition 1\nTechnical condition 2\n\nThen $D$ is both opened and closed in (INSERT SPACE AGAIN).\n\n\nWhat is the point of the above? At some point in your mathematical career you will come to the realisation that the proofs you personally are going to write are all based on the few small set of technical arguments. For what I do the main tool happens to be the Method of Continuity. By the time you have this realisation, it will also be completely obvious to you how to formulate a given proof to fit the template.\nBut how to you come to this realisation? My only suggestion is to read more papers/books/proofs and write more of them yourselfs. Just like a foreign language, the only way to get better and converting your intuitive ideas into formal arguments is through practice and immersion.\nAs an aside, from the examples you gave, it is not quite clear whether your difficulty is with implementing $\\epsilon$ - $\\delta$ s, or with setting up the proof by contradiction.\nNow, besides the usual proofs in textbooks, a good place to read up on proof techniques is Proofs from the BOOK by Aigner and Ziegler. Try to really figure out the details of each proof so you can explain the idea of it, a few days later, without having the book open. Another good resource for problem solving techniques is the Tricki. Of course, reading up on answers (and providing them) on this website would also help.\n\nA bit on the quantifier issue. First you need to mentally nest the various implications. For example, diagrammatically I think (the following is not formal logic notation) uniform continuity to be something like\n$$ \\forall \\epsilon \\to \\left( \\exists \\delta \\to ( \\forall |x-y| < \\delta \\to |f(x) - f(y)| < \\epsilon ) \\right) $$\nSo to contradict it by example, you want something that satisfies the hypothesis $\\forall \\epsilon$ for some $\\epsilon_0$ but not the conclusion $\\exists \\delta \\ldots$. Which means that for $\\epsilon_0$ there cannot exist a $\\delta$ with the requisite property. Which means that for such $\\epsilon_0$ and for each $\\delta$ the requisite property must be false. So after step 1 we have\n$$ \\exists \\epsilon_0 \\forall \\delta \\to \\mbox{ not } ( \\forall |x-y| < \\delta \\to |f(x) - f(y)| < \\epsilon_0 ) $$\nIf the statement inside the parentheses were to be false, since it is a $\\forall$ statement, you just need one example. So you can convert the negation to\n$$ \\exists \\epsilon_0 \\forall \\delta \\to \\left( \\exists |x-y| < \\delta \\mbox{ and } |f(x) - f(y)| > \\epsilon_0 \\right) $$\nSo there, we have converted a \"negative statement\" which we want to contradict, to a \"positive statement\" of a property we want our function to have.\n",
    "tags": [
      "soft-question",
      "proof-writing"
    ],
    "score": 16,
    "answer_score": 11,
    "is_accepted": true,
    "question_id": 8024,
    "answer_id": 8026
  },
  {
    "theorem": "Proving $\\gcd \\left(\\frac{a}{\\gcd (a,b)},\\frac{b}{\\gcd (a,b)}\\right)=1$",
    "context": "How would you go about proving that $$\\gcd \\left(\\frac{a}{\\gcd (a,b)},\\frac{b}{\\gcd (a,b)}\\right)=1$$\nfor any two integers $a$ and $b$?\nIntuitively it is true because when you divide $a$ and $b$ by $\\gcd(a,b)$ you cancel out any common factors between them resulting in them becoming coprime. However, how would you prove this rigorously and mathematically?\n",
    "proof": "Very simply it can be done like this: $\\gcd(a,b)=d$.\nNow we ask can: $\\gcd(\\frac{a}{d},\\frac{b}{d})=e$ for $e>1$?\nWell, this implies $e\\mid\\frac{a}{d},e\\mid\\frac{b}{d} \\Rightarrow em=\\frac{a}{d}, en=\\frac{b}{d} \\Rightarrow dem=a,den=b \\Rightarrow de$ is a common divisor of $a,b$ which is greater than $d$, thus a contradiction as $d$ by definition was supposed as the $\\gcd$. Hence, $e=1$.\n",
    "tags": [
      "elementary-number-theory",
      "proof-writing",
      "divisibility",
      "gcd-and-lcm"
    ],
    "score": 16,
    "answer_score": 6,
    "is_accepted": true,
    "question_id": 752928,
    "answer_id": 752949
  },
  {
    "theorem": "Show that the closure of a set A is the smallest closed set containing A.",
    "context": "I need to prove that $\\bar A$ (the closure of set A) is the smallest closed set containing A. We have already proved that it is a closed set, so now I just need to show that it is the smallest one.\nI have written a preliminary proof that I don't think is particularly rigorous, and would be grateful if someone could give me some pointers. We haven't covered anything regarding metric spaces or anything else in topology, so I had trouble understanding other solutions posted on the site.\n\nLet $A$ be a non-empty set, and $\\bar A$ the closure of $A$ (the union of $A$ and all of its limit points). Let $B$ be a closed set with $A \\subset B \\subset \\bar A$ and $B \\neq \\bar A$.\nSince $A \\subset B$ and $B \\subset \\bar A$, $B$ consists of all the elements of $A$ and some (but not all) of its limit points. However, this means that there are sequences contained entirely within $B$ whose limit points are not elements of $B$. Thus, $B$ is not closed, posing a contradiction to the original statement.\n\n",
    "proof": "I think it is clearer without a contradiction argument:\n\nLet $B$ be a closed set satisfying $A\\subset B$. Let $a$ be a limit point of $A$, so that we can find a sequence $(a_{n})_{n}\\subset A$ with $a_{n}\\rightarrow a$. Since this sequence is also contained in $B$ and $B$ is closed, it follows that $a \\in B$. Because $a$ was an arbitrary limit point, $B$ contains the closure of $A$.\n\n",
    "tags": [
      "general-topology",
      "proof-verification",
      "proof-writing"
    ],
    "score": 16,
    "answer_score": 14,
    "is_accepted": false,
    "question_id": 1996918,
    "answer_id": 1996937
  },
  {
    "theorem": "&quot;The following are equivalent&quot;",
    "context": "What does it mean for several statements to be equivalent? And why does it suffice to prove a \"cyclic\" chain\n$$A_1\\implies A_2\\implies \\cdots\\implies A_n\\implies A_1$$\nin order to show that the conditions $A_1, \\dots, A_n$ are equivalent?\n",
    "proof": "Two statements are equivalent if\n$$A_1\\Rightarrow A_2\\text{ and } A_2\\Rightarrow A_1.$$\nMore than two statements are equivalents if any two of them are equivalent.\nSo if you have for instance \n$$A_1\\Rightarrow A_2\\Rightarrow A_3\\Rightarrow A_1$$\nthen\n$$A_2\\Rightarrow A_1$$\nso $A_1\\iff A_2$. In the same way you could show that $A_2\\iff A_3$ and $A_1\\iff A_3$.\nIt is the basic idea, and it is working for any chain of length $n$ (you can prove it by induction).\n",
    "tags": [
      "logic",
      "proof-writing",
      "terminology"
    ],
    "score": 16,
    "answer_score": 14,
    "is_accepted": false,
    "question_id": 1929453,
    "answer_id": 1929457
  },
  {
    "theorem": "A proof that the Cantor set is Perfect",
    "context": "I found in a book a proof that the Cantor Set $\\Delta$ is perfect, however I would like to know if \"my proof\" does the job in the same way.\n\nTheorem: The Cantor Set $\\Delta$ is perfect.  \nProof:   Let $x \\in \\Delta$ and fix $\\epsilon > 0$. Then, we can take a $n_0 = n$ sufficiently large to have $\\epsilon > 1/3^{n_0}$.\n  Thus, the interval $[a, b]$ where $x$ lies is a subset of $B_\\epsilon\n> (x)$. Hence, by iterating the construction of the Cantor set for $N >\n n_0$, we have intervals of length $1/3^N$ all included in $B_\\epsilon\n (x)$, but with only one of those intervals such that $x$ lies within.\n\nThe intution behind the proof was that we should prove that for every $x$, if $x \\in \\Delta$, then for every $\\epsilon >0$, $B_\\epsilon (x) \\setminus \\{x\\} \\cap \\Delta \\neq \\varnothing$.  \nNow, I do not particularly like my reference to the $[a, b]$ interval that is not mentioned before. Moreover, here – by choosing a closed interval – I am trying to address all at once the case in which $x$ is an endpoint of one of the closed intervals that form $\\Delta$. Finally, I did not close the proof with a statement like \"Thus, there are infinitely many points that differ from $x$ and that lie within $B_\\epsilon (x)$.  \nIn the end, I am not completely sure if this can be considered a proof or not. The intuition is correct (I am kind of positive about it), but I am not sure if I was actually able to write down my intuition in a good way.  \nAs always any feedback is more than welcome.\nThank you!\n",
    "proof": "Your idea is sound, but you’ve not expressed it clearly enough for you to have a real proof. I’ll write up an argument along the general lines that you have in mind.\n\nLet $x\\in\\Delta$ and $\\epsilon>0$ be arbitrary. Choose $n\\in\\Bbb N$ large enough so that $3^{-n}<\\epsilon$. $C_n$, the $n$-th stage in the standard construction of $\\Delta$, is the union of $2^n$ pairwise disjoint closed intervals, each of length $3^{-n}$; let $I$ be the one of these intervals containing $x$, clearly $I\\subseteq B_\\epsilon(x)$.\nNow consider $C_{n+1}$: it’s a disjoint union of $2^{n+1}$ closed intervals, each of length $3^{-(n+1)}$, and exactly two of these intervals, say $I_0$ and $I_1$, are subsets of $I$. Let $I_0$ be the one that contains $x$. $\\Delta\\cap I_1$ is non-empty for the same reason that $\\Delta$ is non-empty (why is that?), so let $y\\in\\Delta\\cap I_1$. Then $y\\in\\Delta\\cap\\big(B_\\epsilon(x)\\setminus\\{x\\}\\big)$, and since $\\epsilon>0$ was arbitrary, $x$ is a limit point of $\\Delta$. Finally, $x\\in\\Delta$ was arbitrary, and $\\Delta$ is closed, so $\\Delta$ is perfect. $\\dashv$\n\nIt isn’t actually necessary to split $I$ into $I_0$ and $I_1$. Let $I=[a,b]$; then $a,b\\in\\Delta\\cap B_\\epsilon(x)$, and since $x$ cannot be equal to both $a$ and $b$, $\\Delta\\cap\\big(B_\\epsilon(x)\\setminus\\{x\\}\\big)\\ne\\varnothing$.\n",
    "tags": [
      "real-analysis",
      "proof-verification",
      "proof-writing",
      "cantor-set"
    ],
    "score": 16,
    "answer_score": 20,
    "is_accepted": true,
    "question_id": 1090736,
    "answer_id": 1091250
  },
  {
    "theorem": "How to explain that proof is important",
    "context": "I don't know if this is the right place to post this or not, but I will go ahead anyway (sorry if it ain't the right place)\nYesterday I was discussing a particular theorem of geometry with my brother which he just learnt in the school. I had asked him if he knew the proof for it, he replied saying his teacher has said that wouldn't be necessary.\nThen, I asked him to sit and try to prove the theorem. He said that knowing the theorem counts for more than knowing the proof. How do I explain to him that knowing the proof is more important and how it can even help expand his thinking?\nI know this question doesn't have a single pointed answer as is pre-requisite for questions posted here, but I would appreciate any replies\n",
    "proof": "In my opinion, they're not important. But let me qualify that in a massive way.\nSuppose we're interested in solving some particular real-life problem. We're going to make some \"let\" statements. Like maybe we've got $(x,y) \\in \\mathbb{R} \\times \\mathbb{R}$. And to solve the problem, we're going to let $(r,\\theta)$ satisfy $x = r\\cos \\theta$ and $y = r\\sin \\theta$. Why are we allowed to do this? We're actually leveraging a theorem:\n\nFor all $x,y \\in \\mathbb{R}$, there exists $r \\in \\mathbb{R}_+$ and $\\theta \\in \\mathbb{R}$ such that $x = r\\cos \\theta$ and $y = r\\sin \\theta$.\n\nThis legitimizes the \"Let\" statement. So theorems tell us the rules of the game; and therefore, the range of problems the human race is capable of solving is hard-limited by the theorems we know. So theorems are important.\nDoes that mean you personally ought to know the proof of a theorem? That we're not allowed to use it 'til we know the proof? Of course not.\nThat being said, if you want the ability to prove your own theorems and thereby move the human race forward, expanding the range of problems that we're capable of solving, well you'd better start going beyond the question: \"What can I do with this theorem?\"\nYou'd better start asking: \"How do we even know its true?\"\nBy the way, most theorems in mathematics aren't proved because of their direct applications to real world problems. This begs that we ask: \"Why should we care about theorems that don't have direct applications to real-life problems?\" You should post another question if this topic interests you.\n",
    "tags": [
      "soft-question",
      "proof-writing"
    ],
    "score": 16,
    "answer_score": 2,
    "is_accepted": true,
    "question_id": 600800,
    "answer_id": 602828
  },
  {
    "theorem": "Checking that a $3$-D diagram is commutative",
    "context": "When proving certain results I need to use commutative diagrams, some of which quite complicated. My question is:\nDo we need to check every small square all the time to make sure that they are all commutative? \nAs an example, if we have the following diagram. If in my proof I wrote \"Consider the following commutative diagram\":\n\nBefore discussing anything else, I need to prove that it is indeed commutative. There are $11$ small squares to verify. When reading papers/books, I seldom see the author verifies every small square is commutative. \nIs there any alternative other than checking all small squares, if I want to claim that a complicated diagram is commutative?\n",
    "proof": "The most common way to get out of checking all the small squares is to have some monic or epic arrows in your diagram. For instance, suppose we knew the square from $B_1$ to $C_2$ was commutative, that $B_2\\to C_2$ was monic, and that the large rectangle from $A_1$ to $C_2$ was commutative (that's a bit strong here since it's already epic, but this is discussion applies more generally.) Then by canceling the monic arrows we could deduce commutativity of the square from $A_1$ to $B_2$. Of course, you can dualize, and you can use this on cubes as well as rectangles: if you knew the top, bottom, front, back, and right faces of the $A,B$ cube were commutative then $A_2'\\to B_2'$ being monic implies the left face is commutative.\nBut in general, even checking all but one square does not suffice, as you can see in your diagram: consider setting the $A_i'$ and $B_i'$ and all four $C$s to zero (then everything but the top left square automatically commutes, but we know nothing about the latter.)\n",
    "tags": [
      "commutative-algebra",
      "proof-writing",
      "alternative-proof",
      "exact-sequence",
      "article-writing"
    ],
    "score": 16,
    "answer_score": 8,
    "is_accepted": false,
    "question_id": 1115910,
    "answer_id": 1135741
  },
  {
    "theorem": "Why is it that when proving trig identities, one must work both sides independently?",
    "context": "Suppose that you have to prove the trig identity:\n$$\\frac{\\sin\\theta - \\sin^3\\theta}{\\cos^2\\theta}=\\sin\\theta$$\nI have always been told that I should manipulate the left and right sides of the equation separately, until I have transformed them each into something identical. So I would do:\n$$\\frac{\\sin\\theta - \\sin^3\\theta}{\\cos^2\\theta}$$\n$$=\\frac{\\sin\\theta(1 - \\sin^2\\theta)}{\\cos^2\\theta}$$\n$$=\\frac{\\sin\\theta(\\cos^2\\theta)}{\\cos^2\\theta}$$\n$$=\\sin\\theta$$\nAnd then, since the left side equals the right side, I have proved the identity. My problem is: why can't I manipulate the entire equation? In this situation it probably won't make things any easier, but for certain identities, I can see ways to \"prove\" the identity by manipulating the entire equation, but cannot prove it by keeping both sides isolated.\nI understand, of course, that I can't simply assume the identity is true. If I assume a false statement, and then derive from it a true statement, I still haven't proved the original statement. However, why can't I do this:\n$$\\frac{\\sin\\theta - \\sin^3\\theta}{\\cos^2\\theta}\\not=\\sin\\theta$$\n$$\\sin\\theta - \\sin^3\\theta\\not=(\\sin\\theta)(\\cos^2\\theta)$$\n$$\\sin\\theta(1 - \\sin^2\\theta)\\not=(\\sin\\theta)(\\cos^2\\theta)$$\n$$(\\sin\\theta)(\\cos^2\\theta)\\not=(\\sin\\theta)(\\cos^2\\theta)$$\nSince the last statement is obviously false, is this not a proof by contradiction that the first statement is false, and thus the identity is true?\nOr, why can't I take the identity equation, manipulate it, arrive at $(\\sin\\theta)(\\cos^2\\theta)=(\\sin\\theta)(\\cos^2\\theta)$, and then work backwards to arrive at the trig identity. Now, I start with a statement which is obviously true, and derive another statement (the identity) which must also be true - isn't that correct?\nAnother argument that I have heard for keeping the two sides isolated is that manipulating an equation allows you to do things that are not always valid in every case. But the same is true when manipulating just one side of the equation. In my first proof, the step\n$$\\frac{\\sin\\theta(\\cos^2\\theta)}{\\cos^2\\theta}$$\n$$=\\sin\\theta$$\nis not valid when theta is $\\pi/2$, for example, because then it constitutes division by zero.\n",
    "proof": "You've got a pretty good handle on the situation.  It's not so much that you can't manipulate the potential identity as an equation as that, in general, most people shouldn't manipulate the potential identity as an equation.  The key part is what you said—use the manipulation to arrive at a true statement (that's your scratch-work), then work backwards to write your proof: starting with a true statement and arriving at the identity.\nIn your last example, since $\\cos\\theta$ is in the denominator, $\\theta=\\frac{\\pi}{2}$ would not be in the domain of the identity, so it's okay to simplify to $\\sin\\theta$.\n",
    "tags": [
      "trigonometry",
      "proof-writing",
      "learning"
    ],
    "score": 16,
    "answer_score": 6,
    "is_accepted": true,
    "question_id": 101053,
    "answer_id": 101057
  },
  {
    "theorem": "Proof of Non-Ordering of Complex Field",
    "context": "Let $\\mathcal F$ be a field. Suppose that there is a set $P \\subset \\mathcal F$ which satisfies the following properties:\n\nFor each $x \\in \\mathcal F$, exactly one of the following statements holds: $x \\in P$, $-x \\in P$, $x =0$. \nFor $x,y \\in P$, $xy \\in P$ and $x+y \\in P$. \n\nIf such a $P$ exists, then $\\mathcal F$ is an ordered field. \nDefine $x \\le y \\Leftrightarrow y -x \\in P \\vee x = y$. \nExercise: Prove that the field of complex numbers $\\mathbb C$ cannot be given the structure of an ordered field. \nMy Work So Far: (Edit 1 note: This section and the Question is at the beginning, simply leaving this up for reference as to where I started)\nLet $i$ be such that $i \\in P, i \\ne 0 \\Rightarrow i > 0$. But $i^2 = -1 \\notin P$. \nMy Question: I am not sure how much I need to redefine, and how I go about rigorously making this patchwork argument airtight. I am aware that I have not addressed how I assumed that $-1 \\notin P$, but I'm not sure how to distinguish between $1$ and $i$ in this proof. \n\nEdit #1 \n1st Step: Showing that $-1 \\notin P$, observe that $(-1)(-1) = 1$ therefore if $-1 \\in P$, both $x, -x \\in P$, a contradiction. \n2nd Step: To show $i \\notin P$, we have that if $i \\in P \\Rightarrow i^2 \\in P$, but $i^2 = -1 \\notin P$, so $i \\notin P$. \n3rd Step: To show $-i \\notin P$, we have $(-i)(-i) = i^2 \\notin P$, so $-i$ cannot be in $P$. \nConclusion: Since $i \\ne 0$, and $i, -i \\notin P$, there is no set $P \\subset \\mathbb C$ that satisfies the above properties, thus $\\mathbb C$ is not ordered. \nThank you André Nicolas and Eric Stucky for your help!\n",
    "proof": "To show that $-1$ is not in $P$, note that if $-1\\in P$ then $(-1)(-1)\\in P$, which contradicts the fact that if $x \\ne 0$ exactly one of $x$ and $-x$ is in $P$.\nNext we show that  $i\\notin P$. Suppose to the contrary that $i\\in P$. Then $i^2\\in P$, which contradicts the fact that $-1\\notin P$.\nThe same argument shows that $-i\\notin P$. This contradicts the fact that if $x\\ne 0$, then exactly one of $x$ and $-x$ is in $P$. \n",
    "tags": [
      "proof-writing",
      "ordered-fields"
    ],
    "score": 16,
    "answer_score": 9,
    "is_accepted": true,
    "question_id": 181720,
    "answer_id": 181732
  },
  {
    "theorem": "Proof that $1729$ is the smallest taxicab number",
    "context": "For homework I have to produce the proof (algebraic or otherwise) to show that $1729$ HAS to be the smallest taxi cab number. A taxicab number means that it is the sum of two different cubes and can be made with $2$ sets of numbers. I have the list of the next ones and I was wondering if it was linked with the fact that it would have to be $0$ cubed if it got any lower which obviously wouldn't work.\nAny help appreciated,\nthanks in advance!\n",
    "proof": "One can prove that the smallest taxicab number is the smallest product $(6n+1)(12n+1)(18n+1)$ consisting of three primes. This means $n=1$, and $7\\cdot 13\\cdot 19=1729$. I do not claim that this proof is much better than brute-force.\n",
    "tags": [
      "elementary-number-theory",
      "proof-writing"
    ],
    "score": 16,
    "answer_score": 3,
    "is_accepted": false,
    "question_id": 487537,
    "answer_id": 487592
  },
  {
    "theorem": "An interesting algorithm about prime numbers that I thought today",
    "context": "I thought up the following algorithm today:\n\nChoose $a_1\\in\\mathbb{Z}^+\\setminus\\{1\\}$. Then let $a_{n+1}=a_n+p_n$, where $p_n$ is the largest prime factor of $a_n$.\n\nThe algorithm is easy, but I have the following questions:\n\n$1)$ Is the sequence $\\{p_n\\}$ monotonically increasing?\n$2)$ Are there infinitely many primes in the sequence $\\{p_n\\}$?\n$3)$ Find all $a,b\\in\\mathbb Z^+$ such that there are infinitely many primes of the form $ak+b$ where $k$ is a nonnegative integer in the sequence $\\{p_n\\}$.\n\nI thought up a solution for $1)$ and $2)$, as follows:\n\n$1)$Assume the contrary, i.e. $p_n>p_{n+1}$ for some $n$.\nAs $p_n|a_n$, $p_n|a_n+p_n\\iff p_n|a_{n+1}$. So $p_n\\le p_{n+1}$. A contradiction rises.\n$2)$ Yes.\nAssume the contrary, there are finitely many primes in the sequence $\\{p_n\\}$.\nThen as $\\{p_n\\}$ is monotonically increasing,  So there exist an $m$ such that $\\forall i\\ge m, p_i=p_m$. So $a_i=a_m+(i-m)p_m$. Also, we let the smallest prime larger than $p_m$ be $p$. But as $(p, p_m)=1$, $\\exists i<p+m$ such that $p|a_i$. A contradiction rises.\n\nI think the above solutions seems correct, but can you help to verify?\nAlso, can someone help me to do $3)$?\nAny help is appreciated!\n",
    "proof": "1) Depending on your definition of monotonically increasing, being $p_{n} > p_{n-1}$ or $p_{n} ≥ p_{n-1}$. The first case is easily disprovable, just pick $a_{n} = 2$, and $p_{1}$ and $p_{2}$ are both 2. For the second case, since $p_{n}|a_{n} \\rightarrow p_{n}|a_{n} + p_{n} \\rightarrow p_{n}|a_{n+1}$, so $p_{n+1}$ is at least $p_{n}$. So depending on your definition, your solution to part 1 is right or wrong. Considering I did it the same way you did, I would say you are correct.\n2) Following what we have proven in part one, we just need to prove that $p_{n}$ isn't a constant eventually. One way to prove this is to assume $p_{n}$ is constant and prove that it must increase no matter the size of $p_{n}$. Call the sequence $r_{n} = \\frac{a_{k+n}}{p_{k}}$ where $r_{n}$ ends when a new value of p arises (or doesn't, but we are proving that it does). $r_{n+1} = r_{n} + 1$ since $a_{k+n} + p_{k} = a_{k+n+1}$. So as long as $\\{r_{n}\\}$ continues, it will eventually reach a prime after $p_{k}$, proving that the sequence is infinite.\n3) Considering that proving infinite primes of the form 11k+2 exist, for example, is already a hard mathematical problem, doing it for infinite cases seems quite out of our reach. Maybe someone on Math.SE can find a solution but I can only give a hypothesis, which is that any pair (a, b) works as long as gcd(b, a) = 1. Call a new sequence $p'_n$, which is just the distinct terms in $p_n$. I have made an interesting observation however, I ran a few trials with maybe 10 or so different starting $a_{1}$, and n = 10,000, and I can prove that $p'_{2}$ to $p'_{n}$ are all consecutive primes. \nSplit it up into two cases: (a) $a_{1} = p'_{1} * k$, where k is less than the next consecutive prime after $p'_{1}$. Then, $a_{2} = p'_{1} * (k+1)$, etc. and eventually $p'_{2}$ is the consecutive prime after $p'_1$. Call $a'_n$ to be the corresponding term in $\\{p'_n\\}$. Therefore, $a'_2 = p'_1*p'_2$. This can be extended to n, since $\\frac{a'_n}{p'_n} = p'_{n-1} < {p'_n}$, $p'_{n+1}$ is the consecutive prime after $p'_n$.\n(b) k is greater than the next consecutive prime after $p'_1$. Then, $p'_2$ is the next prime after k, and yet again, $\\frac{a'_n}{p'_n} = p'_{n-1} < {p'_n}$, so by the same idea as case (a), $p'_{n+1}$ must be the consecutive prime after $p'_n$.\nHopefully someone can help you with part 3, considering all arbitrarily large primes are in $p_n$.\n",
    "tags": [
      "number-theory",
      "proof-verification",
      "proof-writing",
      "prime-numbers",
      "algorithms"
    ],
    "score": 16,
    "answer_score": 2,
    "is_accepted": false,
    "question_id": 3323031,
    "answer_id": 3328444
  },
  {
    "theorem": "How does one perform induction on integers in both directions?",
    "context": "On a recent assignment, I had a question where I had to prove a certain statement to be true for all $n\\in\\mathbb{Z}$. The format of my proof looked like this:\n\nStatement is true when $n=0$\n\"Assume statement is true for some $k\\in\\mathbb{Z}$\"\nStatement must be true for $k+1$\nStatement must be true for $k-1$\n\nMy professor said the logic is flawed because of my second bullet point above. She says that since mathematical induction relies on the well-ordering principle and since $\\mathbb{Z}$ has no least or greatest element, that using induction is invalid.\nInstead, she says my argument should be structured like this:\n\nStatement is true when $n=0$\n\"Assume statement is true for some integer $k\\geq0$\"\nStatement must be true for $k+1$\n\"Assume statement is true for some integer $k\\leq0$\"\nStatement must be true for $k-1$\n\nI am failing to understand where my logic fails and why I need to split the assumptions like she is suggesting. Could someone explain why relying on the well-ordering principle makes the structure of my proof logically unsound?\n",
    "proof": "You are correct. You don't even need to start with $n=0$, you can start at any integer.\nTo prove that your approach is correct, see that if you have proven your version of the induction step, you have automatically proven your teacher's version of the induction step. This in turn then lets you reach your teacher's conclusion step, which is also your conclusion step.\nYour teacher is saying that you need to induct on the positive integers and the negative integers separately. While this is closer to standard (natural number) induction, and also a correct method of proof, it is not necessary.\n(Of course, there is the theoretical possibility that there are statements which can only be proven in the direction $k\\implies k+1$ for positive $k$ and $k\\implies k-1$ for negative $k$, and in those very special cases, your teacher's version of induction will actually lead to a proof, while your induction will fail to reach a conclusion, since you can't get a proof for your induction step. I am not fluent enough in this area of logic to know whether such cases can ever exist, but if they do, then that's a point to your teacher. But if your proof in this particular instance is indeed correct, then I would have no issues calling it a proof by induction.)\n",
    "tags": [
      "logic",
      "proof-writing",
      "induction"
    ],
    "score": 15,
    "answer_score": 23,
    "is_accepted": false,
    "question_id": 4867269,
    "answer_id": 4867274
  },
  {
    "theorem": "Prove that the additive inverse of an odd integer is an odd integer",
    "context": "This is a homework problem, but I don't want the answer, just a little guidance:\n\nProve that the additive inverse of an odd integer is an odd integer.\n\nWhen approaching a problem like this, how much is it safe to assume?  Is it safe to assume that \"the additive inverse of an integer is an integer?\"  Or does that need to be proven first, before we can start talking about odds and evens?\nI have two ideas about how to approach this, and that is to either:\n1) Use absolute value to negate the fact that something is negative so that the absolute values of something like $4$ and $-4$ are both $4$.  But is it safe to assume something like \"the absolute values of any integer positive or negative are equal?\"\n2) Do something like subtract $2$ times a number to get the negative or positive:\ne.g. the additive inverse of $4$ is $(4 - 2(4))$.  The additive inverse of $-4$ is $(-4 -(2(-4))$.\nExactly where I would follow those ideas to, I'm not sure yet, but I'd like to at least know I'm on the right track and not completely going off in the wrong direction.\n",
    "proof": "\nAn integer $n$ is odd if and only if there exists an integer $k$ such that $n = 2k+1$. (Note, $2$ does not divide $n = 2k + 1$: $2$ divides $2k$, but not $1$, hence $n = 2k + 1$ is not even, therefore is odd).\n\nSo let $n$ be an arbitrary odd integer; i.e. $n = 2k+1$ where $k$ is some integer.\nThen \n$\\begin{align} \\;-n & = -(2k+1) \\\\ & = -2k -1 \\\\ & = -2k + (- 2 + 2) - 1 \\\\ & =(-2k - 2) + (2 - 1) \\\\ &=2(-k-1) + 1\\end{align}$. \nNow, $\\;j = (-k - 1)\\,$ is some integer (because $k$ is some integer). So have that $-n = 2(-k -1) = 2j + 1$, which by definition, is an odd integer.\n",
    "tags": [
      "discrete-mathematics",
      "proof-writing"
    ],
    "score": 15,
    "answer_score": 8,
    "is_accepted": true,
    "question_id": 427256,
    "answer_id": 427268
  },
  {
    "theorem": "Proving that $\\sqrt{13+\\sqrt{52}} - \\sqrt{13}$ is irrational.",
    "context": "I'm trying to prove that a certain number is irrational. I've taken a number theory class, so I'm familiar with the proofs that $\\sqrt{2}$ and $\\sqrt{3}$ are irrational (assume it is rational, square both sides, and then arrive at a contradiction). However, I've tried this method with this number to no avail. This is the number in question:\n$\\frac{p}{q} = \\sqrt{13+\\sqrt{52}} - \\sqrt{13}$\n$\\frac{{p}^{2}}{{q}^{2}} = (13+\\sqrt{52}) - 2(\\sqrt{13+\\sqrt{52}})(\\sqrt{13}) + 13$\n$\\frac{{p}^{2}}{{q}^{2}} = 26 + 2\\sqrt{13} - 2\\sqrt{169 + 13\\sqrt{52}}$\nOnce I get here, it just seems that I've made the question more complicated, or that I'm taking the wrong approach, and have no clue how to move on. Any help would be appreciated. Many thanks.\n",
    "proof": "Let us assume $m = \\sqrt{13 + \\sqrt{52} } - \\sqrt {13} $ as rational .\nNext we bring the $\\sqrt {13} $ on the other side and then square both sides.\nThis will give :\n$$ m^2 + 2\\sqrt{13} m = \\sqrt{52} $$\nWe rearrange terms and simplify $\\sqrt{52} = 2 \\sqrt{13} $ and get :\n$$ m^2 = 2 \\sqrt{13} ( 1- m ) $$\nSince we have assumed $m$ as rational , by closure $ m^2 $ and $ 2(1-m) $ are both rational but $\\sqrt{13} $ is irrational . And it is not possible that we multiply a $\\mathbf{non-zero}$ rational and an irrational number to get rational number , hence we arrive at a contradiction . Hence $m$ is irrational .\nBased on comments below , we notice that $ m^2=0  $ will imply $m=1$ and similarly $1-m=0$ implies $ m=0 $ , and hence both are not possible .\n",
    "tags": [
      "proof-writing",
      "radicals",
      "irrational-numbers",
      "rational-numbers"
    ],
    "score": 15,
    "answer_score": 22,
    "is_accepted": false,
    "question_id": 4141440,
    "answer_id": 4141463
  },
  {
    "theorem": "Why are direct proofs often considered better than indirect proofs?",
    "context": "As the title indicates, I'm curious why direct proofs are often more preferable than indirect proofs. \nI can see the appeal of a direct proof, for it often provides more insight into why and how the relationship between the premises and conclusions works, but I would like to know what your thoughts are concerning this.\nThanks!\nEdit: I understand that this question is quite subjective, but that is my intention. There are people who prefer direct proofs more than proof by contradiction, for example. My curiosity is concerning what makes a direct proof preferable to such individuals. In the past, I've had professors grimace whenever I did an indirect proof and showed me that a direct proof was possible, but I never thought to ask them why a direct proof should be done instead. What's the point? \n",
    "proof": "I just did a quick lookup and it suggested that the two flavors of indirect proof was contraposition and contradiction.  What I'm about to say is criticizing contradiction, because contraposition seems fine to me.\nImagine you have a 1000 statement direct proof.  Then every step along that way is provable.  Maybe somebody reads your proof and realizes that an observation you made halfway through is exactly the idea they need to solve a problem they have.  Mathematical history has many examples of lemmas that are more famous than the theorems they originally supported.\nBy contrast, a 1000 statement proof by contradiction starts out with two hypotheses that are inconsistent.  Everything you're building is a logical house of cards that is intended to collapse at the end. Nothing you wrote can be counted on outside that framework without a separate analysis.\nIf it truly takes both hypotheses to get you to the result, then so be it.  But I was rightfully dinged by my professors when I wrote a proof by contradiction that could easily be modified into a direct proof by contraposition.\n",
    "tags": [
      "logic",
      "proof-writing",
      "soft-question"
    ],
    "score": 15,
    "answer_score": 24,
    "is_accepted": true,
    "question_id": 3341323,
    "answer_id": 3341334
  },
  {
    "theorem": "Prove $1+2\\sqrt3$ is not a rational number",
    "context": "How would I go about proving $1+2\\sqrt 3$ is not a rational number assuming $\\sqrt 3$ is not a rational?\nWould direct proof be the easiest? Total beginner here, any insight would be appreciated.\n",
    "proof": "Say $1 + 2 \\sqrt{3} $ is rational by contradiction. So we have \n$$ 1 + 2 \\sqrt{3} = \\frac{p}{q} $$\nfor $p,q \\in \\mathbb{Z}$. Notice we can move $1$ to the other side to obtain \n$$ 2 \\sqrt{3} = \\frac{p}{q} - 1 = \\frac{p-q}{q}$$\nNow, divide by $2$ to obtain \n$$ \\sqrt{3} = \\frac{p-q}{2q} $$\nBut this is a rational number. Hence $\\sqrt{3} \\in \\mathbb{Q}$. And we have reached a contradiction since $\\sqrt{3}$ is not rational.\n",
    "tags": [
      "discrete-mathematics",
      "proof-writing"
    ],
    "score": 15,
    "answer_score": 32,
    "is_accepted": false,
    "question_id": 1171498,
    "answer_id": 1171501
  },
  {
    "theorem": "How does one begin to even write a proof?",
    "context": "I'm in my first proof based class and I'm just having a lot of trouble writing proofs. I mean I know it's not going to come natural and it will take time, but seroiusly, how does someone begin to write a proof and formulate a game plan? For example, I'm trying to write up a solution for this problem but I can't seem to figure out where to start. \n\nGiven $y \\in \\mathbb R, n\\in\\mathbb N$, and $\\epsilon \\gt 0$, show that for some $\\delta \\gt 0$, if $u\\in \\mathbb R$, and $|u-y|\\lt \\delta$, then $|u^n-y^n|\\lt \\epsilon$.\n\nAny suggestions on how to start or even what some first things I should think about are when faced with a \"Show that...\" problem? I'm really looking for a combination of where to start for this and some proof writing in general.\n",
    "proof": "Here are a some steps that I find useful when writing proofs, not necessarily in order.\n\nIf the statement of the problem uses terms you aren't entirely comfortable with, translate it into simpler language, or even paraphrase the problem in regular English. For example, a statement about the linearity of the derivative might be shortened to \"the sum of the derivatives is the derivative of the sum\".\nDetermine the hypotheses you have to work with. One of the worst things you can try and do is to attack the problem from first principles alone. If you are giving specific hypotheses, start with them! It's quite possible that your conclusion is false without those hypotheses.\nFigure out what, exactly, you are being asked to show. You might find yourself running in circles if you don't have a clear direction in mind.\nLook at the relevant chapter(s) of your textbook, and see what theorems relate to the hypotheses you are given. I cannot stress this enough. If your hypothesis is that \"$f$ is continuous on a compact set\", look at every lemma and theorem in the chapter that begins: \"Let $f$ be continuous on a compact set...\", even if further assumptions are necessary.\nOften, if you are given many hypotheses, try and figure out how they work together. A continuous function on a compact set is better than a continuous function and a compact set (as seperate objects).\nAsk yourself: \"Is there any statement or result that would help me get to my conclusion? Is there anything that would help if it were true?\" This is how you make your own lemmas and propositions.\nMost Importantly: After you've translated the problem into a context you understand, ask yourself \"why should this be true?\" It might help to draw a picture, or consider some explicit examples. In my experience, when it comes to writing proofs, especially in analysis, if you don't know why something should be true, you'll have a hell of a time trying to prove it. When you can convince yourself why something should be true, all that's left is to translate that intuition into rigorous mathematics. While that requires some technical skill, that is something you will pick up over time -- in the long run, the intuitive answer is often the most important part. \n\n",
    "tags": [
      "real-analysis",
      "soft-question",
      "proof-writing"
    ],
    "score": 15,
    "answer_score": 24,
    "is_accepted": true,
    "question_id": 290372,
    "answer_id": 290380
  },
  {
    "theorem": "Proof of $\\gcd(a,b)=ax+by\\ $ [Bezout&#39;s identity]",
    "context": "Here is my proof of $\\gcd(a,b)=ax+by$ for $a, b, x, y \\in \\mathbb{Z}$. Am I doing something wrong? Are there easier proofs?\n$a,b \\in \\mathbb{Z}, g=\\gcd(a,b)$ and suppose $g \\neq ax + by$. Let $c$ be a common divisor of $a$ and $b$. Then \n$$\\forall x', y' \\in \\mathbb{Z}: c | ax' + by'\\Longrightarrow\\exists q_1, q_2 \\in \\mathbb{Z}\\,\\,\\, s.t.\\,\\,\\, c q_1 = ax' + by'\\,\\,,\\,\\,cq_2 = g$$ \nSo \n$$gcd(a, b)=g=c q_2 = c q_1 \\frac{q_2}{q_1} = \\left(\\frac{q_2}{q_1}x'\\right)a + \\left(\\frac{q_2}{q_1}y'\\right)b$$ So if we have $q_1|q_2$ then we found $\\gcd(a,b)=ax'+by'$ for all $x', y' \\in \\mathbb{Z}$.\nNow \n$$\\frac{q_1}{q_2}=\\frac{c q_1}{c q_2} = \\frac{ax'+by'}{g}$$ but $g|a$ and $g|b$ so $\\exists q_3 \\in \\mathbb{Z}: \\frac{q_1}{q_2}=q_3 \\Rightarrow q_1=q_2 q_3 \\Rightarrow q_1 | q_2\\,$ . QED.\n",
    "proof": "The nicest proof I know is as follows:\nConsider the set $S = \\{ax+by>0 : a,b \\in \\mathbb{Z}\\}$.  Let $d = \\min S$.  We now show the following:\n\n$d$ is a common divisor of $a$ and $b$.\nAny common divisor of $a$ and $b$ must divide $d$.\n\nIf we can show those two things then it is trivial that $d$ is the greatest common divisor of $a$ and $b$, and therefore that the greatest common divisor of $a$ and $b$ is of the form $ax+by$.\nTo show that $d$ divides $a$ and $b$: suppose for a contradiction that $d$ does not divide $a$.  Then $a=qd+r$, where $q\\ge 0$ and $0<r<d$.  Since $a=qd+r$, $qd=a-r$, and since we have that $d=ax+by$, $q(ax+by)=a-r$, so $r=a(1-qx)-bqy$.  So $r$ is a linear combination of $a$ and $b$, and since $r>0$, that means that $r\\in S$.  Since $r<d$ and we had supposed $d$ to be $\\min S$, we have a contradiction.  So $d$ must divide $a$.\nAn identical argument proves that $d$ must divide $b$.\nWe now want to show that any common divisor of $a$ and $b$ must divide $d$.  This is easy to show: if $a=uc$ and $b=vc$, then $d=ax+by=c(ux+vy)$, so $c$ divides $d$.\nTherefore, $d$ is the greatest common divisor of $a$ and $b$, and is of the form $ax+by$.\n",
    "tags": [
      "abstract-algebra",
      "elementary-number-theory",
      "proof-writing",
      "divisibility",
      "gcd-and-lcm"
    ],
    "score": 15,
    "answer_score": 30,
    "is_accepted": true,
    "question_id": 203383,
    "answer_id": 203388
  },
  {
    "theorem": "A set $A \\subseteq \\mathbb{R}$ is closed if and only if every convergent sequence in $\\mathbb{R}$ completely contained in $A$ has its limit in $A$",
    "context": "Real analysis is a topic I'm unfamiliar with and I'm confused on how to write proofs on them.\nIn order to prove that:\nA set $A \\subseteq \\mathbb{R}$ is closed (1) $\\iff$ Every convergent sequence in $\\mathbb{R}$ completely contained in A has its limit in A (2)\nThe definition of closed I am using is that if A is closed, then $\\mathbb{R} \\setminus A $ is open (i.e. $\\mathbb{R} \\setminus A \\in \\tau$.\nI realize that I have to prove both directions.\nFirst, in order to prove that (1) $\\implies$ (2):\nLet $A \\subseteq \\mathbb{R}$ be a closed set and let $(x_{n})_{n \\in \\mathbb{N}}$ be a sequence of reals with $x_{n} \\in A$ for every $n \\in \\mathbb{N}$ converging to $x_{\\infty} \\in \\mathbb{R}$.\nFrom here, I'm not sure on how to continue writing the proof. I am trying to ultimately prove that $x_{\\infty} \\in A$.\n",
    "proof": "To prove an if and only if, we need to prove the $\\implies$ direction and the $\\impliedby$ direction.\nFor the $\\implies$ direction, suppose $A \\subseteq \\mathbb{R}$ is closed.\nWe want to prove then that if $\\{ x_{n} \\}_{n = 1}^{\\infty}$ is a sequence of real numbers in $A$ that converges, then its limit is in $A$.  If we call $x$ the limit of this sequence (i.e., $\\lim x_n = x$), then by definition of convergence, given $\\epsilon > 0$, there exists an  $N$ such that if $n \\geq N$ then  $d(x_{n}, x) < \\epsilon$ (alternatively, for all $n \\geq N$, $x_{n} \\in B(x, \\epsilon)$).\nTo show that every convergent sequence contained in $A$ has its limit in $A$, suppose by contradiction that there is a convergent sequence $\\{ x_{n} \\}_{n = 1}^{\\infty}$ in $A$, but its limit, $x$, is in $\\mathbb{R} \\setminus A$.  Since $A$ is closed, $\\mathbb{R} \\setminus A$ is open.  Since $x \\in \\mathbb{R} \\setminus A$, and the set is open, we know by definition of open that $\\exists \\epsilon > 0$ such that $B(x, \\epsilon) \\subseteq \\mathbb{R} \\setminus A$.  So we found a ball around $x$ entirely contained in $\\mathbb{R} \\setminus A$.  But by definition of a convergent sequence, $\\exists N$ such that for all $n \\geq N$, $x_{n} \\in B(x, \\epsilon)$.  So there is a point in the sequence after which all terms are in $B(x, \\epsilon)$.  But $B(x, \\epsilon) \\subseteq \\mathbb{R} \\setminus A$.  Which means there are points of the sequence in $\\mathbb{R} \\setminus A$.  This contradicts the assumption that the sequence was entirely contained in $A$.  Thus, every convergent real sequence contained in $A$ has its limit in $A$, as desired.\nHopefully you should now have some idea on how to prove the other direction.  Just remember your definitions.\n",
    "tags": [
      "real-analysis",
      "proof-writing"
    ],
    "score": 15,
    "answer_score": 22,
    "is_accepted": false,
    "question_id": 882876,
    "answer_id": 883178
  },
  {
    "theorem": "$0.101001000100001000001$... is irrational.",
    "context": "How do I show that $0.101001000100001000001...$ is irrational? How do I generalize this to decimal expansions of a similar type, i.e. what is a criterion for decimal expansions with $0$'s and $1$'s involving long gaps of zeros and which guarantees irrationality?\n",
    "proof": "If $0 < p < q$, where $p,\\, q \\in \\mathbb{Z}$, the decimal expansion for $p/q$ is eventually periodic. Indeed, the first $k$ digits after the decimal point are given by $\\lfloor 10^kp/q\\rfloor$. Write $m \\Delta n$ if $10^m$ and $10^n$ have the same remainder when we divide by $q$; the pigeonhole principle implies$$m \\Delta n, \\text{ for some }1 \\le m < n \\le q + 1.$$\nIf $m \\Delta n$, then\n\\begin{align} q\\,|\\,(10^mp - 10^np)\n&\\implies q\\,|\\,(10(10^mp - 10^np))\\\\\n&\\implies q\\,|\\,(10^{m+1}p - 10^{n+1}p)\\implies (m+1)\\Delta(n+1),\\end{align}\nand by induction, $(m+k)\\Delta(n+k)$ for all $k$, Or in other form $a \\Delta b$ if $b-a=n-m$ which $m \\le a$ and $n \\le b$ $(\\text{equivalent to }0 \\le k)$ . Next, note that if $m \\Delta n$, and $r$ is the remainder when $10^mp$ and $10^np$ are divided by $q$, then $\\lfloor10r/q\\rfloor$ is the $(m+1)$st digit of the decimal expansion of $p/q$, and the $(n+1)$st. Thus $a \\Delta b$ similarly results that the $(a+1)$th digit and the $(b+1)$th digit of the expansion of $p/q$ are equal, and because it occurs for any $a$ and $b$ that $b-a=n-m$ which $m \\le a$ and $n \\le b$, Thus $p/q$ has eventually periodic expansion with period $n-m$.\nNow, the given decimal does not have period $P$ for any $P$ because for arbitrarily large $N$, there is a $1$ in the $N$th decimal place followed by $P$ zeros. More generally, if the decimal expansion of any $x \\in \\mathbb{Q}$ has arbitrarily long strings of zeros, and repeats with period $K$, then for any $N$, there exists $K$ consecutive zeros after the $N$th spot, so that the $K$ repeating digits are zeros.\n",
    "tags": [
      "real-analysis",
      "proof-writing",
      "rationality-testing"
    ],
    "score": 15,
    "answer_score": 6,
    "is_accepted": true,
    "question_id": 1423159,
    "answer_id": 1423208
  },
  {
    "theorem": "Mathematical writing guidelines",
    "context": "In section 5.3 from the book Book of Proof by Hammack (3rd edition, this link is to the author's website), the author outlines 12 mathematical writing guidelines to help the young mathematician with writing better proofs.\nThose guidelines, with their examples are as follows:\n\n\nBegin each sentence with a word, not a mathematical symbol:\nWrong: $A$ is a subset of $B$.\nCorrect: The set $A$ is a subset of $B$.\n\nEnd each sentence with a period, even when the sentence ends with\na mathematical symbol or expression:\nWrong: Euler proved that $\\sum_{k=1}^\\infty\\frac{1}{k^s}=\\prod_{p\\in P}\\frac{1}{1-\\frac{1}{p^s}}$\nCorrect: Euler proved that $\\sum_{k=1}^\\infty\\frac{1}{k^s}=\\prod_{p\\in P}\\frac{1}{1-\\frac{1}{p^s}}$.\n\nSeparate mathematical symbols and expressions with words:\nWrong: Because $x^2-1=0$, $x=1$ or $x=-1$.\nCorrect: Because $x^2-1=0$, it follows that $x=1$ or $x=-1$.\n\nAvoid misuse of symbols:\nWrong: The empty set is a $\\subseteq$ of every set.\nCorrect: The empty set is a subset of every set.\n\nAvoid using unnecessary symbols:\nWrong: No set $X$ has negative cardinality.\nCorrect: No set has negative cardinality.\n\nUse first person plural:\nUse the words \"we\" and \"us\" rather than \"I,\" \"you\" or \"me.\"\n\nUse the active voice:\nWrong: The value $x=3$ is obtained through division of both sides by $5$.\nCorrect: Dividing both sides by $5$, we get $x=3$.\n\nExplain each new symbol:\nWrong: Since $a\\mid b$, it follows that $b=ac$.\nCorrect: Since $a\\mid b$, it follows that $b=ac$ for some integer $c$.\n\nWatch out for \"it\":\nWrong: Since $X\\subseteq Y$, and $0<|X|$, we see that it is not empty.\nCorrect: Since $X\\subseteq Y$, and $0<|X|$, we see that $Y$ is not empty.\n\nSince, because, as, for, so:\nThe following statements all mean that $P$ is true (or assumed to be true) and as a consequence $Q$ is true also:\n\n$Q$ since $P$\n$Q$ because $P$\n$Q$, as $P$\n$Q$, for $P$\n$P$, so $Q$\nSince $P$, $Q$\nBecause $P$, $Q$\nAs $P$, $Q$\n\n\nThus, hence, therefore, consequently:\nThese adverbs precede a statement that follows logically from previous sentences or clauses:\nWrong: Therefore $2k+1$.\nCorrect: Therefore $a=2k+1$.\n\nClarity is the gold standard of mathematical writing:\nIf you think breaking a rule makes your writing clearer, then break the rule.\n\n\n\nAre there any other rules or personal experiences that lead to writing a better proof?\n",
    "proof": "Short answer in case there's somebody who hasn't seen this before:\nHalmos, Paul R. \"How to write mathematics.\" Enseign. Math 16.2 (1970): 123-152.\n",
    "tags": [
      "proof-writing",
      "soft-question",
      "big-list"
    ],
    "score": 15,
    "answer_score": 10,
    "is_accepted": true,
    "question_id": 3756434,
    "answer_id": 3756443
  },
  {
    "theorem": "Introduction to Proof via Linear Algebra",
    "context": "Many universities offer a transition course from computational courses like Calculus to proof-oriented courses like Abstract Algebra. Such courses often go by a name like \"Introduction to Proof\" or \"Transition to Higher Mathematics\". They typically contain an introduction to first-order logic (conditionals, conjunctions, negations, quantifiers, etc.) as well as various methods of proof (contradiction, induction, etc.).\nI'm hoping to find a text for a first course in linear algebra that fills the role of a \"transition course\" by deliberately incorporating first-order logic and proof techniques as part of the instruction.\nThe text should be accessible to students with two semesters of Calculus (roughly the basics of single-variable differentiation, integration, and infinite series). In particular, the overwhelming majority of students will have never written a formal proof and will have extremely limited exposure to logic and set theory.\nIdeally, the author would discuss these topics just as they are needed in the treatment of linear algebra (as opposed to supposing the reader is familiar with them already). For example, the author might have a digression on proof by contradiction just prior to using it in some proof about linear independence.\nLess ideal (but still acceptable) would be a text that at the very least makes use of all the relevant ideas from first-order logic and proof techniques that one expects from a transition course. Hopefully, the progression of such a text would be such that the instructor could use a supplemental text to discuss, say, proof by contradiction just as it is about to make its first appearance in the text.\n",
    "proof": "Cherry-pick what you need from multiple books, if you need to. But here are some books that satisfy your requirements. \nIf you want a thorough, from the roots, introduction to higher mathematics, read\n\nBasic Concepts of Mathematics, by E. Zakon, which is freely available in pdf. (Consider donating.)\n\nIt covers logic, naive set theory, the real numbers and linear algebra. It is a great book, but unfortunately it is a bit less well-known. From the author:\n\nThis book helps the student complete the transition from purely manipulative to rigorous mathematics.\n\nFor a more standard course in linear algebra, consider\n\nLinear Algebra as an Introduction to Abstract Mathematics.\n\nI would complement this book with Basic Concepts of Mathematics for the logic and set theory basics.\n\nNot about linear algebra, but you should take a look at the \n\nBook of proofs, by R. Hammack.\n\nIt is sort of a cookbook. For instance, one section is entitled 'How to prove $A \\subseteq B$?'\n",
    "tags": [
      "linear-algebra",
      "reference-request",
      "proof-writing",
      "education",
      "book-recommendation"
    ],
    "score": 15,
    "answer_score": 2,
    "is_accepted": true,
    "question_id": 2196119,
    "answer_id": 2205855
  },
  {
    "theorem": "Can you explain to me why this proof by induction is not flawed? (Domain is graph theory, but that is secondary)",
    "context": "Background\nI am following this MIT OCW course on mathematics for computer science.\nIn one of the recitations they come to the below result:\nOfficial solution\nTask:\nA planar graph is a graph that can be drawn without any edges crossing. Also, any planar graph has a node of degree at most 5. Now, prove by induction that any planar graph can be colored in at most 6 colors.\nSolution.:\nWe prove by induction. First, let n be the number of nodes in the graph.\nThen define\nP (n) = Any planar graph with n nodes is 6-colorable.\nBase case, P (1): Every graph with n = 1 vertex is 6-colorable. Clearly true since it’s\nactually 1-colorable.\nInductive step: P (n) →  P (n + 1): Take a planar graph G with n + 1 nodes. Then take a node v with degree at most 5 (which we know exists because we know any planar graph has a node of degree ≤ 5), and remove it. We know that the induced subgraph G’ formed in this way has n nodes, so by our inductive hypothesis, G’ is 6-colorable.\nBut v is adjacent to at most 5 other nodes, which can have at most 5 different colors\nbetween them. We then choose v to have an unused color (from the 6 colors), and as\nwe have constructed a 6-coloring for G, we are done with the inductive step.\nBecause we have shown the base case and the inductive step, we have proved\n∀n ∈ Z+ : P (n)\n(Note: Z+ refers to the set of positive integers.)\nMy Question\nTo me the inductive step is \"backwards\", because it proves  P(n+1) -> P(n) and then get's to assume P(n) to be true by the inductive hypothesis.\nWhy is this legal? To illustrate my problem, consider the below bogus-proof that sounds \"the same\" to me.\nThm. \"At school, no kid has more than 6 friends.\"\nAssume that there is always one guy who has 5 friends or less.\nProof by induction over the number of kids in school.\nBase Case: n = 1, is obviously true: Kid cannot have any friends, if only child.\nInductive step: P (n) → P (n + 1): Take a school G with n + 1 kids. Then take a kid v with at most 5 friends (which we know exists by assumptions), and remove it.\nWe know that the induced sub-school G’ formed in this way has n kids, so by our inductive hypothesis, in G' no one has more than 6 friends. I can then add v back in and know that he has less than 6 friends. q.e.d.\nThere must be something I am missing, but both proofs look equally flawed to me. I would truly appreciate, if you could break it down to me.\nI am not questioning the theorem, just the procedure of starting with the case n+1 and then very cleverly choosing how to go backwards to n. At best, to me, this proves P(n+1) -> P(n), but not P(n) -> P(n+1).\nUpdate\nThank you everyone! It was so wonderful to get so many answers on such niche problem. I was precisely hung up on starting with n+1 in conjunction with the fact that I get to “choose” how to go to n from there.\nI think I can now verbalize a little better, why I felt unhappy with the proof:\nIn my mind, whenever I look at induction proofs, I model it a bit like a recursive function call in code. I had never explicitly done this, but to illustrate my confusion, here is what they are doing in pseudo-code (in my mind at least):\nis6Colorable(graph):\n  if graph.size == 1: # base case\n    return true\n  else:\n    specialNode = findSomeDegree5orFewerNode(graph)\n    subgraph = graph.drop(specialNode)\n    return is6colorable(subgraph) \n\nNotice how I can rename the function in a meaningful way for any statement that is true for both the base case and the special node and it will always return true for a planar graph?\nE.g. verifyThatNoNodeInThisGraphHasMoreThan5Degrees(graph).\nIn fact, I could rename it most anything that holds for the base case and it would return true and still have some meaning attached (e.g. isNumberOfVerticesOdd(graph) )\nSo that is obviously garbage code (I could just return \"true\") and it felt circular to me.\nThe crux – to me – is that all of the heavy lifting of this proof is done by the fact that adding a degree-5-or-fewer-node to a graph, cannot “taint” the 6-colorability for all the other nodes. (This does not hold for all other statements that are true for the base case and the specialNode. E.g. it might change the number of edges that some of the nodes in the sub-graph have, so I cannot prove that all nodes have degree 5 or less like I suggested above).\nThat means I can deconstruct every graph to the base case, only removing nodes that when added back in, do not taint the 6-color-ability of the induced sub-graph.\nIf I can deconstruct it that way, I can re-construct it that way.\nPerhaps, you can see from my code example, how I struggled with the wording of the official proof, though: The induction they do – to me at least – does not add any insight. It is valid “code” and it returns the correct result, but it is void of insight.\n",
    "proof": "I think what is confusing you is that you are \"starting out\" with a planar graph of $n+1$ nodes.\nBut note that you are not yet saying anything about its colorability. All you're trying to show here is that $P(n) \\to P(n+1)$, or:\n\"Any planar graph with $n$ nodes is $6$-colorable\" $\\to$ \"Any planar graph with $n+1$ nodes is $6$-colorable\"\nThe inductive proof here \"starts out\" with some arbitrary planar graph of $n+1$ nodes, but we're not saying it's $6$-colorable yet. What we do know about this planar graph is that it has some node with degree $\\leq 5$ in it somewhere, something we know is true for all planar graphs. We label this node $v$ and remove it for now.\nWhat do we have left? A planar graph of $n$ nodes, which is $6$-colorable because we've assumed it via $P(n)$.\nThe nodes that $v$ was connected to can have at most $5$ colors among them. So we connect $v$ back to those nodes again, and color it something different (from those nodes), resulting in (at most) $6$ colors between those $\\leq 5$ nodes and $v$. Now we've shown that $P(n) \\to P(n+1)$ because we end with the same arbitrary planar graph we began with, but now we know it's $6$-colorable. The fact that the planar graph is arbitrary is important, because we are trying to prove that $P$ holds for \"any\" planar graph.\nIn other words, the pathway is more like \"arbitrary $n+1$ node planar graph\" -> \"a reduced $n$ node planar graph that is $6$-colorable\" -> \"that arbitrary $n+1$ node planar graph is indeed $6$-colorable too\".\nIt begs the question, why not just \"start out\" with a planar graph of $n$ nodes and then go straight to the graph of $n+1$ nodes? There are some potential issues with such an approach.\nFor example:\nI want to prove that all planar graphs are $3$-colorable. $P(1), P(2), P(3)$, trivially true. Now for $P(n) \\to P(n+1)$, I start out with a graph of $n$ nodes that is $3$-colorable. I add a node to it such that it is connected to $2$ nodes of $2$ different colors, and color my new node a third color. Boom! Now we've made a graph of $n+1$ nodes with at most $3$ colors and we haven't violated the fact that all planar graphs have at least one node of degree $\\leq 5$...\n...wait, not so fast! What have I actually shown here? I've just shown that if you have a $3$-colored graph, you can make another $3$-colored graph with one more node as long as you attach it in a very particular way.\nI need to show it holds for any $3$-colorable planar graph. What if we're talking about a graph with a triangle in it ($3$ nodes, $3$ colors) and I add the extra node right in the middle, and connect it to all three vertices? That's a valid $n+1$ graph too, and yet... I need a fourth color. So this proof fails.\nIn other words, it becomes harder to go from the $n$ case directly to the $n+1$ case because now you have to consider all the possible ways you can hook up that extra node, and all the various ways you might get contradicted.\nThis is why it's way easier to start from the arbitrary $n+1$ graph, because you are immediately granted the existence of a special node, and then you can say something about the colorability of the reduced $n$ graph, and use that to say something about the colorability of the original $n+1$ graph. And since this works for any $n+1$ planar graph you started with, the resulting conclusion holds for all planar graphs.\n",
    "tags": [
      "graph-theory",
      "proof-writing",
      "proof-explanation",
      "induction",
      "planar-graphs"
    ],
    "score": 14,
    "answer_score": 24,
    "is_accepted": true,
    "question_id": 4886128,
    "answer_id": 4886253
  },
  {
    "theorem": "How do we know whether certain mathematical theorems are circular?",
    "context": "There are countless mathematical theorems and lemmata, some of which, obviously, depend on others.\nMy question is: how do we know that, say, Theorem $A_1$- which uses a result proved in Theorem $A_2$ which uses a result proved in ... which uses a result proved in Theorem $A_n$ which, in fact, relies on the fact that Theorem $A_1$ is true - doesn't make the proof of Theorem $A_1$ circular?\nEssentially, what I'm saying is that, since there's no comprehensive list of all mathematical theorems, lemmata and corollaries (and what statements they rely on), how can we assume that no two theorems will be circular (not directly- but down a long chain of theorems)?\nIf I'm not articulating myself properly, please ask me to elaborate.\nThanks\n",
    "proof": "The shortest answer is this: because theorem $A$ can only be proven using theorem $B$ if theorem $B$ is already proven. This way, your circular chain can never happen, since $B$ can only be proven using already proven theorems, meaning $A$ cannot be used to prove neither $B$ nor any theorems used in the proof of $B$ (or any theorem used in the proof of a theorem used in the proof of a theorem used in the.... .... used in the proof of $B$)\n",
    "tags": [
      "soft-question",
      "proof-writing"
    ],
    "score": 14,
    "answer_score": 16,
    "is_accepted": true,
    "question_id": 717107,
    "answer_id": 717116
  },
  {
    "theorem": "The set of all finite subsets of the natural numbers is countable",
    "context": "Could someone verify my proofs?\nProposition: the set of all finite subsets of $\\mathbb{N}$ is countable\nProof 1:\nDefine a set $ X=\\{A\\subseteq\\mathbb{N}\\mid \\text{$A$ is finite} \\}$.\nWe can have a function $g_{n}: \\mathbb{N} \\rightarrow A_{n} $ for each subset such that that function is surjective (by the fundamental theorem of arithmetic). Hence each subset $A_{n}$ is countable. \nBy the \"Union of countable sets is countable\" theorem, X is countable.\nQ. E. D.\nProof 2:\nSuppose we have an ordered list of prime numbers $p_{1}, p_{2}, ..., p_{n}$. Define a function $g: X \\rightarrow \\mathbb{N}$  such that $ g(A) = (p_{1})^{a_{1}}*(p_{2})^{a_{2}}*...*(p_{k})^{a_{k}} $ with $k=$ number of elements of each subset and $ a_{1}, a_{2}, ..., a{k} $ the ordered elements of them. By the fundamental theorem of arithmetic, that function is injective, hence $X$ is countable. \nQ. E. D.\n",
    "proof": "What you have is nowhere near a proof. The definition of $X$ can be accepted, but it is not conveying any insight transgressing the verbal formulation of the problem.\nWe have to construct a bijective map $$f:\\quad {\\mathbb N}_{\\geq0}\\to X,\\qquad n\\mapsto A_n\\ .\\tag{1}$$ This map produces for each $n\\in{\\mathbb N}$  a finite set $A_n\\in X$, and each element $A\\in X$ is produced exactly once.\n(Depending on the theorems of elementary set theory that are available at this point one could make do with a surjective $g:\\ {\\mathbb N}_{\\geq0}\\to X$, or an injective $h:\\ X\\to{\\mathbb N}_{\\geq0}$.)\nThere are various examples of such $f$'s around. The simplest that comes to mind is the following: Any set $A\\subset{\\mathbb N}_{\\geq0}$ can be encoded as a bit string ${\\bf b}_A:=(b_0,b_1,b_2,\\ldots)$ by putting $b_k:=1$ when $k\\in A$ and $b_k=0$ otherwise. When $A\\in X$ this string has only finitely many ones. Now put\n$$\\hat f(A):=\\sum_{k=0}^\\infty b_k2^k\\qquad(A\\in X)\\ .$$\nThis means that we interpret ${\\bf b}_A$ as binary expansion of a certain nonnegative integer. The $\\hat f$ defined in this way is the inverse of a function considered in $(1)$.\n",
    "tags": [
      "elementary-set-theory",
      "proof-writing",
      "proof-verification"
    ],
    "score": 14,
    "answer_score": 21,
    "is_accepted": true,
    "question_id": 908222,
    "answer_id": 908526
  },
  {
    "theorem": "Is the following generalization of Cauchy-Schwarz inequality true?",
    "context": "\nLet $\\{a_{1,i}\\}_{i=1}^k,\\{a_{2,i}\\}_{i=1}^k,\\dots ,\\{a_{n_,i}\\}_{i=1}^k$ be real sequences. Does the following inequality hold\n$$(\\sum_{i=1}^k a_{1,i}^2)\\cdot(\\sum_{i=1}^k a_{2,i}^2)\\cdots(\\sum_{i=1}^k a_{n,i}^2)\\geq (\\sum_{i=1}^k a_{1,i}a_{2,i}\\cdots a_{n,i})^2$$\nfor all $k,n \\in \\mathbb N$?\n\nIt can be easily seen that this is the Cauchy-Schwarz inequality when $n=2$.\nThe motivation for the problem actually comes from the Cauchy-Schwarz inequality. While solving a Cauchy-Schwarz inequality problem, this problem came to my mind. I don't know if this is already a proved theorem in mathematics (because I am a high school student and I don't know much about inequalities). But I didn't find this on internet (I searched on google). So, I assume the problem statement is false. And a proof (or disproof) is needed for that.\nMy workings for $k=2$ and $n=3$:\nHowever, I tried to prove the problem statement for $k=2$ and $n=3$ (and I think I actually proved that!). Here is my workings to do that:\nFor $a,b,c,d,e,f$ real numbers, we have from Cauchy-Schwarz inequality (which is for $n=2$ and $k=2$),\n$$(a^2+b^2)(c^2+d^2) \\geq (ac+bd)^2$$\n$$\\implies (a^2+b^2)(c^2+d^2)(e^2+f^2) \\geq (ac+bd)^2(e^2+f^2)$$\n$$=(a^2c^2+2abcd+b^2d^2)(e^2+f^2)$$\n$$=a^2c^2(e^2+f^2)+2abcd(e^2+f^2)+b^2d^2(e^2+f^2)$$\n$$\\geq a^2c^2e^2+2abcdef+b^2d^2f^2$$\n$$=(ace+bdf)^2$$\nas desired.\n\nI hope my workings are correct. So, I have the following questions:\n\nIs the firstly stated problem statement true? If it is, how to prove that?\nIf it is not true, are there some other values (like $k=2$ and $n=3$ as in the above) for which the statement is true?\n\nAny help would be appreciated and please try to answer the questions so that a high school student can understand them (if it is not possible, then no problem).\n",
    "proof": "I think your proof for the case $k = 2$ and $n = 3$ is valid.\nWithout explicitly using mathematical induction, as in Jorge's\nanswer - although induction is always finally needed to justify an\ninformal proof like this - one can see that the inequality for\ngeneral $n \\geqslant 2$ follows almost immediately from Cauchy's\ninequality, simply by losing most of the terms from the expanded\nproduct of the last $n - 1$ bracketed sums, thus:\n\\begin{multline*}\n\\left(\\sum_{i=1}^ka_{1,i}^2\\right)\n\\left(\\sum_{i=1}^ka_{2,i}^2\\right) \\cdots\n\\left(\\sum_{i=1}^ka_{n,i}^2\\right)\n\\geqslant\n\\left(\\sum_{i=1}^ka_{1,i}^2\\right)\n\\left(\\sum_{i=1}^ka_{2,i}^2 \\cdots a_{n,i}^2\\right) = \\\\\n\\left(\\sum_{i=1}^ka_{1,i}^2\\right)\n\\left(\\sum_{i=1}^k(a_{2,i} \\cdots a_{n,i})^2\\right)\n\\geqslant\n\\left(\\sum_{i=1}^ka_{1,i}(a_{2,i} \\cdots a_{n,i})\\right)^2 =\n\\left(\\sum_{i=1}^ka_{1,i}a_{2,i} \\cdots a_{n,i}\\right)^2.\n\\end{multline*}\nThis proof \"gives away\" so much that the resulting inequality,\nwhen $n > 2,$ is very weak. This is illustrated by the fact that if\nthere are $b_1, b_2, \\ldots, b_n$ such that $a_{j,i} = b_j,$ for\n$j = 1, 2, \\ldots, n,$ and $i = 1, 2, \\ldots, k,$  then the\ninequality reduces to\n$(kb_1^2)(kb_2^2)\\cdots(kb_n^2) \\geqslant (kb_1b_2 \\cdots b_n)^2,$\ni.e., $k^n \\geqslant k^2,$ which is of little interest when $n > 2$!\nThat probably explains why the case $n > 2$ is seldom mentioned.  I\ndid find the case $n = 3$ given as Exercise XVa, problem 37 in\nClement V. Durell, Advanced Algebra, Vol. III\n(Bell, London 1937). A more up-to-date reference is Exercise 1.3 in\nJ. Michael Steele, The Cauchy-Schwarz Master Class\n(Cambridge University Press / Mathematical Association of America\n2004). Steele gives a surprisingly complicated proof, which is why I\nthought it worth giving this very simple one. (In essence it\nduplicates Jorge's proof, but the idea seems worth repeating in\ndifferent words.)\n",
    "tags": [
      "algebra-precalculus",
      "inequality",
      "proof-writing",
      "solution-verification",
      "cauchy-schwarz-inequality"
    ],
    "score": 14,
    "answer_score": 12,
    "is_accepted": true,
    "question_id": 4189361,
    "answer_id": 4189505
  },
  {
    "theorem": "How do people pick $\\delta$ so fast in $\\epsilon$-$\\delta$ proofs",
    "context": "For example, in a proof that shows $f(x) = \\sqrt x$ is uniformly continuous on the positive real line, the proof goes like:\n\nLet $\\epsilon > 0$ be given, and $\\delta = \\epsilon^2$....\n\nOr to show that every Lipschitz continuous function is uniformly continuous\n\nLet $\\epsilon > 0$ be given, and $\\delta = \\epsilon$....\n\nDo these people have a magic ball that let them see what the $\\delta$ value is going to work?\nI often find myself struggling coming up with the $\\delta$ value after doing a bunch of inequalities on $|f(x) - f(y)|< \\delta$ to make it less than $\\epsilon$. How do people know what $\\delta$ is going to be in the first line of their proof?\n",
    "proof": "I have also wondered this too many times, and my analysis professor told us straight up why he does it.\nHe said, \"I prove the limit first, find δ , then write it at the beginning in the final proof to make it seem like I knew all along.\"\n",
    "tags": [
      "calculus",
      "real-analysis",
      "proof-writing",
      "epsilon-delta",
      "proof-explanation"
    ],
    "score": 14,
    "answer_score": 19,
    "is_accepted": true,
    "question_id": 1677604,
    "answer_id": 1961732
  },
  {
    "theorem": "How do I prove the completeness of $\\ell^p$?",
    "context": "Say $\\{x_n\\}$ is Cauchy in $\\ell^p$ and $x$ is its pointwise limit. To argue that $x \\in \\ell^p$ would the following be correct:\nLet $\\varepsilon > 0$ and let $N$ be s.t. $n,m > N$ $\\Rightarrow$ $|x_n - x_m|_p < \\varepsilon$. Then $\\lim_{m \\to \\infty} |x_n - x_m|_p = |x-x_n|_p \\le \\varepsilon$.\nI saw the following different argument: $|x_n - x_m|_p < \\varepsilon$ implies $\\left(\\sum_{k=0}^M |(x_n - x_m)_k|^p\\right)^{1/p} < \\varepsilon$ for all $M$ therefore $\\lim_{m \\to \\infty}\\left(\\sum_{k=0}^M |(x_n - x_m)_k|^p\\right)^{1/p} = \\left(\\sum_{k=0}^M |(x_n - x)_k|^p\\right)^{1/p} \\le \\varepsilon$ for all $M$ therefore $\\lim_{M \\to \\infty} \\left(\\sum_{k=0}^M |(x_n - x)_k|^p\\right)^{1/p} \\le \\varepsilon$.\nThe difference is to use a finite sum step in between. Is it correct to drop it?  And if not: why not? Norm seems to be continuous so one should be able to exchange norm and limit. Thanks.\n",
    "proof": "Edit: The second argument is correct, not the first one. But that's not the only thing you have to prove. There are three steps. See the usual elementary proof below.\nNow if you really want to swap some limits, here is what you can do.\nFix $\\epsilon>0$ and take $N$ such that $\\|x_n-x_m\\|_p\\leq \\epsilon$ for all $n,m\\geq N$. Now fix $n\\geq N$. And consider the sequence $y_m=x_n-x_m$ which converges pointwise to $x_n-x$, so that $\\liminf y_m(k)=x_n(k)-x(k)$ for all $k$. We have \n$$\n\\|y_m\\|_p\\leq \\epsilon\\qquad\\forall m\\geq N \\qquad\\Rightarrow\\qquad \\liminf_m\\|y_m\\|_p\\leq \\epsilon.\n$$\nNow by Fatou's lemma (which is easily proved in $\\ell^p$), you have\n$$\n\\|x_n-x\\|_p=\\|\\liminf_m y_m\\|_p\\leq \\liminf_m \\|y_m\\|_p\\leq \\epsilon\n$$\nfor all $n\\geq N$. So this proves everything at the same time.\nUsual elementary proof:\n\nFind a pointwise limit $x$ by pointwise completeness. You've done that already.\nProve $x$ belongs to $\\ell^p$. You've done that also.\nCheck that $x_n$ tends to $x$ for the $\\ell^p$ norm.\n\nALthough the technique is essentially the same, one needs to treat 2 and 3 separately. \nAbout 2. There is actually an $\\epsilon$ free argument at this point, see the note at the end. Now to prove this the way you did, one usually picks $\\epsilon=1$. Then there is $N$ such that for all $n,m\\geq N$, $\\|x_n-x_m\\|_p\\leq 1$. In particular, for all $K$ and all $m\\geq N$:\n$$\n\\left(\\sum_{k=1}^K|x_m(k)|^p\\right)^{1/p}\\leq \\left(\\sum_{k=1}^K|x_N(k)|^p\\right)^{1/p}+\\left(\\sum_{k=1}^K|x_m(k)-x_N(k)|^p\\right)^{1/p}\n$$\n$$\n\\leq \\|x_N\\|_p+\\|x_m-x_N\\|_p\\leq \\|x_N\\|_p+1.\n$$\nNow let $m$ tend to $+\\infty$. And finally $K$ to $+\\infty$. You get\n$$\n\\|x\\|_p\\leq \\|x_N\\|_p+1\n$$\nso $x\\in\\ell^p$.\nFor 3., now. Fix $\\epsilon>0$. Take $N$ such that $\\|x_n-x_m\\|_p\\leq \\epsilon$ for all $n,m\\geq N$. Then for $n,m\\geq N$ and all $K$:\n$$\n\\left(\\sum_{k=1}^K|x_m(k)-x_n(k)|^p\\right)^{1/p}\\leq \\|x_m-x_n\\|_p\\leq \\epsilon.\n$$\nLet $m$ tends to $+\\infty$. Then $K$. You get $\\|x-x_n\\|_p\\leq \\epsilon$ for all $n\\geq N$.\nNote: Proof of 2 without $\\epsilon$. Since $x_n$ is Cauchy, it is bounded, say by $M$. Then for all $K$ and all $n$:\n$$\n\\left(\\sum_{k=1}^K|x_n(k)|^p\\right)^{1/p}\\leq \\|x_n\\|_p\\leq M.\n$$\nNow let $n$ tend to $=\\infty$. And then $K$. You get $\\|x\\|_p\\leq M$. So $x\\in\\ell^p$.\n",
    "tags": [
      "real-analysis",
      "sequences-and-series",
      "proof-writing",
      "banach-spaces",
      "lp-spaces"
    ],
    "score": 14,
    "answer_score": 16,
    "is_accepted": false,
    "question_id": 328479,
    "answer_id": 328516
  },
  {
    "theorem": "How to learn/speak &quot;mathematical english&quot;?",
    "context": "Good day!\nI was wondering if there is a good way to learn \"maths in english\". I am studying mathematics in Germany (I am from Germany, so english is not my native language) and have recently started to work with english papers, textbooks etc. Reading them is alright so far, although I had to get used to some sentence constructions (but this was also the case when I started studying maths, as mathematicians tend to have their own grammar, so that is fine with me). \nMy question aims to writing/speaking \"mathematical english\". As far as I'm concerned there are (at least) two things I'd like to learn:\n\nGet the \"basic theorems\" right. For example if someone wants to show that there exists a unique solution to $x=\\cos(x),x\\in\\mathbb R$, I'd suggest to use the \"Zwischenwertsatz\". Using wikipedia I can find that the \"Zwischenwertsatz\" is the \"Intermediate value theorem\" and of course I can do that for every specific theorem and eventually learn the translations, but I find it rather annoying to consult wikipedia whenever I want to talk about a specific theorem. I guess I'm looking to suggestions for good textbooks that cover these basics; they don't need to be \"good\" in a way that they have lots of exercises including solutions, online support or anything like that, but have a \"good style\" of mathematical writing, which ties to my second point.\nGet the \"style of writing\" right. Of course, learning by doing should be a good thing here, but living in a german environment so far I didn't have many chances to write in english. My first attempt to writing maths in english actually happened yesterday here on stackexchange, and it took me quite a while until I was satisfied (and at least thought it to be \"readable\"), much longer that it would have taken me to write the same thing in german.\n\nCan you suggest anything that I should read/do to achieve this?\n",
    "proof": "Guten Morgen,\nfellow German here.\nI was in the lucky position, to have professors in the undergrad courses who started their lectures with a quick review of the last lecture (5 minutes) which was given in english. That way, you started to learn the basic math terminology already in the first semesters. But I also had to look up names of theorems which I also did via Wikipedia. \nHowever, what really made me learn the english language (in science) where the lectures given by non-german speakers. This was espacially the case in my Master. I had the possibility to choose my courses rather free, and if possible I took the english version. I have not always written all my exercises in english, but I wrote all the \"larger projects\" I had to do in english: Reports, Seminar work, Projects, Thesis,...\nFurthermore, one simply should not be afraid to get all the textbooks in english. If you take an Analysis course, don't restrict yourself to the german textbooks, simply take an english version of it. You can always also ask the professor to suggest good material, they will be happy to do so! \nBut the most important aspect was already mentioned by fraiem, its learning by doing. \nGood luck!\n",
    "tags": [
      "proof-writing",
      "article-writing"
    ],
    "score": 14,
    "answer_score": 6,
    "is_accepted": false,
    "question_id": 1324759,
    "answer_id": 1324791
  },
  {
    "theorem": "What are statements about the natural numbers where induction is impossible or unnecessary to prove?",
    "context": "I'm looking for statements like \"for all natural numbers, ____\" where induction would be impossible or unnecessarily complicated. This is for pedagogical reasons. When students first learn induction, they want to prove every statement about natural numbers by induction, even when it is entirely not necessary or possible. It would be nice to have examples to break \"natural numbers implies induction\".\nFor example: for all $n\\in \\mathbb{N}$, $\\int_0^{2\\pi} \\cos(nx) dx=0$. If one tries to show this by induction, it would be way more complicated than necessary. You can just compute the integral directly (assuming you know $\\sin(2n\\pi)=0$).\n",
    "proof": "1) For all relatively prime positive integers $a$ and $m$ there is a prime number in the arithmetic progression $a$, $a+m$, $a+2m$, $a+3m,\\dots$ \nOf course the general theorem is that there are infinitely many primes in each such progression, but the way I stated it by quantifying over all $a$ and $m$ is equivalent to the general version; for beginners I think the existence of a single prime in such a progression as $a$ and $m$ vary is easier to comprehend. In fact, because this special case implies the general case it is pretty inconceivable to me how this could be proved by induction.\nThe special case $a = 1$ can be proved in a simpler way, using the $m$-th cyclotomic polynomial, so you have a theorem with a single parameter in it, and I've never seen a proof of that version by induction either.\n2) For $n \\geq 2$, the partial sums $H_n = 1 + 1/2 + 1/3 + \\cdots + 1/n$ are not integers. \nThe proofs I know are based on studying the highest power of $2$ less than or equal to $n$ rather than any kind of direct induction on $n$. If $H_n$ is not an integer how can that raw fact help you show $H_{n+1}$ is not an integer? To use induction you'd be trying to show a \"non-property\" persists from $n$ to $n+1$ (or from all integers from $2$ to $n$ on to $n+1$). If someone says that the proof is by induction on the highest power of $2$ less than or equal to  $n$ then I'd point out that the proof does not use anything about $n$ to get the result for $n+1$.\n3) A partial example: for $n \\geq 2$, the number $\\sqrt[n]{2}$ is irrational.  \nI consider this partial because, while the direct statement is not easily accessible to induction (the numbers $\\sqrt[n]{2}$ and $\\sqrt[n+1]{2}$ are linearly disjoint over $\\mathbf Q$, so it's hard to imagine how, to a beginner, the irrationality of $\\sqrt[n]{2}$ could help you prove irrationality of $\\sqrt[n+1]{2}$), it could be seen as a consequence of the more general result that if a prime -- or just the specific prime $2$ -- divides a product of $n$ integers then it must divide one of them, and that is provable by induction.\n4) Consider giving the class some unsolved problems about integers, e.g., for every $n \\geq 2$ the infinite series $1 + 1/2^n + 1/3^n + 1/4^n + \\dots$ is irrational. It's proved for even $n$ and for $n = 3$, but unproved for $n = 5$. \n5) Bertrand's postulate: for every integer $n \\geq 1$ there is a prime number $p$ with $n \\leq p \\leq 2n$. (I use $\\leq$ instead of $<$ so the result is true all the way down to $n = 1$.) \nMore generally, lots of theorems about the location of prime numbers are not accessible to proofs by induction (so far) since primality is not really a condition that respects the passage from $n$ to $n+1$.\n6) Lagrange's four-square theorem: every integer $n \\geq 1$ is a sum of four perfect squares.  \nThat is proved by doing it first for primes (using special methods exploiting primality of $n$) and then concluding it in general by prime factorization and Euler's four-square identity. You could say this is proved by induction on the number of prime factors of $n$, but to a beginner it's definitely defensible to say this can't be proved as far as we can tell by reasoning from $n$ to $n+1$. Nobody knows how to turn a representation of $n$ as a sum of four squares into a representation of $n+1$ as a sum of four squares.\n",
    "tags": [
      "elementary-number-theory",
      "proof-writing",
      "induction",
      "big-list"
    ],
    "score": 14,
    "answer_score": 6,
    "is_accepted": true,
    "question_id": 1295010,
    "answer_id": 1298886
  },
  {
    "theorem": "When learning mathematics should one prove everything one learns?",
    "context": "I sat through a real analysis class around a year ago and in about two days we partially covered the construction of the real numbers as equivalence classes of Cauchy sequences. Through the teacher didn't do it, it took me about $9$ hours to read and then write out the entire construction in a way I understood it, starting from $\\mathbb{N}$. Most of the process was laborious verification of algebraic manipulations and just checking certain things were satisfiable. Despite doing all of that, I don't think I gained any particularly new insights, it was just a lot of work. \nAt what point should you not verify something? What if you can see there is a proof of it, and you see all the proof requires is verifying a huge amount of algebraic manipulations? In this case even if you go through and check the proof you can be positive manipulating algebraic expressions won't teach you anything new. So then why bother? Where does one draw the line saying  \"I should read a proof of this\" vs \"there isn't anything to be gained here\"?\n",
    "proof": "You think you didn't get any particular new insights. This is probably true for the present, but no longer true for the future. When you will learn about the completion of a metric space using Cauchy sequences, you will just know the contruction already. And if go further on to study the completion of a uniform space using Cauchy filters, you will be pleased to see this is an abstract version of the construction on which you spent 9 hours. \nConclusion. The answer to your question heavily depends on your goals. If you just want to pass an exam and then forget about math, you probably don't need to worry about any single proof. Now if you want to become a mathematician, going deeper is a good idea in the long run.\n",
    "tags": [
      "proof-verification",
      "soft-question",
      "proof-writing",
      "proof-explanation",
      "learning"
    ],
    "score": 14,
    "answer_score": 7,
    "is_accepted": false,
    "question_id": 2506088,
    "answer_id": 2506751
  },
  {
    "theorem": "prove $\\binom{n}{k}\\frac{1}{n^k}\\leq\\frac{1}{k!}$",
    "context": "i am learning maths so fast here in MSE, thank you guys so much for being here to help us!\nso now, my next step towards proficiency: :). \ni am trying to prove that $\\binom{n}{k}\\frac{1}{n^k}\\leq\\frac{1}{k!}$ for $n\\geq1$ and for all $k\\in\\Bbb{N}$. My first problem is that i never tried to prove a statement with induction where i have two dependencies. here $n$ and $k$. \ninduction base: $n=1$ and $k=1$. $\\binom{1}{1}\\frac{1}{1^1}\\leq\\frac{1}{1!}$ which is okay.\ninductive hypothesis: for $n\\geq1$ and for all $$k\\in\\Bbb{N},~~\\binom{n}{k}\\frac{1}{n^k}\\leq\\frac{1}{k!}$$ \n1)first induction step: $$n\\rightarrow n+1~~ \\text{and}~~ k,\\binom{n+1}{k}\\frac{1}{(n+1)^{k}}\\leq\\frac{1}{(k)!}$$ \n2)second induction step: $n$ and $$k\\rightarrow k+1,~~\\binom{n}{k+1}\\frac{1}{(n)^{k+1}}\\leq\\frac{1}{(k+1)!}$$\n1)$$\\binom{n+1}{k}\\frac{1}{(n+1)^{k}}=\\Bigg(\\binom{n}{k}+\\binom{n}{k-1}\\Bigg)\\frac{1}{(n+1)^{k}} =\\\\ \\binom{n}{k}\\frac{1}{(n+1)^{k}} + \\binom{n}{k-1}\\frac{1}{(n+1)^{k}} \\leq \\frac{1}{(k)!}$$\n2) Help.   \nI am having hard time to prove further in calculations, can you pls show me further? is 1) right? i am not sure\n",
    "proof": "In general, for inductions involving two independent natural numbers (here, $n$ and $k$, with $n, k \\ge 1$), the strategy is as follows: \n\nYou first prove the basis case for $n=k=1$, which you have done. \nThen you state an inductive hypothesis that the property holds for $n = a$ and $k = b$. \nThen you need two inductive steps, and for each you assume the inductive hypothesis to be true, i.e. you'll want to use the inductive hypothesis to\n\nFirst, prove the hypothesis also holds for $n=a+1$ and $k = b$; and \nThen, prove that the hypothesis also holds for $ n=a, k = b+1$.\n\n\n",
    "tags": [
      "proof-writing",
      "induction"
    ],
    "score": 14,
    "answer_score": 7,
    "is_accepted": true,
    "question_id": 264266,
    "answer_id": 264269
  },
  {
    "theorem": "Prove that the directrix-focus and focus-focus definitions are equivalent",
    "context": "(NOTE: This is my attempt at answering this question and  this question, but I rewrote it in order to make it easier for me to solve. Also, I've made a YouTube video explaining this whole proof. Note that @Blue has a proof for this question with Dandelin spheres.)\nWe have two definitions of an ellipse and a hyperbola.\n\nAn ellipse is the locus of points that has a constant ratio of distance between a focus (point) and a directrix (line), where that constant ratio is between 0 and 1.\n\n\nHere, the eccentricity is $\\frac C A$, which, by this definition, must be a constant less than 1 for every point on the ellipse.\n\nA hyperbola is the locus of points that has a constant ratio of distance between a focus (point) and a directrix (line), where that constant ratio is greater than 1.\n\n\nHere, the eccentricity is $\\frac{C_1}{A_1}=\\frac{C_2}{A_2}$, which, by this definition, must be a constant greater than 1 for each point on the hyperbola.\n\nAn ellipse is the locus of points which has a constant sum of distances between two foci.\n\n\nHere, the eccentricity is $\\frac{F}{D_1+D_2}$. According to this definition, $D_1+D_2$ is a constant for every point on the ellipse, so we’ll just call it $D$. Obviously, $F$ is constant since it is just the distance between the foci, which doesn’t change.\n\nThe hyperbola is the locus of points which has a constant difference of distances between two foci.\n\n\nHere, the eccentricity is $\\frac{F}{D_2-D_1}$. According to this definition, $D_2-D_1$ is a constant for every point on the hyperbola, so again, we’ll just call it $D$. Obviously, $F$ is constant since it is just the distance between the foci, which doesn’t change.\nMy question is:\n$$\\text{How do I prove these definitions are equivalent?}$$\n",
    "proof": "Now, to prove these definitions are equivalent, we’re going to take the first definition and make an equation out of it. We will then manipulate it a little. Then, we’ll do the same for the second definition and we’ll get the second equation into the form of the first equation. Once we’ve done that, we will prove that the eccentricities are equal for both definitions of the eccentricity.\nFor the first definition, let’s say the focus is $(a, b)$ and the directrix is $Ax+By+C=0$ (this is not the same $A$ and $C$ as discussed in the first definition). The distance to the focus is:\n$$\\sqrt{(x-a)^2+(y-b)^2}$$\nThe distance to the line is:\n$$\\frac{\\lvert Ax+By+C \\rvert}{\\sqrt{A^2+B^2}}$$\nHowever, by dividing $Ax+By+C=0$ by some factor, we can make $\\sqrt{A^2+B^2}$ equal to 1, so we can get rid of the denominator.\nThe ratio of the distance to the point to the distance to the line is the eccentricity, which we will call $e$, so we have:\n$$\\frac{\\sqrt{(x-a)^2+(y-b)^2}}{\\lvert Ax+By+C \\rvert}=e$$\nNow, take the first equation and multiply both sides by the denominator to get rid of the fraction:\n$$\\sqrt{(x-a)^2+(y-b)^2}=e\\lvert Ax+By+C \\rvert$$\nSquare both sides to simplify things:\n$$(x-a)^2+(y-b)^2=(eAx+eBy+eC)^2$$\nNote that since $\\sqrt{A^2+B^2}=1$, we have that:\n$$e=e\\sqrt{A^2+B^2}=\\sqrt{(eA)^2+(eB)^2}$$\nThus, the eccentricity is the square root of the sum of the squares of the coefficients of $x$ and $y$ on the right side. This will become relevant later.\n\nFor the second definition, let’s say one focus is $(a, b)$ and the other is $(c, d)$. The sum/difference of the distances is $D$, so we have:\n$$\\sqrt{(x-a)^2+(y-b)^2}\\pm\\sqrt{(x-c)^2+(y-d)^2}=D$$\nAgain, $+$ is for ellipses, $-$ is for hyperbola.\n\nAlso, since $F$ is the distances between the foci, we know that:\n$$F=\\sqrt{(a-c)^2+(b-d)^2}$$\nSquare both sides:\n$$F^2=(a-c)^2+(b-d)^2$$\nDivide both sides by $D^2$:\n$$\\left(\\frac F D\\right)^2=\\left(\\frac{a-c}{D}\\right)^2+\\left(\\frac{b-d}{D}\\right)^2$$\nTake the square root of both sides and substitute $e=\\frac F D$:\n$$e=\\sqrt{\\left(\\frac{a-c}{D}\\right)^2+\\left(\\frac{b-d}{D}\\right)^2}$$\nThis will come up again later on.\n\nNow, back to where we were before this tangent. Square both sides:\n$$\\left(x-a\\right)^2+\\left(y-b\\right)^2+\\left(x-c\\right)^2+\\left(y-d\\right)^2\\pm2\\sqrt{\\left(\\left(x-a\\right)^2+\\left(y-b\\right)^2\\right)\\left(\\left(x-c\\right)^2+\\left(y-d\\right)^2\\right)}=D^2$$\nGet the radicand alone on the right side:\n$$\\left(x-a\\right)^2+\\left(y-b\\right)^2+\\left(x-c\\right)^2+\\left(y-d\\right)^2-D^2=\\mp2\\sqrt{\\left(\\left(x-a\\right)^2+\\left(y-b\\right)^2\\right)\\left(\\left(x-c\\right)^2+\\left(y-d\\right)^2\\right)}$$\nSquare both sides:\n$$\\left(\\left(x-a\\right)^2+\\left(y-b\\right)^2\\right)^2+\\left(\\left(x-c\\right)^2+\\left(y-d\\right)^2\\right)^2+2\\left(\\left(x-a\\right)^2+\\left(y-b\\right)^2\\right)\\left(\\left(x-c\\right)^2+\\left(y-d\\right)^2\\right)+D^4-2D^2\\left(\\left(x-a\\right)^2+\\left(y-b\\right)^2+\\left(x-c\\right)^2+\\left(y-d\\right)^2\\right)=4\\left(\\left(x-a\\right)^2+\\left(y-b\\right)^2\\right)\\left(\\left(x-c\\right)^2+\\left(y-d\\right)^2\\right)$$\nNote that the $\\mp$ has been removed by squaring. Now, subtract both sides by the expression on the right side:\n$$\\left(\\left(x-a\\right)^2+\\left(y-b\\right)^2\\right)^2+\\left(\\left(x-c\\right)^2+\\left(y-d\\right)^2\\right)^2-2\\left(\\left(x-a\\right)^2+\\left(y-b\\right)^2\\right)\\left(\\left(x-c\\right)^2+\\left(y-d\\right)^2\\right)+D^4-2D^2\\left(\\left(x-a\\right)^2+\\left(y-b\\right)^2+\\left(x-c\\right)^2+\\left(y-d\\right)^2\\right)=0$$\nUse the $a^2+b^2-2ab=(b-a)^2$ identity to simplify and rearrange the terms so it looks more like a quadratic in terms of $D^2$:\n$$D^4-2D^2\\left(\\left(x-a\\right)^2+\\left(y-b\\right)^2+\\left(x-c\\right)^2+\\left(y-d\\right)^2\\right)+\\left(\\left(x-c\\right)^2+\\left(y-d\\right)^2-\\left(x-a\\right)^2-\\left(y-b\\right)^2\\right)^2=0$$\nFix the $D^2$ term so that it has the $(x-c)^2+(y-d)^2-(x-a)^2-(y-b)^2$ that’s in the constant term:\n$$D^4-2D^2\\left(\\left(x-c\\right)^2+\\left(y-d\\right)^2-\\left(x-a\\right)^2-\\left(y-b\\right)^2\\right)-2D^2\\left(2\\left(x-a\\right)^2+2\\left(y-b\\right)^2\\right)+\\left(\\left(x-c\\right)^2+\\left(y-d\\right)^2-\\left(x-a\\right)^2-\\left(y-b\\right)^2\\right)^2=0$$\nAdd both sides by the new term that is not part of the quadratic:\n$$D^4-2D^2\\left(\\left(x-c\\right)^2+\\left(y-d\\right)^2-\\left(x-a\\right)^2-\\left(y-b\\right)^2\\right)+\\left(\\left(x-c\\right)^2+\\left(y-d\\right)^2-\\left(x-a\\right)^2-\\left(y-b\\right)^2\\right)^2=2D^2\\left(2\\left(x-a\\right)^2+2\\left(y-b\\right)^2\\right)$$\nUse the $a^2-2ab+b^2=(a-b)^2$ identity to simplify the left side. Also, factor out the $2$ from the right side:\n$$\\left(\\left(x-c\\right)^2+\\left(y-d\\right)^2-\\left(x-a\\right)^2-\\left(y-b\\right)^2-D^2\\right)^2=4D^2\\left(\\left(x-a\\right)^2+\\left(y-b\\right)^2\\right)$$\nSimplify the left side more by expanding the quadratics and subtracting like terms. Also, write $(2D)^2$ on the right side:\n$$\\left(2\\left(a-c\\right)x+c^2-a^2+2\\left(b-d\\right)y+d^2-b^2-D^2\\right)^2=\\left(2D\\right)^2\\left(\\left(x-a\\right)^2+\\left(y-b\\right)^2\\right)$$\nNow, divide both sides by $(2D)^2$:\n$$\\left(\\frac{a-c}{D}x+\\frac{b-d}{D}y+\\frac{c^2-a^2+d^2-b^2-D^2}{2D}\\right)^2=\\left(x-a\\right)^2+\\left(y-b\\right)^2$$\nSwitch both sides of the equation:\n$$\\left(x-a\\right)^2+\\left(y-b\\right)^2=\\left(\\frac{a-c}{D}x+\\frac{b-d}{D}y+\\frac{c^2-a^2+d^2-b^2-D^2}{2D}\\right)^2$$\nNow, we have the equation in the same form as the first equation. However, we still need to check that the definition of eccentricity from the first equation is equal to the definition of eccentricity from this equation. As we said above from the first equation, $e$ is the square root of the sum of the squares of the coefficients of $x$ and $y$ on the right side, which means that here, according to the first equation, the eccentricity is:\n$$e=\\sqrt{\\left(\\frac{a-c}{D}\\right)^2+\\left(\\frac{b-d}{D}\\right)^2}$$\nHowever, this is the exact same eccentricity we derived for the second equation, so the eccentricities are equal.\nThus, we are done and both definitions are equivalent with equivalent eccentricities for both ellipses and hyperbolas.\n",
    "tags": [
      "proof-verification",
      "proof-writing",
      "conic-sections"
    ],
    "score": 14,
    "answer_score": 6,
    "is_accepted": true,
    "question_id": 1833973,
    "answer_id": 1833974
  },
  {
    "theorem": "Ordinal interpretation of Friedman&#39;s $n$?",
    "context": "I heard that Kruskal's tree theorem can be turned into a finite form that creates an extremely fast growing function because ordinals could be encoded into trees.\nOn this wiki page it mentions that there is another very fast growing function (but it's not quite as fast) called $n$.\n\n$n(k)$ is defined as the length of the longest possible sequence that can be constructed with a k-letter alphabet such that no block of letters $x_i$,...,$x_{2i}$ is a subsequence of any later block\n\nI think that is related to Higman's lemma\nso I was wondering what is the interpretation of this in terms of ordinals? I guess it must be related to an ordinal between $\\epsilon_0$ and $\\Gamma_0$.\n\nHow can I explain it better. The Ackermann and Goodstein function is fast growing and the TREE function is fast growing, because they are produced from ordinal notations (for up to omega^2 (or is it omega^omega of Primitive recursive arithmetic?), epsilon_0 and Gamma_0 respectively) that measure the proof strength of some theories.. so they are faster than any total function in those theories. I want to know what ordinal and theory is $n$ associated with.\nIf you understand my meaning please feel free to edit my question. I am only starting at logic so I don't know how to state it formally.\n",
    "proof": "There is indeed an ordinal interpretation of Friedman's block subsequence theorem;  however, the associated ordinal is not between $\\varepsilon_0$ and $\\Gamma_0$;  it is $\\omega^{\\omega^\\omega}$.  More precisely, k-labelled sequences have ordinal $\\omega^{\\omega^{k-1}}$.  Friedman's block subsequence theorem is provable in the theory $I\\Sigma_3$ but not the theory $I\\Sigma_2$; the latter has poof theoretic ordinal $\\omega^{\\omega^\\omega}$.\nThe ordering on sequences of order type $\\omega^{\\omega^\\omega}$ is as follows: \n1-labelled sequences are determined by their length, i.e. 1 < 11 < 111 ... .\nAssume now that have ordered k-labelled sequences; we will now order k+1-labelled sequences.  Let S and T be two k+1-labelled sequences.  If one of S or T has more appearances of k+1 than the other, that sequence will be considered greater.  Otherwise,  suppose both S and T have n appearances of k+1;  express S and T as\n$S = S_0$ k+1 $S_1$ k+1 ... k+1 $S_n$\n$T = T_0$ k+1 $T_1$ k+1 ... k+1 $T_n$\nwhere the $S_i$ and $T_i$ are k-labelled sequences.  Compare the $S_i$ and the $T_i$ lexicographically, i.e. first compare $S_0$ to $T_0$, then $S_1$ to $T_1$, etc.  For the smallest $i$ for which $S_i$ and $T_i$ differ, if $S_i > T_i$, then $S > T$;  if $S_i < T_i$, then $S < T$.  This defines the ordering.\nNow I will demostrate that k-labelled sequences have order type $\\omega^{\\omega^{k-1}}$.  Clearly, 1-labelled sequences have order type $\\omega = \\omega^{\\omega^{1-1}}$.  Suppose that we have proven that k-labelled sequences have order type $\\omega^{\\omega^{k-1}}$.  Let S be a k+1 labelled sequence, and express S as\n$S = S_0$ k+1 $S_1$ k+1 ... k+1 $S_n$.\nwhere the $S_i$ represent k-labelled sequences. Let $\\alpha_i$ be the ordinals representing $S_i$.  We have $\\alpha_i < \\omega^{\\omega^{k-1}}$ for all $i$.  Then $S$ will be associated with the ordinal\n$\\omega^{\\omega^{k-1}n} (1+\\alpha_0) + \\omega^{\\omega^{k-1}(n-1)} \\alpha_1 + \\ldots + \\omega^{\\omega^{k-1}} \\alpha_{n-1} + \\alpha_n$.\nI leave it to the reader to show that this association will yield the same ordering as our previously defined ordering.  This identifies k+1-labelled sequences with ordinals up to $\\omega^{\\omega^{k}}$, as desired.\nFriedman's n(k) grows at a rate similar to that of $H_{\\omega^{\\omega^{\\omega}}}(k)$ in the Hardy hierarchy, which is equal to $F_{\\omega^{\\omega}}(k)$ in the fast-growing hierarchy;  you lose an $\\omega$ when converting to the fast-growing hierarchy.\n",
    "tags": [
      "logic",
      "proof-writing",
      "order-theory",
      "ordinals",
      "trees"
    ],
    "score": 14,
    "answer_score": 8,
    "is_accepted": true,
    "question_id": 319813,
    "answer_id": 326083
  },
  {
    "theorem": "Why are linear combinations of independent standard normal random variables also normally distributed?",
    "context": "My professor has given a list of questions that will not be appearing on my test, with this being one of them. I still feel this is extremely important to understand.\nHow can I prove the following\n\nIf $X$ and $Y$ are independent, standard normal random variables, then the\n  linear combination $aX+bY,\\;\\forall a,b>0$ is also\n  normally distributed.\n\n\nIf I am not mistaken, I believe I can find the distribution of the linear combination\n\nIf we let $Z=aX+bY$, knowing $X,Y \\sim N(0,1)$, we can find the expectation and variance as\n  $$\\mathbb{E}(Z)=\\mathbb{E}(aX+bY)=a\\mathbb{E}(X)+b\\mathbb{E}(Y)=0$$\n  $$Var(Z)=Var(aX+bY)=a^2Var(X)+b^2Var(Y)=a^2+b^2$$\n  $$$$\n  Thus, $Z \\sim N(0,a^2+b^2)$.\n\nI just don't think this proves that linear combination is normally distributed. I tried looking in some reference books that my professor reserved at the library, but they all just state the fact and I can't figure out how to prove it.\n",
    "proof": "The most direct way is to look at the characteristic function.\nThe characteristic function of an r.v. characterizes its probability distribution completely. If you can show that the characteristic function of the linear combination is that of a normal r.v., you are done.\n",
    "tags": [
      "probability",
      "probability-theory",
      "probability-distributions",
      "proof-writing"
    ],
    "score": 14,
    "answer_score": 4,
    "is_accepted": false,
    "question_id": 961765,
    "answer_id": 961772
  },
  {
    "theorem": "Prove that $\\,\\int_0^1 \\sqrt{1+x^2}\\,\\mathrm dx\\le \\int_0^1 \\sqrt{1+x}\\,\\mathrm dx\\,,\\,$ proof verification.",
    "context": "\nProve that $\\displaystyle\\int_0^1 \\sqrt{1+x^2}\\mathrm dx\\le \\int_0^1 \\sqrt{1+x}\\,\\mathrm dx$\n\nSo these are my first days of integral calulus. I think that's well done and is all clear, but would like to know if there's a better way to work it, or even if I've omitted something, would like to know what you will add or improve, thanks!\nMy proof:\nLet $f(x)=\\sqrt{1+x^2}$ and $g(x)=\\sqrt{1+x}$ continuous functions on the interval $[0,1]$, then, for both functions, the condition $x\\in [0,1] \\to 0\\le x\\le 1$ must be satisfied. So, we start by building the functions based on this condition:\n$$x\\in [0,1] \\to 0\\le x\\le 1$$\n$$0\\le x^2\\le x$$\n$$1\\le 1+x^2\\le 1+x$$\n$$1\\le \\sqrt{1+x^2}\\le \\sqrt{1+x}$$\nSo, that implies that:$$f(x)\\le g(x), \\forall x\\in[0,1]$$\n(For this last step, I'm just applying a known proposition: If $f(x)\\ge g(x), $ then $\\int_a^b f(x)\\,\\mathrm dx\\ge \\int_a^b g(x)\\,\\mathrm dx$)\n$$\\int_0^1 \\sqrt{1+x^2}\\,\\mathrm dx\\le \\int_0^1 \\sqrt{1+x}\\,\\mathrm dx$$\n",
    "proof": "Let\n$$\\text{I}＝\\int_{0}^{1}\\sqrt {1＋x}\\;dx＝\\int_{0}^{1}\\left({1＋x}\\right)^{\\frac {1}{2}}dx＝\\left[{\\frac {2\\left({1＋x}\\right)^{\\frac {3}{2}}}{3}}\\right]^1_0＝\\frac {4\\sqrt {2}-2}{3}\\\\~\\\\$$\nLet\n$$\\text{J}＝\\int_{0}^{1}\\sqrt {1＋x^2}\\;dx＝\\left[{x\\sqrt {1＋x^2}}\\right]^1_0-\\int_{0}^{1}\\frac {x^2}{\\sqrt {1＋x^2}}dx\\\\~\\\\＝\\sqrt {2}-\\int_{0}^{1}\\left({\\sqrt {1＋x^2}-\\frac {1}{\\sqrt {1＋x^2}}}\\right)dx\\\\~\\\\2\\text{J}＝\\sqrt {2}＋\\int_{0}^{1}\\frac {1}{\\sqrt {1＋x^2}}dx\\\\~\\\\$$\nand therefore\n$$\\int_{0}^{1}\\frac {1}{\\sqrt {1＋x^2}}dx＝\\int_{0}^{\\frac {\\pi}{4}}\\frac {1＋\\tan^2(x)}{\\sqrt {1＋\\tan^2(x)}}dx＝\\int_{0}^{\\frac {\\pi}{4}}\\sec(x)\\;dx＝\\left[{\\ln(\\sec(x)＋\\tan(x))}\\right]^{\\frac {\\pi}{4}}_{0}＝\\ln(1＋\\sqrt {2})\\\\~\\\\$$\nFinally\n$$2\\text{J}＝\\sqrt {2}＋\\ln(1＋\\sqrt {2})\\implies\\;\\text{J}＝\\frac {\\sqrt {2}＋\\ln(1＋\\sqrt {2})}{2}$$\nSo:$I>J$\n",
    "tags": [
      "calculus",
      "solution-verification",
      "definite-integrals",
      "proof-writing"
    ],
    "score": 14,
    "answer_score": 1,
    "is_accepted": false,
    "question_id": 4756126,
    "answer_id": 4756247
  },
  {
    "theorem": "My first proof that uses the well-ordering principle (very simple number theory). Please mark/grade.",
    "context": "What do you think about my first proof that uses the well-ordering principle? Please mark/grade.\n\nTheorem\nThe sum of the cubes of three consecutive natural numbers is a multiple of 9.\nProof\nFirst, introducing a predicate $P$ over $\\mathbb{N}$, we rephrase the theorem as follows.\n        $$\\forall n \\in \\mathbb{N}, P(n)\n\t\t\\quad \\text{where} \\quad\n\t\tP(n) \\, := \\, n^3 + (n + 1)^3 + (n + 2)^3 \\text{ is a multiple of 9}$$\n        We prove the theorem by contradiction.\n        To that end, in step 1, we start by assuming that the negation of the theorem holds.\n        Then, in step 2, we show that some natural number satisfies our predicate.\n        In step 3, we move along with the help of the well-ordering principle.\n        Finally, in step 3, we deduce our contradiction.\nStep 1: Assuming the negation\n            We assume the negation of the theorem:\n            There is a $n \\in \\mathbb{N}, \\neg P(n)$.\n            With that said, we are done as soon as a contradiction is deduced.\nStep 2: Satisfying our predicate\n            Consider the natural number $0$.\n            Since $0^3 + (0 + 1)^3 + (0 + 2)^3 = 0 + 1 + 8 = 9$, we see that $P(0)$ is true.\nStep 3: Employing the well-ordering principle\n            By our assumption, there is a natural number for which the predicate is false.\nThus there is a non-empty set of such numbers.\n            According to the well-ordering principle, the set contains a least element $k$.\nSince $P(0)$ is true, we infer that $k \\ne 0$, more precisely $k > 0$.\n            Hence, $k - 1$ is a natural number;\n            and by choice of $k$, we have that $P(k - 1)$ is true.\nStep 4: The contradiction\n            As $P(k - 1)$ is true, there is a natural number $i$ such that\n            $$i \\cdot 9 = (k - 1)^3 + k^3 + (k + 1)^3\\text{.}$$\n            We use this fact in the following equivalent transformation.\n            In the transformation, the first line does not represent a multiple of $9$,\n            since $P(k)$ is false; however, the last line clearly does represent a multiple of $9$.\nThis is our contradiction, which completes the proof.\n\\begin{align}\nk^3 + (k + 1)^3 + (k + 2)^3 &= (k - 1)^3 + k^3 + (k + 1)^3 + (k + 2)^3 - (k - 1)^3 \\\\\n\t\t\t&= (k - 1)^3 + k^3 + (k + 1)^3 + k^3 + 6k^2 + 12k + 8 - k^3 + 3k^2 - 3k + 1 \\\\\n\t\t\t&= (k - 1)^3 + k^3 + (k + 1)^3 + 9k^2 + 9k + 9 \\\\\n\t\t\t&= 9i + 9k^2 + 9k + 9 \\\\\n\t\t\t&= 9 \\cdot (i + k^2 + k + 1)\n\\end{align}\n",
    "proof": "While I have no objection to the proof as such, this problem seems to me to be an example in which the machinery of well-ordering/induction is unnecessary, and avoiding it leads to a much shorter proof. $n^3 + (n+1)^3 + (n+2)^3 = 3n^3 + 9n^2 + 15n + 9 \\equiv 3n^3-3n$ (mod 9). This latter will be congruent to 0 mod 9 so long as $n^3 - n \\equiv 0$ (mod 3).  But $n^3-n = (n-1)n(n+1)$ and one of any three consecutive numbers is divisible by 3.\n",
    "tags": [
      "elementary-number-theory",
      "proof-writing",
      "induction",
      "proof-verification",
      "divisibility"
    ],
    "score": 14,
    "answer_score": 1,
    "is_accepted": false,
    "question_id": 736177,
    "answer_id": 1522088
  },
  {
    "theorem": "Is it legitimate to specify a statement as a Theorem if it is proved using numerical methods (since it can&#39;t be proved analytically)?",
    "context": "I have some statement, which is \"proved\" by numerical methods (particularly, using simulations), since it isn't possible to prove analytically. Is it legitimate to articulate this statement as a Theorem, if it isn't proved analytically. If no, what should I call it? \nP.S. More details regarding the statement. \nI have some data generating process and need to prove that the generated data as a function of some variable has some properties. For example, I have $x^1,...,x^n$ generated data (n=100000) and need to show that the function $x(\\gamma)$ is positive defined  over the domain. Therefore, on the generated data I apply interpolation technique in order to construct the function and I check the property. \n",
    "proof": "In general, numerical methods don't constitute proofs. If all we have is an unknown blackbox function $f:\\mathbb{R} \\to \\mathbb{R}$ that we know nothing about, and all we can do is compute its value at (finitely) many points, then we simply can't prove that $f$ is positive.\nHowever, in specific cases, we could have arguments based on numerical methods that are valid. Typically, we'd need to make a numerical approximation, and then prove, using non-numerical methods, that our numerical approximation is accurate enough for the theorem to follow. As such, how numerical methods can aid us in proving a statement is very statement-specific.\nTake, for example the following problem: Prove that $f(x) = x^2 + 1 > 0 \\ \\forall x \\in [-1, 1]$.\nInvalid proof: We computed $f(x)$ at $10^{10^{10000}}$ random points and used linear interpolation between them. Here's a plot. We can see that $f(x)$ is always positive.\nValid proof 1: We computed $f(x)$ at points three points: $f(-1) = 2$, $f(0) = 1$, and $f(1)=2$. Let $g(x)$ be the linear interpolation of the points $(-1, 2)$, $(0, 1)$, and $(1, 2)$. $g$ attains its minimum at $g(0) = 1$. Since $f^{\\prime \\prime} = 2$, we can compute an error bound on our interpolation (see https://ccrma.stanford.edu/~jos/resample/Linear_Interpolation_Error_Bound.html): $|f(x) - g(x)| \\leq \\frac{2}{8}$. Therefore, we can conclude that $f(x) \\geq \\frac{3}{4} > 0$.\nNote: Often, if we need to resort to numerical methods, if would be just as hard to compute derivatives. However, we don't need the actual derivatives, we just need an upper bound. The better the bound, the less points we would need to evaluate $f(x)$ at. Furthermore, bound to the first derivative is enough, but having second could also reduce the number of points needed.\nValid proof 2 (sketch): We know that $f(x)$ is convex. We use a numerical method to compute its minimum. find that $\\min f(x) \\approx 1.0000000075$. We also have an (true, non-numerical) error bound on our approximation: $|1.0000000075 - \\min f(x)| < 0.001$. Therefore, $f(x) > 1.0000000075 - 0.001 > 0$.\nFinally, it doesn't really matter whether analytical proofs exist or not. The validity of any proof is only determined by that proof and no others.\nIn fact, it has been proven that not all true statements can be proven. But that is no reason to reduce our standards of rigor.\n",
    "tags": [
      "proof-writing",
      "numerical-methods"
    ],
    "score": 13,
    "answer_score": 53,
    "is_accepted": true,
    "question_id": 3063680,
    "answer_id": 3063729
  },
  {
    "theorem": "Show that $e^n&gt;\\frac{(n+1)^n}{n!}$ without using induction.",
    "context": "I have got an inequality problem which is as follow:\n\nShow that $e^n>\\frac{(n+1)^n}{n!}$\n\nI can do it by induction but I have been told to prove it without induction.\nMy Work:\n$$e^n=1+n+\\frac{n^2}{2!}+\\frac{n^3}{3!}+........$$\n$$e^n>1+n+\\frac{n^2}{2!}+\\frac{n^3}{3!}+........+\\frac{n^n}{n!}$$\n$$e^n>\\frac{n^n}{n!}+\\frac{n^{n-1}}{(n-1)!}.......+\\frac{n^2}{2!}+n+1$$\nFrom here I can't go further. \nI shall be thankful if you guys can provide me a complete solution/proof of this inequality. A hint will also work.\nThanks in advance.\n",
    "proof": "$$e^n=1+n+\\frac{n^2}{2!}+\\frac{n^3}{3!}+........$$\n$$e^n>1+n+\\frac{n^2}{2!}+\\frac{n^3}{3!}+........+\\frac{n^n}{n!}$$\n$$e^n>\\frac{n^n}{n!}+\\frac{n^{n-1}}{(n-1)!}.......+\\frac{n^2}{2!}+n+1$$ $$e^n>n^n\\left[\\frac{1}{n!}+\\frac{1}{n(n-1)!}+\\frac{1}{n^2(n-2)!}...+\\frac{1}{n^{n-1}}+\\frac{1}{n^n}\\right] $$ $$e^n>\\frac{n^n}{n!}\\left[1+\\frac{1}{n}n+\\frac{1}{n^2}n(n-1)+\\frac{1}{n^3}n(n-1)(n-2)...+\\frac{n!}{n^n}\\right] $$  $\\because$ $$n(n-1)>\\frac{n(n-1)}{2!}$$and $$n(n-1)(n-2)>\\frac{n(n-1)(n-2)}{3!}$$and $$n!>1$$ \n$\\therefore $ $$e^n>\\frac{n^n}{n!}\\left[1+n\\frac{1}{n}+\\frac{n(n-1)}{2!}\\frac{1}{n^2}+...+\\frac{1}{n^n}\\right]$$ $$e^n>\\frac{n^n}{n!}(1+\\frac1n)^n$$ $$e^n>\\frac{n^n}{n!}\\frac{(n+1)^n}{n^n}$$  $$e^n>\\frac{(n+1)^n}{n!}$$\n",
    "tags": [
      "inequality",
      "proof-writing",
      "exponential-function",
      "alternative-proof"
    ],
    "score": 13,
    "answer_score": 6,
    "is_accepted": true,
    "question_id": 2051203,
    "answer_id": 2066148
  },
  {
    "theorem": "Which do you recommend for learning how to write proofs — How to Prove it by Velleman, or How to Solve it by Polya?",
    "context": "Which of these two books is suited for a student looking to learn how to write proofs?\nI have a working knowledge of calculus and linear algebra, but I'm not good at writing proofs. My intention is to learn proofs in general, not necessarily for the calculus and linear algebra.\n\nHow to Prove it by Daniel Velleman\n\nHow to Solve It by George Polya\n\n\nI ask because the latter is suggested on a highly voted question here, but the former has a more apt name. The reviews aren't helping. You can suggest other books.\n",
    "proof": "Velleman's How to Prove it is quite a bit more relevant to your needs.  It is organized like a conventional text, and pays a lot of attention to proof writing.\nPolya's book focuses on problem-solving. One can view it as a better book, certainly a historically far more important book. But it focuses on how one finds the idea that will crack a concrete problem.  \nThere is quite a bit of material in Velleman that is useful for writing proofs in linear algebra, in particular on how to proceed from definitions. There is none of that in Polya.  There is also essentially nothing in Polya on basic analysis. Polya beautifully accomplishes his aims: they just happen to be different from what you said you wanted.\n",
    "tags": [
      "reference-request",
      "proof-writing",
      "book-recommendation"
    ],
    "score": 13,
    "answer_score": 17,
    "is_accepted": true,
    "question_id": 166603,
    "answer_id": 166610
  },
  {
    "theorem": "Show that every local homeomorphism is continuous and open therefore bijective local homeomorphism is a homeomorphism",
    "context": "Follow up on another question I asked recently: Topology: Show restriction of continuous function is continuous, and restriction of a homeomorphism is a homeomorphism\n\nDefinition: Let $(X, \\mathcal{T})$ and $(Y, \\mathcal{J})$ be topological spaces. A\n  function ${\\displaystyle f:X\\to Y\\,}$  is a local homeomorphism if for\n  every point $x \\in X$ there exists an open set $U \\subseteq X$\n  containing $x$ and an open set $V \\subseteq Y$ such that the restriction ${\\displaystyle f|_{U}:U\\to V\\,}$  is a  homeomorphism.\n\nThis definition is a bit alarming because it starts with \"...if for\n every point $x \\in X$ there exists an open set $U \\in \\mathcal{T}$...\", makes it seem like a property of the underlying space. Can we always find an open $U$? But anyways. \n\nObjective: Show that every local homeomorphism is continuous and open\n  therefore bijective local homeomorphism is a homeomorphism\n\nProof: \n(Honestly not sure what I am doing but proceed regardless)\nLet $(X, \\mathcal{T})$ and $(Y, \\mathcal{J})$ be topological spaces and \n function ${\\displaystyle f:X\\to Y\\,}$  is a local homeomorphism. We will show that $f$ is continuous and open.\n\nFirst show $f$ is continuous. \n$f$ is continuous if for all $V \\in \\mathcal{J}, f^{-1}(V) \\in \\mathcal{T}$. Take some $V \\in \\mathcal{J}$, then $V$ is a subspace equipped with subspace topology $\\mathcal{J}_V = \\{V \\cap W| W \\in \\mathcal{J}\\}$. \nConsider the inverse of the  restriction $f^{-1}|_U$ on an open set in $\\mathcal{J}_V$, then $f^{-1}|_U(V \\cap W) = f^{-1}|_U(V) \\cap f^{-1}|_U(W) $$= f^{-1}(V) \\cap U \\cap f^{-1}(W) \\cap U =  f^{-1}(V) \\cap  f^{-1}(W) \\cap U$.\nThen $f^{-1}(V) = f^{-1}(W) \\cup U \\cup f^{-1}|_U(V \\cap W)$. We note all the sets on the right hand side are open. In particular, $U$ is open,  $f^{-1}|_U(V \\cap W)$ is open by definition of homeomorphism (??  $f^{-1}(W)$ ??), hence $f$ is continuous. ($\\leftarrow$ something wrong here!)\n\nNext show $f$ is open. \n$f$ is open if $\\forall U \\in \\mathcal{T}, f(U) \\in \\mathcal{J}$. Consider the restriction $f|_U$ on the subspace topology on $U$, $\\mathcal{T}_U = \\{U \\cap M | M \\in \\mathcal{T}\\}$.  $f|_U(U \\cap M) = f|_U(U) \\cap f|_U(M) = V \\cap f(M) \\cap f(U)$\nThen $f$ is open since $f(U) = f|_U(U \\cap M) \\cup V \\cup f(M)$ and $f|_U(U \\cap M)$ is open by definition of homeomorphism, $V$ is open in $\\mathcal{T}$ (?? $\\cup f(M)$ ??) ($\\Leftarrow$ another mistake here)\n\nI'm not quite sure how to proceed with showing bijective + continuous + open + local = homeomorphism part. \n\nCan someone help me fix those two problems and give me some ideas how\n  to conclude that bijective local homeomorphisms are homeomorphisms?\n\n",
    "proof": "Your attempts are, unfortunately, flawed.\nSince you know about local properties of $f$, it is better showing that $f$ is continuous at each point.\nLet $x\\in X$; we want to show that, for every open neighborhood $V$ of $f(x)$, there exists a neighborhood $U$ of $x$ such that $f(U)\\subseteq V$. Let $U_x$ be an open neighborhood of $x$ and $V_x$ an open set in $Y$ such that $f$ induces a homeomorphism $f_{U_x}\\colon U_x\\to V_x$ and choose any open neighborhood $V$ of $f(x)$. \nThen $V\\cap V_x$ is an open set in $Y$ containing $f(x)$, \n\nso there exists an open neighborhood $U$ of $x$ in $U_x$ such that $f(U)\\subseteq V\\cap V_x$; since $U$ is open in $U_x$ it is open in $X$ as well and $f(U)\\subseteq V$ as requested.\n\nNow you want to prove that $f$ is open. Let $A$ be open in $X$ and, for each $x\\in A$, choose open sets $U_x\\subseteq X$ and $V_x\\subseteq Y$ so that $x\\in U_x$ and $f$ induces a homeomorphism between $U_x$ and $V_x$.\nFor each $x\\in A$, $f(U_x\\cap A)$ is open in $V_x$, so it is open in $Y$ as well. Therefore\n$$\n\\bigcup_{x\\in A}f(U_x\\cap A)\n$$\n\nequals $f(A)$ and is open in $Y$.\n\nIf $f$ is bijective, then $f^{-1}$ exists and it is continuous \n\n because $f$ is open.\n\n",
    "tags": [
      "general-topology",
      "functions",
      "proof-verification",
      "proof-writing",
      "proof-explanation"
    ],
    "score": 13,
    "answer_score": 18,
    "is_accepted": true,
    "question_id": 1826878,
    "answer_id": 1826952
  },
  {
    "theorem": "Prove that $\\mathcal{P}(A)⊆ \\mathcal{P}(B)$ if and only if $A⊆B$.",
    "context": "Here is my proof, I would appreciate it if someone could critique it for me:\nTo prove this statement true, we must proof that the two conditional statements (\"If $\\mathcal{P}(A)⊆ \\mathcal{P}(B)$, then $A⊆B$,\" and, If $A⊆B$, then $\\mathcal{P}(A)⊆ \\mathcal{P}(B)$) are true.\n\nContrapositive of the first statement: If $A \\nsubseteq B$, then $\\mathcal{P}(A) \\nsubseteq \\mathcal{P}(B)$\nIf $A \\nsubseteq B$, then there must be some element in $A$, call it $x$, that is not in $B$: $x \\in A$, and $x \\notin B$. Since $x \\in A$, then $\\{x\\} \\in \\mathcal{P}(A)$; moreover, since $x \\notin B$, then $\\{x\\} \\notin \\mathcal{P}(B)$, which proves that, if $A \\nsubseteq B$, then $\\mathcal{P}(A) \\nsubseteq \\mathcal{P}(B)$. By proving the contrapositive true, the original proposition must be true.\n\nTo prove the second statement true, I would implement nearly the same argument, so that isn't necessary to write. So, does this proof seem correct? Also, was the contrapositive necessary? Or is there another way to prove the initial statement?\n",
    "proof": "Your argument is fine, if a little more roundabout than necessary: both directions can be done easily with direct proofs. \nSuppose first that $\\wp(A)\\subseteq\\wp(B)$. $A\\subseteq A$, so $A\\in\\wp(A)\\subseteq\\wp(B)$, so $A\\in\\wp(B)$, and hence $A\\subseteq B$.\nNow suppose that $A\\subseteq B$. Then for any $X\\in\\wp(A)$ we have $X\\subseteq A\\subseteq B$, so $X\\subseteq B$, and therefore $X\\in\\wp(B)$. Thus, $\\wp(A)\\subseteq\\wp(B)$.\n",
    "tags": [
      "discrete-mathematics",
      "proof-writing"
    ],
    "score": 13,
    "answer_score": 18,
    "is_accepted": true,
    "question_id": 332130,
    "answer_id": 332148
  },
  {
    "theorem": "How do I show that the sum of two random variables is random variable?",
    "context": "How do I prove the following?\n\nIf $X$ and $Y$ are random variables on a probability space $(\\Omega, F, \\mathbb P)$, then so is $X+Y$.\n\nThe definition of a random variable is a function $X: \\Omega \\to \\mathbb R$, with the property that $\\{\\omega\\in\\Omega: X(\\omega)\\leq x\\}\\in F$, for each $x\\in\\mathbb R$.\n\nFurthermore, how to approach $X+Y$ and $\\min\\{X, Y\\}$?\n",
    "proof": "There are a number of ways to do it.  A standard trick for proving things like this is by noticing that $\\{X+Y\\leq x\\}^c=\\{X + Y > x\\} = \\displaystyle \\bigcup_{r \\in \\mathbb{Q}} \\{X > r\\} \\cap \\{Y > x - r\\} $, and that showing that this is in $F$ is enough to show that $X + Y$ is measurable.  Then use the properties of $X$, $Y$, and $\\sigma-$algebras to deduce that this set is measurable.\n",
    "tags": [
      "probability",
      "probability-theory",
      "proof-writing"
    ],
    "score": 13,
    "answer_score": 14,
    "is_accepted": true,
    "question_id": 305989,
    "answer_id": 305993
  },
  {
    "theorem": "Proof that there are infinitely many positive rational numbers smaller than any given positive rational number.",
    "context": "I'm trying to prove this statement:- \"Let $x$ be a positive rational number. There are infinitely many positive rational numbers less than $x$.\"\nThis is my attempt of proving it:-\nAssume that $x=p/q$ is the smallest positive rational number. \nConsider $p/q - 1$\n       $= (p-q)/q$\nCase I: $p$ and $q$ are both positive\nThen, $p-q<p$\nAnd hence, $(p-q)/q < p/q$\nSince $p$ and $q$ are integers, $(p-q)$ is also an integer. Thus, $(p-q)/q$ is a rational number smaller than $p/q$. Therefore, our assumption is wrong, and there always exists a rational number smaller than any given rational number $x$.\nCase II: $p$ and $q$ are both negative\nThen, let $p/q = -s/-t$, where $s$ and $t$ are both positive integers.\nThen, $-s-(-t)>-s \\implies (-s+t)/-t < -s/-t \\implies (p-q)/q <p/q$ \nSince $p$ and $q$ are integers, $(p-q)$ is also an integer. Thus, $(p-q)/q$ is a rational number smaller than $p/q$. Therefore, our assumption is wrong, and there always exists a rational number smaller than any given rational number $x$.\nQ.E.D\nIs my proof correct? And there are a couple of questions that I've been pondering over:-\n1) How do I justify the subtraction of $1$ from $p/q$? I mean, I assumed that $p/q$ is the smallest rational number, so how do I even know if this operation is valid?\n2) I proved that there always exists a smaller rational number given any positive rational number. But how do I prove that there's always a smaller positive rational number?\n3) Also, I don't seem to have proved that there are infinitely many smaller rational numbers than $x$. If I use a general integer $k$ instead of $1$, this would be taken care of, right? But then again, how do I justify this subtraction?\nI'd be really grateful, if someone could help me with this! Thanks!\n",
    "proof": "First, you don't need Case II. If $x\\in\\mathbb Q_{>0}$, then you can assume, that $x=\\frac{p}{q}$, where $p,q>0$.\nYour general idea is good: Assume, that $x$ is smallest possible and find an even smaller one, which then is a contradiction. Now let's answer your questions:\n2) You already noticed, that you only proved that there is a smaller rational number, not necissarily positive. Your proof is basically \"If $x$ is the smallest, then $x-1$ is smaller, a contradiction.\" \n1) Of course, this a valid operation, it actually disproved your assumption, that $x$ was the smallest rational number.\n3) is right, too. With an arbitrary $k$, you get infintely many smaller rationals.\nTo prove the positive case, notice, that if $x\\in\\mathbb Q_{>0}$, then $0<\\frac{x}{k}<x$ for all $k\\in\\mathbb Z$.\n",
    "tags": [
      "proof-writing",
      "rational-numbers"
    ],
    "score": 13,
    "answer_score": 5,
    "is_accepted": true,
    "question_id": 428695,
    "answer_id": 428701
  },
  {
    "theorem": "Logarithm proof problem: $a^{\\log_b c} = c^{\\log_b a}$",
    "context": "I have been hit with a homework problem that I just have no idea how to approach. Any help from you all is very much appreciated. Here is the problem\nProve the equation: $a^{\\log_b c} = c^{\\log_b a}$\nAny ideas?\n",
    "proof": "$\\large{a^{\\log_b c}=e^{\\ln a \\cdot\\log_b c}= e^{\\ln a\\cdot\\ln c/\\ln b}}=c^{\\ln a/\\ln b}=c^{\\log_b a}$.\n",
    "tags": [
      "proof-writing",
      "logarithms"
    ],
    "score": 13,
    "answer_score": 13,
    "is_accepted": false,
    "question_id": 320116,
    "answer_id": 451658
  },
  {
    "theorem": "Help to clarify proof of Euler&#39;s Theorem on homogenous equations",
    "context": "\nWhy is the last step (setting $\\lambda = 1$ allowed?\nI have trouble accepting this because if I set $\\lambda =1 $ at the very start, then:\n$f(\\lambda x , \\lambda y)=\\lambda^r f(x,y)$\nbecomes\n$f(x,y) = f(x,y)$\nand\nso I can't prove the theorem.\nWhy does setting $\\lambda = 1$ at the end of the proof work?\nEDIT\nFor example, let $f(x,y)=x^2y^2$\nTherefore, $f(x,y)$ is homogeneous of degree 4.\nTherefore, the second to last step will be:\n$4x^2y^2=4\\lambda^3x^2y^2$\nThe above equation is only true if $\\lambda=1$\nWhy doesn't the theorem make a qualification that $\\lambda$ must be equal to 1?\nIt seems to me that this theorem is saying that there is a special relationship between the derivatives of a homogenous function and its degree but this relationship holds only when $\\lambda=1$. Please correct me if my observation is wrong.\n",
    "proof": "You can't set $\\lambda = 1$ in the line $f(\\lambda x,\\lambda y) = \\lambda^r f(x,y)$ because the very next step is to differentiate with respect to $\\lambda$ which makes no sense when the variable isn't present.\nAt the end of the proof, you are taking advantage of the fact that $\\lambda$ is an arbitrary element of $\\mathbb{R}$. So you pick an element that makes the calculations easy.\nVariant of proof\nDefine the function $g : \\mathbb{R} \\rightarrow \\mathbb{R}$ by $g(t) = f(tx,ty)$. Since $f$ is homogeneous, we can write $g(t) = t^r f(x,y)$. Find $g'(t)$. \nUsing $g(t) = t^r f(x,y)$, it is clear that $g'(t) = rt^{r-1} f(x,y)$.\nUsing $g(t) = f(tx,ty)$, we get that $g'(t) = \\frac{\\partial f}{\\partial (tx)}\\cdot\\frac{d(tx)}{dt} + \\frac{\\partial f}{\\partial (ty)}\\cdot\\frac{d(ty)}{dt} = x\\frac{\\partial f}{\\partial (tx)}+y\\frac{\\partial f}{\\partial (ty)}$.\nSo we have that for all $t$, $rt^{r-1} f(x,y) = x\\frac{\\partial f}{\\partial (tx)} + y\\frac{\\partial f}{\\partial (ty)}$. If we let $t=1$, then we have that $g(1) = f(x,y)$, our original function, and $rf(x,y) = x\\frac{\\partial f}{\\partial x}+ y\\frac{\\partial f}{\\partial y}$, the desired result.\n\nTo address the specific example of $f(x,y) = x^2y^2$. You have the second to last step wrong on the RHS. \nWhen doing the proof, we are working with $f(\\lambda x, \\lambda y)$ throughout. So when calculating $\\dfrac{df}{d\\lambda}$ we get:\n$\\begin{align}\n\\dfrac{df}{d\\lambda} &= \\dfrac{\\partial f}{\\partial (\\lambda x)}\\cdot \\dfrac{d (\\lambda x)}{d\\lambda} + \\dfrac{\\partial f}{\\partial (\\lambda y)}\\cdot \\dfrac{d (\\lambda y)}{d\\lambda} \\\\\n&= 2(\\lambda x)(\\lambda y)^2\\cdot x + 2(\\lambda x)^2(\\lambda y)\\cdot y \\\\\n&= 4\\lambda^3 x^2y^2 = \\dfrac{d}{d\\lambda}\\left(\\lambda^4 f(x,y)\\right)\n\\end{align}$\nSo we could choose any $\\lambda$ we want and it would still be a true equation. But then to get the desired result, we would have to divide that back out of both sides. Choosing $\\lambda = 1$ saves a bit of algebra.\n",
    "tags": [
      "calculus",
      "proof-writing"
    ],
    "score": 13,
    "answer_score": 16,
    "is_accepted": false,
    "question_id": 657472,
    "answer_id": 657487
  },
  {
    "theorem": "Vector spaces - Multiplying by zero scalar yields zero vector",
    "context": "Please rate and comment. I want to improve; constructive criticism is highly appreciated.\nPlease take style into account as well.\nThe following proof is solely based on vector space related axioms.\nAxiom names are italicised.\nThey are defined in Wikipedia (see vector space article).\nVector spaces - Multiplying by zero scalar yields zero vector\n\\begin{array}{lrll}\n\\text{Let} & \\dots & \\text{be} & \\dots \\\\\n\\hline\n& F && \\text{a field.} \\\\\n& V && \\text{a vector space over $F$.} \\\\\n& 0 && \\text{an identity element of addition of $F$.} \\\\\n& \\mathbf{0} && \\text{an identity element of addition of $V$.} \\\\\n& \\mathbf{v} && \\text{an arbitrary vector in $V$.} \\\\\n\\end{array}\n$$\\text{Then, }0\\mathbf{v} = \\mathbf{0}.$$\nProof. We will denote by $1$ an identity element of scalar multiplication;\nwe will denote by $(-\\mathbf{v})$ an additive inverse of $\\mathbf{v}$.\n\\begin{align*}\n0\\mathbf{v}\n&= 0\\mathbf{v} + \\mathbf{0}                     && \\text{by }\\textit{Identity element of vector addition} \\\\\n&= 0\\mathbf{v} + (\\mathbf{v} + (-\\mathbf{v}))   && \\text{by }\\textit{Inverse elements of vector addition} \\\\\n&= (0\\mathbf{v} + \\mathbf{v}) + (-\\mathbf{v})   && \\text{by }\\textit{Associativity of vector addition} \\\\\n&= (0\\mathbf{v} + 1\\mathbf{v}) + (-\\mathbf{v})  && \\text{by }\\textit{Identity element of scalar multiplication} \\\\\n&= ((0 + 1)\\mathbf{v}) + (-\\mathbf{v})          && \\text{by }\\textit{Distributivity of scalar multiplication (field addition)} \\\\\n&= ((1 + 0)\\mathbf{v}) + (-\\mathbf{v})          && \\text{by }\\textit{Commutativity of field addition} \\\\\n&= (1\\mathbf{v}) + (-\\mathbf{v})                && \\text{by }\\textit{Identity element of field addition} \\\\\n&= \\mathbf{v} + (-\\mathbf{v})                   && \\text{by }\\textit{Identity element of scalar multiplication} \\\\\n&= \\mathbf{0}                                   && \\text{by }\\textit{Inverse elements of vector addition} \\\\\n\\end{align*}\nQED\n",
    "proof": "To shorten the proof, we may write as suggested by André Nicolas, \n\nProof. Let $(V,+,\\cdot)_F$ be a vector space over the field $F$. We wish to show that $\\forall v\\in V$ one has $0\\cdot v=\\mathbf{0}$, where $0$ is the zero scalar and $\\mathbf{0}$ is the zero vector. Let $v$ be an element of the vector space $V$;\nBy one of the axioms of field addition, $$0\\cdot v=(0+0)\\cdot v.$$\n  Since scalar multiplication is distributive over addition, $$(0+0)\\cdot v=0\\cdot v+0\\cdot v.$$\n  From the previous two equalities we conclude that $$0\\cdot v=0\\cdot v+0\\cdot v.$$ \n  Adding to both sides the inverse element for addition of $0\\cdot v$, which we'll denote by $-0\\cdot v$: $$0\\cdot v+(-0\\cdot v)=0\\cdot v+0\\cdot v+(-0\\cdot v).$$\n  By the inverse axiom, $$\\mathbf{0}=0\\cdot v+\\mathbf{0},$$\n  hence by the identity axiom, $$0\\cdot v=\\mathbf{0}.\\tag*{$\\square$}$$\n\n",
    "tags": [
      "linear-algebra",
      "vector-spaces",
      "proof-writing",
      "proof-verification",
      "learning"
    ],
    "score": 13,
    "answer_score": 17,
    "is_accepted": false,
    "question_id": 893350,
    "answer_id": 1694413
  },
  {
    "theorem": "Given $f(x)$ is continuous on $[0,1]$ and $f(f(x))=1$ for $x\\in[0,1]$. Prove that $\\int_0^1 f(x)\\,dx &gt; \\frac34$.",
    "context": "\nLet $f$ be a continuous function whose domain includes $[0,1]$, such that $0 \\le f(x) \\le 1$ for all $x \\in [0,1]$, and such that $f(f(x)) = 1$ for all $x \\in [0,1]$. Prove that $\\int_0^1 f(x)\\,dx > \\frac34$.\n\nHere's all that I have, from the Mean Value Theorem, we have some $c\\in[0,1]$, and $a$, such that $$a=f(c)=\\int_0^1 f(x)dx.$$\nBy the Extreme Value Theorem, there exist some $m$, $n\\in[0,1]$ such that $$f(m)\\ge f(x)\\ge f(n).$$ I'm stuck here. Is this the right approach? Where do I go from here?\nI also got to know what the very fact that $f(f(x))=1$ shows that there is some $x$ such that $f(x)=1$ because the range of $f(x)$ is the domain of $f(x)$ (which I'm still trying to understand; I know what it means, I'm just trying to take it in).\n",
    "proof": "$f(1)=f(f(f(1)))=(f\\circ f) (f(1))=1$\n$f([0,1])=[a,1]$ for some $a >0$ since the image is connected hence an interval ending at $1$ and compact hence the interval is closed while obviously $f([a,1])=1$ so $a >0$\nBut now on $[0,a], f(x) \\ge a$ so $\\int_0^1f(x)dx=\\int_0^af(x)dx+\\int_a^1f(x)dx \\ge a^2+1-a \\ge 3/4$ and we cannot have equality since then $a=1/2$ and because $f(1/2)=1, f(x) \\to 1, x \\to 1/2, x<1/2$ so $f$ cannot be identically $1/2$ on $[0,1/2)$ and it is bigger on at least a small interval near $1/2$\nNote that by choosing that interval very small and making $f$ linear there (and $1/2$ before, $1$ after) we can get the integral $3/4+\\epsilon$ so the result is sharp.\nDone!\n",
    "tags": [
      "calculus",
      "integration",
      "continuity",
      "proof-writing",
      "extreme-value-theorem"
    ],
    "score": 13,
    "answer_score": 16,
    "is_accepted": true,
    "question_id": 3750528,
    "answer_id": 3750542
  },
  {
    "theorem": "How to structure long proofs",
    "context": "How do you structure proofs that are longer than say half a page?\nI have already encountered a variety of styles (in my short math life), some of which I list below and I just hoped to hear some wise words or your ideas on the topic, for example in which situation to use which (lecture, tutorial, research paper, thesis, book, etc.)\n\nWrite the proof down in running text, proof auxiliary claims that are need within the proof where they are needed. \nWrite the proof down in running text; where auxiliary claims are needed refer to them and \n2.1 make lemmata before the theorem that contain the auxiliary claims\n2.2 make lemmata after the theorem that contain the auxiliary claims\nGive a list of steps, the last one being the conclusion using the prior steps.\n3.1 Proof step 1 before writing down step 2 etc.\n3.2 First write all steps down, then proof them in the same order\n3.3 First write all steps down, then write down the conclusion including running text proof, then prove the remaining steps\n\n",
    "proof": "I agree with the comment above saying that it depends on your audience. If you are writing a proof of a theorem that you will submit to a journal, then you will probably leave out a lot of details. If you are teaching a class and writing notes, then you will probably give a lot of details. \nIf you audience is not familiar with the background, then you might want to provide this.\nIt also depends on how long exactly your proof is. You might want to write a whole book for the proof and in that case, you can split things into chapters as well.\nI (personally) think that the important thing is that your proof is clear. \n\nIntroduction: A long proof/argument could start by stating what you are going to do. If you reduce the proof to some technicality, then state that you will do this. Also in the introduction you can give the background background for the result. This background can contain an explanation of why the result is interesting.\nMeat: Try to split the proof into lemmas/claims.\nExamples: Don't under estimate examples. Reading long proofs containing a lot of technical details can be hard because one doesn't have a good example to think about. So before or after a lemma, you might provide an example illustrating the lemma/claim. \nConclusion: I would end by summarizing what you have proved. You might write something like: \"We have now proved out result. From lemma X we got that ... Combining this with lemma Y with reach the conclusion that ...\n\n",
    "tags": [
      "soft-question",
      "proof-writing"
    ],
    "score": 13,
    "answer_score": 9,
    "is_accepted": false,
    "question_id": 854629,
    "answer_id": 854642
  },
  {
    "theorem": "In proofs, is &#39;write&#39; equivalent to &#39;let&#39; or &#39;suppose&#39;?",
    "context": "In my textbook, the proof of the statement \"If A = [a b c d], then...\" starts with 'Write A = [a b c d]...'.\nIs this similar to 'Let A = [a b c d]...' or 'Suppose A = [a b c d]...'?\n",
    "proof": "Yes, \"write\" is about the same as \"let\" or \"suppose\". Using \"write\" instead of just plain \"let\" sometimes occurs when the author is establishing notation as well as naming something to be used in the text that follows.\n",
    "tags": [
      "proof-writing",
      "article-writing"
    ],
    "score": 13,
    "answer_score": 8,
    "is_accepted": true,
    "question_id": 4408184,
    "answer_id": 4408185
  },
  {
    "theorem": "Prove that $\\log X &lt; X$ for all $X &gt; 0$",
    "context": "I'm working through Data Structures and Algorithm Analysis in C++, 2nd Ed, and problem 1.7 asks us to prove that $\\log X < X$ for all $X > 0$.\nHowever, unless I'm missing something, this can't actually be proven. The spirit of the problem only holds true if you define several extra qualifiers, because it's relatively easy to provide counter examples.\nFirst, it says that $\\log_{a} X < X$ for all $X > 0$, in essence.  \nBut if $a = -1$, then $(-1)^{2} = 1$. Therefore $\\log_{-1} 1 = 2$. Thus, we must assume \n$a$ is positive.\nif $a$ is $< 1$, then $a^2 < 1$.  Therefore we must assume that $a \\geq 1$.\nNow, the book says that unless stated otherwise, it's generally speaking about base 2 for logarithms, which are vital in computer science.\nHowever, even then - if $a$ is two and $X$ is $\\frac{1}{16}$, then $\\log_{a} X$ is $-4$. (Similarly for base 10, try taking the log of $\\frac{1}{10}$ on your calculator: It's $-1$.) Thus we must assume that $X \\geq 1$.\n...Unless I'm horribly missing something here. The problem seems quite different if we have to prove it for $X \\geq 1$.\nBut even then, I need some help solving the problem. I've tried manipulating the equation as many ways as I could think of but I'm not cracking it.\n",
    "proof": "One way to approach this question is to consider the minimum of $x - \\log_a x$ on the interval $(0,\\infty)$. For this we can compute the derivative, which is\n$1 - 1/(\\log_e a )\\cdot x$.  Thus the derivative is zero at a single point, namely $x = 1/\\log_e a,$ and is negative to the left of that point and positive to the right. Thus $x - \\log_a x$ decreases as $x$ approaches $1/\\log_e a$ from the left, and then increases as we move away from this point to the right.  Thus the minimum\nvalue is achieved at $x = 1/\\log_e a$.  (Here I'm assuming that $a > 1$, so that $\\log_e a > 0$; the analysis of the problem is a little different if $a < 1$, since then for $x < a < 1$, we have $log_a x > 1 > x,$ and the statement is\nnot true.)\nNow this value is equal to $1/\\log_e a + (\\log_e \\log_e a)/\\log_e a,$  and you want this to be $> 0 $.  This will be true provided $a > e^{1/e}$ (as noted in the comments).  \n",
    "tags": [
      "logarithms",
      "proof-writing"
    ],
    "score": 13,
    "answer_score": 12,
    "is_accepted": false,
    "question_id": 380963,
    "answer_id": 394202
  },
  {
    "theorem": "Why don&#39;t mathematicians introduce intuition behind concepts as physicists do?",
    "context": "First of all please don't be angry - if anyone might be - and thoughtlessly downvote this post. I'll make it clear that I'm not here to criticise mathematicians - but rather to understand.\nI understand the importance and significance of accurate statements of definitions and theorems and their respective proofs.\nI read some mathematical methods in physics books and felt that they lack systematic approaches to concepts. They are more like well-written novels with a good plot.\nSo I always refer to some (relatively) more rigorous books that discuss each area of mathematical methods in more detail. These are usually books with titles subject name + \"with applications\" or something like that.\nBut at the same time, I felt that all of these proper maths books only contain, from the beginning till the end, definition - theorem - example - proof - remark kind of approach. Honestly, I find them very boring yet solidly written.\nI saw maths department professors take the identical approach in their classes. Here comes what I think is a more serious problem - such ways of teaching have no intuition.\nI'll compare two different ways of explaining a single concept of differentiation.\nThe first way I would do is to introduce the $\\varepsilon-\\delta$ definition of a derivative and provide proofs of various theorems about it - like linearity, product rule, quotient rule, chain rule and so on.\nThe second way I would do is to initially very informally introduce the notion of tangent vector, differential forms $dx, dy$, linear approximation and Taylor expansion. And then talk about how it is historically originated from Newton's study of mechanics, and how it is contemporarily applied to various optimisation problems. And then start dealing with the basic proofs of facts, perhaps by exploiting $\\varepsilon-\\delta$.\nI don't see how the first way of teaching has any pedagogical advantage which is how mathematicians are doing mathematics. It definitely trains you to have a rigorous understanding. However, it does not only de-motivate students but also stops them from taking the intuitions out of it.\nSo, my questions come.\n\nHow come mathematicians have intuitions though? I felt that I would never be able to have an intuitive understanding if this was the only way in which I was taught mathematics.\n\nWhy do mathematicians hate to include intuitions? To phrase it a bit more offensively, what will then be the point of countless repetitions of theorems and proofs if there's no motivation? Why not nicely mix intuition and rigorousness?\n\n\n",
    "proof": "The question is a reasonable one. I think part of the issue is that there are secretly a few types of math texts and typically it's not clear which book is of which type.\n\nSome are written for people who \"already know the material\" but need to either fill in minor gaps or have a reference for some proofs. These will typically be written concisely with not so much motivation, because the target audience would not need these things as much.\nSome of them are written for the newcomer. These texts will typically have more filler text and background information. Often, they have sections discussing motivation and historical context. More often than not, these texts are directed at undergraduates.\n\n\nOpinion: My personal opinion is that motivations are best learned from asking friends working in the respective fields, though I do think it is important to have these motivations written down in some places. However, it does seem that sometimes a little too much emphasis on carrying out the details of proofs is present in some math courses. I feel that the most precious information that a lecturer can impart is the context and motivation of a field - after all, the proofs are usually in many different books.\nIt can be quite important to understand historical context in learning math. For instance, it's easier to understand the definition of a \"scheme\" if you understand at least a little bit the classical theories that preceded schemes and what problems the definition was meant to address.\n",
    "tags": [
      "proof-writing",
      "physics",
      "mathematical-physics",
      "education",
      "philosophy"
    ],
    "score": 13,
    "answer_score": 9,
    "is_accepted": true,
    "question_id": 4125230,
    "answer_id": 4125676
  },
  {
    "theorem": "IMO 2011: Prove that, for all integers $m$ and $n$ with $f(m)&lt;f(n)$, the number $f(n)$ is divisible by $f(m)$",
    "context": "\nProblem: Let $f$ be a function from the set of integers to the set of positive integers. Suppose that, for any two integers $m$ and $n$, the difference $f(m) - f(n)$ is divisible by $f(m-n)$. Prove that, for all integers $m$ and $n$ with $f(m)<f(n)$, the number $f(n)$ is divisible by $f(m).$ (Resource: IMO $2011$)\n\nMy method:\n$$\\frac {f(m)-f(n)}{f(m-n)}\\in\\mathbb{Z}$$\nIf $f(m)=f(n)$ , $\\frac{f(n)}{f(m)}=1\\in \\mathbb {Z^{+}}$\nI can accept $f(n)>f(m)$.\nIt is obvious, $f(n)-f(m)≥f(m-n)$\n\n$$ \\begin{cases} m \\mapsto m & \\\\ n \\mapsto m-n& \\end{cases} \\Rightarrow \\begin{cases} f(m) \\mapsto f(m) & \\\\ f(n) \\mapsto f(m-n) & \\end{cases} $$\nNow,  I will prove that $f(m-n)=f(m)$ must be. \nIt is obvious $$\\frac {f(m)-f(n)}{f(m-n)}\\in\\mathbb{Z} \\Rightarrow \\frac {f(m)-f(m-n)}{f(n)}\\in\\mathbb{Z}$$\nIf $f(m)≠f(m-n)$, we can write $\\mid  f(m)-f(m-n) \\mid ≥f(n)$. Considering $f(m)>0 , f(m-n)>0$ and $f(n)>f(m)$ we get  $f(m-n)>f(m)$ must be.\nCase $1.$ \n$$f(m-n)-f(m)≥f(n) $$\nCase $2.$\n$$f(m)=f(m-n)$$\nLet $n=0$, for Case $1$, we can write $f(n)≤f(m-n)-f(m) \\Rightarrow f(0)≤0$  But, this is a contradiction. Because, $E(f)>0$. So, we get,  if $f(n)>f(m)$ then $f(m)=f(m-n)$ must be.\nFinally,\n$$\\frac {f(m)-f(n)}{f(m-n)}\\in\\mathbb{Z} \\Rightarrow \\frac {f(m)-f(n)}{f(m)}\\in\\mathbb{Z} \\Rightarrow \\frac {f(n)}{f(m)} \\in \\mathbb{Z^{+}} $$ Q.E.D.\n\nCan You verify my solution? Because, I'm not so sure. I don't have a teacher to approve the solution.\n",
    "proof": "Your proof looks correct to me.\n\nNow,  I will prove that $f(m-n)≥f(m)$ must be.\n\nI think that you have a typo here. It should be $f(m-n)=f(m)$.\n\nIt is obvious $$\\frac {f(m)-f(n)}{f(m-n)}\\in\\mathbb{Z} \\Rightarrow \\frac {f(m)-f(m-n)}{f(n)}\\in\\mathbb{Z}$$\n\nYes, $f(m)-f(m-n)$ is divisible by $f(m-(m-n))=f(n)$.\n\nIf $f(m)≠f(m-n)$, we can write $\\mid  f(m)-f(m-n) \\mid ≥f(n)$. Considering $f(m)>0$ and $f(m-n)>0$, we get  $f(m-n)>f(m)$ must be. Because, $f(n)>f(m).$\nCase $1.$\n$$f(m-n)-f(m)≥f(n) $$\nCase $2.$\n$$f(m)=f(m-n)$$\nLet $n=0$, for Case $1$, we can write $f(n)≤f(m-n)-f(m) \\Rightarrow f(0)≤0$  But, this is a contradiction. Because, $E(f)>0$. So, case $1$ is impossible.\n\nI think there is no need to separate it into cases as follows :\n\"Suppose that $f(m)\\not =f(m-n)$. Then, we can write $\\mid  f(m)-f(m-n) \\mid \\ge f(n)$. Considering $f(m)>0$ and $f(m-n)>0$, we get  $f(m-n)>f(m)$ because $f(n)>f(m).$ It follows that $f(m-n)-f(m)\\ge f(n)$. Let $n=0$. Then, we can write $f(n)≤f(m-n)-f(m) \\implies f(0)≤0$  which contradicts $f(0)\\gt 0$. So, we have $f(m)=f(m-n)$.\"\n\nAnother way to prove $f(m-n)=f(m)$.\nWe have $$-f(n)\\lt f(m)-f(n)\\le -f(m-n)\\lt 0$$ from which\n$$0\\lt f(m-n)\\lt f(n)$$\nfollows.\nFrom  $$f(m)-f(m-n)\\lt f(m)+f(m-n)\\le f(n)$$\nand\n$$f(m-n)\\lt f(n)\\lt f(n)+f(m)\\implies -f(n)\\lt f(m)-f(m-n)$$\nwe get\n$$-f(n)\\lt f(m)-f(m-n)\\lt f(n)$$\nSince $f(m)-f(m-n)$ is divisible by $f(m-(m-n))=f(n)$, we get $f(m)-f(m-n)=0$.\n",
    "tags": [
      "algebra-precalculus",
      "number-theory",
      "proof-writing",
      "contest-math",
      "solution-verification"
    ],
    "score": 13,
    "answer_score": 4,
    "is_accepted": true,
    "question_id": 2954032,
    "answer_id": 2957590
  },
  {
    "theorem": "Why are proofs not written as collections of logic symbols but are instead written in sentences?",
    "context": "Mathematical proofs are written as sentences and not as collections of logic symbols.\nThrough logical operations, it is much easier for me to visualize what the symbols are trying to tell us rather than English text filled with grammar. This is my personal opinion, others may have different opinions.\nI just asked this question on another website to find out logical mistakes in my work which is entirely done in the language of propositional logic.\nSome people suggested to write it down in sentences in English. Is there any kind of tragedy in writing proofs as collections of logic symbols?\n",
    "proof": "You have not translated the pages from Apostol's book into mathematical logic.\nWhat you have done is to transcribe them into your own idiosyncratic shorthand, which may be useful to you but is less than meaningless to anyone else.\nLet's start with the use of the symbol $\\stackrel{\\mathrm{def}}=.$\nIn normal mathematics, this tells us that the notation on the left is defined to represent the expression on the right in a general way.\nFor example, when we write\n$$ \\cosh x \\stackrel{\\mathrm{def}}= \\frac{e^x + e^{-x}}{2}, \\tag1$$\nit is a definition of the $\\cosh$ function.\nIn a definition of this sort, a symbol such as $x$ is a variable that can be substituted, so Definition $(1)$, above, tells us not only how to interpret\n$\\cosh x$; it also says how to interpret\n$\\cosh y,$ $\\cosh t,$ $\\cosh a,$ or $\\cosh b.$\nFor example, Definition $(1)$ informs us that\n$$ \\cosh b = \\frac{e^b + e^{-b}}{2}.$$\nIn your notes, you start with the definition\n$$ [a, b] \\stackrel{\\mathrm{def}}= \\text{closed interval in $x$-axis}. $$\nNow, setting aside the fact that there are four English words on the right-hand side of that definition (what were you saying about using symbols rather than English text?), you have just defined a bracket notation for us,\n\"[\" followed by a variable followed by \",\" followed by another variable\nfollowed by \"]\" and you have informed us that this is a closed interval on the $x$-axis. Now it seems strange that your variable names do not occur on the right-hand side of this definition, and in fact this does make the definition relatively useless in strict logic: which closed interval is denoted by $[a,b]$?\nBut worse still, on the next line we find out that changing the variable names changes the definition to a closed interval on the $y$-axis, not the $x$-axis.\nIf you actually succeeded in translating the pages to pure logic, along the way you would realize that the labels \"$x$-axis\" and \"$y$-axis\" are hints to help you visualize things, not part of the strict logic of the mathematics itself.\nYou really need only define the closed-interval notation once.\nI would say that some of your uses of $\\stackrel{\\mathrm{def}}=$ are actually logical definitions of symbols and notation. But many are not.\nIf you have a good definition of the product of two sets, it is not necessary to write out your interpretation of $P_x \\times P_y$ as a \"definition.\"\nIt would already be defined and (logically) unnecessary to write.\nBy the, way, symbols such as \"$\\ldots$\" do not belong to the notation of mathematical logic; they are (again) merely hints to understanding.\nYou also seem to tend to use \"$=$\" to signify \"is a\" rather than the standard symmetric, transitive, and reflexive notion of equality.\nFor example:\n$$ Ƃ:Q \\to \\mathbb R = \\mathrm{SF} $$\nwould mean the same thing as\n$$ \\mathrm{SF} = Ƃ:Q \\to \\mathbb R $$\nif you were writing in the language of mathematical logic;\nand the meaning of the line in which it appears would still be ambiguous.\n(Is SF a mathematical constant like $\\pi$?)\nIf you actually were writing in mathematical logic you might have defined SF as a predicate, written in the form\n$$ \\mathrm{SF}(Ƃ:Q \\to \\mathbb R). $$\nLater on that same line, however, you write $Ƃ:Q_{ij} \\to \\mathbb R,$\ncontradicting what you wrote earlier.\nThe domain of $Ƃ$ could be either $Q$ or $Q_{ij},$ but it cannot be both in the same definition. It seems you want to say that the restriction of $Ƃ$ to $Q_{ij}$ is a constant function, but you have neither the logical notation to describe a restriction of a function to a subdomain nor to say that a function is constant.\nYou end up defining $©_{ij}$ as a synonym for $Ƃ:Q_{ij} \\to \\mathbb R$ but not saying anything about new what the function does.\nFrankly, without using Apostol's text as a Rosetta Stone for your work,  I think it would be very difficult for anyone else to guess what you mean by all your notations.\nI see nothing wrong with making your own notes on a passage of text and equations in which you break everything out in a tabular format with displayed equations and no paragraphs of text.\nJust don't expect anyone else to read it. It is for your own use in organizing your thoughts, and that is all.\nIf you really want to write things like this in mathematical logic, there are various computer-aided proof systems in which you can write your definitions and theorems in completely symbolic language and feed them into the software, which will check them for you.\nBut I don't know if you would actually find this easier to work with than the text in a book like Apostol's.\n",
    "tags": [
      "real-analysis",
      "multivariable-calculus",
      "proof-writing",
      "propositional-calculus"
    ],
    "score": 12,
    "answer_score": 64,
    "is_accepted": true,
    "question_id": 3808881,
    "answer_id": 3808949
  },
  {
    "theorem": "$A \\in B$ vs. $A \\subset B$ for proofs",
    "context": "I have to prove a few different statements. \nThe first is if $A \\subset B$ and $B \\subset C$ then prove $A \\subset C$. This one is fairly straight forward, but I'm stuck on how the next one differs. \nProve that if $A \\in B$ and $B \\in C$ then $A \\in C$. \nI don't really understand how to put this in logical symbols. I've only seen $a \\in A$ written out but never \"a set $A$ is an element of a set $B$\".\nHere's what I have for a proof at this point, assuming I understand what \"a set $A$ is an element of a set $B$\" means: suppose $A \\in B$ and $B \\in C$. \nThen $A \\in C$.\n",
    "proof": "The implication $(A\\in B) \\wedge (B\\in C) \\implies A\\in C$ is false. Just take $B=\\{A\\}$ and $C=\\{B\\} = \\{\\{A\\}\\}$ to have a counterexample.\n",
    "tags": [
      "elementary-set-theory",
      "proof-writing"
    ],
    "score": 12,
    "answer_score": 28,
    "is_accepted": true,
    "question_id": 1430594,
    "answer_id": 1430598
  },
  {
    "theorem": "Prove that $x = 2$ is the unique solution to $3^x + 4^x = 5^x$ where $x \\in \\mathbb{R}$",
    "context": "Yesterday, my uncle asked me this question:\n\nProve that $x = 2$ is the unique solution to $3^x + 4^x = 5^x$ where $x \\in \\mathbb{R}$.\n\nHow can we do this? Note that this is not a diophantine equation since $x \\in \\mathbb{R}$ if you are thinking about Fermat's Last Theorem.\n",
    "proof": "For all $x_j>x_i$ and $0<a<1$, $a^{x_i}>a^{x_j}$ . \nHence \n\\begin{align}\n\\left(\\frac{3}{5}\\right)^{x} + \\left(\\frac{4}{5}\\right)^{x} - 1 < \\left(\\frac{3}{5}\\right)^{2} + \\left(\\frac{4}{5}\\right)^{2} - 1 = 0\n\\end{align}\nfor all $x>2$. Hence, there is no solution for $x>2$.\nSimilarly\n\\begin{align}\n\\left(\\frac{3}{5}\\right)^{x} + \\left(\\frac{4}{5}\\right)^{x} - 1 > \\left(\\frac{3}{5}\\right)^{2} + \\left(\\frac{4}{5}\\right)^{2} - 1 =0\n\\end{align}\nfor all $x<2$. \n",
    "tags": [
      "algebra-precalculus",
      "proof-writing"
    ],
    "score": 12,
    "answer_score": 16,
    "is_accepted": true,
    "question_id": 272114,
    "answer_id": 272123
  },
  {
    "theorem": "Proving the so-called &quot;Well Ordering Principle&quot;",
    "context": "Is there anything wrong with the following proof?\nTheorem. Every non-empty subset $B \\subset\\mathbb{N}$ has a least member.\nProof. Assume not. Then, of necessity, we'd have to have $B=\\varnothing$, for if $B$ contained even one $b\\in\\mathbb{N}$, then that $b$ would satisfy $b=min(B). $ Contradiction, end proof.\nI get the sense there is something wrong here, but I can't seem to define exactly what. Also, is there a context where one can simply take this principle as an axiom, and not have to prove it? After all, it is extremely intuitive. Thanks.\n",
    "proof": "You should be more detailed in what you claim. Your proof is wrong, and it is very hard to understand why you obtain said conclusions. My point is:\n\"Then, of necessity...\" Why?\n\"...if $B$ contained even one $b∈\\Bbb N$, then that $b$ would satisfy $b=\\min(B)$? Why?\n\nA correct proof would go as follows:\n\nP Let $B\\subseteq \\Bbb N$ be nonempty. We prove by induction that $B$ has a least element. Assume by contradiction that $B$ has no least element. Let $J$ be the set of elements that are not in $B$. Since $0$ is a lower bound of $\\Bbb N$, $0\\notin B$ (else it would be a least element) so $0\\in J$. We proceed to prove the induction step $0,1,\\dots,n\\in J\\implies n+1\\in J$. Indeed, suppose that $0,1,\\dots,n\\in J$. Then $n+1$ cannot be in $B$ since then it would be a lower bound of $B$, and since  $0,1,\\dots,n\\notin B$, it would be a least element. It follows $n+1\\in J$. By induction, $J=\\Bbb N$ so $B=\\varnothing$ which is impossible.\n\nAs Pete is saying, WOP is equivalent to PMI.\nPROP Suppose every nonempty subset of $\\Bbb N$ has a least element. Let $B$ be a subset of $\\Bbb N$ with the following properties\n$(1)$ $0\\in B$.\n$(2)$ $n\\in B\\implies n+1\\in B$\nWe show that $B=\\Bbb N$.\n\nP Let $B$ be as above. Consider the set of $\\Bbb N\\setminus B$, and assume by contradiction it is not empty. By the WOP, it has a least element, call it $a$. Since $0\\in B$, this element must be of the form $a=n+1$ for some $n\\in \\Bbb N$. Since $n+1$ is the first element that is not in $B$, $n$ is an element of $B$. But then $n+1\\in B$, which is absurd. It follows that $\\Bbb N\\setminus B$ must be empty, so $B=\\Bbb N$, as claimed.\n\n",
    "tags": [
      "elementary-number-theory",
      "proof-writing"
    ],
    "score": 12,
    "answer_score": 16,
    "is_accepted": true,
    "question_id": 358979,
    "answer_id": 358988
  },
  {
    "theorem": "The role of &#39;arbitrary&#39; in proofs",
    "context": "Generally, when one is going to prove a result regarding a set of elements, they begin their proof with those first few pleasing words: \"Suppose...is an arbitrary element in...\"\nMy question is, why does considering an arbitrary element in a proof imply that the proven result applies to every element in the set? Although I think I have an vague about this, I am still interested in seeing what others have to say in relation to this idea. Perhaps, if possible, answer the question in the manner that you would if a student of yours posed this question to you.\nThank you!\n",
    "proof": "You could instead say \"let $x$ be any element of our set\" that is the same as saying \"let $x$ be an arbitrary element of our set\". This just means we are not assuming anything about $x$ other than it is in our set. So, anything we prove about $x$ holds for any element in our set.\n",
    "tags": [
      "soft-question",
      "proof-writing"
    ],
    "score": 12,
    "answer_score": 21,
    "is_accepted": true,
    "question_id": 833009,
    "answer_id": 833013
  },
  {
    "theorem": "Prove that $1^2+2^2+\\cdots+n^2=\\frac{n(n+1)(2n+1)}{6}$ for $n \\in \\mathbb{N}$.",
    "context": "Problem: Prove that $1^2+2^2+\\cdots+n^2=\\frac{n(n+1)(2n+1)}{6}$ for $n \\in \\mathbb{N}$.\nMy work: So I think I have to do a proof by induction and I just wanted some help editing my proof.\nMy attempt:\nLet $P(n)=1^2+2^2+\\cdots+n^2=\\frac{n(n+1)(2n+1)}{6}$ for $n \\in \\mathbb{N}$. \nThen $$P(1)=1^2=\\frac{1(1+1)(2+1)}{6}$$\n$$1=\\frac{6}{6}.$$\nSo $P(1)$ is true.\nNext suppose that $P(k)=1^2+2^2+\\cdots+k^2=\\frac{k(k+1)(2k+1)}{6}$ for $k \\in \\mathbb{N}$. Then adding $(k+1)^2$ to both sides of $P(k)$ we obtain the following:\n$$1^2+2^2+\\cdots+k^2+(k+1)^2=\\frac{k(k+1)(2k+1)}{6}+(k+1)^2$$\n$$=\\frac{2k^3+3k^2+k+6(k^2+2k+1)}{6}$$\n$$=\\frac{2k^3+9k^2+13k+6}{6}$$\n$$=\\frac{(k^2+3k+2)(2k+3)}{6}$$\n$$=\\frac{(k+1)(k+2)(2k+3)}{6}$$\n$$=\\frac{(k+1)((k+1)+1)(2(k+1)+1)}{6}$$\n$$=P(k+1).$$\nThus $P(k)$ is true for $k \\in \\mathbb{N}$.\nHence by mathematical induction, $1^2+2^2+\\cdots+n^2=\\frac{n(n+1)(2n+1)}{6}$ is true for $n \\in \\mathbb{N}$.\n",
    "proof": "I am going to provide what I think is a nice way of writing up a proof, both in terms of accuracy and in terms of communication. You be the judge(s).\n\nClaim: For $n\\geq 1$, let $S(n)$ be the statement\n$$\nS(n) : 1^2+2^2+3^2+\\cdots+n^2=\\frac{n(n+1)(2n+1)}{6}.\n$$\nBase step $(n=1)$: The statement $S(1)$ says $1^2=1(2)(3)/6$ which is true.\nInductive step $(S(k)\\to S(k+1))$: Fix some $k\\geq 1$ and suppose that\n$$\nS(k) : 1^2+2^2+3^2+\\cdots+k^2=\\frac{k(k+1)(2k+1)}{6}\n$$\nholds. To be shown is that\n$$\nS(k+1) : 1^2+2^2+3^2+\\cdots+k^2+(k+1)^2=\\frac{(k+1)(k+2)(2(k+1)+1)}{6}\n$$\nfollows. Starting with the left-hand side of $S(k+1)$,\n\\begin{align}\n\\text{LHS} &= 1^2+2^2+3^2+\\cdots+k^2+(k+1)^2\\tag{definition}\\\\[1em]\n  &= \\frac{k(k+1)(2k+1)}{6}+(k+1)^2\\tag{by $S(k)$}\\\\[1em]\n  &= (k+1)\\left[\\frac{k(2k+1)}{6}+(k+1)\\right]\\\\[1em]\n  &= (k+1)\\frac{k(2k+1)+6(k+1)}{6}\\\\[1em]\n  &= (k+1)\\frac{2k^2+k+6k+6}{6}\\\\[1em]\n  &= (k+1)\\frac{2k^2+7k+6}{6}\\\\[1em]\n  &= (k+1)\\frac{(k+2)(2k+3)}{6}\\\\[1em]\n  &= \\frac{(k+1)(k+2)(2(k+1)+1)}{6}\\\\[1em]\n  &= \\text{RHS},\n\\end{align}\nthe right-hand side of $S(k+1)$ follows. This completes the inductive step. \nThus, by mathematical induction, for every $n\\geq 1, S(n)$ is true. $\\Box$\n",
    "tags": [
      "proof-verification",
      "proof-writing",
      "induction"
    ],
    "score": 12,
    "answer_score": 8,
    "is_accepted": true,
    "question_id": 1166027,
    "answer_id": 1166038
  },
  {
    "theorem": "are two consecutive numbers relatively prime?",
    "context": "I have a question.\nI have been given this proof: \"For any $n$ in the integers where $n>2$, show there are at least $2$ elements in $U(n)$ that satisfy $x^2=1$.\"\nI have gone through and actually proved this, (that the numbers are $1$ and $n-1$) but i didn't' know how to prove that $n-1$ is in fact in the set $U(n)$. Is it because two consecutive numbers are always relatively prime?\n",
    "proof": "$n$ is coprime to $n-1$, for if $d$ divides $n$ and $d$ divides $n-1$, then $d$ divides $n-(n-1)=1$.\n",
    "tags": [
      "prime-numbers",
      "proof-writing"
    ],
    "score": 12,
    "answer_score": 25,
    "is_accepted": false,
    "question_id": 296747,
    "answer_id": 296748
  },
  {
    "theorem": "If $a+b+c$ divides the product $abc$, then is $(a,b,c)$ a Pythagorean Triple?",
    "context": "Firstly, I will define what Pythagorean Triples are for those who do not know.\n\n\nDefinition:\n\nA Pythagorean Triple is a group of three integers $a$, $b$ and $c$ such that $a^2+b^2=c^2$, since the Pythagorean Theorem asserts that for any $90^\\circ$ (right-angle) triangle $ABC$ with sides $a$, $b$ and $c$, one will always have the equation, $a^2+b^2=c^2$.\n\n\n\nI was looking at Pythagorean Triples and noticed another property apart from how $a^2+b^2=c^2$. Here are the first $30$ Pythagorean Triples $(a,b,c)$ ordered from smallest to greatest value, i.e. $$(a,b,c)\\qquad\\text{ s.t. }\\qquad a<b<c.\\tag*{$\\big(\\text{s.t. = such that}\\big)$}$$\n\n\nI noticed that $a^2=(c+b)(c-b)$, but that is trivial since $$\\begin{align}a^2&=(c+b)(c-b)\\tag{given} \\\\ &=c^2-b^2 \\\\ \\Leftrightarrow\\,\\,\\,\\, a^2+b^2&=c^2.\\end{align}$$\n\nHowever, I also noticed that by having \"$u\\mid v$\" be read as \"$u$ divides $v$\", it appears that $$a+b+c\\mid abc.$$ For example, $(a,b,c)=(3,4,5)$ is a classic Pythagorean Triple; $3^2+4^2=5^2$.\nAlso, $$\\begin{align}3+4+5&=12 \\\\ \\& \\quad3\\times 4\\times 5 &= 60. \\\\ \\\\ 12 &\\,\\mid 60 \\\\ \\Leftrightarrow \\,\\,\\,\\,3+4+5&\\,\\mid 3\\times 4\\times 5.\\end{align}$$ This, I cannot prove to be true $-$ but I tested with all the $30$ Pythagorean Triples above, and I have come across no counter-example. Is there a proof? I do not know where to begin myself.\n\n\n\nConjecture:\n\nGiven three positive integers $a$, $b$ and $c$, if $a < b<c$ and $a^2+b^2=c^2$, then $$a+b+c\\mid abc.$$\n\n\nThank you in advance.\nEdit:\nMy conjecture was originally the other way round; i.e. if $a+b+c\\mid abc$ then $a^2+b^2=c^2$. But $6$ is a counter-example, namely because it is a  Perfect Number.\n",
    "proof": "You actually want it the other way around: if $a^2+b^2=c^2$ then $a+b+c|abc$. That you can prove very quickly from the general form of primitive Pythagorean triples $(a,b,c)=(m^2-n^2,2mn,m^2+n^2)$.\n",
    "tags": [
      "geometry",
      "proof-writing",
      "triangles",
      "conjectures",
      "pythagorean-triples"
    ],
    "score": 12,
    "answer_score": 12,
    "is_accepted": true,
    "question_id": 2825945,
    "answer_id": 2825959
  },
  {
    "theorem": "Prove that $2^n$ does not divide $n!$",
    "context": "I want to prove that $2^n$ does not divide $n!$.\nI was trying by induction and I'm confused about if what I'm doing is right.\nFirst I test it with $n=1$. In fact:\n$$2^1 \\nmid 1!$$\nSo if i take the I.H. as $2^n \\nmid n!$ and I try to prove it for $n+1$:\n$$2^{n+1} \\nmid (n+1)!$$\n$$2^{n} \\cdot 2 \\nmid (n+1) \\cdot n!$$\nAs  $2^n \\nmid n!$ it must be that $2^n$ divides $n+1$, so I need to prove that it doesn't. If I try by induction again I must have $n>1$ for it to be true:\n$$P(n) = 2^n \\nmid n+1$$\n$$P(n+1) = 2^n \\cdot 2 \\nmid n+1+1$$\nbut $2^n \\nmid n+1$ and $2^n \\nmid 1$\nbecause $n>1$\nSo my question is obviously if this is correct. I'm doubting because the exception I have to do with $n$ being greater than 1 for the second part. If I made a mistake could you point to me in a better direction? I'm sure there must be a simpler way to prove this.\nThanks!\n",
    "proof": "A proof by induction\nwill be difficult\nbecause, as $n$ increments,\n$2^n$ adds one factor of $2$\nwhile $n!$ can add many.\nThe way I would do it\nis use\nLegendre's theorem,\nstated here, for example:\nhttp://www.cut-the-knot.org/blue/LegendresTheorem.shtml\nIf $p$ is a prime,\nand\n$v_p(n!)$\nis the exponent of\nthe greatest power of $p$\ndividing $n!$,\nso\n$p^{v_p(n!)} \\mid n!$\nand\n$p^{v_p(n!)+1} \\not\\mid n!$,\nthen\n$v_p(n!)\n=\\sum_{k=1}^{\\lfloor \\log_p n \\rfloor} \\lfloor \\dfrac{n}{p^k} \\rfloor\n$.\nIt follows that\n$\\begin{array}\\\\\nv_p(n!)\n&=\\sum_{k=1}^{\\lfloor \\log_p n \\rfloor} \\lfloor \\dfrac{n}{p^k} \\rfloor\\\\\n&\\le\\sum_{k=1}^{\\lfloor \\log_p n \\rfloor}  \\dfrac{n}{p^k} \\\\\n&=n\\sum_{k=1}^{\\lfloor \\log_p n \\rfloor}  \\dfrac{1}{p^k} \\\\\n&<n\\sum_{k=1}^{\\infty}  \\dfrac{1}{p^k} \n\\qquad\\text{(here is where we get a strict inequality)}\\\\\n&=n\\dfrac{\\frac1{p}}{1-\\frac1{p}}\\\\\n&=\\dfrac{n}{p-1}\\\\\n\\end{array}\n$\nIn particular,\nfor $p=2$,\n$v_2(n!)\n< n\n$,\nso\n$2^n\n\\not\\mid n!\n$.\nNote that if\n$n=p^m$\nfor some integer $m$,\nthen\n$\\begin{array}\\\\\nv_p(n!)\n&=v_p((p^m)!)\\\\\n&=n\\sum_{k=1}^{\\lfloor \\log_p n \\rfloor}  \\dfrac{1}{p^k}\\\\\n&=n\\sum_{k=1}^{m}  \\dfrac{1}{p^k}\\\\\n&=p^m\\dfrac{\\frac1{p}-\\frac1{p^{m+1}}}{1-\\frac1{p}}\\\\\n&=p^m\\dfrac{1-\\frac1{p^{m}}}{p-1}\\\\\n&=\\dfrac{p^m-1}{p-1}\\\\\n\\end{array}\n$ \nIf $p=2$,\n$v_2((2^m)!)\n=2^m-1\n$,\nso this misses by just $1$,\nso that\n$2^{2^m-1} \\mid (2^m)!$\nand\n$2^{2^m} \\not\\mid (2^m)!$.\n",
    "tags": [
      "elementary-number-theory",
      "proof-verification",
      "proof-writing",
      "divisibility"
    ],
    "score": 12,
    "answer_score": 3,
    "is_accepted": true,
    "question_id": 1808670,
    "answer_id": 1808687
  },
  {
    "theorem": "how to prove $f^{-1}(B_1 \\cap B_2) = f^{-1}(B_1) \\cap f^{-1}(B_2)$",
    "context": "I am given this equation: \n$f^{-1}(B_1 \\cap B_2) = f^{-1}(B_1) \\cap f^{-1}(B_2)$\nI want to prove it: what i did is\nI take any $a \\in f^{-1}(B_1 \\cap B_2)$, then there is $b \\in (B_1 \\cap B_2)$ so that $f(a)=b$. Because of $b \\in (B_1 \\cap B_2)$, it is true that $b \\in B_1$ and $b \\in B_2$, so $a \\in f^{-1}(B_1)$ and $a \\in f^{-1}(B_2)$.  \nthis means $f^{-1}(B_1 \\cap B_2) \\subseteq f^{-1}(B_1) \\cap f^{-1}(B_2)$. \nis it ok? \n",
    "proof": "Yeah...this can be actually written in this way;\n$a\\in f^{-1}(B_1\\cap B_2)$, means $f(a)\\in B_1\\cap B_2$ and so $f(a)\\in B_1$ and $f(a)\\in B_2$. Hence, $a\\in f^{-1}(B_1)$ and $a\\in f^{-1}(B_2)$\n",
    "tags": [
      "elementary-set-theory",
      "proof-writing",
      "problem-solving"
    ],
    "score": 12,
    "answer_score": 10,
    "is_accepted": true,
    "question_id": 516374,
    "answer_id": 516376
  },
  {
    "theorem": "&quot;$n$ is even iff $n^2$ is even&quot; and other simple statements to teach proof-writing",
    "context": "I am supposed to teach undergraduate students who do not major in mathematics and I would like to give them a short introduction to mathematical reasoning and to the concept of proof. I am looking for very simple mathematical statements (true or false) to help them get familiar with logic and proof writing.\nI am not sure if what I ask is clear, so here are some ideas I came up with:\n\nLet $n$ be an integer. If $n$ is a multiple of $42$, then $n$ is even.\nLet $n$ be an integer. If $n$ is a multiple of $43$, then $n$ is odd.\nIf $x$ is a positive real number, then $x^2 \\geq x$.\nFor all $n \\in \\Bbb Z$, $n$ is even iff $n^2$ is even.\nThere is no $x \\in \\Bbb Q$ such that $x^2 = 2$.\nThere exist irrational numbers $\\alpha,\\beta > 0$ such that $\\alpha^\\beta$ is rational.\nFor all $x \\in \\Bbb R$, if [for all $\\epsilon > 0$, $|x| < \\epsilon$], then $x = 0$.\nThere exists $m \\in \\Bbb Z$ such that, for all $n \\in \\Bbb Z$, one has $n \\leq m$.\n$(-1)\\times (-1) = 1$\nFor all $x \\in \\Bbb R$, one has $0\\times x = 0$. (proposed by Robert Auffarth)\nLet $n$ be a positive integer. If $x$ is real and $x^n=0$ then $x = 0$. (ibidem)\nSuppose $b,d \\neq 0$ and $\\dfrac{a}{b} = \\dfrac{c}{d}$. Then $\\dfrac{a}{b} + \\dfrac{c}{d} = \\dfrac{a+c}{b+d}$.\n\nIt would be great to cover different type of statements.\n",
    "proof": "I left the following exercises for a maths education calculus class --- mainly to test their knowledge of the definitions actually. A few soft questions and maybe more material covered than what you are talking about but many are good proof-writing exercises.\nEDIT: I have added some more calculus problems after 36... this time without italics.\n$\\newcommand{\\R}{\\mathbb{R}}$ $\\newcommand{\\raw}{\\rightarrow}$ $\\newcommand{\\N}{\\mathbb{N}}$ $\\newcommand{\\Q}{\\mathbb{Q}}$ $\\newcommand{\\Raw}{\\Rightarrow}$\n\nSuppose that $f:\\R\\raw\\R$ and define a function $g:\\R\\raw\\R$ by $g(x)=1/f(x)$. Prove that $g$ has no roots.\nLet $n\\in\\N$ and $f:\\R\\raw\\R$. Prove that $k$ is a root of $[f(x)]^n$ if and only if $k$ is a root of $f$.\nLet $f:\\R\\raw\\R$. Prove that if $k$ is a root of $f$, then $0$ is a root of $f(x+k)$.\nProve that the product of two even functions is an even function.\nProve that the composition of two even functions is an even function.\nUse the unit circle to  prove that the cosine function is an even function.\nProve that the product of two odd functions is an even function.\nSuppose that $f:\\R\\raw\\R$ is an odd function defined on the entire real line. Prove that $f$ has a root.\nUsing the fact that sine is an odd function, prove that the tangent function is odd.\nProve that $g:\\R\\raw\\R$, defined by $g(x)=-x$ is decreasing.\nGive an example of a function $f:\\R\\raw\\R$ which is both increasing and decreasing for all $x\\in\\R$.\nGive an example of a function that is strictly increasing for all $x\\in\\R$ but has no roots.\nSuppose that $f:\\R\\raw\\R$ is strictly increasing on a non-empty closed interval $[a,b]\\subset\\R$. Show that if $a\\leq x_1<x_2\\leq b$, then the (secant) line joining $(x_1,f(x_1))$ to $(x_2,f(x_2))$ has positive slope.\nSuppose that $g:\\R\\raw\\R$ is strictly decreasing and has a root at $a\\in\\R$. Prove that $g$ has no other roots.\nSuppose that $f:\\R\\raw\\R$ is a positive increasing function and that $g:\\R\\raw\\R$ is a positive decreasing function. Prove that $q=f/g$, $q(x)=f(x)/g(x)$ is an increasing function.\nUse the formula for the roots of a quadratic function $p(x)=ax^2+bx+c$ to find an expression for the sum of the roots of $p$; and the product of the roots of $p$.\nSuppose that $q(x)=ax^2+bx+c$ is a quadratic function with real roots $\\alpha$ and $\\beta$. Use the fact that quadratic functions are symmetric about the line $x=-b/2a$ --- and that  their maxima/ minima are found there to find an expression for $\\alpha+\\beta$.\nLet $r(x)=ax^2+bx+c$ be a quadratic function. Use the factor theorem to find an expression for the sum of the roots of $r$; and the product of the roots of $r$.\nProve that all polynomials of odd degree have at least one root.\nProve the factor theorem for the polynomial $c(x)=ax^3+bx^2+cx+d$.\nGive an example of degree $4$ polynomials $p$ and $q$ such that $p+q$ is a polynomial of degree $3$.\nSuppose that $p$ and $q$ are polynomials and let $r$ be the rational function defined by $r(x)=p(x)/q(x)$. Prove that if $k$ is a root of $r$ then $k$ is a root of $p$. By finding a counterexample, show that the converse does not hold.\nSuppose that $p$ and $q$ are polynomials and let $r$ be the rational function $r(x)=p(x)/q(x)$. If $q(a)=0$, then $r(a)$ is not defined at $a$ and hence discontinuous at $a$. Find examples of polynomials $p$ and $q$ such that: (a) $r$ is continuous, and (b) $r$ is not continuous---but is bounded (there exists a positive number $M>0$ such that $|r(x)|<M$ for all $x\\in\\R$).\nProve that $|x^2+1|=x^2+1$ for all $x\\in\\R$.\nProve that the absolute value function is even.\nSuppose that $f:\\R\\raw \\R$ has the property that $f(x)<0$ for all $x\\in\\R$. Describe the relationship between the graph of $f(x)$  and the graph of $|f(x)|$.\nLet $k\\in\\R$ be a constant and $a\\in\\R$. Use the $\\varepsilon-\\delta$ definition of a limit to prove that\n$$\\lim_{x\\raw a}k=k\\text{, and }\\lim_{x\\raw a}x=a.$$\nSuppose that $f:\\R\\raw\\R$ and\n$$\\lim_{x\\raw 1}f(x)=0.$$\nDoes this imply that $1$ is a root of $f$?\nShow that there are two values of $a\\in\\R$ such that the left- and right-handed limits of $f:\\R\\raw\\R$ at $x=1$ agree where:\n$$f(x)=\\left\\{\\begin{array}{cc}(ax)^2 & \\text{ if }x<1\n    \\\\ ax+6 & \\text{ if }x\\geq 1\\end{array}\\right.$$\nProduce a rough sketch of $f$ in each case.\nAssuming we know what $2^x$ is, we can define a function $f:\\R\\raw\\R$ by:\n$$f(x)=\\frac{1}{1+2^{-1/x}}$$\nSketch an argument that suggests that the left- and right-hand limits of $f(x)$ at $0$ are, respectively, $0$ and $1$.\nConstruct a function $f:\\R\\raw\\R$ such that\n$$\\lim_{x\\raw0^-}f(x)=+\\infty\\text{ , and }\\lim_{x\\raw 0^+}f(x)=1.$$\nSuppose that $f:\\R\\raw\\R$ and that\n$$\\forall\\varepsilon>0,\\,\\exists\\,\\delta>0\\text{ such that if }0<|x|<\\delta\\Rightarrow|f(x)|<\\varepsilon.$$\nDoes this imply that\n$$\\lim_{x\\raw 0}f(x)=0.$$\nFor all real numbers $x,\\,y\\in\\R$ with $x\\neq y$, there exists a fraction between $x$ and $y$ --- i.e. a $q\\in(x,y)$.  Consider the function\n$$f(x)=\\left\\{\\begin{array}{cc}1&\\text{ if }x\\in\\Q\n\\\\0 & \\text{ if }x\\not\\in\\Q\\end{array}\\right.$$\nProve that $f$ is not continuous at any point.\nConsider the function\n$$g(x)=\\left\\{\\begin{array}{cc}x&\\text{ if }x\\in\\Q\n\\\\0 & \\text{ if }x\\not\\in\\Q\\end{array}\\right.$$\nProve that $g$ is continuous at $0$.\nSuppose that functions $f_1:\\R\\raw \\R$ and $f_2:\\R\\raw\\R$ have the property that, for $i=1,2$\n$$f_i(x)=f_i(y)\\Raw x=y.$$\nProve that $f=f_1\\circ f_2$ has this property also.\nFind a set $A\\subseteq\\R$, and functions $f:A\\raw\\R$ and $g:\\R\\raw \\R$ such that $(g\\circ f)(x)=x$ for all $x\\in A$ but that there exists a $y\\in \\R$ such that $(f\\circ g)(y)\\neq y$.\nSuppose that $f:[0,1]\\raw[0,1]$ is continuous and strictly increasing on $[0,1]$ such that $f(0)=0$ and $f(1)=1$. Suppose further that $g:[0,1]\\raw[0,1]$ is a  function such that\n$$(g\\circ f)(x)=x$$\nfor all $x\\in[0,1]$. How is the graph of $g$ related  to the graph of $f$.\nProve that a continuous function is  continuous on any non-empty closed interval $[a,b]\\subset \\R$.\nConstruct a function which is not continuous (everywhere) but is  {continuous on} $[0,1]$.\nThe singleton set $\\{x\\}\\subset\\R$ is equal to the closed interval $[x,x]$. Verify that all the conclusions of the  Intermediate Value Theorem hold when $f:\\{x\\}\\raw\\R$ is a  continuous function got by restricting a continuous function $f:\\R\\raw\\R$ to $\\{x\\}$. Find  {ALL} such continuous functions $\\{x\\}\\raw\\R$.\nProve that the  Mean Value Theorem holds on any non-empty closed interval for a smooth function $f:\\R\\raw\\R$.\nFind the  local maxima and minima of the constant function $f(x)=0$.\nShow that $f:\\R\\raw\\R$, $f(x)=x^4+12x^3+54x^2-12x+5$ is  concave up on $\\R$.\nProve that $\\tan x$ has a  vertical asymptote at $x=\\pi/2$.\nGive an example of a function which is  continuous on $(0,1]$ but not  on $[0,1]$.\nUse the  Intermediate Value Theorem to prove that $\\sqrt{2}\\in\\R$ exists.\nUse the  Intermediate Value Theorem to prove that if $f:[a,b]\\raw\\R$ and $g:[a,b]\\raw\\R$ are continuous functions such that $f(a)=g(b)$ and $f(b)=g(a)$, then there exists a point $c\\in[a,b]$ such that $f(c)=g(c)$.  \nIt can be shown that a continuous function obtains its  absolute maximum and minimum on finite unions of closed intervals. Consider the set $S$ defined by:\n\\begin{align*}\n    S&=[0,2\\pi]\\cup\\left[2\\pi,2\\pi+\\frac{1}{2\\pi}\\right]\\cup\\left[3\\pi,3\\pi+\\frac{1}{3\\pi}\\right]\\cup\\left[4\\pi,4+\\frac{1}{4\\pi}\\right]\\cup\\cdots\\cup\\left[100\\pi,100\\pi+\\frac{1}{100\\pi}\\right],\n    \\\\ &=[0,2\\pi]\\cup\\left(\\bigcup_{i=2}^{100}\\left[i\\pi,i\\pi+\\frac{1}{i\\pi}\\right]\\right).\n    \\end{align*}\nFind the absolute maxima and minima of $\\sin:S\\raw\\R$.\nIf $f:(a,b)\\raw\\R$ is a smooth function such that $f'=0$ for all $x\\in(a,b)$ then $f$ is constant on $(a,b)$. Find an example of a differentiable function $g:\\R\\raw\\R$ such that\n$$\\lim_{x\\raw\\infty}g'(x)=0\\text{ , but}\\lim_{x\\raw\\infty}g(x)$$\nis not a constant.\nSketch continuous functions $f_i:[0,1]\\raw\\R$ for $i=1,2,3,4,\\dots$ such that $f_i$ has $i$ stationary points; i.e. solutions to $f'(c)=0$ for $c\\in(0,1)$.\nIs it possible for a function $f:\\R\\raw\\R$ to have a  local maximum/ minimum at a point $a\\in\\R$ where $f$ is discontinuous?.\nIn the proof of the  Closed Interval Method, we make the assumption that $f:[a,b]\\raw\\R$ is continuous. Is this assumption necessary?.\nLet $\\varepsilon\\in (0,1)$ be a constant. Using algebraic techniques, find the\n             critical points of $f:[0,1]\\raw\\R$\n            $$f(x):=\\left\\{\\begin{array}{cc}0 & \\text{ if }-1\\leq x<-\\varepsilon\n                \\\\[1.5ex]\\ \\frac{1}{\\varepsilon}x+1 & \\text{ if }-\\varepsilon\\leq x<0\n                \\\\[1.5ex] -\\frac{1}{\\varepsilon}x+1 & \\text{ if }0\\leq x<\\varepsilon\n                \\\\[1.5ex] 0 &\\text{ if }\\varepsilon\\leq x\\leq 1\n                \\end{array}\\right.$$\nConstruct a function $f:\\R\\raw\\R$ that is  twice differentiable but not  three-times differentiable.\nShow that\n$$\\lim_{h\\raw0}\\frac{\\cos(\\pi+h)-\\cos(\\pi)}{h}=0.$$\nShow that if $f:[a,b]\\raw\\R$ is  twice differentiable, there exists a point $c\\in(a,b)$ such that\n$$f''(c)=\\frac{f'(b)-f'(a)}{b-a}$$\nSuppose that a smooth function $f:[-1,1]\\raw\\R$ is  concave down on $[-1,1]$ and further that $f'(0)=0$. Draw a sketch suggesting that $f$ attains its absolute maximum at $0$.\nUse the geometric definition of  concavity to show that $f(x)=|x|$ is  concave up on $\\R$.\nShow that $|x|$ is an  asymptotic of $\\sqrt{x^2+3}$.\nExplain why there is no polynomial  asymptotic of $\\sin x$.\nFind all  vertical asymptotes of $f(x)=\\frac{\\sin x}{x}$.\nUse the  Intermediate Value Theorem to prove that if $f:[0,1]\\rightarrow[0,1]$ is a continuous function, then $f$ has a  fixed point; i.e. a point $x\\in[0,1]$ such that $f(x)=x$.\nSuppose that $f:[a,b]\\raw \\R$ is a differentiable function such that $f'(x)\\leq K$ for all $x\\in[a,b]$. Prove that $f(a)+K(b-a)$ is an upper bound for the {absolute maximum of $f$ on $[a,b]$} [HINT: Assume that $f(x)>f(a)+K(b-a)$ for some $x\\in[a,b]$ and show that this contradicts the  Mean Value Theorem.].\nCall a non-empty set of points $\\{x_1,x_2,\\dots,x_n\\}\\subset\\R$ an  antipodal set for $f:\\R\\raw\\R$ if the tangents to $f$ at $x_1,x_2,\\dots,x_n$ are all parallel. Prove that an antipodal sets of a cubic function contain at most two elements.\nVerify  Rolles's Theorem for $\\sin:[0,\\pi]\\raw\\R$ geometrically by considering the unit circle with parametric equation $(x,y)=(\\cos\\theta,\\sin\\theta)$.\nLet $[a,b]$ be a non-empty closed interval and consider $\\sin:[a,b]\\raw\\R$. Explain why $\\sin$ satisfies the hypothesis of the  Mean Value Theorem. Apply the Mean Value Theorem to show that there exists an $x\\in(a,b)$ such that\n        $$|\\cos(x)|=\\left|\\frac{\\sin(b)-\\sin(a)}{b-a}\\right|.$$\nNow use this result to show that for any $x_1,\\,x_2\\in \\R$\n$$|\\sin (x_2)-\\sin(x_1)|\\leq |x_2-x_1|.$$\nWe could then use this result to prove that $\\sin$ is a continuous function. What would be wrong with the proof?\nIn this exercise we see why we might call the quantity\n $$\\frac{f(b)-f(a)}{b-a}$$\n the  average slope across $[a,b]$ when we talk about the  Mean Value Theorem. Let $f:[a,b]\\raw\\R$ be a differentiable function. Partition the interval $[a,b]$ into $n$ equally spaced points:\n$$a=:x_0<x_1<x_2<\\cdots x_n:=b.$$\nIf $[a,b]$ is divided into $n$ sub-intervals then each will have width $h=(b-a)/n$ ---\nso that $x_i=a+ih$. Produce a sketch of a smooth function $f:[a,b]\\raw\\R$ and a partition of $[a,b]$. Now for large $n$, and hence intervals of small length,  smooth curves look like lines so approximate the slope of $f$ on the interval $[x_{i-1},x_{i}]$ by the secant line joining $(x_{i-1},f(x_{i-1}))$ to $(x_{i},f(x_{i}))$ (a sketch should help):\n$$f'(x)\\approx \\frac{f(x_i)-f(x_{i-1})}{h}\\,\\text{ , for }x\\in[x_{i-1},x_i].$$\nNow average over all of the $n$ sub-intervals:\n$$\\text{average}(f')\\approx\\frac{\\sum_{i=1}^n\\left(\\frac{f(x_i)-f(x_{i-1})}{h}\\right)}{n}.$$\nNow use the relationship between $a,\\,b,\\,n$ and $h$ and telescoping series techniques(see Wikipedia) to show that\n$$\\text{average}(f')\\approx \\frac{f(b)-f(a)}{b-a}.$$\nTo make this interpretation precise --- by taking $n\\raw\\infty$ --- we actually have to use the  Mean Value Theorem! More of this in Integration.\nArgue that if $f:\\R\\raw\\R$ is a smooth, even function such that the only solution to $f'(x)=0$ is $x=0$, then $x=0$ cannot be a  saddle point (a point $a\\in\\R$ such that $f'(a)=0$ but $a$ is not a local maxima or minima). Suppose there is a  {local maximum} at $x=0$. Explain why this is an absolute maximum for $f$ on $\\R$.\nConstruct a function with  vertical asymptotes at $x=0,1,2,3,4,4,\\dots,10^6$.\n\n",
    "tags": [
      "soft-question",
      "proof-writing",
      "big-list",
      "education"
    ],
    "score": 12,
    "answer_score": 12,
    "is_accepted": true,
    "question_id": 509984,
    "answer_id": 517788
  },
  {
    "theorem": "Big O notation sum rule",
    "context": "I understand that when adding functions, the behavior is dominated by the highest power. But what I am having trouble is understanding the proof. Could anyone help me step by step in explaining the proof behind $T_1(n) + T_2(n) = O(max (f(n), g(n)))$ ? Thank you very much.\n",
    "proof": "Given $T_1(n)=O(f(n))$ and $T_2(n)=O(g(n))$, we are to prove $$T_1(n)+T_2(n)=O(\\max(f(n),g(n))\\tag0$$ \n\nWrite down exactly what the first assumption says: there exists a constant $C_1$ and an index $N_1$ such that \n$$|T_1(n)| \\le C_1f(n)\\quad \\text{when } n\\ge N_1 \\tag1$$\nWrite down exactly what the second assumption says: there exists a constant $C_2$ and an index $N_2$ such that \n$$|T_2(n)| \\le C_2g(n)\\quad \\text{when } n\\ge N_2 \\tag2$$\nPrepare to combine (1) and (2) by introducing $N=\\max(N_1,N_2)$ and $C=\\max(C_1,C_2)$.\nAdd (1) and (2): \n$$\n|T_1(n)|+|T_2(n)|\\le  C_1f(n)+C_2g(n) \\le C(f(n)+g(n))\\quad \\text{when } n\\ge N \\tag3\n$$\nCheck that for any two real numbers $a,b$ we have $$a+b\\le 2\\max(a,b)\\tag4$$\nUse (4) in (3) to obtain \n$$\n|T_1(n)|+|T_2(n)|\\le 2C\\max(f(n),g(n))\\quad \\text{when } n\\ge N \\tag5\n$$\nConclude that (0) holds.\n\n",
    "tags": [
      "asymptotics",
      "proof-writing"
    ],
    "score": 12,
    "answer_score": 20,
    "is_accepted": false,
    "question_id": 324200,
    "answer_id": 450718
  },
  {
    "theorem": "Proof of an inequality using analytic geometry",
    "context": "If $p,q,r$ are real numbers and $0<p<q<r$, then $$\\frac pq +\\frac qr +\\frac rp >\\frac qp +\\frac rq +\\frac pr$$\nIs this a well-known inequality?\nMy proof of it is based on analytic geometry:\nIf you plot the points $P=(p,1/p)$, $Q=(q,1/q)$, $R=(r,1/r)$ in this order, you get the vertices of a triangle circumscribed by an equilateral hyperbola $xy=1$ entirely on the first quadrant (as all numbers $p,q,r$ are positive and because a line cannot intercept a conic in three points). The area of this triangle PQR can be given by the following formula: $$\\frac 12 \\begin {vmatrix} p & \\frac 1p & 1 \\\\ q & \\frac 1q & 1 \\\\ r & \\frac 1r & 1 \\\\ \\end {vmatrix}$$\nAnd as these coordinates are written anticlockwise in this determinant, it has to be positive:\n$$\\begin {vmatrix} p & \\frac 1p & 1 \\\\ q & \\frac 1q & 1 \\\\ r & \\frac 1r & 1 \\\\ \\end {vmatrix}>0$$\nFrom that we get \n$$\\frac pq +\\frac qr +\\frac rp >\\frac qp +\\frac rq +\\frac pr$$\nIs this proof correct?\nIs there another, simpler proof for this inequality?\n",
    "proof": "Just a slightly different view at your proof: If the function $f: I \\to \\Bbb R$ is\nstrictly convex on the interval $I \\subset \\Bbb R$ then for $p < q < r$ in $I$\n$$\n f(q) < \\frac{r-q}{r-p} \\, f(p)  + \\frac{q-p}{r-p} \\, f(r) \\\\\n\\iff (r-q) \\, f(p) + (p-r) \\, f(q) + (q-p) \\, f(r) > 0 \\quad (*) \n$$\nChoosing $f(x) = \\frac 1x$ gives the desired inequality. \nThe connection to your solution is that $(*)$ can be written as\n$$\n\\begin {vmatrix} p & f(p) & 1 \\\\ q & f(q) & 1 \\\\ r & f(r) & 1 \\\\ \\end {vmatrix}>0\n\\, ,\n$$\ni.e. the (oriented) area of the triangle is positive because \n$f(x) = \\frac 1x$ is strictly convex.\n",
    "tags": [
      "algebra-precalculus",
      "proof-verification",
      "inequality",
      "proof-writing",
      "analytic-geometry"
    ],
    "score": 12,
    "answer_score": 7,
    "is_accepted": true,
    "question_id": 2923057,
    "answer_id": 2923161
  },
  {
    "theorem": "What is the correct way of disproving a mathematical statement?",
    "context": "This question is motivated by my midterm exam. In this exam there was a question as follow:\n\nQuestion: If the following statement is true, prove it, otherwise disprove it.\nIf $\\mathbf{u}$ and $\\mathbf{v}$ are vectors in three dimensions, then\n  $\\mathbf{u}\\times\\mathbf{v}=\\mathbf{v}\\times\\mathbf{u}$. The $\\times$\n  operation here means cross product.\n\nFor this question I actually proved that $\\mathbf{u}\\times\\mathbf{v}=-\\mathbf{v}\\times\\mathbf{u}$ and so the statement is false but my lecturer deducted some marks and said that my solution is not correct. She said for disproving question, you need one counter example. But still I think that I disproved it because I clearly showed that the given statement does not hold.\nSo my question after this long story is, what is the correct way of disproving a mathematical statement?\n",
    "proof": "A correct solution is any solution that works. That is like saying \"it's correct if it's correct\" but the point is that you don't have to use a counterexample. In most cases, if you can get away by showing a short and sweet counterexample to a claim then you should certainly do that. Always opt for the short and simple demonstration.\nAs for your professor's comment. If he said that you proof is incorrect because you did not provide a counterexample, then that is not a correct criticism, as it seems to imply that the only way to give an answer to this question is by counterexample, which is not the case. If, however, he claimed that you did not entirely prove the claim, then that is correct, since you proved an other equality holds, not the the original one does not. \n",
    "tags": [
      "proof-writing"
    ],
    "score": 12,
    "answer_score": 9,
    "is_accepted": false,
    "question_id": 834772,
    "answer_id": 834794
  },
  {
    "theorem": "How to show that the fabius function is nowhere analytic?",
    "context": "Consider the fabius function\nhttps://en.m.wikipedia.org/wiki/Fabius_function\nhttps://people.math.osu.edu/edgar.2/selfdiff/\nHow does one show that this function is nowhere analytic ?\nProbably related , Maybe even a step in the answer : how to evaluate this function for nonreals ? Is it defined there ? Can it be extended to the complex ? \nIs the best strategy to show it does not equal its Taylor series anywhere ?\nAlso : is it sufficient to show it is not analytic at rational $x$ ? Is there Some theorem that says nowhere analytic for rationals implies nowhere analytic for reals ?\n\nHow about other self-differentiating functions ; are all of them either constant or nowhere analytic ??\n",
    "proof": "Note $F(x)=0\\iff x=0$. From\n$$F'(x)=\\begin{cases}2F(2x)&x\\in[0,1/2]\\\\ 2F(2(1-x))& x\\in[1/2,1]\\end{cases}$$\nIt follows $F'(x)=0\\iff x\\in\\{0,1\\}$. Differentiating again gives $F''(x)=0\\iff x\\in\\{0,1/2,1\\}$. One can see that this pattern continues (do it with induction if necessary) so that:\n$$F^{(n)}(x)=0\\iff x\\in\\left\\{\\frac k{2^n}\\mid k\\in\\{0,...,2^n\\}\\right\\}$$\nThe important consequence is this: If $x = k/2^n$ for some $k\\in\\mathbb N$ then only finitely many derivatives of $F$ are non-zero at $x$. This means the taylor series of $F$ at this point is a polynomial.\nFor $F$ to be analytic at $x$ it is necessary and sufficient that there exist an open neighbourhood of $x$ in which $F$ is equal to its taylor series, here a polynomial. There cannot be any such neighbourhood however, since if $y$ is not of the form $k/2^{n}$ then no derivative of $F$ vanishes at $y$ and $F$ cannot be a polynomial in a neighbourhood of $y$. Since every neighbourhood of $x$ contains irrational points it follows $F$ is not analytic at $x$.\nThe set $\\left\\{\\frac k{2^n}\\mid n\\in\\mathbb N, k\\in\\{0,...,2^n\\}\\right\\}$ is dense in $[0,1]$ and the set of non-analyticities is always closed ($*$), so $F$ is not analytic anywhere on $[0,1]$.\n( ($*$) follows from power series being analytic, if $F$ is analytic at some $y$ it must be equal to a power series on some open neighbourhood of $y$ and thus analytic on this entire neighbourhood)\nThis was the step in the original paper by Fabius:\nJ. Fabius, \"A probabilistic example of a nowhere analytic\n$C^\\infty$-function\".  Zeitschrift für Wahrscheinlichkeitstheorie und Verwandte Gebiete 5 (1966) 173--174\n\nAs to other self differentiating functions (I'm not entirely sure what this means?): $\\exp'(x)=\\exp(x)$ and $\\exp$ is analytic.\n",
    "tags": [
      "complex-analysis",
      "proof-writing",
      "implicit-differentiation"
    ],
    "score": 12,
    "answer_score": 13,
    "is_accepted": true,
    "question_id": 2087361,
    "answer_id": 2090257
  },
  {
    "theorem": "Do eigenvectors always form a basis?",
    "context": "Suppose we have a $n \\times n $ matrix  over $\\Bbb R$. \nIs it necessary that we should have $n$ linearly independent eigenvectors associated with eigenvalues so that they form a basis? \nCan you give a proof or counterexample?\nHow about if you have the same question over complex numbers?\n",
    "proof": "No, of course not. For example, $\\begin{pmatrix} 0 &1 \\\\ 0 &0\\end{pmatrix}$ has $0$ as its only eigenvalue, with eigenspace $\\begin{pmatrix} x \\\\ 0 \\end{pmatrix}$. Thus there are not enough independent eigenvectors to form a basis.\n",
    "tags": [
      "linear-algebra",
      "proof-writing",
      "eigenvalues-eigenvectors"
    ],
    "score": 12,
    "answer_score": 12,
    "is_accepted": true,
    "question_id": 268176,
    "answer_id": 268184
  },
  {
    "theorem": "How much rigour in proofs?",
    "context": "Just a quick pointer: you could probably get away with skipping to the last 2 paragraphs, as the rest covers my motivation for answering this question.\nOver this coming holidays I'm hoping to spend a little time reading through Spivak's Calculus, and answering some of the more difficult/interesting looking questions. While I've learnt most of the material before, the calculus courses in first year at my Uni were all taught primarily for engineers, and I feel I have a very 'handwavy' understanding of ideas like continuity, Riemann sums, etc., etc. \nAnyway, I ran into this question in chapter 2 of the book last night:\nProve that:\n$$\\sum_{k=0}^{l} \\binom{n}{k}\\binom{m}{l-k}=\\binom{n+m}{l} $$\nSpivak recommended considering the binomial expansion of $(1+x)^a(1+x)^b$ and after a minute or so I saw the 'reason' why this statement is true. Briefly, we need to consider the coefficient of $x^l$ in the binomial expansion suggested above. Expanding out $(1+x)^{n+m}$ with the binomial theorem gives the coefficient $\\binom{n+m}{l}$, the RHS of the expression above. On the other hand, if we expand out $(1+x)^n$ and $(1+x)^m$ separately and multiply them together, we're going to have to add up all of the different cross-multiplications that give an l'th order term: there's $\\binom{n}{0}x^0\\times \\binom{m}{l}x^l$,$\\binom{n}{1}x^1\\times \\binom{m}{l-1}x^{l-1}$, etc. And if we add these up we get the expression on the LHS of the equality above. These 2 expressions for the coefficient of $x^l$ must be equal, so therefore the equality above must hold.\nQ.E.D.\nDone.\nCool.\nHowever, I'm still very uncomfortable with leaving the question with just this answer. The entire reason I'm reading through Spivak is to try to learn some mathematical rigour, and yet, while I'm sure this argument is valid, I have a feeling many analysts would want a little more... ummm...something :)\nAnd this where I'm stuck. On one end of the scale, we could expect all proofs to be entirely based on axioms or previously proven theorems - Euclid style. At the other end we get arguments like this one above, or like Newton's intuitionistic view of a limit. My question is simply 'when should I be satisfied that a proof is a proof?' How much rigour is sufficient? Indeed, how much rigour is used in proofs in professional mathematics (I'd gather that's quite field-dependent)? \nThank you very much for your thoughts\n",
    "proof": "The highest degree of rigor would be achieved this way: Write down a list of axioms and a list of rules of inference. Start from an axiom and modify its logical formula using only one rule at a step and at each step clearly stating what rule you have used. \nI'm not a logician and my description is probably not very good, but my point is that, whether or not this or some similar approach could be called \"absolutely rigorous\", it is highly impractical. At some stage in your development you know the law of associativity and need not be reminded of it every time it is used. At a later stage in your development the same is true about, say, the binomial theorem. \nSo, in practice, rigor is a relative concept. Proofs are written for the reader to check them. The goal of the author is to make this checking as quick and effortless as possible (this is often not true for textbook authors). To this end he must find the right amount of detail. The reader should not lose time by having to check four steps for a statement he could have easily understood in one step. On the other hand, the reader should not be forced to brood a long time over a statement that could also be written up for example with three intermediary steps each taking only a tenth of this time to check. \nBut this checking process depends on the reader. In my opinion, you cannot talk about rigor without talking about the \"mathematical maturity\" your average reader has. In a research paper proofs are considered rigorous that would be called handwaving or incomprehensible in an undergraduate textbook. \nOf course, giving the right amount of steps in proofs is only one aspect of rigor. Another is not speaking about concepts you haven't properly defined. But this is relative as well. In a research paper about mathematical physics you don't have to clearly state the axioms for the real numbers. In a calculus textbook you do. \nIn your particular example: If you can expect your readers to know for example that the coefficients of polynomial functions are unique, your proof (or at least that part of it) is rigorous. If not, it is not, and you have to give some explanation. \nYou ask about when should you be satisfied that a proof is a proof. My opinion: if and only if you are sure you could, if required, fill in all thinkable intermediary steps and trace each fact you use back to the very axioms. If you are already asking yourself \"is this really a proof?\", it is most certainly not for you (though it is for Spivak, and, if he succeeded, for his intended readership). You have to break it down to steps that are completely obvious to you. In your mathematical development more and more arguments come to fulfill this criterion. For example, when you first learn about induction, you need to clearly state the base case, the induction hypothesis and so on. Once you have seen your share of proofs by induction, you are satisfied and often grateful if the author just writes \"by induction we get...\".    \n",
    "tags": [
      "proof-writing",
      "learning"
    ],
    "score": 12,
    "answer_score": 11,
    "is_accepted": true,
    "question_id": 83856,
    "answer_id": 83893
  },
  {
    "theorem": "How can I learn about proofs for computer science?",
    "context": "I study computer science at a university. My school offers several courses where various proofs are expected, but there is no course that introduces the fundamental concepts of proofs and how to write your own proofs. It's possible to \"muddle through\" some of these courses without really understanding proofs, but I want to spare myself the frustration and actually learn about them.\nHow can I learn about proofs for computer science?\nI want to start at the very basics of proofs. I'm only interested in proofs as they relate to computer science, since I will have to spend my spare time on this. However, if I really do need to learn about mathematical proofs in general first, then so be it.\nNote: Just reading the proofs in our course literature is not enough; this is what me and my classmates do right now, and none of us have any deeper understanding of what constitutes an actual proof.\n",
    "proof": "Proofs are proofs. The basic logical structures are independent of the mathematical subject area, so there’s no real loss in starting with a general introduction to the subject. You’ll find a list of books on the subject here; it includes the book by Daniel Solow that Gerry Myerson mentioned. It also includes How to Prove It: A Structured Approach, by Daniel J. Velleman, which I think is a significantly better book. (It’s also cheaper and gets some very good reviews at Amazon.) I taught from the first edition of Transition to Advanced Mathematics, by Richard St. Andre, D. Smith, and M. Eggen a good many years ago; if I remember correctly, it was perfectly usable but not so good as the Velleman. I’m not familiar with the others listed.\nThe discussion and links here are also useful. In particular, the first edition of Bridge to Abstract Mathematics: Mathematical Proof and Structures, by Ronald P. Morash, is available for free download; I also taught from it a number of years ago and remember it as being quite decent.\nFinally, some introductory discrete math texts devote significant space to introducing students to reading and writing proofs (and also contain mathematics that students in computer science ought to know). One such is Edward Scheinerman, Mathematics: A Discrete Introduction, which I recommend highly. Another is Susanna S. Epp, Discrete Mathematics with Applications.\n",
    "tags": [
      "computer-science",
      "proof-writing",
      "proof-theory"
    ],
    "score": 12,
    "answer_score": 9,
    "is_accepted": false,
    "question_id": 74537,
    "answer_id": 74542
  },
  {
    "theorem": "Determinant of a companion matrix",
    "context": "I have to find determinant of  $$A := \\begin{bmatrix}0 & 0 & 0 & ... &0 & a_0 \\\\ -1 & 0 & 0 & ... &0 & a_1\\\\ 0 & -1 & 0 & ... &0 & a_2 \\\\ 0 & 0 & -1 & ... &0 & a_3 \\\\ \\vdots &\\vdots &\\vdots & \\ddots &\\vdots&\\vdots \\\\0 & 0 & 0 & ... &-1 & a_{n-1}     \\end{bmatrix} + t I_{n \\times n}$$\nIt is not a difficult thing to do. My method is as follows :\n$$\\begin{bmatrix}0 & 0 & 0 & ... &0 & a_0 \\\\ -1 & 0 & 0 & ... &0 & a_1\\\\ 0 & -1 & 0 & ... &0 & a_2 \\\\ 0 & 0 & -1 & ... &0 & a_3 \\\\ \\vdots &\\vdots &\\vdots & \\ddots &\\vdots&\\vdots \\\\0 & 0 & 0 & ... &-1 & a_{n-1}     \\end{bmatrix} + t I_{n \\times n} = \\begin{bmatrix}t & 0 & 0 & ... &0 & a_0 \\\\ -1 & t & 0 & ... &0 & a_1\\\\ 0 & -1 & t & ... &0 & a_2 \\\\ 0 & 0 & -1 & ... &0 & a_3 \\\\ \\vdots &\\vdots &\\vdots & \\ddots &\\vdots&\\vdots \\\\0 & 0 & 0 & ... &-1 & a_{n-1} + t     \\end{bmatrix} $$\nPerforming the row reduction of type $R_{k+1} \\to R_{k+1} + \\dfrac{1}{t}R_k$\nI get an upper triangular matrix\n$$\\begin{bmatrix}t & 0 & 0 & ... &0 & a_0 \\\\ 0 & t & 0 & ... &0 & a_1 + \\dfrac {a_0} t\\\\ 0 & 0 & t & ... &0 & a_2 + \\dfrac{a_1}{t} + \\dfrac {a_0} {t^2} \\\\ 0 & 0 & 0 & ... &0 & a_3 + \\dfrac{a_2}{t} + \\dfrac{a_1}{t^2} + \\dfrac {a_0} {t^3} \\\\ \\vdots &\\vdots &\\vdots & \\ddots &\\vdots&\\vdots \\\\0 & 0 & 0 & ... &0 & a_{n-1} + t   + \\sum_{k=0}^{n-2} \\dfrac{a_{k}}{t^{(n-1) - k }}  \\end{bmatrix} $$\nDeterminant of which is $t^n + \\sum^{n-1}_{k = 1} a_k t^{k}$.\nMy friend says this is not a rigorous proof and that I have to use induction to prove $$\\det A = t^n + \\sum^{n-1}_{k = 1} a_k t^{k}$$ She says that I have only found a formula for $\\det A$ and I can't be sure if it works for all $n\\in \\Bbb N$ without a proof. Is she correct? \n",
    "proof": "The argument can be made rigorous by the following identity:\nlet $U$ be the final triangular matrix and let $D$ be the matrix containing only a subdiagonal of $1, \\cdots, 1$. Then one has\n$$\n\\left(I - \\frac{1}{t} D\\right) U = (A + t I) \n$$\nhence $\\det(A+tI)= \\det(I - \\frac{1}{t} D) \\det (U) = \\det(U)$\nIt means that this process is actually an $LU$ decomposition of $A+tI$.\nFor a complete calculation, let\n$$\\renewcommand{\\arraystretch}{2}  \\begin{array}{rcl}{P}_{0}&=&0\\\\\n{P}_{i}&=&\\displaystyle  \\sum _{p = 0}^{i-1} {a}_{p} {t}^{p} \\quad  i  \\geqslant  1\\\\\nL&=&{\\left({{\\delta}}_{i}^{j}-{t}^{{-1}} {{\\delta}}_{i}^{j+1}\\right)}_{i , j}\\\\\nU&=&{\\left(t {{\\delta}}_{i}^{j}+{t}^{1-i} {P}_{i} {{\\delta}}_{j}^{n}\\right)}_{i , j}\n\\end{array}$$\nThen, using ${P}_{i}-{P}_{i-1} = {a}_{i-1} {t}^{i-1}$ for $i  \\geqslant  1$,\n$$\\renewcommand{\\arraystretch}{2}  \\begin{array}{rcl}{\\left(L U\\right)}_{i , j}&=&\\displaystyle  \\sum _{k = 1}^{n} \\left({{\\delta}}_{i}^{k}-{t}^{{-1}} {{\\delta}}_{i}^{k+1}\\right) \\left(t {{\\delta}}_{k}^{i}+{t}^{1-k} {P}_{k} {{\\delta}}_{j}^{n}\\right)\\\\\n&=&t {{\\delta}}_{i}^{j}+{t}^{1-i} {P}_{i} {{\\delta}}_{j}^{n}-{{\\delta}}_{i}^{j+1}-{t}^{1-i} {P}_{i-1} {{\\delta}}_{j}^{n}\\\\\n&=&t {{\\delta}}_{i}^{j}-{{\\delta}}_{i}^{j+1}+{a}_{j-1} {{\\delta}}_{j}^{n}\\\\\n&=&{\\left(A+t I\\right)}_{i , j}\n\\end{array}$$\nwhere $\\delta_i^j$ is Kronecker's delta.\n",
    "tags": [
      "linear-algebra",
      "matrices",
      "proof-writing",
      "determinant",
      "companion-matrices"
    ],
    "score": 12,
    "answer_score": 4,
    "is_accepted": false,
    "question_id": 2515679,
    "answer_id": 2515701
  },
  {
    "theorem": "There is a free group $F_2$ in $SO(3)$",
    "context": "I'd like an exposition of the proof that there is a subgroup of $SO(3)$ isomorphic to $F_2$, the free group on two elements. This is a key step in the proof of the Banach-Tarski paradox. The usual generators chosen for this are rotations about the $x$ and $z$ axes by $\\theta=\\arccos\\left(\\frac13\\right)$. Wikipedia attempts to describe a proof, but it is too terse for my liking; perhaps someone here can fill in the gaps.\n\nLet $a$ be a rotation of $\\theta=\\arccos\\left(\\frac{1}{3}\\right)$ about the first, $x$ axis, and $b$ be a rotation of $\\theta$ about the $z$ axis (there are many other suitable pairs of irrational multiples of $\\pi$, that could be used here as well).\nThe group of rotations generated by $a$ and $b$ will be called $H$. \n  Let $\\omega$ be an element of $H$ which starts with a rotation on the $z$ axis, of the form $\\omega=\\ldots a^{k_1}b^{k_2} \\ldots b^{n}$ [I think this is trying to say that the shortest word expressing $\\omega$ is of the form $xb$ for some word $x$].\nIt can be shown by induction that $\\omega$ maps the point $(1,0,0)$ to $\\displaystyle\\left(\\frac i {3^N}, \\frac{j\\sqrt 2}{3^N}, \\frac k {3^N}\\right)$, for some $i,j,k \\in \\mathbb Z,N \\in \\mathbb N$. Analysing $i,j$ and $k$ modulo 3, one can show that $j\\neq 0$. The same argument repeated (by symmetry of the problem) is valid for the opposite rotation about the $z$ axis, as well as rotations about the $x$ axis. This shows that for any non trivial word $\\rho \\in H$, then $\\rho \\neq e$. Therefore, the group $H$ is a free group, isomorphic to $F_2$.\n\nEdit: To clarify what I am looking for, I want a concise but rigorous proof of the above statement. The WP proof says \"one can show\" too much.\n",
    "proof": "For the \"by induction\" statement: Let $T$ be the set of matrices with entries $a_{ij}$ such that $a_{ij}\\in\\Bbb Z$ if $i+j$ is even and $a_{ij}\\in\\Bbb Z\\sqrt 2$ if $i+j$ is odd, that is, of the form $$\\begin{pmatrix}\\Bbb Z&\\Bbb Z\\sqrt2&\\Bbb Z\\\\\\Bbb Z\\sqrt2&\\Bbb Z&\\Bbb Z\\sqrt2\\\\\\Bbb Z&\\Bbb Z\\sqrt2&\\Bbb Z\\end{pmatrix}.$$ Then $T$ forms a subring of $M(3,\\Bbb R)$. Clearly $I,0\\in T$ and $T$ is closed under addition. To show that $T$ is closed under multiplication, suppose $A,B\\in T$, with elements $(a_{ij}),(b_{ij})$. Then the product is $c_{ij}=\\sum_{k=1}^3a_{ik}b_{kj}$. If $i+j$ is even, then every element in the sum is either $a_{ik}b_{kj}\\in\\Bbb Z\\cdot \\Bbb Z$ if $k$ is even or $a_{ik}b_{kj}\\in\\sqrt 2\\Bbb Z\\cdot \\sqrt 2\\Bbb Z=2\\Bbb Z$ if $k$ is odd, so the sum is also in $\\Bbb Z$, or else $i+j$ is odd, in which case one of the two terms is in $\\Bbb Z$ and the other is in $\\sqrt 2\\Bbb Z$ hence the product is in $\\sqrt 2\\Bbb Z$ and the sum of these is also in $\\sqrt 2\\Bbb Z$. Thus $AB\\in T$.\nNote also that $$3a=\\begin{pmatrix}3&0&0\\\\0&1&-2\\sqrt2\\\\0&2\\sqrt2&1\\end{pmatrix}\\in T\\qquad3b=\\begin{pmatrix}1&-2\\sqrt2&0\\\\2\\sqrt2&1&0\\\\0&0&3\\end{pmatrix}\\in T,$$\nso for any product $M$ of $n$ terms selected from $\\{a,b,a^{-1},b^{-1}\\}$, we have $3^nM\\in T$, and since applying this to $v=(1,0,0)$ extracts the top row, we have $3^nMv=(i,j\\sqrt 2,k)$ for some $i,j,k\\in\\Bbb Z$ as desired.\n\n\"Analyzing modulo 3\": Consider taking the coefficients of the matrix $\\bmod 3$ (treated as elements of $\\Bbb Z[\\sqrt2]$). This has the effect of simply reducing the coefficient of an element of the form $\\Bbb Z\\sqrt2$ (there are no mixed terms $a+b\\sqrt2$). Dropping the $\\sqrt2$ from the $a_{ij}$, $i+j$ odd entries and denoting $-1$ as $\\bar 1$, we get:\n$$[3a]=\\begin{pmatrix}0&0&0\\\\0&1&1\\\\0&\\bar 1&1\\end{pmatrix}\\quad\n[3a^{-1}]=\\begin{pmatrix}0&0&0\\\\0&1&\\bar 1\\\\0&1&1\\end{pmatrix}\\quad\n[3b]=\\begin{pmatrix}1&1&0\\\\\\bar 1&1&0\\\\0&0&0\\end{pmatrix}\\quad\n[3b^{-1}]=\\begin{pmatrix}1&\\bar 1&0\\\\1&1&0\\\\0&0&0\\end{pmatrix}$$\nThese matrices share the common pattern of having a $2\\times2$ block of $1$'s, with a single $\\bar 1$ in the block and $0$ outside. Now consider the following four matrices:\n$$A=[3a]=\\begin{pmatrix}0&0&0\\\\0&1&1\\\\0&\\bar 1&1\\end{pmatrix}\\quad\nB=\\begin{pmatrix}0&0&0\\\\0&1&1\\\\0&1&\\bar 1\\end{pmatrix}\\quad\nC=\\begin{pmatrix}0&1&\\bar 1\\\\0&1&1\\\\0&0&0\\end{pmatrix}\\quad\nD=\\begin{pmatrix}0&\\bar 1&1\\\\0&1&1\\\\0&0&0\\end{pmatrix}$$\nIt turns out that matrices of this form and their negatives are closed under left multiplication by the generators, which constitutes a proof of the goal because the identity matrix is not of this form. Here is the multiplication table:\n\\begin{array}{c|cccc}\\times&A&B&C&D\\\\\\hline\n[3a]=A&-A&0&A&A\\\\\n[3a^{-1}]&0&-B&B&B\\\\\n[3b]&C&C&-C&0\\\\\n[3b^{-1}]&D&D&0&-D\n\\end{array}\nThe $0$ elements are because $[3a^{-1}][3a]=[9I]$ is divisible by $3$ and so is equal to $0$, but can only occur if the word $M$ is not reduced (contains adjacent cancelling pairs) - for example $[3a]B=0$, but $\\pm B$ only arises from a product $[3a^{-1}]x$ for some $x\\in\\{A,B,C,D\\}$. This proves that if $M$ is any product of $n$ terms selected from $\\{a,b,a^{-1},b^{-1}\\}$ whose last term is $a$ and with no cancelling pairs, $[3^nM]\\in\\{\\pm A,\\pm B,\\pm C,\\pm D\\}$, but $[3^nI]=0$. Thus $M\\ne I$. Argumentation by symmetry (or with a different set of matrices) also establishes the result for words that end in $a^{-1},b,b^{-1}$, so we have that $M\\ne I$ for any nontrivial word.\n",
    "tags": [
      "group-theory",
      "proof-writing",
      "rotations",
      "free-groups"
    ],
    "score": 12,
    "answer_score": 7,
    "is_accepted": true,
    "question_id": 1492799,
    "answer_id": 1493249
  },
  {
    "theorem": "Lebesgue - Radon - Nikodym Theorem: Question about $\\sigma$-finite case",
    "context": "Lebesgue Radon Nikodym Theorem\n\nLet $\\nu$ be a $\\sigma$-finite signed measure on $(X,\\mathcal{A})$ and $\\mu$ a $\\sigma$-finite positive measure on $(X,\\mathcal{A})$\n\nThere exist unique $\\sigma$-finite signed measures $\\rho,\\lambda$ on $(X,\\mathcal{A})$ such that $$\\nu=\\rho+\\lambda\\qquad \\rho \\ll\\mu,\\qquad\\lambda \\perp \\mu.$$\nThere exists an extended $\\mu$-integrable function $f$ such that $d\\rho=f\\,d\\mu$ i.e.$$\\nu=f\\,d\\mu+\\lambda$$\nIf we also have $\\nu=\\tilde{f}\\,d\\mu+\\lambda$ where $\\tilde{f}$ is an extended $\\mu$-integrable function, then $$\\tilde{f}=f\\quad\\mu\\text{-a.e}$$\n\n\nProof.\nCase 1\nSuppose first that $\\mu$ and $\\nu$ are both finite, positive measures. On this first step I have no problems\nCase 2\nSuppose that $\\mu$, $\\nu$ are both $\\sigma$-finite positive measure. We can write $$X=\\bigcup_l E_j\\quad\\text{and}\\quad X=\\bigcup_k F_k,$$ with $\\mu(E_j)<\\infty$, $\\nu(F_k)<\\infty.$ Then $$X=\\bigcup_{j,k}(E_j\\cap F_k)=\\bigcup_l A_l$$ disjointly with $\\mu(A_l), \\nu(A_l)<\\infty.$ Define $$\\mu_k(E)=\\mu(E\\cap A_k)\\quad \\nu_k(E)=\\nu(E\\cap A_k),$$ so by case 1. we can write $\\nu_k=\\rho_k+\\lambda_k$ for some unique measures with $\\rho_k \\ll \\mu_k$ and $\\lambda_k\\perp \\mu_k.$ Note that $$\\mu_k(A_k^c)=\\mu(A_k^c\\cap A_k)=0,$$ so $A_k^c$ is a $\\mu_k-$null set. Therefore $$f^{'}_k=f_k\\chi_{A_k}$$ equals $f_k$ $\\mu_k-$ a.e, so we can replace $f_k$ with $f^{'}_k$ without changing $\\lambda_k$ or $\\rho_k.$ In other words, we  can assume that $f_k(x)=0$ $\\forall x\\notin A_k.$\nSince the $A_k$ are disjoint, we can therefore define $$f=\\sum_{k=1}^\\infty f_k.$$ Since $f\\ge 0$, $$d\\rho=f\\,d\\mu$$ defines a positive measure. Also, $$\\lambda=\\sum_{k=1}^\\infty \\lambda_k$$ is a positive measure, since each $\\lambda_k\\ge 0$\n\nQuestion I'm trying and trying again to show the following but I can't:\n\n$\\lambda, \\rho$ are $\\sigma$-finite;\n$\\nu=\\rho+\\lambda$;\n$\\rho \\ll \\mu$;\n$\\lambda\\perp \\mu$;\nThe uniqueness statements hold.\n\nCould you please give me some suggestions on the basis of what I have already shown?\n\n",
    "proof": "Let $\\mu$ and $\\nu$ be two $\\sigma$-finite measures. Then we can write $$\\bigsqcup_{j=1}^\\infty E_j=X=\\bigsqcup_{k=1}^\\infty F_k,$$ where $\\mu(E_j)<\\infty,\\nu(F_k)<\\infty$, for all $k,j$. Let's write $$X=\\bigsqcup_{k,j}E_j\\cap F_k=\\bigsqcup_{l=1} A_l.$$ Then, $\\mu(A_l)<\\infty,\\nu(A_l)<\\infty$ for all $l$.\nDefine two finite measure $\\mu_l:\\mathcal A\\ni E\\longmapsto \\mu(A_l\\cap E)$ and $\\nu_l:\\mathcal A\\ni E\\longmapsto \\nu(A_l\\cap E)$ for each $l\\geq 1$. Now, we have $$\\nu_l=\\lambda_l+\\rho_l\\text{ for some measures }\\lambda_l,\\rho_l\\text{ with }\\lambda_l\\perp \\mu_l, \\rho_l\\ll\\mu_l\\text{ and }\\mathrm d\\rho_l=f_l\\ \\mathrm d\\mu_l$$$$\\text{ for some }\\mu_l\\text{-integrable real valued non-negative function }f_l.$$\nSince $\\mu_l(A_l^c)=\\nu_l(A_l^c)=0$ we have $\\displaystyle\\lambda_l(A_l^c)=\\mu_l(A_l^c)-\\int_{A_l^c}f_l\\ \\mathrm d\\mu_l=0$. We can also assume $f_l=0$ on $A_l^c$.\n\n$\\color{red}{\\text{Proof of (1):}}$ Now, let $\\displaystyle \\lambda=\\sum_l\\lambda_l$ and $\\displaystyle f=\\sum_lf_l$. Note that $\\displaystyle\\lambda(A_i)=\\sum_l\\lambda_l(A_i)=\\lambda_i(A_i)<\\infty$ as $A_l$ are disjoint, i.e., $\\displaystyle A_i\\cap \\bigsqcup_{l\\not= i}A_l=\\emptyset$ and $\\lambda_l(A_l^c)=0$. So, $\\displaystyle X= \\bigsqcup_{l=1} A_l$ is a decompostion of $X$ into finite $\\lambda$-measure sets.\nDefine $\\mathrm d\\rho:=f\\ \\mathrm d\\mu$. Then $$\\begin{align}\\rho(A_i) &=\\int_{A_i} f\\ \\mathrm d\\mu\\\\ &=\\int f1_{A_i}\\ \\mathrm d\\mu\\\\ & =\\int\\left(\\sum_lf_l 1_{A_i}\\right)\\ \\mathrm d\\mu\\\\ &=\\sum_l\\int f_l1_{A_i}\\ \\mathrm d\\mu\\\\ &=\\int f_i1_{A_i}\\ \\mathrm d\\mu\\end{align}$$ as $f_l(A_l^c)=0$. Now, $\\displaystyle\\int f_i1_{A_i}\\ \\mathrm d\\mu=\\int f_i\\ \\mathrm d\\mu_i<\\infty$, see definition of $\\mu_i$.\nNote that interchange of summation and integral is possible here as all $f_l$ are non-negatives, so apply monotone convergence to partial sums. So, $\\displaystyle X= \\bigsqcup_{l=1} A_l$ is a decomposition of $X$ into finite $\\rho$-measure sets. $\\square$\n\n$\\color{red}{\\text{Proof of (2):}}$ Notice that $\\displaystyle\\nu=\\sum_l\\nu_l=\\sum_l\\lambda_l+\\sum_l\\rho_l=\\lambda+\\sum_l\\rho_l$. Note that for any $B\\in \\mathcal A$ we have $\\displaystyle B=\\bigsqcup_l B\\cap A_l$. So, $\\displaystyle\\rho(B)=\\sum_l\\rho(B\\cap A_l).$ But as in the previous paragraph we have $$\\begin{align}\\rho(A_i\\cap B) &=\\int_{A_i\\cap B} f\\ \\mathrm d\\mu\\\\ &=\\int f1_{A_i\\cap B}\\ \\mathrm d\\mu\\\\ &=\\int\\left(\\sum_lf_l 1_{A_i\\cap B}\\right)\\ \\mathrm d\\mu\\\\ &=\\sum_l\\int f_l1_{A_i\\cap B}\\ \\mathrm d\\mu\\\\ &=\\int f_i1_{A_i\\cap B}\\ \\mathrm d\\mu\\end{align}$$ as $f_l(A_l^c)=0$. Now, $\\displaystyle\\int f_i1_{A_i\\cap B}\\ \\mathrm d\\mu=\\int_B f_i\\ \\mathrm d\\mu_i=\\rho_i(B)$.\nSo, $\\displaystyle\\rho(A_i\\cap B)=\\sum_l\\rho(B\\cap A_l)=\\rho_i(B).$ Hence, $\\displaystyle\\sum_l\\rho_l=\\rho$. Therefore, $\\displaystyle\\nu=\\lambda+\\rho$. $\\square$\n\n$\\color{red}{\\text{Proof of (3):}}$ Note that $\\mu(B)=0$ implies $\\displaystyle\\mu_i(B)=\\mu(B\\cap A_i)=0$ for all $i$, then $\\displaystyle\\rho_i(B)=\\int_Bf_i\\ \\mathrm d\\mu_i=0$. Since $\\displaystyle\\rho(B)=\\sum_l\\rho_l(B)=0$, we are done. $\\square$\n\n$\\color{red}{\\text{Proof of (4):}}$ Since $\\mu_l\\perp\\lambda_l$, write $X=W_l\\sqcup U_l$ with $U_l$ is null for $\\mu_l$, $W_l$ is null for $\\lambda_l$. Let, $\\widetilde W_l=W_l\\cap A_l,\\widetilde U_l=U_l\\cap A_l$. Then let $$W=\\bigsqcup_l \\widetilde W_l, U=\\bigsqcup_l \\widetilde U_l,$$ see $A_l$ are disjoint. Now, $$W\\cap U=\\emptyset\\text{ and }W\\cup U=\\bigcup_l\\big(\\widetilde W_l\\cup \\widetilde U_l\\big)=\\bigcup_lA_l=X.$$ Next, $U$ is null for $\\mu$ and $W$ is null for $\\lambda$ as $\\displaystyle\\mu(U)=\\sum_l\\mu_l(U)=\\sum_l\\sum_j\\mu_l(\\widetilde U_j)=0$. Similarly for other. $\\square$\n\n$\\color{red}{\\text{Proof of (5):}}$ Now, if $f_l'$ also satisfies the above then, $f_l$ is $\\mu_l$ equal to $f_l'$, i.e., $\\mu(Z_l\\cap A_l)=\\mu_l(Z_l)=0$, where $Z_l=\\{f_l\\not =f_l'\\}$.\nNote that we can assume, as previously said,  $f_l(A_l^c)=0$ and $f_l'(A_l^c)=0$, so that $Z_l\\subseteq A_l$, so that $\\mu(Z_l)=\\mu(Z_l\\cap A_l)=0$. Now let $$f'=\\sum_lf_l',$$ then $\\{f\\not=f'\\}=\\cup_l Z_l$ and so, $\\mu\\big(\\{f\\not=f'\\}\\big)=\\sum_l \\mu(Z_l)=0$. $\\square$\n\n",
    "tags": [
      "real-analysis",
      "measure-theory",
      "proof-writing",
      "solution-verification"
    ],
    "score": 12,
    "answer_score": 6,
    "is_accepted": true,
    "question_id": 3709489,
    "answer_id": 3713882
  },
  {
    "theorem": "Prove that for every three non-zero integers, a,b and c, at least one of the three products ab,ac,bc is positive",
    "context": "The question is:\nProve that for every three non-zero integers, a,b and c, at least one of the three products ab,ac,bc is positive. Use proof by contradiction.\nMy general approach to doing contradiction is as follow:\nI always like to turn the statements into propositional logic, with an implication. In this case it would be like:\n[For all a,b,c in the domain of non-zero integer If a,b and c are three non-zero integers], then at least one of the three products ab,ac,bc is positive.\nThen I take the negation of it: (P ^ ~Q)\nAssume to the contrary,there exist a,b and c that are non-zero integers and none of the three products ab,ac and bc are positive.\nNow, I pick a = 1,  b = -1 , c = 2\na.b = -1\na.c = 2\nb.c = -2\nSince a.c is positive, our assumption is false and we have a contradiction. Hence, the original statement is true.\nPlease feel free to share any other alternatives...(contradiction ones)\n",
    "proof": "The product of three negative numbers is negative.  So if $ab$, $ac$, and $bc$ are all negative, then $(ab)(ac)(bc)\\lt0$.  But $(ab)(ac)(bc)=a^2b^2c^2$ is the product of three squares, which are all positive.  \n",
    "tags": [
      "elementary-number-theory",
      "proof-verification",
      "proof-writing"
    ],
    "score": 11,
    "answer_score": 64,
    "is_accepted": true,
    "question_id": 1527843,
    "answer_id": 1527866
  },
  {
    "theorem": "How do we know all numbers alternate between odd and even numbers?",
    "context": "Basically, I tried proving that multiplying an odd and even number together gives you an even number to a friend of mine. This is what I said.\nLet's first take a random number $k$. It doesn't matter if its odd or even. If we then multiply this by $2$, we get an even number (from the definition of a even number, which is basically a number that can be divided by $2$). Now we know that in the number line, it goes odd, even, odd, even,..., and so if we add $1$ to this, then we get an odd number. We now have an odd number and an even number and so lets multiply them.\n$$2k \\cdot (2k + 1) = 2k(2k + 1) = 4k^2 + 2k) = 2(2k^2 + k)$$\nWe now have a random number $(2k^2 + k)$ multiplied by $2$ and so this is an even number (by definition of an even number).\nMy friend then said that we make the assumption that $2k \\pm 1$ is odd because the number line alternates between odd and even numbers, how do know that this is true?\nI was thinking how to prove this. Would it be by some form of induction?\nEDIT: I think I need to change \"number\" to \"integer\" in this post don't I?\n",
    "proof": "I wouldn't appeal to the fact that the integers alternate between even and odd.\nI would say this:\nAn even number is defined as any integer of the form $2k$ for $k$ any integer (maybe excluding $k=0$).\nAn odd number is defined as any integer of the form $2k +1$ for any integer $k$.\nYou can just take this as the definition. And then (as you have) the proof that even times odd is even is just\n$$\n2k(2k'+1) = 2[k(2k'+1)]\n$$\nwhere of course $k(2k'+1)$ is just an integer.\n",
    "tags": [
      "elementary-number-theory",
      "proof-writing"
    ],
    "score": 11,
    "answer_score": 22,
    "is_accepted": true,
    "question_id": 322911,
    "answer_id": 322921
  },
  {
    "theorem": "Prove $|a+b|+|a-b| \\geq |a|+|b|$",
    "context": "I am fighting with this proof-writing problem for a while. The statement says $$|a+b|+|a-b| \\geq |a|+|b|.$$\nI know the triangle inequality which says$$|a+b| \\leq |a|+|b|.$$ \nHow can I use this inequality to prove the statement above? I am adding the distance between $a$ and $b$ to $|a+b|$ which then turns the triangle inequality to another direction. How can I prove this in a mathematical way? \n",
    "proof": "$2|a|=|a+b+a-b|\\leq |a+b|+|a-b|$\n$2|b|=|b+a+b-a|\\leq |b+a|+|b-a|=|a+b|+|a-b|$\nAdd these inequalities and then halve.\n",
    "tags": [
      "inequality",
      "proof-writing",
      "problem-solving"
    ],
    "score": 11,
    "answer_score": 24,
    "is_accepted": true,
    "question_id": 258969,
    "answer_id": 258973
  },
  {
    "theorem": "Trigonometry Identity: Prove that $\\sin(a-b)=\\sin a \\cos b - \\cos a \\sin b$",
    "context": "First, I do not want a proof using $\\sin(a+b)=\\sin a \\cos b + \\cos a \\sin b$. Second, I suspect that it has something to do with Euler's formula; $e^{ix}=\\cos x + i\\sin x$, but I am not sure. Can anyone give me some direction? Thanks in advance.\nP.S. I apologize if this question is a duplicate. I did not find any when scrolling down the \"Questions that may already have your answer\" list.\n",
    "proof": "\nSomething to help you visualize this geometrically.\n",
    "tags": [
      "trigonometry",
      "proof-writing"
    ],
    "score": 11,
    "answer_score": 16,
    "is_accepted": true,
    "question_id": 705021,
    "answer_id": 705039
  },
  {
    "theorem": "Proof: Is there a line in the xy plane that goes through only rational coordinates?",
    "context": "Question: Is there a line in the XY plane that has all rational coordinates. Prove your answer. \nIdea: There is most certainly not. I believe it can be shown that between any 2 rational points that there is at least one irrational coordinate. Therefore, there can not be a line that contains only rational points. The issue is that I am not sure how to show this. Any ideas? I am also open to any other ideas of how to do this. Thanks.\nNote: this is for an intro to proofs study guide. So, I would prefer not to use advanced theorems. \n",
    "proof": "Two proofs.\nFirst one based on cardinality. A line has the cardinality of the continuum like $\\mathbb R$, while $\\mathbb Q$ is countable.\nSecond one. A line has an equation $ax+by+c=0$ with $(a,b) \\neq (0,0)$. If $a \\neq 0$ then $(-\\frac{b \\pi +c}{a},\\pi)$ belongs to the line and the second coordinate of that point is not rational. While if $a=0$, $(\\pi,-\\frac{c}{b})$ belongs to the line (thanks to immibis comment).\n",
    "tags": [
      "proof-verification",
      "proof-writing",
      "irrational-numbers",
      "rational-numbers"
    ],
    "score": 11,
    "answer_score": 23,
    "is_accepted": true,
    "question_id": 1572526,
    "answer_id": 1572537
  },
  {
    "theorem": "APICS Mathematics Contest 1999: Prove $\\sin^2(x+\\alpha)+\\sin^2(x+\\beta)-2\\cos(\\alpha-\\beta)\\sin(x+\\alpha)\\sin(x+\\beta)$ is a constant function of $x$",
    "context": "This is question 3 from the APICS Mathematics Competition paper of 1999:\n\nProve that $$\\sin^2(x+\\alpha)+\\sin^2(x+\\beta)-2\\cos(\\alpha-\\beta)\\sin(x+\\alpha)\\sin(x+\\beta)$$ is a constant function of $x$.\n\nExpanding it seems rather daunting, in particular the last term, and nothing I've tried has been useful towards cancelling terms out.\nIt was assigned in a pre-calculus course, so it should be possible to solve without using derivatives. However, showing that $f'(x)=0$ would obviously be a valid solution.\nAny ideas are greatly welcome.\n",
    "proof": "\nTaking @SiongthyeGoh's comment, in the above picture, $AE=1$, $\\angle BAE=x$, $\\angle BAC=\\beta$, and $\\angle BAD=\\alpha$.\nIt follows that $DE=\\sin(x+\\alpha)$ and $CE=\\sin(x+\\beta)$. Applying the cosine theorem to $\\triangle DCE$, the given expression is equal to $DC^2$, hence is a constant.\n",
    "tags": [
      "functions",
      "trigonometry",
      "proof-writing"
    ],
    "score": 11,
    "answer_score": 17,
    "is_accepted": false,
    "question_id": 1869525,
    "answer_id": 1869584
  },
  {
    "theorem": "Prove that there is no smallest positive real number",
    "context": "I have to prove the following: \n$$\\text{Prove that there is no smallest positive real number}$$\nArgument by contradiction\nSuppose there is a smallest positive real number. Let $x$ be the smallest positive real number: \n$$x : x \\gt 0, x \\in \\mathbb{R}$$\nLet $y$ be $\\frac{x}{10}$. Contradiction. This implies that $y < x$ which implies that you can always construct a number that is less than the \"smallest positive real number\". QED. \nCan someone please verify the write up of the proof and the proof itself? \nThanks for your time!\nP.S. I have seen this and this but I'm not looking for a way to approach the problem but rather verification and write up help.\nP.P.S If there is another novel way of approaching this problem, I would like to know! \n",
    "proof": "Here is a slightly different way to organize the proof.  What we will do is split it into two parts:\n\nFor every positive real number there is another positive real number less than it.  Proof: Let $x>0$.  Then since $0<\\frac{1}{2}<1$, we have $x>\\frac{1}{2}x>0$, and so $\\frac{1}{2}x$ is such a number.\nThere is no smallest positive real number.  Proof: Assume for sake of contradiction that $x$ is the smallest such.  Then by 1 there is a smaller such number, contradicting minimality.\n\nThe idea with splitting the proof into two statements is that we have isolated the proof by contradiction into a very small part.  The risk with proof by contradiction is that, since you are in fact assuming something which is false from the beginning, any mistaken reasoning after that will look like a valid completion to the contradiction proof.\n",
    "tags": [
      "proof-verification",
      "proof-writing",
      "alternative-proof"
    ],
    "score": 11,
    "answer_score": 10,
    "is_accepted": true,
    "question_id": 1247700,
    "answer_id": 1247720
  },
  {
    "theorem": "Prove $1+\\frac{1}{\\sqrt{2}}+\\frac{1}{\\sqrt{3}}+\\dots+\\frac{1}{\\sqrt{n}} &gt; 2\\:(\\sqrt{n+1} − 1)$",
    "context": "Basically, I'm trying to prove (by induction) that:\n$$1+\\frac{1}{\\sqrt{2}}+\\frac{1}{\\sqrt{3}}+\\dots+\\frac{1}{\\sqrt{n}} > 2\\:(\\sqrt{n+1} − 1)$$\nI know to begin, we should use a base case.  In this case, I'll use $1$.  So we have:\n$$1 > 2\\:(1+1-1) = 1>-2$$\nWhich works out.\nMy problem is the next step.  What comes after this?  Thanks!\n",
    "proof": "Mean Value Theorem can also be used,\nLet $\\displaystyle f(x)=\\sqrt{x}$\n$\\displaystyle f'(x)=\\frac{1}{2}\\frac{1}{\\sqrt{x}}$\nUsing mean value theorem we have:\n$\\displaystyle \\frac{f(n+1)-f(n)}{(n+1)-n}=f'(c)$ for  some $c\\in(n,n+1)$\n$\\displaystyle \\Rightarrow \\frac{\\sqrt{n+1}-\\sqrt{n}}{1}=\\frac{1}{2}\\frac{1}{\\sqrt{c}}$....(1)\n$\\displaystyle \\frac{1}{\\sqrt{n+1}}<\\frac{1}{\\sqrt{c}}<\\frac{1}{\\sqrt{n}}$\nUsing the above ineq. in $(1)$ we have,\n$\\displaystyle \\frac{1}{2\\sqrt{n+1}}<\\sqrt{n+1}-\\sqrt{n}<\\frac{1}{2\\sqrt{n}}$\nAdding the left part of the inequality we have,$\\displaystyle\\sum_{k=2}^{n}\\frac{1}{2\\sqrt{k}}<\\sum_{k=2}^{n}(\\sqrt{k}-\\sqrt{k-1})=\\sqrt{n}-1$\n$\\Rightarrow \\displaystyle\\sum_{k=2}^{n}\\frac{1}{\\sqrt{k}}<2\\sum_{k=2}^{n}(\\sqrt{k}-\\sqrt{k-1})=2(\\sqrt{n}-1)$\n$\\Rightarrow \\displaystyle1+\\sum_{k=2}^{n}\\frac{1}{\\sqrt{k}}<1+2\\sum_{k=2}^{n}(\\sqrt{k}-\\sqrt{k-1})=2\\sqrt{n}-2+1=2\\sqrt{n}-1$\n$\\Rightarrow \\displaystyle\\sum_{k=1}^{n}\\frac{1}{\\sqrt{k}}<2\\sqrt{n}-1$\nSimilarly adding the right side of the inequality we have,\n$\\displaystyle\\sum_{k=1}^{n}\\frac{1}{2\\sqrt{k}}>\\sum_{k=1}^{n}(\\sqrt{k+1}-\\sqrt{k})=\\sqrt{n+1}-1$\n$\\Rightarrow \\displaystyle\\sum_{k=1}^{n}\\frac{1}{\\sqrt{k}}>2(\\sqrt{n+1}-1)$\nThis completes the proof.\n$\\displaystyle 2\\sqrt{n+1}-2<\\sum_{k=1}^{n}{\\frac{1}{\\sqrt{k}}}<2\\sqrt{n}-1.$\nThis is a much better proof than proving by induction(Ofcourse if someone knows elementary calculus).\n",
    "tags": [
      "discrete-mathematics",
      "proof-writing"
    ],
    "score": 11,
    "answer_score": 19,
    "is_accepted": true,
    "question_id": 313834,
    "answer_id": 313836
  },
  {
    "theorem": "I am looking for class of math problems which are provable in ZF if and only if they are provable in ZFC",
    "context": "I know that P vs NP and Riemannian hypothesis are of this class but could not find any article on that. I would also appriciate links or books on related theme. My question is: what are some other problems of the same class and how one proves such property? I am by the way undergrad of math.\n",
    "proof": "The relevant term here (but see the end of this answer) is absoluteness. Basically, say that a statement is absolute between two structures if its truth in one implies its truth in the other and vice versa. There is also \"directional\" absoluteness: if $A$ is a substructure of $B$ we say that a statement $\\varphi$ is upwards-absolute (with $A$ and $B$ implicit from context) if its truth in $A$ implies its truth in $B$, and downwards-absolute if the reverse holds.\nAbsoluteness first rears its head (although often via different terminology, e.g. \"preserved\" and \"reflected\") in basic model theory and universal algebra. For example, one of the earliest results of this type one sees is the following:\n\nSuppose $\\mathcal{A}$ is a substructure of $\\mathcal{B}$ and $\\varphi(x_1,...,x_n)$ is a quantifier-free first-order formula. Then if $\\forall x_1,...,x_n\\varphi(x_1,...,x_n)$ is true in $\\mathcal{B}$, it is also true in $\\mathcal{A}$.\n\nPut another way, this says that universal sentences are downwards-absolute between any structure-substructure pair. A more intricate result is Birkhoff's HSP theorem, which develops a connection both ways: it gives a characterization of when two algebraic structures have the same \"equational\" theory. If memory serves, Hodges' big model theory book has a lot of interesting information about this sort of thing.\n\nHowever, \"simple\" absoluteness results such as the above are quite limited and certainly don't come close to applying to situations of the type we care about here. In order to bring absoluteness into play in set theory, we have to do a lot more work.\nThe key (initially at least; later on it gets supplanted by more complicated analogues) is the following result of Godel, which I'll phrase informally for readability:\n\nAny model $V$ of $\\mathsf{ZF}$ has a substructure $L$ satisfying $\\mathsf{ZFC}$. (Moreover, $L$ is \"nicely definable\" inside $V$ and generally has a number of other excellent properties.)\n\nThis tells us the following: if $\\varphi$ is upwards-absolute between models of $\\mathsf{ZF}$, then its $\\mathsf{ZFC}$-provability implies its $\\mathsf{ZF}$-provability. This is because if $\\varphi$ is upwards-absolute between models of $\\mathsf{ZF}$ and is $\\mathsf{ZFC}$-provable, we must have that $\\varphi$ is true in every model $V$ of $\\mathsf{ZF}$ (hence be $\\mathsf{ZF}$-provable) since it is true in $L$ due to $L$'s satisfying $\\mathsf{ZFC}$.\nThe idea then is that similarities between $V$ and $L$ can be turned into absoluteness results. For example, any arithmetical statement (basically, referring only to natural numbers) cannot rely on choice. This comes from looking at the construction of $L$: it turns out that $V$ and $L$ \"have the same natural numbers\" in a precise sense, and so they cannot disagree about arithmetical facts. Many mathematical principles, including the vast majority of results and conjectures in number theory, are either explicitly arithmetical (e.g. Fermat's last theorem or P=NP) or $\\mathsf{ZF}$-provably equivalent to an arithmetical statement (e.g. see here re: RH).\n\nThe next two absoluteness theorems important enough to have names are Mostowski absoluteness which says that all \"statements about well-foundedness\" are absolute between $V$ and $L$ (hence cannot rely on choice), and Shoenfield absoluteness which applies Mostowski to prove that all $\\Pi^1_2$ statements (a rather technical notion, but one which is incredibly encompassing - my understanding is that all the Millenium Problems for example, fall into this category) are absolute between $V$ and $L$. We also have higher absoluteness results contingent on additional set-theoretic hypotheses, the latter of which are called \"large cardinal axioms\" (and have some pretty neat names). Like Shoenfield these higher absoluteness results tend to come from clever applications of Mostowski absoluteness, but these applications are much more intricate, and in particular require us to develop more complicated analogues of Godel's $L$ as hinted at above.\nThese stronger absoluteness results are genuinely hard to summarize, let alone sketch proofs of. The above paragraph should however have indicated that Mostowski's principle plays an especially central role here. This is indeed true; moreover, it is by far the most approachable of the higher absoluteness theorems. Like arithmetical absoluteness, it comes from a simple observation, namely that $V$ and $L$ \"have the same ordinal numbers.\" This, together with some thinking about transfinite recursion, ultimately gives the desired result.\nBut going back to the start of this answer, even before Mostowski - even before arithmetical absoluteness! - is that beautiful result of Godel about a thing called \"$L$.\" While quite difficult, this is one of the absolute (hehehehe) jewels of set theory, and it is well worth your time.\n\nOK, there is one useful bit of terminology I've omitted above. I chose to focus on structures, since I think that's the most intuitive framing. However the question itself was posed at the level of theories instead: $\\mathsf{ZF}$ and $\\mathsf{ZFC}$ are sets of sentences describing structures, not structures themselves.\nWith this in mind, there is another relevant term: conservativity. Given two theories $T,S$ with $S$ possibly in a larger language, we say $S$ is a conservative extension of $T$ iff $(i)$ $S$ proves everything that $T$ does in the language of $T$ but $(ii)$ those are the only things which $S$ proves in that language. Basically, $S$ is a conservative extension of $T$ iff\nIf the language of $S$ is no bigger than the language of $T$, then $S$ is a conservative extension of $T$ iff $S$ and $T$ are \"basically the same\" (= prove the same things). However, there are also weakenings of conservativity which are non-trivial even if the languages are the same: namely, we can talk about conservativity restricted to sentences of a certain form. For example, the arithmetical absoluteness result above has the following consequence, which is really what you were asking about:\n\n$\\mathsf{ZFC}$ is conservative over $\\mathsf{ZF}$ for arithmetical sentences.\n\nSo properly speaking, \"absoluteness\" is about comparing models while \"conservativity\" is about comparing theories. It's also worth noting that there are many variations on the notion of conservativity - see e.g. here.\n",
    "tags": [
      "logic",
      "proof-writing",
      "set-theory",
      "riemann-hypothesis",
      "decidability"
    ],
    "score": 11,
    "answer_score": 21,
    "is_accepted": true,
    "question_id": 4166224,
    "answer_id": 4166265
  },
  {
    "theorem": "Proof of Wolstenholme&#39;s theorem",
    "context": "According to the theorem, if\n$$1+\\frac{1}{2}+\\frac{1}{3}+\\frac{1}{4}+\\frac{1}{5}+\\cdots+\\frac{1}{p-1} =\\frac{r}{q}$$\nthen we have to prove that $r\\equiv0 \\pmod{p^2}$.\n(Given $p>3$, otherwise $1+\\dfrac{1}{2}=\\dfrac{3}{2}$, $3 \\not\\equiv  0 \\pmod 9$.)\nI guess there's a $(\\bmod p)$ solution for this, but I don't really get how to start it. \n",
    "proof": "The group theoretic proof on Wikipedia is good.\nAlternatively, another nice way is, the second line is working in $\\mathbb{Z}/p\\mathbb{Z}$,\n$$ \\frac2p\\sum_{n=1}^{p-1} \\frac{1}{n} =  \\frac2p \\sum_{n=1}^{(p-1)/2} \\left(\\frac{1}{n} + \\frac{1}{p-n}\\right) = 2 \\sum_{n=1}^{(p-1)/2} \\frac{1}{n(p - n)}$$ $$ \\equiv -\\sum_{n=1}^{p-1} \\frac{1}{n^2} \\equiv - \\sum_{n=1}^{p-1} n^2 \\equiv -\\frac{(p-1)p(2p - 1)}{6}$$\nwhich appears in this article by Christian Aebi and Grant Cairns.\nA more hands-on approach can be found in these notes by Timothy Choi.\nFinally, an excellent survey and additional results here by Romeo Mestrovic.\n",
    "tags": [
      "elementary-number-theory",
      "proof-writing",
      "prime-numbers",
      "divisibility"
    ],
    "score": 11,
    "answer_score": 13,
    "is_accepted": true,
    "question_id": 325491,
    "answer_id": 325524
  },
  {
    "theorem": "Prove every maximal ideal of $A$ is a prime ideal (Hint: Use the fact that $J$ is a maximal ideal iff $A/J$ is a field.)",
    "context": "\nLet $A$ be a commutative ring with unity. Prove: Proof every maximal ideal of $A$ is a prime ideal (Hint: Use the fact that $J$ is a maximal ideal iff $A/J$ is a field.)\n\nIn the question before this I proved that $J$ is a prime ideal iff $A/J$ is an integral domain.  Now, I have what I think is a \"pseudoproof\" and as such, am not satisfied.\n$\\rightarrow$ Let $J$ denote an arbitrary maximal ideal of $A$.  Since $J$ is a maximal ideal of $A$, $A/J$ is a field.  Because every field is an integral domain, $A/J$ is an integral domain.  Since $A/J$ is an integral domain, $J$ is a prime ideal.  Thus, every maximal ideal $J$ of $A$ is a prime ideal.\nIs there another way to prove this directly?\n",
    "proof": "You could always just use the obvious elementary proof, if someone forced you to.\nSuppose $ab\\in M$ and $a\\notin M$. Then $(a,M)=R$, so $1=ax+m$ for some $x\\in R$, $m\\in M$.  ($(a,M)$ denotes the ideal generated by $a$ and $M$, which is equal to the ideal $aR+M$.)\nThis yields $b=abx+bm\\in M$.\n",
    "tags": [
      "abstract-algebra",
      "ring-theory",
      "proof-writing",
      "alternative-proof"
    ],
    "score": 11,
    "answer_score": 20,
    "is_accepted": false,
    "question_id": 1047603,
    "answer_id": 1047611
  },
  {
    "theorem": "Every finite set contains its supremum: proof improvement.",
    "context": "\nEvery finite subset of $\\mathbb R$ contains its supremum (and its infimum)\n\nProof Let $A=\\{a_1,...,a_n\\}$ be a finite subset of $\\mathbb{R}$. Since it is non-empty and it is bounded ($\\max A$ is an upper bound), it has supremum, that is $\\exists \\sup A$ and by definition $\\forall a \\in A \\;\\, a \\leq \\sup A$. Let's suppose that $\\sup A  \\not\\in A$ then, since $\\max A \\in A$ we have that $\\max A < \\sup A$. But considering that $\\mathbb Q$ is dense in $\\mathbb R$ we can conclude that $\\exists r \\in \\mathbb Q$ s.t. $\\max A < r < \\sup A$, but this is absurd since $r$ is an upper bound of $A$ and it is lower than the supremum. Necessarily, $\\sup A\\in A$.\nIs there anything wrong? Is there any way to prove this without using density of $\\mathbb Q$ or another property? Thanks in advance.\n",
    "proof": "Your proof starts right away with using $\\max A$. But if you know the existence of $\\max$ then it is automatically also the $\\sup$. So this is probably not the way you are expected to proceed.\nIf $A=\\{a\\}$ is a singleton set, then clearly $\\sup A=\\max A = a$.\nIf $A$ has cardinality $n>1$ and we know as induction hypothesis that all sets of cardinality $<n$ have a maximal element, let $a\\in A$ be an arbitrary element and let $A'=A\\setminus\\{a\\}$.\nSince $A'$ has less than $n$ elements, let $a'= \\max A'$.\nIf $a'\\ge a$, then $a'$ is a maximal element of $A$.\nIf $a'< a$, then $a$ is a maximal element of $A$. \n",
    "tags": [
      "real-analysis",
      "proof-writing"
    ],
    "score": 11,
    "answer_score": 24,
    "is_accepted": true,
    "question_id": 259893,
    "answer_id": 259904
  },
  {
    "theorem": "Prove that there is only one unique base b representation of any natural number.",
    "context": "I have been asked to prove that for any integer base $b \\geqslant 2$, every natural number has a unique base $b$ representation. I am not sure if this has been answered somewhere already, but I could not find a general answer that can be applied to any base. \nWould the division algorithm apply here? I know that it would serve to give every number a unique quotient and remainder, but I'm not sure how exactly to craft the proof. I appreciate any input, thanks in advance!! \n",
    "proof": "Let $N$ be a natural number. You want to write it as\n$$\nN = a_{0} + a_{1} b + a_{2} b^{2} + \\dots + a_{k} b^{k}\n$$\nfor a suitable $k$, and $0 \\le a_{i} < b$.\nNow rewrite the above as\n$$\n\\begin{cases}\nN = a_{0} + q b,\\\\\n0 \\le a_{0} < b\n\\end{cases}$$\nto see that $a_{0}$ is uniquely determined as the remainder of the division of $N$ by $b$.\nNow consider\n$$\nq = a_{1} + a_{2} b + \\dots + a_{k} b^{k-1}\n$$\nand repeat, that is, use induction.\n",
    "tags": [
      "elementary-number-theory",
      "proof-writing",
      "modular-arithmetic"
    ],
    "score": 11,
    "answer_score": 14,
    "is_accepted": true,
    "question_id": 2174546,
    "answer_id": 2174564
  },
  {
    "theorem": "Center of the Quaternions: Proof and Method",
    "context": "I have to calculate the center of the real quaternions, $\\mathbb{H}$.  \nSo, I assumed two real quaternions, $q_n=a_n+b_ni+c_nj+d_nk$ and computed their products.  I assume since we are dealing with rings, that to check was to check their commutative product under multiplication.  So i'm looking at $q_1q_2=q_2q_1$.  When I do this, I find that clearly the constant terms are identical, so it is clear that the subset $\\mathbb{R}$ is in the center.  So, perhaps then that $\\mathbb{C}\\le\\mathbb{H}$.  However i ended up, after direct calculation with the following system;\n$$c_1d_2=c_2d_1$$\n$$b_1d_2=b_2d_1$$\n$$b_1c_2=b_2c_1$$\nSo the determination is then found by solving this system.  Intuitively, I felt that this lead to $0$'s everywhere and thus the center of $\\mathbb{H}$, $Z(\\mathbb{H})=\\mathbb{R}$.  I then checked online for some confirmation and indeed it seemed to validate my result.  However, the proof method used is something I haven't seen.  It was pretty straight forward and understandable, but again, I've never seen it.  It goes like this;\nSuppose $b_1,c_1,$ and $d_1$ are arbitrary real coefficients and $b_2, c_2,$ and $d_2$ are fixed.  Considering the first equation, assume that $d_1=1$ (since it is arbitrary, it's value can be any real...).  This leads to \n$$c_1=\\frac{c_2}{d_2}$$\nAnd that this  is a contradiction, since $c_1$ is no longer arbitrary (it depends on $c_2$ and $d_2$)\nI really like this proof method, although it is unfamiliar to me.  I said earlier that for my own understanding, it seemed intuitively obvious, but that is obviously not proof:\n1) What are some other proof methods for solving this system other than the method of contradiction used below?  I was struggling with this and I feel I sholnd't be. \n2) What other proofs can be found in elementary undergraduate courses that use this method of \"assume arbitrary stuff\", and \"fix some other stuff\" and get a contradiction?  I found this method very clean and fun, but have never seen it used (as far as I know) in any elementary undergraduate courses thus far...\n",
    "proof": "I am not sure where the contradiction lies exactly in your proof by contradiction. But here is another method.\nAn element $x\\in \\mathbb H$ belongs to the center if and only if $[x,y]=0$ for all $y\\in \\mathbb H$, where $[x,y]=xy-yx$ denotes the commutator of two elements.\nWe see immediately that $[x,1]=0$, whereas if $x=a+bi+cj+dk$ we have\n$$\n[x,i]=-2ck+2dj.\n$$\nThus $[x,i]=0$ if and only if $c=d=0$. Similarly $[x,j]=0$ if and only if $b=d=0$. Thus the only elements $x$ which commute with both $i$ and $j$ are $x\\in \\mathbb R$; in particular, it follows that $Z(\\mathbb H)\\subset \\mathbb R$. Since it is clear that $\\mathbb R\\subset Z(\\mathbb H)$, the result follows.\nIdea behind the proof: There are three special copies of the complex numbers sitting inside $\\mathbb H$: the subspaces\n$$\n\\mathbb C_i=\\mathbb R[i],\\qquad \\mathbb C_j=\\mathbb R[j],\\qquad \\mathbb C_k=\\mathbb R[k].\n$$\nOver $\\mathbb H$, all of these subspaces are their own centers: $Z_{\\mathbb H}(\\mathbb C_i)=\\mathbb C_i$ and so forth. Since $$\\mathbb H=\\mathbb C_i+ \\mathbb C_j+ \\mathbb C_k,$$\nit follows that $Z(\\mathbb H)=Z(\\mathbb C_i)\\cap Z(\\mathbb C_j)\\cap Z(\\mathbb C_k)=\\mathbb R$.\n",
    "tags": [
      "abstract-algebra",
      "ring-theory",
      "proof-writing",
      "proof-explanation"
    ],
    "score": 11,
    "answer_score": 15,
    "is_accepted": true,
    "question_id": 1595738,
    "answer_id": 1595752
  },
  {
    "theorem": "Proof that there are infinitely many primes congruent to 3 modulo 4",
    "context": "I'm having difficult proving this.\nAs a hint the exercise to prove first, that if $a\\lneqq \\pm 1$ satisfies $a \\equiv 3 \\pmod4$, then exist $p$ prime, $p \\equiv 3 \\pmod 4$ such $p\\mid4$. But I'm not really getting for what purpose can this be used.\n",
    "proof": "If there are only finitely many primes $\\equiv 3 \\pmod 4$, take the product of them and denote that product by $a$. Now look at $2a + 1$, and try to deduce a contradiction.\n",
    "tags": [
      "elementary-number-theory",
      "prime-numbers",
      "proof-writing",
      "congruences"
    ],
    "score": 11,
    "answer_score": 12,
    "is_accepted": true,
    "question_id": 671440,
    "answer_id": 671491
  },
  {
    "theorem": "Function is continuous if and only if preimages of elements of a subbase are open.",
    "context": "I recently started studying Topology, and functions defined from one topological space to another.\nI found on Wikipedia, where I was reading about subbase, that 'continuity of a function need only be checked on a subbase of the range.'\nThat is, if $\\mathcal B$ is a subbase for $Y$, a function  $f  : X \\to Y$ is continuous if and only if  $f^{−1}(U)$ is open in $X$ for each $U \\in\\mathcal B$.\nI tried to prove it by taking an open set in $Y$ and showing that its inverse is also open if $f$ is continuous, after that I studied the structure of $O$ and found that each $O$ is in fact union of finite intersections of elements of $\\mathcal B$, and I got stuck, how to show that the inverse of unions of finite intersections of elements of $\\mathcal B$ to be open?\nMay be it's very easy,but it's not striking my head, hope to get a help.\nThank you.\n",
    "proof": "You use the fact that, for a family of sets $(A_i\\,:\\,i\\in I)$, $$f^{-1}\\left(\\bigcap_{i\\in I}A_i\\right)=\\bigcap_{i\\in I} f^{-1}(A_i)\\\\ f^{-1}\\left(\\bigcup_{i\\in I}A_i\\right)=\\bigcup_{i\\in I} f^{-1}(A_i)$$\nLet's prove the first one. \\begin{align}x\\in f^{-1}\\left(\\bigcap_{i\\in I}A_i\\right)&\\iff f(x)\\in\\bigcap_{i\\in I}A_i\\iff \\forall i\\in I,\\ f(x)\\in A_i\\\\&\\iff \\forall i\\in I,\\ x\\in f^{-1}(A_i)\\iff x\\in \\bigcap_{i\\in I} f^{-1}(A_i)\\end{align}\n The other one is similar. Once you know that, you're done: preimage of an open set is union of finite intersections of preimages of elements of the subbasis, thus union of finite intersections of open sets, thus open.\n",
    "tags": [
      "general-topology",
      "proof-writing"
    ],
    "score": 11,
    "answer_score": 11,
    "is_accepted": true,
    "question_id": 2190471,
    "answer_id": 2190480
  },
  {
    "theorem": "role of definitions in proofs",
    "context": "Definitions are needed to define objects and such, however I am confused as to where definitions come from. I feel that they cannot be something that we arbitrarily define because simply saying something exists does not ensure its existence and further defining something from nothing may eventually lead to a contradiction. So it seems a strange situation that we need definitions to have something to work with and prove stuff about, and yet these definitions surely need to go thru a process of proof themselves. Can someone explain what definitions are in the most primitive sense and where they come from? \nEdit: \nlet me elaborate on my confusion, I have been reading up on different logic books and somewhere came upon a definition of proof as  \n\n\"Let T be a set of first order formulas. A proof from (proper axioms) T is a finite sequence of formulas (\"steps\") such that every step is either a logical axiom, a member of T, or the result of applying a rule of inference to previous steps in the proof.\"\n  -a proper axiom set is for example set theory axioms.\n\nNow from this way of defining proof, it seems like the only definitions that can be used are those of the proper axioms.\nAs per Henning Makholm's request, lets talk about the definition of a function. Where does the $(\\forall x(x\\in A\\implies\\exists! y((x,y)\\in f))$ come from? in regards to the definition just given of proof, it is not a logical axiom nor a proper axiom. Is this something that is always assumed and therefore added to a premise in any proof?\nEdit2:\nI think Alfred yerger answered this best in his comment.I'm not saying anyone is wrong I just got the connection after reading his comment. This makes most sense when I think about it in the context of the definition of proof I provided. Definitions are shorthand for concepts we wish to define and prove about and this comes in 2 possible ways, in the hypothesis (e.g. 'let f be a function such that...') or is the property which we try and prove some particular object has (e.g. 'show f=... is a function') in this way, the definition does not claim something exists. That happens during the hypothesis of a proof as was noted by yerger. Sorry if I was being too vague when first posing this question. I will award the answer to someone who could maybe flesh this answer out a little more and maybe provide some good special cases or instances of ways definitions come about being defined. Thank you all for the contributions and I really wish I could award more than 1 point.\n",
    "proof": "Definitions are just shorthand.\nFor example \"$f$ is a function from $A$ to $B$\" is shorthand, in set theory, for $$f\\subseteq A\\times B \\land \\left(\\forall x(x\\in A\\implies\\exists_1 y((x,y)\\in f))\\right)$$\nAnd here, even $\\exists_1$ is a shorthand. \nDefinitions are a way to avoid writing the same thing over and over again.\nAnother example: Saying \"$p,q$ are prime\" would be really cumbersome if we couldn't define \"prime.\" But we can say it without the definition. \nDefinitions do two things: They shorten language, and they clarify what concepts are important. As a rule, we name things if we want to talk about them for more than a brief moment, because it makes our discussions so much clearer.\n",
    "tags": [
      "logic",
      "proof-writing",
      "proof-theory"
    ],
    "score": 11,
    "answer_score": 8,
    "is_accepted": true,
    "question_id": 1253603,
    "answer_id": 1253622
  },
  {
    "theorem": "Doubly stochastic matrix proof",
    "context": "\nA transition matrix $P$ is said to be doubly stochastic if the sum\n  over each column equals one, that is $\\sum_i P_{ij}=1\\space\\forall i$.\n  If such a chain is irreducible and aperiodic and consists of $M+1$\n  states $0,1,\\dots,M$ show that the limiting probabilities are given by\n  $$\\pi_j=\\frac{1}{1+M},j=0,1,\\dots,M$$\n\nI have no idea how to prove it but\n\nIf a chain is irreducible then all states communicate i.e $$P_{ij}>0\\space \\text{and}\\space P_{ji}>0\\space\\forall i,j$$\nIf $d$ denotes the period of any state, if a chain is irreducible aperiodic, then $d(i)=1\\forall i$\n\nIf $P_{(M+1)\\times (M+1)}$ matrix and $\\pi$ is the stationary distribution \n$$\\pi_j=\\sum_iP_{ij}\\pi_i\\space j=0,1,\\dots,M+1$$\nbut how I can get this expression? \n",
    "proof": "Proof:\nWe first must note that $\\pi_j$ is the unique solution to $\\pi_j=\\sum \\limits_{i=0} \\pi_i P_{ij}$ and $\\sum \\limits_{i=0}\\pi_i=1$.\nLet's use $\\pi_i=1$. From the double stochastic nature of the matrix, we have  $$\\pi_j=\\sum_{i=0}^M \\pi_iP_{ij}=\\sum_{i=0}^M P_{ij}=1$$\nHence, $\\pi_i=1$ is a valid solution to the first set of equations, and to make it a solution to the second we must normalize it by dividing by $M+1$.\nThen by uniqueness as mentioned above, $\\pi_j=\\dfrac{1}{M+1}$.\n$$ \\blacksquare$$\n\nNote : To understand this proof, one must recall the definition of a stationary distribution. \nA vector $\\mathbf{\\pi}$ is called a stationary distribution vector of a Markov process if the elements of $\\mathbf{\\pi}$ satisfy:\n$$\n \\mathbf{\\pi} = \\mathbf{\\pi} \\cdot \\mathbf{P}, \\sum_{i \\in S} \\pi_{i} = 1 \\text{ ,  and } \\pi_{i} > 0\\text{  }\\forall \\text{  } i \\in S\n$$\nNote that a stationary distribution may not exist, and may not be unique.\n",
    "tags": [
      "stochastic-processes",
      "proof-writing",
      "self-learning",
      "markov-chains"
    ],
    "score": 11,
    "answer_score": 10,
    "is_accepted": true,
    "question_id": 1361358,
    "answer_id": 1361365
  },
  {
    "theorem": "Prove $ x^n-1=(x-1)(x^{n-1}+x^{n-2}+...+x+1)$",
    "context": "So what I am trying to prove is for any real number x and natural number n, prove $$x^n-1=(x-1)(x^{n-1}+x^{n-2}+...+x+1)$$ \nI think that to prove this I should use induction, however I am a bit stuck with how to implement my induction hypothesis. My base case is when $n=2$ we have on the left side of the equation $x^2-1$ and on the right side: $(x-1)(x+1)$ which when distributed is $x^2-1$. So my base case holds. \nNow I assume that $x^n-1=(x-1)(x^{n-1}+x^{n-2}+...+x+1)$ for some $n$. However, this is where I am stuck. Am I trying to show $x^{n+1}-1=(x-1)(x^n + x^{n-1}+x^{n-2}+...+x+1)$? I am still a novice when it comes to these induction proofs. Thanks\n",
    "proof": "To conclude your induction proof, just multiply x both sides :\n$x^n-1=(x-1)(x^{n-1}+x^{n-2}+...+x+1) $\nmultiply $x$ both sides :\n$\\begin{align} \\\\ x^{n+1}-x &=(x-1)(x^n+x^{n-1}+x^{n-2}+...+x^2+x) \\\\ \nx^{n+1}-1 -(x-1) &=(x-1)(x^n+x^{n-1}+x^{n-2}+...+x^2+x) \\\\ x^{n+1}-1  &=(x-1)(x^n+x^{n-1}+x^{n-2}+...+x^2+x)+(x-1) \\\\ \\end{align}$\nfactor $(x-1)$ and you're done !\n",
    "tags": [
      "elementary-number-theory",
      "proof-writing"
    ],
    "score": 11,
    "answer_score": 15,
    "is_accepted": true,
    "question_id": 900869,
    "answer_id": 900875
  },
  {
    "theorem": "Being too pedantic with writing proofs",
    "context": "A little background:\nAlmost two months ago I started to seriously self-study mathematics and so I searched the web for the best first book to expose myself. I found the following invaluable resources:\nhttp://www.stumblingrobot.com/best-math-books/\nhttps://hbpms.blogspot.com/\nand based on the above websites I decided to go with Velleman's How to Prove It. This was the first time that I was seeing proofs.\nMy problem:\nFrom my experience on this website it seems that my proofs are too pedantic or wordy. But in the above mentioned book it seems that the author emphasizes such proofs. So I am really confused!\nHere are some of the examples that I was told my proofs were too pedantic:\nSuppose $\\{A_i | i ∈ I\\}$ is an indexed family of sets and $I \\neq \\emptyset$. Prove that $\\bigcap_{i\\in I}A_i\\in\\bigcap_{i\\in I}\\mathscr P(A_i)$.\nSuppose $A$, $B$, and $C$ are sets. Prove that $C\\subseteq A\\Delta B$ iff $C\\subseteq A\\cup B$ and $A\\cap B\\cap C=\\emptyset$.\nProve that for any family of sets $\\mathcal F$, $\\bigcup!\\mathcal F=\\bigcup\\mathcal F$ iff $\\mathcal F$ is pairwise disjoint.\nIn the second question the answer by halrankard really opened my eyes to a whole new world. That I should try to work at the level of sets. From then on I tried to do exactly that but sometimes I really have a hard time doing it or simply I cannot see it.\nIn the third question as in many others the answer by Brian M. Scott helped me to see how I was wordy about a certain problem but in general whenever I try to prove statements from the above mentioned book my proofs automatically become too pedantic. I simply do not know which parts of my proofs are redundant.\nHow can I fix this problem? Is it too soon to fix this problem? Does everybody experience such a problem when they are at the beginning of the road?\nThanks for your attention.\nEdit:\nI was going to accept the answer by Mike but since the answer by CogitoErgoCogitoSum was controversial I decided to put a bounty on my question to see more perspectives.\n",
    "proof": "I've looked at your three proofs, but I only analyzed the first one very closely (since the answerers of the last two provided detailed comments on your proofs). I've added an answer to the first question.\nYou'll notice that in my answer I use a very common lemma: $B \\subseteq \\cap_{i \\in I} A_i$ iff $\\forall i \\in I \\ B \\subseteq A_i.$\nEveryone learns these kinds of lemmas eventually, usually from reading proofs that use them. Sometimes one discovers them on one's own, but this usually ultimately depends on inspiration from encountering broadly similar arguments in other people's proofs first.\nI'm not very familiar with Velleman's book, but from looking at some of it casually, it seems that most of the arguments presented go back to the level of elements rather than using any kind of higher-level lemmas on sets. So you can hardly be blamed for reproducing the same style of proof the author uses.\nYour proofs will naturally become more sophisticated when you start reading more sophisticated mathematics. In the meantime, you're doing the right thing by breaking things down so that you understand every detail of a proof. That's the main thing.\nAnother way you can improve your proofs is by selecting textbooks or problem books with full solutions. That way you can compare your solution with the book's. You seem to have the discipline to do things on your own before looking at a solution, so this is likely to be a help to you, not a hindrance.\n",
    "tags": [
      "proof-writing",
      "soft-question",
      "self-learning"
    ],
    "score": 11,
    "answer_score": 7,
    "is_accepted": true,
    "question_id": 3772628,
    "answer_id": 3773966
  },
  {
    "theorem": "Please verify my proof of: There is no integer $\\geq2$ sum of squares of whose digits equal the integer itself.",
    "context": "While going to sleep, I just started thinking about numbers, their squares, cubes and after thinking for about $20$ minutes I got that:\n\nThere is no integer (having any numbers of digits) except $0$ and $1$, sum of squares of whose digits is equal to number itself.\n\nIsn't it interesting ??\nNow I came out blanket and started writing everything on paper (Or started finding a proof of what I have got).\nMY WORK\nSuppose there exist an integer $a_0a_1a_2.......a_{n-1}$  having n digits.\nFirstly, we can easily exclude negative integers from the race as the number will be negative and sum of squares will be positive (we can never equate negative and positive).\nNow comes the turn of non negative integers.\nAs we have assume that there exist an integer which satisfy our condition so, it should yield us:\n$$10^{n-1}a_0+10^{n-2}a_1+.........10a_{n-2}+a_{n-1}={a_0}^2+{a_1}^2+.......{a_{n-2}}^2+{a_{n-1}}^2$$\nWhich on further solving becomes,\n$$a_0(10^{n-1}-a_0)+a_1(10^{n-2}-a_1)+.........+a_{n-2}(10-a_{n-2})+a_{n-1}(1-a_{n-1})=0$$\n$$a_0(10^{n-1}-a_0)+a_1(10^{n-2}-a_1)+.........+a_{n-2}(10-a_{n-2})=a_{n-1}(a_{n-1}-1)$$\nNow, the hardest part for me comes:\nThe terms of left are all positive and they increase as we keep on going towards left.\nWe can only compare the terms $$a_{n-1}(a_{n-1}-1)$$ and $$a_{n-2}(10-a_{n-2})$$ because the shortest of other terms is too far from comparison.\nNow if we make some comparison, we will have $$a_{n-1}(a_{n-1}-1)=a_{n-2}(10-a_{n-2})$$\nNow let's form a table for the function on RHS:\n$$\\begin{array}{c|c}\na_{n-2}&            a_{n-2}(10-a_{n-2})\\\\\\hline\n0&                         0\\\\\n1&                         9\\\\\n2&                         16\\\\\n3&                         21\\\\\n4&                         24\\\\                         \n5&                         25\\\\\n6&                         24\\\\\n7&                         21\\\\\n8&                         16\\\\\n9&                         9\n\\end{array}$$\nAgain, we form a table and this time for the function on LHS:\n$$\\begin{array}{c|c}\na_{n-1}&                       a_{n-1}(a_{n-1}-1)\\\\\\hline\n0&                                  0\\\\\n1&                                  0\\\\\n2&                                  2\\\\\n3&                                  6\\\\\n4&                                 12\\\\\n5&                                 20\\\\\n6&                                 30\\\\\n7&                                 42\\\\\n8&                                 56\\\\\n9&                                 72\n\\end{array}$$\nNow we have all possible values of  $a_{n-1}(a_{n-1}-1)$ and  $a_{n-2}(10-a_{n-2})$ and A quick look at tables yield that the only common value $a_{n-1}(a_{n-1}-1)$ and  $a_{n-2}(10-a_{n-2})$ have is $0$. \nSo the condition is followed when ($a_{n-1}$ ,$a_{n-2}$ )=($0,0$) and ($1,0$). Or $1$ and $0$ are the only numbers which follow the condition.{ There is no integer (having any numbers of digits) except $0$ and $1$, sum of squares of whose digits is equal to number itself is proved}.\nI hope you guys will understand that such questions are hard to write (specially for someone like me who is a beginer at Mathjax). So, if you have any problem in understanding, leave comment. I shall be thankful if someone can verify my proof or can give a new one (with completely different approach). You can suggest modification in my work or give suggestion that where it can be improved.Thanks\n",
    "proof": "The problem requires\n$$\n\\begin{align}\n0\n&=\\sum_{k=0}^n\\left(10^kd_k-d_k^2\\right)\\\\\n&=\\sum_{k=0}^nd_k\\left(10^k-d_k\\right)\n\\end{align}\n$$\nThe only negative term can be $d_0(1-d_0)\\ge-72$.\nFor $k\\ge2$, if $d_k\\ne0$, then $d_k\\left(10^k-d_k\\right)\\ge10^k-1\\ge99$.\nTherefore, only $d_0,d_1$ can be non-zero. So we just need to find $d_0,d_1$ so that\n$$\nd_1(10-d_1)=d_0(d_0-1)\n$$\nHowever\n$$\nd_1(10-d_1)\\in\\{0,9,16,21,24,25\\}\n$$\nwhile\n$$\nd_0(d_0-1)\\in\\{0,2,6,12,20,30,42,56,72\\}\n$$\nThe only way they can be equal is if both are $0$ and that requires $d_1=0$ and $d_0\\in\\{0,1\\}$.\nThat is, the only solutions are $0$ and $1$.\nYour proof was a bit difficult to read, but it looks similar.\n",
    "tags": [
      "number-theory",
      "elementary-number-theory",
      "proof-verification",
      "proof-writing",
      "alternative-proof"
    ],
    "score": 11,
    "answer_score": 3,
    "is_accepted": true,
    "question_id": 2031334,
    "answer_id": 2031512
  },
  {
    "theorem": "Proof of matrix norm property: submultiplicativity",
    "context": "I've been searching for the definition of the submultiplicative (I think it has multiple names from what I've seen) property in proof form. Some books define it as part of the properties that define matrix norms, and some include it as an additional property. I still haven't been able to work it out for myself or find it anywhere.\nLet $A$ and $B$ be $n\\times m$ and $m\\times l$ matrices respectively, prove that:\n$$\\begin{align}\n\\|AB\\| \\le \\|A\\|\\|B\\| \n\\end{align}$$\n",
    "proof": "If you have a norm on $K^n$ and if you define a matrix norm (the induced matrix norm) \n$$\\lVert A\\rVert=\\sup\\limits_{\\lVert x\\rVert =1}\\{\\lVert Ax\\rVert :x\\in K^n\\}$$\nthen one can readily check $$\\lVert Ax\\rVert \\leqslant \\lVert A\\rVert \\lVert x\\rVert $$\nThere is no problem that $A,B$ aren't square, as long as $AB$ makes sense. In such a case\n$$\\lVert ABx\\rVert \\leqslant \\lVert A\\rVert \\lVert Bx\\rVert \\leqslant \\lVert A\\rVert \\lVert B\\rVert  \\lVert x\\rVert $$\nYou can check Rudin's Princples Chapter 9, which has this on the very first few pages.\n",
    "tags": [
      "matrices",
      "proof-writing",
      "normed-spaces"
    ],
    "score": 11,
    "answer_score": 10,
    "is_accepted": true,
    "question_id": 487855,
    "answer_id": 487857
  },
  {
    "theorem": "Maximise $\\left( \\sum_{i=1}^{n} p_i \\cdot i \\right) - \\left( \\max_{j=1}^{n} p_j \\cdot j \\right)$ with $p$ permutation of size $n$",
    "context": "I'm trying to maximise the following value:\n$\\left( \\sum_{i=1}^{n} p_i \\cdot i \\right) - \\left( \\max_{j=1}^{n} p_j \\cdot j \\right)$\nwhere $p$ is an array consisting of $n$ distinct integers from $1$ to $n$ in arbitrary order (a permutation of $\\{1, 2, \\dots, n\\}$). I have to find an optimal permutation $p$.\n\nA brute force approach in Python can allow us detect a pattern:\nfrom __future__ import annotations  # noqa\nfrom itertools import permutations  # noqa\n\nif __name__ == \"__main__\":\n    for n in range(1, 12):\n        elements = range(1, n+1)\n        perms = permutations(elements, n)\n        best: int = 0\n        best_perm: list[int] | None = None\n        for perm in perms:\n            perm: list[int] = list(perm)\n            s = 0\n            max = 0\n            for i in range(0, n):\n                s += (i+1) * perm[i]\n                if (i+1) * perm[i] > max:\n                    max = (i+1) * perm[i]\n            s -= max\n            if s >= best:\n                best = s\n                best_perm = perm\n        print(f\"{n=}, {best_perm=}, {best=}\")\n\nOutputs:\nn=1, best_perm=[1], best=0\nn=2, best_perm=[2, 1], best=2\nn=3, best_perm=[1, 3, 2], best=7\nn=4, best_perm=[1, 4, 3, 2], best=17\nn=5, best_perm=[1, 2, 5, 4, 3], best=35\nn=6, best_perm=[1, 2, 3, 6, 5, 4], best=62\nn=7, best_perm=[1, 2, 3, 7, 6, 5, 4], best=100\nn=8, best_perm=[1, 2, 3, 4, 8, 7, 6, 5], best=152\nn=9, best_perm=[1, 2, 3, 4, 5, 9, 8, 7, 6], best=219\nn=10, best_perm=[1, 2, 3, 4, 5, 6, 10, 9, 8, 7], best=303\nn=11, best_perm=[1, 2, 3, 4, 5, 6, 7, 11, 10, 9, 8], best=406\n\nWe can notice that the numbers in the permutation \"go up\", then \"go down starting from $n$\". It makes sense (small numbers in the beginning, and we can't put the largest number at the end since there is the $\\max_{j=1}^{n} p_j \\cdot j$ term subtracted in the function we are trying to maximise). Could it potentially fail for large values of $n$? However, I have thoroughly tested all computationally-feasible cases and have not discovered a counterexample.\nI'm struggling to find a proof. This post is adapted from the Problem - C competition from codeforces that ended on the 12th of August 2023.\nEDIT: I'm looking for a proof of the pattern, but there might be a proof to find an permutation that maximises the value we are trying to maximise, which would be even more satisfying.\n",
    "proof": "This is not a full proof but I hope it provides some insight\nIf we assume the optimal solution is in form of\n$p_i = \\begin{cases}\n        i & \\text{if  }~~i=1,2,...,m\\\\\n        n+m+1-i & \\text{if}~~i=m+1,...,n\n    \\end{cases}$\nfor $i>m$, it we can show that $a_i = ip_i$ has maximum at $i=\\lceil \\frac{m+n}{2} \\rceil$. This can be shown considering the fact that $a_i = ip_i$ is a concave function of $i$ and\n$i^* = \\min_{i>m} {a_{i+1}\\le a_{i}}$.\n$a_{i+1}\\le a_i \\Rightarrow (i+1)(n+m-i)\\le i(n+m+1-i) \\Rightarrow i\\ge \\frac{n+m}{2} \\Rightarrow i^* = \\lceil \\frac{n+m}{2} \\rceil$.\nAlso it is clear that $a_i>a_j$ if $i>m$ and $j\\le m$. Therefore, $\\max_{i \\in \\{1,2,..,n\\}}{a_i} =\\lceil \\frac{n+m}{2} \\rceil (n+m+1-\\lceil \\frac{n+m}{2} \\rceil) =\\begin{cases}\n        (\\frac{n+m}{2})(\\frac{n+m}{2}+1) & \\text{if  }~~n+m~~\\text{even}\\\\\n        (\\frac{n+m+1}{2})^2 & \\text{if}~~~~n+m~~\\text{odd}\n    \\end{cases}$\nNow, we try to find the the value of $m$ that maximizes our objective function\n$f(m)=\\sum_{i=1}^m i^2 + \\sum_{i=m+1}^n a_i - a_{\\lceil \\frac{n+m}{2} \\rceil}$. We further simplify as\n$f(m)=\\sum_{i=1}^m i^2 - \\sum_{i=m+1}^n i^2+ (n+m+1)\\sum_{i=m+1}^n i  - a_{\\lceil \\frac{n+m}{2} \\rceil} = 2\\sum_{i=1}^m i^2 - \\sum_{i=1}^n i^2+ (n+m+1)\\sum_{i=m+1}^n i  - a_{\\lceil \\frac{n+m}{2} \\rceil}$\n$=\\frac{m(m+1)(2m+1)}{3}-\\frac{n(n+1)(2n+1)}{6}+\\frac{(n+m+1)^2(n-m)}{2}-a_{\\lceil \\frac{n+m}{2} \\rceil}$. It is easy to show that this function is also concave as\n$\\frac{\\partial^2 f}{\\partial m^2} = (4m+2)-(n+3m+2)-\\frac{1}{2}=-(n-m+\\frac{1}{2})<0$. So,\n$m^* = min_m {f(m+1)\\le f(m)}$.\n$f(m+1)\\le f(m) \\Rightarrow 2\\sum_{i=1}^{m+1} i^2 - \\sum_{i=1}^n i^2+ (n+m+2)\\sum_{i=m+2}^n i  - a_{\\lceil \\frac{n+m+1}{2} \\rceil} \\le 2\\sum_{i=1}^m i^2 - \\sum_{i=1}^n i^2+ (n+m+1)\\sum_{i=m+1}^n i  - a_{\\lceil \\frac{n+m}{2} \\rceil}$\n$\\Rightarrow 2(m+1)^2 - (n+m+1)(m+1) + \\sum_{i=m+2}^n i + a_{\\lceil \\frac{n+m}{2} \\rceil}-a_{\\lceil \\frac{n+m+1}{2} \\rceil}\\le 0$\n$\\Rightarrow -(n-m-1)(m+1) + \\frac{(n+m+2)(n-m-1)}{2} - \\lceil \\frac{n+m+1}{2} \\rceil\\le 0$\n$\\text{if  }~~n+m~~\\text{even} \\Rightarrow {(n-m)^2 -n+m} - 2 -n -m \\le 0 \\Rightarrow (n-m)^2 \\le 2n+2) \\rightarrow m\\ge n-\\sqrt{2n+2}$\n$\\text{if  }~~n+m~~\\text{odd} \\Rightarrow {(n-m)^2 -n+m} - 1 -n -m \\le 0 \\Rightarrow (n-m)^2 \\le 2n+1 \\rightarrow m\\ge n-\\sqrt{2n+1}$\nIt can be shown that $m^* = n- \\lfloor \\sqrt{2n+1}\\rfloor$ is the solution for any case. Hence, the optimal solution of proposed form is:\n$p_i = \\begin{cases}\n        i & \\text{if  }~~i=1,2,...,n- \\lfloor \\sqrt{2n+1}\\rfloor\\\\\n        2n- \\lfloor \\sqrt{2n+1}\\rfloor+1-i & \\text{if}~~i=n- \\lfloor \\sqrt{2n+1}\\rfloor+1,...,n\n    \\end{cases}$\n",
    "tags": [
      "combinatorics",
      "proof-writing",
      "permutations",
      "contest-math",
      "discrete-optimization"
    ],
    "score": 11,
    "answer_score": 6,
    "is_accepted": false,
    "question_id": 4751863,
    "answer_id": 4757038
  },
  {
    "theorem": "How do I know what we already know in a proof when the assumptions we can use are even more complex?",
    "context": "I think one reason why I find proofs so difficult is that I don't know what it is we're permitted to know and what we assume we don't know.\nFor example, \"proving $0x = 0$\" seems so incredibly obvious and yet I have to prove it with \"simpler\" assumptions and try to find some clever workaround:\n$$\\begin{array} {rcll} 0x &=& 0x  &\\text{  by reflexivity $x=x$}\\\\\n(0+0)x &=& 0x  &\\text{  by additive identity $x+0=x$ and... substitution?}\\\\\n0x + 0x &=& 0x  &\\text{  by distributive axiom}\\\\\n0x + 0x &=& 0x + 0  &\\text{  by additive identity again}\\\\\n0x &=&  0 &\\text{  by cancellation law}\\\\\n\\end{array}$$\nBut anyways these proofs already feel unsatisfactory to me. How do I know what \"rules\" I'm allowed to work with? For instance the distributive law being an axiom I never would have guessed in a hundred years was an axiom because it seems far more \"complicated\" than the idea of proving $0x=0$.\nIs there a general way to know what we can and cannot use in order to prove something? Why are these \"more complicated\" assumptions like distributive law (which seem to encapsulate several rules at once, like addition, multiplication, restructuring of the statement, etc) allowed?\n",
    "proof": "As others have said in the comments, you know what rules and axioms you can use because you are given or choose a particular collection. One of the most common misunderstandings I see is the thought that there is one set of rules and axioms that all mathematicians agree on. This is especially pernicious in logic but exists in other areas too. Usually what happens is a student sees one definition and assumes that that is the definition. It usually takes a while before they realize that all of these things have a variety of different but equivalent definitions, and in most cases also have genuinely different definitions. In the case of logic, just looking at the sheer number of entries under \"logic\" in the Stanford Encyclopedia of Philosophy, many of which correspond to different logics, gives an indication. And it is not comprehensive by any means.\nFor your example, all the reasoning is equational so which logic we're using is not as critical. We do need the laws for equality which could be roughly described as \"a congruence with respect to everything\". First, equality is an equivalence relation meaning it is reflexive, $x=x$; symmetric, if $x = y$ then $y = x$; and transitive, if $x = y$ and $y = z$ then $x = z$. Then what makes equality equality is the indiscernibility of indenticals which is usually expressed as a rule rather than an axiom and states: if $x = y$ and $P$ is some predicate with free variable $z$, then if $P[x/z]$ is provable so is $P[y/z]$ where $P[x/z]$ means $P$ with all free occurrences of $z$ replaced with $x$, i.e. substituting $x$ for $z$, and similarly for $P[y/z]$. (Again, there are other ways of presenting these rules and axioms. Indeed, this set is redundant...)\nThe proof of a statement like $0x=0$ is common in the theory of rings. For example, using the definition of a ring given on Wikipedia, this is a theorem but not an axiom. The aspect I mentioned before strikes here too. There are other choices you could take for the axioms of a ring, including ones where $0x=0$ is taken axiomatically. Also, the term \"ring\" is ambiguous as many authors consider \"rings without unit\" (i.e. which don't necessarily have an element that behaves like $1$). The definition Wikipedia gives is a ring with unit. These definitions are not equivalent.\nAnyway, using the definition on Wikipedia, one way to prove $0x=0$ is the following: $$\\begin{align}\n&0x-0x=0 \\tag{additive inverse}\n\\\\ \\iff & (0+0)x-0x=0 \\tag{additive identity}\n\\\\ \\iff & (0x+0x)-0x = 0 \\tag{left distributivity}\n\\\\ \\iff & 0x+(0x-0x) = 0 \\tag{additive associativity}\n\\\\ \\iff & 0x + 0 = 0 \\tag{additive inverse}\n\\\\ \\iff & 0x = 0 \\tag{additive identity}\n\\end{align}$$\nEach $\\iff$ is hiding a use of the indiscernibility of identicals. For example, the first step is: let $P$ be $zx-0x=0$, the additive identity axiom for $0$ states $0+0=0$ or, by symmetry, $0=0+0$, if $P[0/z]$ is provable, then $P[(0+0)/z]$ is provable. This gives the $\\Rightarrow$ direction, the $\\Leftarrow$ direction just uses the same equality the other way.\nSo why don't we just take $0x=0$ as an axiom. Well, we could. However, doing so wouldn't let us derive the other axioms and given the other axioms we can derive this one. Using a minimal collection of axioms makes it easier to verify if something is a ring (or a ring homomorphism). We would have to explicitly verify $0x=0$ while having it be a theorem means we can derive it once and for all for all rings. Another factor affecting the choice of axioms is also evident in Wikipedia's presentation. We often want to build our definitions in a modular fashion (which often leads to non-minimal lists of axioms). In this case, Wikipedia's definition starts with the axioms of an commutative group. That is, a ring is an commutative group and simultaneously a monoid whose \"multiplication\" distributes over the group operation. This way of presenting rings allows us to \"import\" theorems about commutative groups and monoids and apply them to rings. We could, of course, still do this if we had a different presentation of the axioms of a ring, but we'd have to derive the commutative group/monoid structure first, and this structure may not be obvious from the alternate presentation.\nIf you really want to get a visceral feel for all of this, I recommend getting familiar with a proof assistant like Agda, Coq, LEAN, or several others. I particularly recommend Agda as it puts all the gory details of the proofs right in your face. Most other proof assistants use a tactics-based approach which means you typically write \"proof scripts\" which are little programs that search for proofs for you. You don't typically see the proofs in those systems. Nevertheless, any of them will make it very apparent what it means to work with a given definition, what is and is not available at any point in time, and why structuring definitions one way versus another may be desirable. They all have pretty steep learning curves though and LEAN and Coq have better introductory material than Agda.\n",
    "tags": [
      "logic",
      "proof-writing",
      "soft-question"
    ],
    "score": 11,
    "answer_score": 7,
    "is_accepted": true,
    "question_id": 2675203,
    "answer_id": 2675361
  },
  {
    "theorem": "Pascal&#39;s Triangle and Binary Representations",
    "context": "In the article that I am currently reading, it is stated as a well-known fact that positions $2^i$ or equivalently $(n-2^i)$ in the $n^{th}$ row in Pascal's Triangle modulo $2$ spell out the binary representation of $n$:\n$$\n\\newcommand{\\red}{\\color{red}}\n\\newcommand{\\blue}{\\color{blue}}\n\\begin{array}{rrc}\n0:&0&1\\\\\n1:&1&\\red1\\ 1\\\\\n2:&10&\\blue1\\ \\red0\\ 1\\\\\n3:&11&1\\ \\blue1\\ \\red1\\ 1\\\\\n4:&100&\\red1\\ 0\\ \\blue0\\ \\red0\\ 1\\\\\n5:&101&1\\ \\red1\\ 0\\ \\blue0\\ \\red1\\ 1\\\\\n6:&110&1\\ 0\\ \\red1\\ 0\\ \\blue1\\ \\red0\\ 1\\\\\n7:&111&1\\ 1\\ 1\\ \\red1\\ 1\\ \\blue1\\ \\red1\\ 1\\\\\n8:&1000&\\blue1\\ 0\\ 0\\ 0\\ \\red0\\ 0\\ \\blue0\\ \\red0\\ 1\\\\\n9:&1001&1\\ \\blue1\\ 0\\ 0\\ 0\\ \\red0\\ 0\\ \\blue0\\ \\red1\\ 1\\\\\n10:&1010&1\\ 0\\ \\blue1\\ 0\\ 0\\ 0\\ \\red0\\ 0\\ \\blue1\\ \\red0\\ 1\n\\end{array}\n$$\nTo be more precise and/or technical, if $n$'s binary expansion is $b_t b_{t-1}\\cdots b_1 b_0$ or equivalently\n$\nn=\\sum_{i=0}^t b_i\\cdot 2^i\n$\nthen we have\n$$\nb_i=\\binom n{2^i}\\pmod 2\n$$\n\nNow I was thinking about the cleanest and simplest way to prove this result. I find the self-similarity of Sierpinski's Triangle to provide a nice visual argument, yet it fails to be simple to communicate succintly in a paper, I think.\nMy suggested proof\nThus I thought it would be simpler to consider which powers of $2$ divide respectively the numerator and denominator of\n$$\n\\binom n{2^i}=\\frac{\\prod_{s=1}^{2^i}(n-2^i+s)}{\\prod_{t=1}^{2^i} t}\n$$\nNow note that the factors of the numerator $n-2^i+s$ cover a full set of residues modulo $2^i$. Those with non-zero remainder $n-2^i+s\\equiv t$ modulo $2^i$ will have the same divisibility by $2$ as $t$ has, namely some power $2^j<2^i$ will be the maximal power of $2$ dividing both $t$ and that factor.\nExactly one factor will be divisible by $2^i$, namely the single factor $n-2^i+s$ whose binary representation ends in $i$ zeros. Now if the $i^{th}$ bit is zero then this factor will at least be divisible by $2^{i+1}$ because then it ends in at least $i+1$ zeros. If on the other hand the $i^{th}$ bit is $1$, then this factor is divisible by no higher power of $2$ than $2^i$.\nThus we see that if the $i^{th}$ bit is $1$ there is a 1:1 correspondance between the factors of the numerator and denominator with respect to their divisibility by $2$ thus resulting in an odd number. But if the $i^{th}$ bit is zero then the numerator has at least one more factor $2$ than the numerator - counted by multiplicity.\n\nQuestion: Do you have suggestions to simplify this argument or can you point me to a completely different approach making everything simpler? Perhaps there even is a clean and simple combinatorial proof?\n",
    "proof": "Another approach uses generating functions (for a similar example, see the proof of Lucas's Theorem). Let $p(x) = \\sum_{k=0}^n\\binom{n}{k}x^k$.\nIt is easy to check that for primes $p$ and nonnegative integers $k$, we have $(1+x)^{p^k}\\equiv 1 + x^{p^k}\\pmod p$.\nThen\n$$p(x) = (1+x)^n = \\prod_{i=0}^t \\left((1+x)^{2^i}\\right)^{b_i} \\equiv \\prod_{i=0}^t \\left(1+x^{2^i}\\right)^{b_i}\\pmod 2.$$\nThus $\\binom{n}{2^j}$ is congruent to the coefficient of $x^{2^j}$ in $\\prod_{i=0}^t \\left(1+x^{2^i}\\right)^{b_i}$ mod $2$.\nSince all the $b_i$ are 0 or 1, the coefficient of $x^{k}$ in $\\prod_{i=0}^t \\left(1+x^{2^i}\\right)^{b_i}$ is the number of ways to write $k=2^{i_1}+2^{i_2}+\\cdots+2^{i_m}$ for some $i_1<i_2<\\cdots<i_m$ where $b_{i_1}=b_{i_2}=\\cdots=b_{i_m} = 1$.\nSince binary representation is unique, all the coefficients of $\\prod_{i=0}^t \\left(1+x^{2^i}\\right)^{b_i}$ are 0 or 1.\nIn particular, the coefficient of $x^{2^j}$ is 1 if $b_j=1$ and 0 if $b_j=0$, so we have $b_j\\equiv \\binom{n}{2^j}\\pmod 2$.\nI believe by the same argument you can show for all primes $p$, writing $n=\\overline{b_tb_{t-1}\\dots b_0}_p$ in base $p$, we have\n$$b_j\\equiv\\binom{n}{p^j}\\pmod p. $$\nEDIT: For this problem and the problem for general $p$ you can actually can just apply Lucas's Theorem directly:\n$$\\binom{n}{p^j} \\equiv \\prod_{i=0}^t\\binom{b_i}{[i=j]}\\equiv b_j\\pmod p$$\nwhere we denote $[i=j]$ to be 1 if $i=j$ and 0 otherwise.\n",
    "tags": [
      "elementary-number-theory",
      "proof-verification",
      "proof-writing"
    ],
    "score": 11,
    "answer_score": 4,
    "is_accepted": true,
    "question_id": 1954098,
    "answer_id": 1968556
  },
  {
    "theorem": "Is there such a thing as &quot;finite&quot; induction?",
    "context": "I am not sure of the terminology that I am looking for, but I would like to use an inductive proof on the following type of structure. I have something of the form, for every $n \\geq 2$ and for any $1 \\leq d \\leq n-1$, some property $P(n, d)$ is true. I chose to do induction on $d$ since it appears to make the proof simpler (fewer cases in the case-by-case analysis for my problem). So my base-case was $P(n, 1)$, then I showed that $P(n, d-1)$ implies $P(n, d)$ for any $2 \\leq d \\leq n-1$. Typically, induction has this \"infinite domino effect\", like in the proof that $\\sum_{i=1}^n i = n(n+1)/2$, but in my proof, this is not the case as the \"domino effect\" stops at $d = n-1$. Is this an okay thing to do? Do I still call this a proof by induction? Or does it have a different name?\n",
    "proof": "If you have $n$ as a variable (not as a fixed constant like $n=30$) then I am quite sure (though I did to try to write down the details) that you can prove the full induction principle (for natural numbers) from the validity of your special induction principle.\nFrom this point of view, I would really call it a proof by induction.\nTo see that your special induction principle is a special case of the general induction principle (for natural numbers) is simple: Given a natural number $n$, just apply the general induction principle to the property\n$$``\\text{$d\\ge n$ or $P(n,d)$``}$$\n",
    "tags": [
      "proof-verification",
      "proof-writing",
      "induction"
    ],
    "score": 11,
    "answer_score": 4,
    "is_accepted": false,
    "question_id": 1807724,
    "answer_id": 3833820
  },
  {
    "theorem": "Julius K&#246;nig&#39;s proof of Schr&#246;der–Bernstein theorem",
    "context": "I found that Julius König's proof is short and simple to understand, but Wikipedia only provides a sketch and omits details. Here I present a proof with full detail.\nPlease have a check on it! Thank you so much!\nTheorem:\n\nLet $f:A \\to B$ and $g:B \\to A$ be injections. Then there exists a bijection from $A$ to $B$.\n\nProof:\n\nWithout loss of generality, we can safely assume that $A \\cap B=\\varnothing$.\nFor any $x \\in A \\cup B$, we can form a unique sequence by repeatedly applying $f$ and $g$ to go right, and $g^{-1}$ and $f^{-1}$ to go left whenever $g^{-1}(x)$ and $f^{-1}(x)$ are defined.\nSuch sequence looks like: $$\\cdots \\rightarrow f^{-1}(g^{-1}(x)) \\rightarrow g^{-1}(x) \\rightarrow x \\rightarrow f(x) \\rightarrow g(f(x)) \\rightarrow \\cdots$$\nFor any particular $x$, the sequence may terminate to the left or not, at a point when $f^{-1}$ or $g^{-1}$ is not defined. Since $f$ and $g$ are injective, each $x$ is in exactly one such sequence to within identity (if an element occurs in two sequences, all elements to the left and to the right must be the same in both. So these two sequences are identical). Therefore, the sequences form a partition of $A \\cup B$.\nCall a sequence an A-stopper if it stops at an element of $A$, or a B-stopper if it stops at an element of $B$. Otherwise, call it doubly infinite. It suffices to generate bijection for each sequence as follows.\n\nA-stopper\n\nLet $A_1$ be the set of its elements in $A$, $B_1$ be the set of its elements in $B$.\nLet $h:A_1 \\to B_1$ such that $h(a)=f(a)$ for all $a \\in A_1$.\n$h(a_1)=h(a_2) \\implies f(a_1)=f(a_2) \\implies a_1=a_2$ [Since $f$ is injective] $\\implies h$ is injective.\nFor $b \\in B_1$, there exists $x=f^{-1}(b) \\in A_1$ [If not, this sequence will stop at $b \\in B$, which contradicts to the fact that it is A-stopper). $h(x)=f(f^{-1}(b)=b \\implies h$ is surjective.\nThus $h:A_1 \\to B_1$ is bijective.\n\nB-stopper\n\nLet $A_2$ be the set of its elements in $A$, $B_2$ be the set of its elements in $B$.\nLet $k:B_2 \\to A_2$ such that $k(b)=g(b)$ for all $b \\in B_2$.\n$k(b_1)=k(b_2) \\implies g(b_1)=g(b_2) \\implies b_1=b_2$ [Since $g$ is injective] $\\implies k$ is injective.\nFor $a \\in A_2$, there exists $y=g^{-1}(a) \\in B_2$ [If not, this sequence will stop at $a \\in A$, which contradicts to the fact that it is B-stopper). $k(y)=g(g^{-1}(a)=a \\implies k$ is surjective.\nThus $k:B_2 \\to A_2$ is bijective. Then $k^{-1}:A_2 \\to B_2$ is bijective.\n\nDoubly infinite\n\nLet $A_3$ be the set of its elements in $A$, $B_3$ be the set of its elements in $B$.\nLet $t:A_3 \\to B_3$ such that $t(a)=f(a)$ for all $a \\in A_3$.\n$t(a_1)=t(a_2) \\implies f(a_1)=f(a_2) \\implies a_1=a_2$ [Since $f$ is injective] $\\implies t$ is injective.\nFor $b \\in B_3$, there exists $x=f^{-1}(b) \\in A_3$ [If not, this sequence will stop at $b \\in B$, which contradicts to the fact that it is doubly infinite). $t(x)=f(f^{-1}(b)=b \\implies t$ is surjective.\nThus $t:A_3 \\to B_3$ is bijective.\n\n",
    "proof": "There are few flaws that exists not because of a mistake, but because you were sloppy:\n\n\"...the sequences form a partition\"\n\nShould be\n\n\"...the range of the sequences form a partition\"\n\nAlso\n\n\"It suffices to generate bijection for each sequence as follows\"\n\nThis is correct but needs a proof:\n\nLet $P$ be the partition, and let $$P_A=\\{x\\in\\mathcal P(\\bigcup P)\\mid\\forall a\\in x(a\\in A)\\land(x\\subseteq Q\\subseteq\\bigcup P\\implies(\\forall b\\in Q\\setminus x(b\\notin A)))\\}\\\\P_B=\\{x\\in\\mathcal P( \\bigcup P)\\mid\\forall a\\in x(a\\in B)\\land(x\\subseteq Q\\subseteq\\bigcup P\\implies(\\forall b\\in Q\\setminus x(b\\notin B)))\\}$$ in other words: you separate the partition to 2 partitions for $A,B$.\nNow, let $f$ be bijective from $P_A$ to $P_B$ such that $f(x)=y\\iff x\\cup y\\in P$, therefore there exists bijective $F_x: x\\to f(x)$, thus $G: A\\to B$ will be bijective defined as $\\bigcup_{x\\in P_A}F_x$.\n\n\nNow, apart from this, the proof is good, but (in my opinion) it is easier to prove this theorem assuming $B\\subseteq A$ instead of $A\\cap B=\\emptyset$(Although both are good)\n",
    "tags": [
      "elementary-set-theory",
      "proof-verification",
      "proof-writing"
    ],
    "score": 11,
    "answer_score": 3,
    "is_accepted": true,
    "question_id": 2749527,
    "answer_id": 2917948
  },
  {
    "theorem": "Is there any way to systematically do all epsilon delta proofs?",
    "context": "If you want to prove that the limit of $f(x)$ as $x$ to $a$ is equal to $L$ using the epsilon-delta definition of the limit, you need to solve the inequality\n$$|f(x)-L|<\\epsilon$$\nfor $x$, getting it into the form \n$$|x-a|<\\delta$$\nfor some $\\delta$, which will in general be a function of $\\epsilon$.\nMy question is, is there some way to calculate the function $\\delta(\\epsilon)$, short of solving the inequality above using the function $f$ you have?\nIs it at least possible if $f$ is sufficiently well behaved?  Like if $f$ is differentiable, can you calculate $\\delta(\\epsilon)$ using the derivative of $f$?\nEDIT: This journal paper shows a formula for polynomials.  If $f(x) = \\sum_{n=0}^{k} a_n (x-a)^n$, then to prove that the limit of $f(x)$ as $x$ goes to $a$ equals $f(a)$, we can let $\\delta = min(1,\\frac{\\epsilon}{ \\sum_{n=1}^{k} |a_n|})$.\nCan this be generalized to Taylor series?  If $f(x) = \\sum_{n=0}^{\\infty} a_n (x-a)^n$, then can we prove that the limit of $f(x)$ as $x$ goes to $a$ equals $f(a)$ by letting $\\delta = min(1,\\frac{\\epsilon}{ \\sum_{n=1}^{\\infty} |a_n|})$ ?\n",
    "proof": "Below I deal with the power series question. I'll use your notation and assume WLOG that $a=0.$\nHere's a simple solution to the general $\\delta = \\varphi(\\epsilon)$ question that uses a different idea. Suppose the radius of convergence of the series is $r\\in (0,\\infty).$ Then\n$$f'(x) = \\sum_{n=1}^{\\infty}na_nx^{n-1},\\,\\,|x|<r.$$\nDefine $D=\\sum_{n=1}^{\\infty}n|a_n|(r/2)^{n-1}.$ Then for $|x|<r/2,$ the mean value theorem gives\n$$|f(x)-f(0)| = |f'(c_x)||x| \\le D|x|.$$\nThus $\\delta = \\min(r/2,\\epsilon/D)$ is a solution.\nNote that since $r = 1/\\limsup |a_n|^{1/n},$ we really do have a formula for $\\delta $ as a function of $\\epsilon$ that depends only on the coefficients $a_1,a_2, \\dots.$ Note also that in the case $r=\\infty,$ we can replace $r/2$ by $1$ in the above, and everything goes through. \nNow to your specific question: Does $\\delta = \\min(1,\\epsilon/(\\sum_{n=1}^{\\infty}|a_n|))$ work? The answer is yes, assuming $\\sum|a_n| < \\infty.$ \nProof: Because $\\sum|a_n| < \\infty,$ the power series defining $f$ has radius of convergence at least $1.$ Let $\\epsilon>0.$ Set $\\delta = \\min(1,\\epsilon/(\\sum_{n=1}^{\\infty}|a_n|)).$ If $|x|<\\delta,$ then \n$$|f(x)-f(0)| = |\\sum_{n=1}^{\\infty}a_nx^n|\\le \\sum_{n=1}^{\\infty}|a_n||x|^n$$ $$ = |x| \\sum_{n=1}^{\\infty}|a_n||x|^{n-1} \\le  |x| \\sum_{n=1}^{\\infty}|a_n| <\\epsilon.$$\nThis result covers all cases where the radius of convergence is greater than $1.$ But obviously the result fails if $\\sum|a_n| = \\infty.$ Here we are in the case where the radius of convergence $r$ is a number in $(0,1].$ This can be handled by scaling into the  $\\sum|a_n| < \\infty$ situation, and then scaling back. But the answer isn't as simple in this case. Since Micah's answer already covers this argument, I'll omit it here. (Note that the first method I mentioned, involving $f'(x),$ does not require this scaling argument.)\n",
    "tags": [
      "calculus",
      "real-analysis",
      "limits",
      "proof-writing",
      "epsilon-delta"
    ],
    "score": 11,
    "answer_score": 1,
    "is_accepted": true,
    "question_id": 2844201,
    "answer_id": 2850135
  },
  {
    "theorem": "Is there a standard name for this &quot;continuous induction&quot; principle?",
    "context": "I am working on a paper, and I want to prove that some statement $P(x)$ holds for every value of a parameter $x \\in [0,\\infty)$.  I plan to proceed as follows:\n\nShow that $P(0)$;\nShow that if $P(x)$ then there exists $\\epsilon > 0$ such that $P(y)$ for all $x \\le y \\le x+\\epsilon$;\nShow that if $x_n \\uparrow x$ and $P(x_n)$ for every $n$, then $P(x)$.\n\nIt will follow that $P(x)$ holds for every $x \\in [0,\\infty)$.  Is there a standard concise word or phrase for this principle?\nOf course, I could include a proof of a lemma saying that 1,2,3 implies $\\forall x P(x)$.  But that seems overly pedantic; this fact must be familiar to professional mathematicians, but I want to know what word will remind them of it.\nIt's sort of a continuous analogue of (transfinite) induction (1 is like the base case, 2 like the successor case, and 3 the case of a limit ordinal).  It is also somewhat akin to showing that a property holds for every $x$ in a connected space by showing that the set on which it holds is nonempty and clopen; indeed, if I had both upper and lower limits, that is exactly what it would be.  But I can't seem to find a phrase that fits exactly.\nAs an analogy, suppose I wanted to show that some statement $Q(n)$ holds for every natural number $n$.  I might show (1) $Q(0)$ and (2) if $Q(n)$ then $Q(n+1)$.  I would then write \"By induction, $Q(n)$ for every $n$\"; I would not bother to give a proof of the induction principle.  Basically I want to know what phrase can play the role of \"by induction\" for the principle described above.\n",
    "proof": "Yes, a standard term is “connectedness”. Namely, let $S$ be the set of all $x$ such that $P(x)$ holds. You are just proving, in this order:\n - $S$ is not empty;\n - $S$ is open [actually, open for a slightly unusual toplogy, but depending on your property $P$ it might be as easy to show that $S$ is open for the usual topology]: if $S$ contains a point $x$ then it contains some ball centered at $x$;\n - $S$ is closed: a limit of points belonging to $S$ also belongs to $S$.  \nSince $[0,+\\infty[$ is connected, any closed-and-open subset is either empty or the full set $[0,+\\infty[$. Since $S$ is not empty, it is the full set.\n",
    "tags": [
      "proof-writing",
      "induction",
      "article-writing"
    ],
    "score": 11,
    "answer_score": 2,
    "is_accepted": false,
    "question_id": 1276691,
    "answer_id": 1276951
  },
  {
    "theorem": "Proof of the Pythagorean Theorem via $\\frac{d}{dx}\\sin^2 x + \\frac{d}{dx}\\cos^2 x = 0$",
    "context": "Any one seen this proof before?\n$$\\frac{d}{dx} \\sin(x)^2=2\\cos(x)\\sin(x)$$\n$$\\frac{d}{dx} \\cos(x)^2=-2\\cos(x)\\sin(x)$$\n$$\\frac{d}{dx} \\sin(x)^2+\\frac{d}{dx} \\cos(x)^2=0$$\n$$\\sin(x)^2+\\cos(x)^2=c$$\n$$\\sin(0)^2+\\cos(0)^2=c$$\n$$1=c,$$\n$$\\sin(x)^2+\\cos(x)^2=1$$\nLet C, A, and B be the hypotinuse, opposite, and ajacent sides of a right triangle, then\n$$((C\\sin(x))^2+(C\\cos(x))^2=C^2$$\n$$A^2+B^2=C^2$$\nIs this proof valid, i.e. is the Pythagorean theorem used in defining the above trig relations? \n",
    "proof": "I think this proof is OK. \nYou can get the derivatives from the trigonometric addition formulas, which can be in turn proved without the Pythagorean theorem, but only from (other) geometry.\n",
    "tags": [
      "calculus",
      "geometry",
      "proof-writing"
    ],
    "score": 11,
    "answer_score": 2,
    "is_accepted": true,
    "question_id": 261208,
    "answer_id": 261525
  },
  {
    "theorem": "New Proof of Pythagorean Theorem (using inscribed circle)?",
    "context": "I was solving an easy problem for fun when I stumbled onto this, and was wondering if this was a correct and possibly a new proof of the Pythagorean Theorem.\nGiven right triangle $\\triangle ABC$, and side lengths $a$, $b$, and $c$.  Inscribe in  $\\triangle ABC$ a circle, which has radius $r$, and origin point $O$.  Connect $O$ to vertices $A$, $B$ and $C$, such that you form $\\overline{AO}$, $\\overline{BO}$, and $\\overline{CO}$.  This creates three trianlges: $\\triangle ABO$, $\\triangle BCO$, and $\\triangle ACO$.  Obviously the area of these three new triangles equals that of $\\triangle ABC$.  Notice that the radius, $r$, of the inscribed circle is the height of the three new triangles.  Adding the areas together, we get: $$\\frac{ar}{2}+\\frac{br}{2}+\\frac{cr}{2}=\\frac{ab}{2}$$  Solving for $r$, you get: $$r=\\frac{ab}{a+b+c}$$\nNow look at this picture:\n\nBy the property of tangential distances, we know that: $$(a-r)+(b-r)=c$$  So solving for $r$ again, we get: $$r=\\frac{a+b-c}{2}$$  Now setting the two equations equal to $r$ equal to each other and some slight algebra: \\begin{align}\n\\ \\frac{a+b-c}{2}&=\\frac{ab}{a+b+c}\n\\\\ 2ab&=a^2+ab-ac+ab+b^2-bc+ac+bc-c^2\n\\\\ 2ab&=a^2+2ab+b^2-c^2\n\\\\ c^2&=a^2+b^2\n\\end{align}\nQ.E.D.\nThoughts?\n",
    "proof": "It is important to state the axiomatic framework the OP is coming from. It appears that they are working within the confines of 'Euclidean Plane Geometry' (a.k.a. high school plane geometry), and not the $\\mathbb R \\times \\mathbb R$ Cartesian Coordinate Space.\nThe OP's proof is completely valid in that setting, and if carefully argued there is no circular reasoning.\nThe next section is just for fun.\n\nAnother title for the OP's question:\n\nNew Proof of Pythagorean Theorem (using the incenter of a triangle)?\n\n(they can erase the picture of the circle).\nThe OP's proof doesn't rely on the concept of a circle or tangential distances.\nA Theory of (tick-marked) Ray Lines could be postulated that describes the plane, and using the OP's logic, the simultaneous truth of the two equations \n$$\\tag 1 \\frac{ar}{2}+\\frac{br}{2}+\\frac{cr}{2}=\\frac{ab}{2}$$\n$$\\tag 2 (a−r)+(b−r)=c$$\ncan be argued.\nYou will find the two-dimensional Pythagorean Theorem to be true if you believe in the following:\n$\\quad \\text{The Bisection of Two Rays Emanating from the Same Point}$\n$\\quad \\text{The Perpendicular Distance From a Point to a Line}$\n$\\quad \\text{The Area of a Rectangle}$\n$\\quad \\text{Similar Triangles}$\n$\\quad \\text{The Area of a Triangle}$\nWith that under your belt, you prove the following:\n\nTheorem 1: Concurrency of Angle Bisectors of a Triangle.\n  In a triangle, the angle bisectors intersect at a point that is equidistant from the sides of the triangle; this point is called the incenter of the triangle.\n\nIf want to feel more comfortable before throwing out your Compass-and-Straightedge, please read\nHow to Bisect an Angle Using Only a Ruler\n",
    "tags": [
      "geometry",
      "proof-verification",
      "proof-writing",
      "circles"
    ],
    "score": 11,
    "answer_score": 1,
    "is_accepted": false,
    "question_id": 1853695,
    "answer_id": 3073202
  },
  {
    "theorem": "How many values of $x$ are there such that $\\sqrt{x(x+p)}$ is a positive integer for some $p$?",
    "context": "\nHow many values of x are there such that there exists positive integer solutions for S, such that  $S=\\sqrt{x(x+p)}$ where $x$ is an integer and $p$ is a prime number $>2$?\n\nThis is a problem I made and will submit to brilliant.org, but before I do that I want some advice and your viewpoints.\n\nHere is my proof:\nFirst we can say that since (p>2), it will always be odd since it a prime number.\nIn order to have (S) as positive we must have (x)  as  a perfect square. This also implies that (x+p) should also be a perfect square.\n(Proof):\nSuppose, let us assume that (x) is not a perfect square. It implies, that (x+p) also cannot be a perfect square.\n(CASE 1:)\nWhen (x) is an even number(not a perfect square), we have ((x+p)) as odd. This implies that we will always remain with the irrational number (\\sqrt{2}), since (x) will have an odd power of (2) as it is not a perfect square and (x+p) is odd. (S) thus cannot be a positive integer.\n(CASE 2:)\nSimilarly, when (x) is odd, (x+p) is even:\nWhen (x+p) is not a perfect square then we can easily conclude that we will never find a positive solution for (S) since (x) and (x+p) are in opposite parity and both are imperfect squares.\nThus we reach a contradiction.\nTherefore we must have (x=N^2) and (x+p=Y^2) where (N) and (Y) are integers other than (0).(Since we already considered the cases with 0).\nNow,\n$Y^2-N^2=p$\n$\\Rightarrow (Y+N)(Y-N)=p$\nThus, it follows that (Y+N) and (Y-N)  are factors of (p) thus they can either be equal to (1) or (p).\nOn solving we get $Y=\\frac{p+1}{2}$ and $N=\\frac{1-p}{2}$ or $N=\\frac{p-1}{2}$ depending upon what we take (Y+N) and (Y-N) as, and (N) and (Y) can easily be verified to be integers.\nThus we can conclude that, there exists a solution for which (x) and (x+p) are perfect squares.\nThere is another solution when $x=-N^2$ and $x+p=-Y^2$.\nSo there are $2$ solutions in total.\n",
    "proof": "It it valid to state that whenever $x$ is a multiple of $p$ there is no solution.\nNext, whenever $x$ is not a multiple of $p$ $gcd(x,x+p)=1$.\nAs such, it follows that if $x$ and/or $x+p$ are imperfect squares, then there exists no solutions.\nSo,\n$x=N^2$\n$x+p=Y^2$\n",
    "tags": [
      "elementary-number-theory",
      "proof-writing",
      "alternative-proof"
    ],
    "score": 11,
    "answer_score": 1,
    "is_accepted": true,
    "question_id": 414758,
    "answer_id": 414843
  },
  {
    "theorem": "Solving graph theory proofs",
    "context": "I am trying to study for an exam on graph theory and I have a few questions. How would you start a proof? For example, when I see a problem like this:\n\nLet G be a graph with n vertices where every vertex has degree at least n/2. Prove that G is\n  connected.\n\nHow would I start this off? I know how to solve this now but for problems like that, how would I start the question? And for things like, prove that a graph has a bridge, etc. Like, how do I know what to start looking for when I see a problem like that?\n",
    "proof": "I realised how old this post is after I had written my answer but figured I would post anyway since the question has been viewed many times and hopefully my answer will be helpful to others. For the record, I am about to start a PhD in graph theory.\nPractice, practice, practice. This will help you to become more familiar with which proof methods tend to work well for which kinds of problems (as in other areas of maths, often there is more than one possible method, some of which will reach the answer more quickly than others). Look at examples, practice questions.\n\nMy graph theory lecturer often advised us to start by considering \"small examples\" and look for a pattern. In your example, draw/think about some graphs with small numbers of vertices. What can we say about such graphs and does that help us make a general statement about them (i.e. why must they be connected)?\nAnother good method for proving some statements in graph theory is proof by contradiction. I often find this to be a good method to try first (and is what I used to prove your statement) since it requires me to think more about exactly what the statement is saying i.e. what does it mean for a graph to be connected?\nOther methods include proof by induction (use this with care), pigeonhole principle, division into cases, proving the contrapositive and various other proof methods used in other areas of maths.\n\nAnother possibly obvious but important starting point is to spend a moment thinking about the definitions used in the statement. In your example, start by thinking carefully about what it means for a graph to be connected and what will a graph where every vertex has degree $n/2$ look like? I will now run through my thought process in proving the given statement by contradiction.\nSuppose for a contradiction that $G$ is not connected. What does this mean? It means that there is some vertex $v$ which cannot be reached (via a path) from some vertex $u$. What does that mean (try to picture the situation)? It means that any vertex $w$ which is adjacent to $u$ also cannot reach $v$ (otherwise we could join the edge from $u$ to $w$ to the path from $w$ to $v$ to form a path from $u$ to $v$). In fact, any vertex which can be reached from $u$ cannot reach $v$. Similarly, any vertex which can be reached from $v$ cannot reach $u$. \nDenote those vertices in $G$ which can be reached from $u$ by $U$ and denote those vertices in $G$ which can be reached from $v$ by $V$. None of the vertices in $U$ can be adjacent to any vertices in $V$. Since every vertex in $G$ has degree at least $n/2$, every vertex in $U$ must be adjacent to at least half of the vertices in $G$. Similarly, every vertex in $V$ must be adjacent to at least half of the vertices in $G$. Since $U$ and $V$ each contain at most $n/2$ vertices, this is not possible: we have a contradiction.\nThe above is an extreme example and some will disagree, but I believe a few extra words and explanation can make a proof much clearer than symbols alone.\n",
    "tags": [
      "graph-theory",
      "proof-writing"
    ],
    "score": 10,
    "answer_score": 18,
    "is_accepted": false,
    "question_id": 461456,
    "answer_id": 2351664
  },
  {
    "theorem": "Proof by Contradiction: &quot;Bad Form&quot; or &quot;Finest Weapon&quot;? Reconciling Perspectives",
    "context": "G.H. Hardy famously described proof by contradiction as \"one of a mathematician's finest weapons.\" However, I've encountered claims that some schools of thought consider proof by contradiction to be a less desirable form of proof.\nI'm interested in understanding the reasoning behind both perspectives. Could someone elaborate on:\nWhy proof by contradiction is considered powerful and elegant by some, like Hardy? What are its strengths and advantages compared to other proof techniques?\nWhat are the potential criticisms or drawbacks of proof by contradiction? Why might some schools of thought prefer alternative methods?\nAre there specific areas of mathematics where proof by contradiction is more or less favored? If so, what are the reasons for this preference?\nHow do these differing perspectives impact the practice of mathematics? Do they lead to different approaches or conclusions?\nI myself think that proof by contradiction is a placeholder for advanced direct proofs in the future of that proof. For example, historically, The most famous proof of the irrationality of √2 is a proof by contradiction. It assumes √2 is rational and derives a contradiction. This proof is elegant but doesn't offer much insight into why √2 is irrational. Later mathematicians developed direct proofs of the irrationality of √2. These proofs often use properties of continued fractions, providing a more intuitive understanding of why √2 cannot be expressed as a fraction.\nI'd appreciate any insights or references that can help me reconcile these seemingly conflicting viewpoints.\n",
    "proof": "One disadvantage of proofs by contradiction is that any intermediate results you derive along the way can't be re-used elsewhere, since they are only valid under the contradiction hypothesis. For this reason, if you're writing a paper whose central argument is a proof by contradiction, it's a good idea to state and prove as many lemmas as you can \"outside\" of the contradiction argument before applying them in the main proof.\n",
    "tags": [
      "proof-writing",
      "soft-question",
      "math-history",
      "philosophy"
    ],
    "score": 10,
    "answer_score": 14,
    "is_accepted": false,
    "question_id": 4942881,
    "answer_id": 4942932
  },
  {
    "theorem": "Prove that $\\cot^2{(\\pi/7)} + \\cot^2{(2\\pi/7)} + \\cot^2{(3\\pi/7)} = 5$",
    "context": "Prove that $\\cot^2{(\\pi/7)} + \\cot^2{(2\\pi/7)} + \\cot^2{(3\\pi/7)} = 5$ .\nI am sure this is derived from using roots of unity and Euler's complex number function, but I am very uncomfortable in these areas so some help would be great. It is evident that $(a + b + c)^2 - 2(ab + ac + bc) = a^2 + b^2 + c^2$ .\nSo, using a polynomial of degree 3 and the coefficients on the $x^2$ and $x$ terms will get where we need to be.\n",
    "proof": "First of all, by noticing $\\cot^2(\\pi - x) = \\cot^2(x)$, we can write this identity as\n$$\\sum_{k=1,3,5} \\cot^2(\\frac{2\\pi k}{14}) = 5$$\nBy writing $\\cot^2(x) = \\frac{1}{\\sin^2(x)} - 1$, and using symmetries of $\\cos$ and $\\sin$ ($\\cos(x)=\\cos(-x)$, $\\sin(\\frac{\\pi}{2}-x)=\\cos x$), we can write this sum as follows:\n$$\\sum_{k=1,3,5} \\frac{1}{\\cos^2(\\frac{\\pi k}{14})} = 8$$\nIf we let $a_i = \\cos(\\frac{\\pi (2i-1)}{14}), i=1,2,3$, we can write this expression as\n$$(*) \\frac{(\\sum_{i<j} a_i a_j)^2 - 2\\prod a_i \\sum a_i}{(\\prod a_i)^2}$$\nThe 7'th Chebyshev Polynomial (of the first kind) vanishes exactly on $\\cos(\\frac{2k-1}{14}\\pi)$, $1\\le k \\le 7$. Those roots are actually $\\pm (a_1, a_2, a_3)$ and $0$, each is a simple root.\nWe can compute the polynomial recursively and find that it equals \n$$T_7(x) = 64x^7-112x^5+56x^3-7x=x(64x^6-112x^4+56x^2-7)$$\nWe'll work with $P_7(x)=\\frac{T_7(x)}{64x}$, a monic polynomial with roots $\\pm(a_1,a_2,a_3)$.\nThis shows, by using Vieta and the symmetry of roots (it requires some manipulation on symmetric polynomials):\n\n$(\\prod a_i)^2=\\frac{7}{64}$ (by considering coefficient of $x^0$)\n$(\\sum_{i<j} a_i a_j)^2 - 2\\prod a_i \\sum a_i = \\frac{56}{64}$ (by considering coefficient of $x^2$ - this one required some computation)\n\nSo the sum $(*)$ equals $\\frac{56}{64} / \\frac{7}{64} = 8$, which implies your identity. $\\blacksquare$\nEDIT: I'll describe some of the philosophy behind the answer.\nThe first half - I knew I wanted to you Chebyshev polynomials in some way (because its roots are related to the expression), so I did basic manipulations that helped me use the coefficients of the Chebyshev polynomial. I didn't know apriori that there are any 'good' manipulations, but I hoped and it indeed worked out.\nThe second half - What I really wanted is a polynomial $Q(x)$  whose roots are  $a_1,a_2,a_3$. Unfortunately, I had managed only to construct the polynomial $P_7(x)$ which equals $-Q(x)Q(-x)$. Fortunately, the coefficients of $P_7$ encode enough information about the coefficients of $Q$. Explicitly, by comparing coefficients:\n$$P_7[X^k] = \\sum_{i+j=k} (-1)^{1+j} Q[X^i]Q[X^j]$$\nI used this for $k=0,2$ and it was enough. $k=0$ gave $P_7(0)=-Q(0)^2$, i.e. we have the product of the $a_i$! (up to sign, but we don't even need it.)\n$k=2$ gave $P_7[X^2] = Q[X^1]^2-2Q[X^2]Q[X^0]$, which luckily was exactly the missing ingredient in calculating the rational expression $(*)$, so that's it.\nEDIT 2: I feel that I need to expand on the \"theory\" of Chebyshev polynomial, because using it might scare people away.\nThe $n$'th Chebyshev polynomial of the first kind is the unique polynomial satisfying $T_n(\\cos (\\theta)) = \\cos(n\\theta)$, for any $\\theta$. Evidently, $\\cos(\\frac{\\pi}{2n}(2k+1))$ is a root for any $k$ - just plug $\\theta = \\frac{\\pi}{2n}(2k+1))$. As $\\deg T_n = n$ (see the next paragraph), there can be no other roots.\nWhy is $T_n$ necessarily a polynomial? Well, for $n=0$ we have $T_0 = 1$, and for $n=1$ we have $T_1(x)=x$. For $n=2$ we already need some trigonometry: $\\cos(2\\theta)=2\\cos^2(\\theta)-1$, so $T_2(x)=2x^2-1$. We can define $T_n$ recursively by trigonometric insights:\n$$\\cos(\\alpha)+\\cos(\\beta)=2\\cos(\\frac{\\alpha+\\beta}{2})\\cos(\\frac{\\alpha-\\beta}{2})$$\n$$\\implies \\cos((n+1)\\theta) + \\cos((n-1)\\theta) = 2\\cos(n\\theta)\\cos(\\theta)$$\n$$\\implies T_{n+1}(x) + T_{n-1}(x) = 2T_{n}(x)x$$\nThis is how I calculated $T_7$. In practice I just used the recurrence relation $T_{n+1}(x) = 2T_{n}(x)x-T_{n-1}$ and the table here.\nThere are some shortcuts, since the leading coefficient of $T_n$ is $2^{n-1}$ and the last coefficient is $0$ when $n$ is odd. \n",
    "tags": [
      "trigonometry",
      "proof-writing",
      "roots-of-unity"
    ],
    "score": 10,
    "answer_score": 14,
    "is_accepted": true,
    "question_id": 265229,
    "answer_id": 265274
  },
  {
    "theorem": "Is collapsing considered a legitimate proof?",
    "context": "For example if I want to prove that $2^n - 1 = 1 + 2 + 4 + 8 +...+ 2^{n-1}$ I can obviously use induction and that is accepted. But I can also collapse it like:\nTo Prove $2^n = S(n)$:\n\n$S(n) = (1 + 1) + 2 +...+ 2^{n-1}$\n$S(n) = (2 + 2) + 4 + 8 +...+2^{n-1}$\n$S(n) = (4 + 4) + 8 +...+2^{n-1}$\n\nand so on until\n$S(n) = 2^{n-1} + 2^{n-1} = 2^n$\nIs this method of collapsing considered a legitimate and presentable proof? \n",
    "proof": "Well, sort of, but in fact, writing proofs like the one you want to write is why induction exists. Whenever people say something like \"and so on until\", they're expressing your intuition that it's possible to continue the argument by induction. The whole point of the method of induction is to make intuitions like this one precise.\nLet $S(k)=2^k + 2^k + 2^{k+1} + 2^{k+2} + ... + 2^{n-1}$. Then what we want to show is that $S(0) = 2^n$. Your proof basically amounts to saying $S(0) = S(1) = S(2) = ...$ \"and so on\", until we get $S(0) = S(n-1)$. Notice that $S(n-1) = 2^{n-1} + 2^{n-1}$, which obviously equals $2^n$. So we get $S(0) = 2^n$. To phrase this as a proof by induction, we're going to prove by induction that $S(0) = S(k)$ for all $k<n$, thus we'll obtain $S(0)=S(n-1)$ at the end.\nObviously, $S(0) = S(0)$. Now suppose $S(0) = S(k)$. Then:\n$$\\begin{align}S(0) &= S(k) \\\\&= 2^k + 2^k + 2^{k+1} + 2^{k+2} + ... + 2^{n-1} \\\\&= 2\\cdot2^k + 2^{k+1} + 2^{k+2} + ... + 2^{n-1} \\\\&= 2^{k+1} + 2^{k+1} + 2^{k+2} + ... + 2^{n-1}\\\\&=S(k+1)\\end{align}$$\n",
    "tags": [
      "sequences-and-series",
      "proof-writing"
    ],
    "score": 10,
    "answer_score": 23,
    "is_accepted": true,
    "question_id": 1598280,
    "answer_id": 1598297
  },
  {
    "theorem": "Theorems with simple proofs with one method of proof, and incredibly difficult with another.",
    "context": "This is a very soft and potentially naive question, but I've always wondered about this seemingly common phenomenon where a theorem has some method of proof which makes the statement easy to prove, but where other methods of proof are incredibly difficult.\nFor example, proving that every vector space has a basis (this may be a bad example).  This is almost always done via an existence proof with Zorn's lemma applied to the poset of linearly independent subsets ordered on set inclusion. However, if one were to suppose there exists a vector space $V$ with no basis, it seems (to me) that coming up with a contradiction given so few assumptions would be incredibly challenging.\nWith that said, I had a few questions:\n\nAre there any other examples of theorems like this?\nIs this phenomenon simply due to the logical structure of the statements themselves, or is it something deeper?  Is this something one can quantize in some way? That is, is there any formal way to study the structure of a statement, and determine which method of proof is ideal, and which is not ideal?\nWith (1) in mind, are there ever any efforts to come up with proofs of the same theorem using multiple methods for the sake of interest?\n\n",
    "proof": "Very often the first proof of a result which appears in the literature is extremely messy because the mathematician who proved it is working at the very edge of what is possible with the tools of the day; then it gets simplified over time as other mathematicians better understand what is going on and develop better machinery for streamlining the proofs. These first proofs are typically not presented to students because they are terrible, but the disadvantage of not knowing them is that you don't see how valuable the machinery that streamlines the modern proofs is.\nThere are many examples of this sort of thing, some of which you can find at this MO question; here's one that I came across while writing a blog post about the Sylow theorems. It is about\n\nCauchy's theorem: if a finite group $G$ has the property that its order $|G|$ is divisible by a prime $p$, then $G$ has an element of order $p$.\n\nThere is an extremely slick proof of this theorem which comes from considering the set of solutions to the equation\n$$\\{ (g_1, \\dots g_p) \\in G^p : g_1 g_2 \\dots g_p = e \\}$$\nand then considering the action of the cyclic group by rotation $(g_1, g_2 \\dots g_{p-1}, g_p) \\mapsto (g_2, g_3, \\dots g_p, g_1)$, which you can see in the link. It takes maybe three sentences to give.\nBy contrast, Cauchy's original proof took 9 pages. He does it by explicitly constructing the Sylow $p$-subgroups of the symmetric group, then (I believe Cauchy was working at a time when \"finite group\" always meant \"finite group of permutations\" so for him all finite groups were already embedded into symmetric groups) using a clever counting argument to show that if a finite group $G$ has the property that $p \\mid |G|$ and also embeds into another finite group which has Sylow $p$-subgroups, then $G$ has an element of order $p$; you can see the details in the link. I give a very abbreviated sketch of the proof; the full construction of the Sylow $p$-subgroups of the symmetric group is very tedious (I have never seen anyone give it in full, and tried doing it in a follow-up blog post but gave up because it was too tedious).\nThis is a good example of what I mean; Cauchy was working at a very early time in group theory before anyone had even defined an abstract group, and people just didn't understand group theory that well yet. There was not even the notion of a quotient group at the time. Once group theory was better understood better proofs were possible. Actually I have no idea who the above slick proof of Cauchy's theorem is due to nor how many decades it took after Cauchy's original proof for someone to find it.\nCauchy's original proof does have the advantage that it is much closer to being a proof of the first Sylow theorem. It has a generalization due to Frobenius which shows that if a finite group $G$ embeds into a finite group $H$ which has a Sylow $p$-subgroup, then $G$ must have a Sylow $p$-subgroup. And then you can prove Sylow I by exhibiting the Sylow $p$-subgroups of the symmetric groups, or somewhat more easily, the general linear groups $GL_n(\\mathbb{F}_p)$, then invoking Cayley's theorem.\n",
    "tags": [
      "logic",
      "proof-writing",
      "soft-question"
    ],
    "score": 10,
    "answer_score": 15,
    "is_accepted": true,
    "question_id": 4621227,
    "answer_id": 4621261
  },
  {
    "theorem": "Prove $GL_2(\\mathbb{Z}/2\\mathbb{Z})$ is isomorphic to $S_3$",
    "context": "I'm asked to show that $G=GL_2(\\Bbb Z/2\\Bbb Z)$ is isomorphic to $S_3$. I have few ideas but I don't manage to put them all together in order to obtain a satisying answer.\nI first tried using Cayley's theorem ($G$ is isomorphic to a subgroup of $S_6$), and I also noticed that $\\operatorname{Card}(G)=\\operatorname{Card}(S3)=6$ & that they're both non-abelian group.\nIs this enough to say that considering $S_3$ is a subgroup of $S_6$ with the same cardinality than $G$, it has to be isomorphic to it ? \nCould anyone give me some elements to get a more rigorous proof or lead me to an other path to show this statement ? \nThanks in advance\n",
    "proof": "If you don't know that the unique non-abelian group of order $6$ is $S_3$, you can go with a more explicit approach:\nFind an isomorphism by showing that $GL_2(\\mathbb Z/2\\mathbb Z)$ is a group of permutations of a $3$-element set, the nonzero vectors in $\\mathbb (\\mathbb Z/2\\mathbb Z)^2$. Number these vectors, then argue that every element of $GL_2$ induces a permutation of this set, which gives you a homomorphism between the two groups by sending each element of $GL_2$ to the corresponding permutation in $S_3$. Then show that this homomorphism is injective: two different elements of $GL_2$ permute the elements differently. Finally, count the number of elements in each group. Since the groups have the same order, your injective homomorphism is also surjective, and thus an isomorphism.\n",
    "tags": [
      "abstract-algebra",
      "group-theory",
      "proof-writing",
      "group-isomorphism"
    ],
    "score": 10,
    "answer_score": 16,
    "is_accepted": false,
    "question_id": 567045,
    "answer_id": 567056
  },
  {
    "theorem": "Prove that every bounded sequence in $\\Bbb{R}$ has a convergent subsequence",
    "context": "I am to prove that every bounded sequence in $\\Bbb{R}$ has a convergent subsequence. I am stuck not knowing how and where to start. \n",
    "proof": "This is the so called Bolzano-Weierstrass Theorem.\nI will prove that any sequence of real numbers has a monotone subsequence and leave the rest to you\nFirst some terminology:\nLet $\\left(a_n\\right)$ be a sequence of real numbers and $m\\in \\mathbb{N}$. We say that $m$ is a peak of the sequence $\\left(a_n\\right)$ if $$n> m\\implies a_n<a_m$$\nNow the proof:\nLet $\\left(a_n\\right)$ be a sequence of real numbers.\nSuppose that $\\left(a_n\\right)$ has an infinite number of peaks $$k_0<k_1<k_2<...<k_n<...$$ and consider the subsequence $(a_{k_n})$. Then, $(a_{k_n})$ is strictly decreasing since $$k_n>k_m\\implies a_{k_n}<a_{k_m}$$ and thus $(a_{k_n})$ is monotone.\nSuppose that $\\left(a_n\\right)$ has an finite number of peaks and let $N\\in \\mathbb{N}$ be the last (greatest) peak. Then $k_{0}=N+1$ is not a peak and so \n\\begin{equation}\\exists k_1>k_0\\in \\mathbb{N}:a_{k_{1}}>a_{k_{0}}\\end{equation}\nHaving defined $k_n$ such as that $k_n>k_{n-1}>N$ then \n\\begin{equation}\\exists k_{n+1}>k_n\\in \\mathbb{N}:a_{k_{n+1}}>a_{k_{n}}\\end{equation}\nThe subsequence  $(a_{k_n})$ is obviously increasing and so it is monotone\nNow if $(a_n)$ is in addition bounded, so is $(a_{k_n})$ and applying the monotone convergence theorem yields....\n",
    "tags": [
      "sequences-and-series",
      "limits",
      "proof-writing"
    ],
    "score": 10,
    "answer_score": 13,
    "is_accepted": true,
    "question_id": 264049,
    "answer_id": 264051
  },
  {
    "theorem": "Formally prove that every finite language is regular",
    "context": "I know how to prove this informally, but don't know what the formal proof should look like. \n",
    "proof": "One-line proof: A finite language can be accepted by a finite machine.\nDetailed construction: Suppose the language $\\mathcal L$ consists of strings $a_1 ,a_2, \\ldots, a_n$.\nConsider the following NFA to accept $\\mathcal L$: It has a start state $S$ and an accepting state $A$.  In between $S$ and $A$ there are $n$ different paths of states, one for each $a_i$.  The machine can only get from the beginning of the i'th path to the end if it sees exactly the string $a_i$. \nThere are $\\epsilon$-transitions from $S$ to the beginning of each path, and from the end of each path to $A$.\nFor example, suppose $\\mathcal L$ consists of exactly the three strings \"fish\", \"dog\", and \"carrot\".  Then the NFA looks like this:\n  .-------- f - i - s - h --.\n /                           \\\nS---- d - o - g --------------A  \n \\                           /\n  '- c - a - r - r - o - t -`\n\n",
    "tags": [
      "proof-writing",
      "regular-language"
    ],
    "score": 10,
    "answer_score": 12,
    "is_accepted": true,
    "question_id": 216047,
    "answer_id": 216075
  },
  {
    "theorem": "Prove that $\\sum_{k=0}^r {m \\choose k} {n \\choose r-k} = {m+n \\choose r}$",
    "context": "Prove that $$\\sum_{k=0}^r {m \\choose k} {n \\choose r-k} = {m+n \\choose r}.$$\nThis problem is in the chapter about discrete random variables, but I have no idea what to go about substituting.\nI can't get it to be a featured formula without screwing stuff up, sorry about that.\n",
    "proof": "Standard Binomial Coefficients\nUse the Binomial Theorem\n$$\n\\begin{align}\n(1+x)^m(1+x)^n\n&=\\sum_{k=0}^m\\binom{m}{k}x^k\\sum_{j=0}^n\\binom{n}{j}x^j\\\\\n&=\\sum_{k=0}^m\\sum_{j=0}^n\\binom{m}{k}\\binom{n}{j}x^{k+j}\\\\\n&=\\sum_{k=0}^m\\sum_{r=k}^{k+n}\\binom{m}{k}\\binom{n}{r-k}x^r&&j=r-k\\\\\n&=\\sum_{r=0}^{m+n}\\color{#C00000}{\\sum_{k=0}^r\\binom{m}{k}\\binom{n}{r-k}}x^r\\tag{1}\\\\\n(1+x)^{m+n}\n&=\\sum_{r=0}^{m+n}\\color{#C00000}{\\binom{m+n}{r}}x^r\\tag{2}\n\\end{align}\n$$\nCompare coefficients of $x^r$ in $(1)$ and $(2)$.\n\nGeneralized Binomial Coefficients\nAs Michael Hardy mentions, the formula is true, even when $m$ and $n$ are not integers. The binomial coefficients can be generalized to any real number in the top argument:\n$$\n\\binom{x}{k}=\\frac{x(x-1)(x-2)\\dots(x-k+1)}{k!}\\tag{3}\n$$\nWhen $x$ is a negative integer, $(3)$ gives the formula for the negative binomial coefficients:\n$$\n\\begin{align}\n\\binom{-n}{k}\n&=\\frac{-n(-n-1)(-n-2)\\dots(-n-k+1)}{k!}\\\\\n&=(-1)^k\\frac{(n+k-1)(n+k-2)(n+k-3)\\dots n}{k!}\\\\\n&=(-1)^k\\binom{n+k-1}{k}\\tag{4}\n\\end{align}\n$$\nThe Generalized Binomial Theorem states that for any real $m$,\n$$\n(1+x)^m=\\sum_{k=0}^\\infty\\binom{m}{k}x^k\\tag{5}\n$$\nNote that when $m$ is a non-negative integer, $\\binom{m}{k}=0$ for $k\\gt m$ and so $(5)$ is a polynomial in that case.\nUsing $(5)$, we can imitate the proof for the standard binomial coefficients:\n$$\n\\begin{align}\n(1+x)^m(1+x)^n\n&=\\sum_{k=0}^\\infty\\binom{m}{k}x^k\\sum_{j=0}^\\infty\\binom{n}{j}x^j\\\\\n&=\\sum_{k=0}^\\infty\\sum_{j=0}^\\infty\\binom{m}{k}\\binom{n}{j}x^{k+j}\\\\\n&=\\sum_{k=0}^\\infty\\sum_{r=k}^\\infty\\binom{m}{k}\\binom{n}{r-k}x^r&&j=r-k\\\\\n&=\\sum_{r=0}^\\infty\\color{#C00000}{\\sum_{k=0}^r\\binom{m}{k}\\binom{n}{r-k}}x^r\\tag{6}\\\\\n(1+x)^{m+n}\n&=\\sum_{r=0}^\\infty\\color{#C00000}{\\binom{m+n}{r}}x^r\\tag{7}\n\\end{align}\n$$\nCompare coefficients of $x^r$ in $(6)$ and $(7)$.\n",
    "tags": [
      "probability",
      "combinatorics",
      "proof-writing",
      "summation"
    ],
    "score": 10,
    "answer_score": 9,
    "is_accepted": false,
    "question_id": 502835,
    "answer_id": 503382
  },
  {
    "theorem": "Proof of a binomial identity $\\sum_{k=0}^n {n \\choose k}^{\\!2} = {2n \\choose n}.$",
    "context": "Prove that $$\\sum_{k=0}^n {n \\choose k}^{\\!2} = {2n \\choose n}.$$\nThe exercise provides the following hint: $\\,\\,\\displaystyle{n \\choose k}={n\\choose n-k}$.\nAny help?\n",
    "proof": "Following the hint given by the OP, it suffices to show that\n$$\n\\sum_{k=0}^n {n \\choose k}^{\\!2}=\\sum_{k=0}^n {n \\choose k}{n \\choose {n-k}} = {2n \\choose n}.\n$$\nThe last equality is a consequence of the following more general identity (known as Vandermonde's identity)\n$$\n\\sum_{j=0}^k\n\\binom{n-m}{k-j}\\binom{m}{j}=\\binom{n}{k},\n$$\nwhere $n\\ge m,k\\ge 0$,\nwhich in turn is just an equality of the coefficients of $x^k$ is the left and right hand side of the binomial expansions of\n$$\n(1+x)^{n-m}(1+x)^m=(1+x)^n.\n$$\n",
    "tags": [
      "combinatorics",
      "summation",
      "proof-writing",
      "binomial-coefficients",
      "taylor-expansion"
    ],
    "score": 10,
    "answer_score": 10,
    "is_accepted": true,
    "question_id": 690076,
    "answer_id": 690081
  },
  {
    "theorem": "Prove that if $2^n - 1$ is prime, then $n$ is prime for $n$ being a natural number",
    "context": "\nProve that if $2^n - 1$ is prime, then $n$ is prime for $n$ being a natural number\n\nI've looked at https://math.stackexchange.com/a/19998\n\nIt is known that $2^n-1$ can only be prime if $n$ is prime.  This is because if $jk=n$, $2^n-1=\\sum_{i=0}^{n-1}2^i$ $=\\sum_{i=0}^{j-1} 2^i \\sum_{i=0}^{k-1} 2^{ij}$.\n\nSo they will only continue to alternate at twin primes.  In particular, $2^{6k+2}-1, 2^{6k+3}-1$ and $2^{6k+4}-1$ will all be composite\nWhat I dont understand is how do I get:\n$$\\sum_{i=0}^{n-1}2^i=\\sum_{i=0}^{j-1} 2^i \\sum_{i=0}^{k-1} 2^{ij}$$\nIf I am looking at the wrong answer, how can I proof the above?\n\nThen again, maybe I am indeed looking at the wrong thing?\n\nIn particular, $2^{6k+2}-1, 2^{6k+3}-1$ and $2^{6k+4}-1$ will all be composite\n\nThis doesnt say why $2^{6k+2}-1$ is composite?\n",
    "proof": "If $2^n-1$ is prime, then $n$ cannot be $1$. We show that $n$ cannot be composite. \nSuppose to the contrary that $n=ab$, where $a$ and $b$ are greater than $1$. Then \n$$2^n-1=2^{ab}-1=(2^a)^b-1.$$\nLet $x=2^a$. Then $2^n-1=x^b-1$.\nBut \n$$x^b-1=(x-1)(x^{b-1}+x^{b-2}+\\cdots +x+1.\\tag{$1$})$$\nTo see this, just multiply out: almost everything cancels.\nNow $x=2^a \\ge 4$, so $x-1\\gt 1$. It is easy to see that $x^{b-1}+x^{b-2}+\\cdots +x+1\\gt 1$. It follows from $(1)$ that $x^b-1$, that is, $2^n-1$, is the product of two numbers that are $\\gt 1$. This contradicts the given fact that $2^n-1$ is prime.\n",
    "tags": [
      "discrete-mathematics",
      "proof-writing"
    ],
    "score": 10,
    "answer_score": 12,
    "is_accepted": false,
    "question_id": 186587,
    "answer_id": 186592
  },
  {
    "theorem": "A challenging geometry proof?",
    "context": "\nGiven: $C$ on $\\overline{AB}$ such that $BC=3AC$ and $m\\angle B=2m\\angle XCB$.\nTo show: $AX=2AC+BX$\nI have verified this result with trigonometry and analytic geometry and double-checked my work with GeoGebra.  But it seems like such an elegant result that there should be a purely geometric proof.  Any ideas?\nI was inspired to investigate this diagram trying to solve a different problem here on MSE.  As far as trying to come up with a proof on my own I tried constructing $M,N$ on $\\overline{AX}$ such that $AM=AC$ and $NX=BX$ and drawing some isosceles triangles. That might be fruitful (since you'd only have to show $AM=MN$), but nothing leapt out at me quickly.\n",
    "proof": "OP's own answer shows that the key to the solution is to recognize that point $X$ lies on a hyperbola with foci $A$ and $C$ passing through $B$. Here's a \"geometric\" derivation of that fact.\n\nLet the trisecting points of $\\overline{BC}$ be $S$ and $T$. Let the angle bisector at $C$ meet $\\overline{BX}$ at $D$, creating isosceles $\\triangle BCD$. Let $\\overleftrightarrow{DM}$ (with $M$ the midpoint of $\\overline{BC}$ be the extended altitude of this triangle, and let $P$ be the projection of $X$ onto this line. \n\nThen we have\n$$\\left.\\begin{align}\n\\text{Angle Bis. Thm} &\\implies \\frac{|CX|}{|DX|}=\\frac{|BC|}{|BD|} = \\frac{2|BM|}{|BD|} \\\\[4pt]\n\\triangle DXP\\sim\\triangle DBM &\\implies \\frac{|DX|}{|PX|}=\\frac{|BD|}{|BM|}\n\\end{align}\\right\\}\\implies\n\\frac{|CX|}{|PX|}=\\frac{|CX|}{|DX|}\\cdot\\frac{|DX|}{|PX|}=2$$\nTherefore, $\\overleftrightarrow{DM}$ is the directrix, and $C$ the focus, of a hyperbola through $X$ with eccentricity $2$. \nSince trisection point $T$ divides $\\overline{MC}$ in the ratio $1:2$, it must be a vertex of the hyperbola. Moreover, since $|ST|:|SC|=1:2$, it follows that $S$ is the center of the hyperbola. By symmetry across that center, $B$ and $A$ are the other vertex and focus, respectively, and the result follows. $\\square$\n",
    "tags": [
      "geometry",
      "proof-writing",
      "euclidean-geometry",
      "conic-sections"
    ],
    "score": 10,
    "answer_score": 13,
    "is_accepted": true,
    "question_id": 3325533,
    "answer_id": 3325613
  },
  {
    "theorem": "Prove that if a function $f$ has a jump at an interior point of the interval $[a,b]$ then it cannot be the derivative of any function.",
    "context": "Prove that if a function $f$ has a jump at an interior point of the interval $[a,b]$ then it cannot be the derivative of any function.\nI know that for $f$ is differentiable in $(a,b)$ and that it has one-sided derivative $f_+' (a)≠f_-' (b)$ at the endpoints. If $C$ is a real number between $f_+' (a)$  and $f_-' (b)$, then there exists $c∈(a,b)$ such that $f' (c)=C $. How can I use this to prove the above?\n",
    "proof": "This a standard result that derivatives don't have jump discontinuity.\nLet $c \\in (a, b)$ then $f'(c) = \\lim_{x \\to c}\\dfrac{f(x) - f(c)}{x - c}$ exists. Let us assume that limits $\\lim_{x \\to c^{+}}f'(x) = A, \\lim_{x \\to c^{-}}f'(x) = B$ exist. Now let's handle the case for $x \\to c^{+}$ first. Clearly then $x > c$ and we have $\\dfrac{f(x) - f(c)}{x - c} = f'(d)$ for some $d \\in (c, x)$. As $x \\to c^{+}$, $d \\to c^{+}$ and we get $$f'(c) = \\lim_{x \\to c^{+}}\\dfrac{f(x) - f(c)}{x - c} = \\lim_{d \\to c^{+}}f'(d) = A$$\nSimilarly by considering $x \\to c^{-}$ we can show that $B = f'(c)$ so that $A = B$ and $f'(x)$ is continuous at $c$ and therefore does not have jump discontinuity. It may happen however that one or both of the limits $A, B$ don't exist or are $\\pm\\infty$.\n\nUpdate: I am bit surprised to see that in the comments to the question people have linked this result with IVT (intermediate value theorem) for derivatives. These two properties of derivatives (IVT and no jump discontinuity) are not derivable from each other. Rather they are both derived from Mean Value Theorem in completely different ways.\nFurther Update: I had a look at the Wikipedia article dealing with Darboux theorem (IVT for derivatives). Even the wikipedia makes a mistake that any function satisfying IVT can't have jump discontinuity. This is totally unexpected from wikipedia and I don't know whom to complain for this.\nHere is a very simple example to prove my point. Let $f(0) = 0, f(1) = 1$ and $ f(x) = 1 - x$ for $x\\in (0, 1)$. This function satisfies IVT on $[0, 1]$ and is yet having jumps at the end-points.\nWhat is true is the following:\nA function $f$ which is monotone and satisfies IVT on $[a,b]$ does not have jump discontinuity and is therefore continuous in $[a, b]$\n\nEven more update: Due to the paraphrasing of the comment by copper.hat in the question I misinterpreted the Wikipedia article. According to copper.hat comment if $g(x)$ takes all values in interval $[g(a), g(b)]$ as $x$ varies in $[a, b]$ then $g(x)$ can't have jumps in $[a, b]$. This statement is wrong.\nWikipedia however has a different definition. It says that a function is Darboux function if it satisfied intermediate value property. The intermediate value property is defined as follows: let $f$ be defined on interval $I$. If for any $[a, b] \\subseteq I$ the function $f$ takes all values between $f(a)$ and $f(b)$ for some value of $x \\in (a, b)$ then it is said to have intermediate value property on $I$.\nI missed the part of any $[a, b] \\subseteq I$ and thought that intermediate value property of $f$ on an interval $[a, b]$ is supposed to mean that $f$ must take all values between $f(a)$ and $f(b)$ for some $x \\in (a, b)$. Note the subtle difference in Wikipedia version and my interpretation. Wikipedia prescribes a very strong condition where we have to check every subinterval $[a, b]$ of the domain of defintion $I$ of function $f$ whereas in my interpretation we only need to check this for $I$ and not any subintervals of $I$.\nTo put formally let Wikipedia version of IVT be called WIVT and my version be called PIVT. Then a function $f$ satisfies WIVT if it satisfies PIVT on every sub-interval of $I$. A function satisfying WIVT does not have jumps whereas a function satisfying PIVT may have jumps.\n",
    "tags": [
      "calculus",
      "proof-writing"
    ],
    "score": 10,
    "answer_score": 14,
    "is_accepted": true,
    "question_id": 563771,
    "answer_id": 577978
  },
  {
    "theorem": "Math proof vs Logic Proof.",
    "context": "I am trying to learn math \"from scratch\" and started by reading an introductory logic textbook (A Concise introduction to logic) that did a great job explaining predicate logic. Among other things, proofs, that is, syntactic string manipulation where from a set of premises P, via a number of inference rules, a new statement, aka conclusion C, can be derived.\nI am now profoundly stuck trying to make the leap to mathematical proofs. And here is where. The first introductory proof I saw looks like the following:\n\nSlide 42 from here\nBut how does this connect to predicate logic?\nThe predicate logic proofs begin with premises and here the theorem to be proven looks like the conclusion but no premises are given. Which I can understand: in propositional logic, a theorem is \"a sentence that can be derived without premises\" (ctrl + f for \"theorem\" here).\nBut where do I go from here?\nI can rewrite the theorem in predicate logic like the following\n\nBut how can I go about proving it? The techniques in the textbook call for a sequence of conditional derivations and I would guess some universal instantiation or generalizations.\nWhat am I missing to connect these 2 proofs?\nWhat is the link between a predicate logic proof and a math proof written in English?\nI see that there is a square function involved that can be thought of as a relation (a form of predicate) and Even(x) is also a predicate, but aside from that i see no clues.\nThe math proof almost feels like a different underlying language and the predicate logic is like its skeleton that can not really be used here. Is that a valid notion?\nProving in math almost feels like using rules of arithmetic to get a matching definition while proving in predicate logic is more about producing a new string from the existing ones.\nI see how this is a lot of thoughts pointing in potentially different directions  but I am hoping that someone more experienced could notice/relate to the struggle and point me in the right direction.\nThank you everyone!\nI believe my question is similar to the mathematical proof vs. first-order logic deductions but I did not find anything to answer my particular example.\n",
    "proof": "Adding to Ethan's answer and specifically addressing your questions:\n\nThe predicate logic\nproofs begin with premises, and here the theorem to be proven looks\nlike the conclusion but no premises are given. Which I can understand:\nin propositional logic, a theorem is \"a sentence that can be derived\nwithout premises\".\n\nA mathematical theorem is premised ultimately on mathematical axioms and an interpretation of its mathematical symbols. The axiom system and background toolkit (definitions, rules, laws, lemmas, other theorems) is tacit. This is in contrast to pure theorems, which are logical validities/truths; here, the adjective 'logical' is not merely indicating that the statement is logically derivable, but that it is true regardless of interpretation.\n\nProving in math almost feels like using rules of arithmetic to get a\nmatching definition\n\nQuibble: proofs derive results, not definitions.\n\n$$\\forall n.\\,(n\\in\\mathbb N\\to(\\text{Even}(n)\\leftrightarrow\\text{Even}(n^2)))\\tag0$$\n\nCorrection: see $(2)$ below.\n\nWhat is the link between a predicate logic proof and a math proof written in English?\n\nProof: Pick an arbitrary even integer $n.$ We need to show that $n²$ is even.\n\n\nHere, the author means that $$(n\\in\\mathbb Z \\land \\text{Even}(n))\\to \\text{Even}(n^2)\\tag1$$ logically implies $$n\\in\\mathbb Z\\to(\\text{Even}(n)\\to\\text{Even}(n^2)),$$ and that invoking universal generalisation then gives the required result $$\\forall n\\;(n\\in\\mathbb Z\\to(\\text{Even}(n)\\to\\text{Even}(n^2))).\\tag2$$ So far, no mathematics has been performed yet.\n\n\nSince $n$ is even, there is some integer $k$ such that $n = 2k.$\n\n\nHere, the author is using multiplication, an arithmetical operation, and invoking the definition of \"even number\", that is, what it means for the predicate Even$(n)$ to be true/false.\n\n\nThis means that $n^2 = (2k)^2= 4k^2 = 2(2k^2).$ From this, we see that there is an integer $m$ (namely, $2k^2$) where $n^2 = 2m.$ Therefore, $n^2$ is even,\n\n\nHere, the author is also using exponentiation, another arithmetical operation, and  informally—which is not to say unrigorously—proving statement $(1).$ Conditional introduction, existential instantiation and generalisation, etc. are all implicit.\n\n\nwhich is what we wanted to show. ■\n\n\nP.S. That period/colon/comma suffixing the quantifier in $(0)$ potentially introduces ambiguity (though not in $(0)$ itself as it is thoroughly parenthesised): $$∀x\\, Px\\to Q,$$ due to the precedence convention of logical symbols, definitely means $$(∀x\\, Px)\\to Q,\\tag{A}$$ while in $$∀x. Px\\to Q,$$ is the '.' merely superfluous or intended as a delimiter like so $$∀x\\, (Px\\to Q)\\;?\\tag{B}$$ $(\\text A)$ and $(\\text B)$ are logically inequivalent.\n",
    "tags": [
      "discrete-mathematics",
      "proof-writing",
      "proof-explanation",
      "predicate-logic"
    ],
    "score": 10,
    "answer_score": 0,
    "is_accepted": true,
    "question_id": 4647178,
    "answer_id": 4647438
  },
  {
    "theorem": "Does continuity of $f$ imply $f^{-1}(\\bar A)\\subset\\overline{f^{-1}(A)}$?",
    "context": "I'm struggling to prove or disprove that the continuity of $f$ implies $f^{-1}(\\bar A)\\subset\\overline{f^{-1}(A)}$.\n$f:X\\to Y$ is a map between metric spaces $(X,d),(Y,d')$ while $\\bar M$ denotes the closure of $M$.\nThe definition of continuity I'm supposed to use for this exercise is:\n$f$ is continuous$\\ \\Leftrightarrow\\ f^{-1}(M)$ is open if $M\\subset Y$ is open$\\ \\Leftrightarrow\\ f^{-1}(M)$ is closed if $M\\subset Y$ is closed.\n",
    "proof": "Let $Y = \\mathbb{R}$ with usual metric and $X = (0,1) \\cup \\{2\\}$ with metric inherited from the standard metric on $\\mathbb{R}$. let $f(x) = x$ on $(0,1)$ and $f(2) = 1$ and take $A = (0,1)$. it is then easy to see that for this particular case your statement doesn't hold\n",
    "tags": [
      "general-topology",
      "metric-spaces",
      "proof-writing",
      "continuity",
      "examples-counterexamples"
    ],
    "score": 10,
    "answer_score": 7,
    "is_accepted": true,
    "question_id": 881730,
    "answer_id": 881741
  },
  {
    "theorem": "Show that $\\gcd(a, 0)$ exists and equals $|a|$ for all $a$ in $\\mathbb Z$",
    "context": "I came up with the proof in the paragraph below. My question is about how I expressed the proof, and about the first part of the question above.\nFor one, my proof seems to me very wordy compared to proofs in my textbook or shown by my professors, so I'd appreciate input on how to express it in a more formal way. Also, I haven't shown that $d$ (where $d = \\gcd(a, 0)$) exists and I don't see how I'd do so.\n\nPROOF: Suppose $d = \\gcd(a, 0)$, where $d$ is an integer. Then $d \\mid a$ and $d \\mid 0$. As every integer divides $0$, $d$ will be the largest divider of $a$. The largest divider of any integer is itself. However, $a$ may be negative and $d$, by definition, is greater or equal than zero, so $d = |a|$.\n\nI appreciate any answers. Thanks!\n",
    "proof": "You're on the right track! The zero thing is true, but the gcd isn't by definition nonnegative (greater than or equal to 0.) Since the gcd is, well, the greatest number that divides both, consider all divisors of each. Here's how I would prove it:\nSince any nonzero number divides 0, the divisors of 0 are $ ..., -2, -1, 1, 2, ..., |a|-1, |a|, |a|+1, ... $.\nAnd $a$ will have some number of divisors, but it's largest will be $|a|$, since $|a|$ divides $a$, and anything larger than $|a|$ cannot possibly divide $a$.\nTherefore, the largest number that divides both is just the largest number that divides $a$, which is $|a|$.\n",
    "tags": [
      "elementary-number-theory",
      "proof-writing",
      "integers"
    ],
    "score": 10,
    "answer_score": 5,
    "is_accepted": false,
    "question_id": 3053333,
    "answer_id": 3053406
  },
  {
    "theorem": "Cannot find a mistake in an incorrect proof.",
    "context": "I am reading an introductory book on proof-writing techniques. One of the exercises asks to demonstrate why the proof is incorrect. I spent quite a while thinking about it and still feel puzzled.\n$\\textbf{Incorrect theorem:}$ Suppose $F$ and $G$ are families of sets. If $\\cup F$ and $\\cup G$ are disjoint, then so are $F$ and $G$.\n$\\textbf{Proof:}$ (by contradiction) Suppose $\\cup F$ and $\\cup G$ are disjoint. Suppose $F$ and $G$ are not disjoint. Then we can choose some set $A$ such that $A \\in F$ and $A \\in G$. Since $A \\in F$, then $A \\subset \\cup F$ (this result is true and was proven by me before as an exercise, so I just use it here). Similarly, since $A \\in G$, every element of $A$ is in $\\cup G$. But then every element of $A$ is in both $\\cup F$ and $\\cup G$, and this is impossible since $\\cup F$ and $\\cup G$ are disjoint. Thus, we have reached a contradiction, so $F$ and $G$ must be disjoint. QED.\n$\\textbf{Note:}$ So far I have been able to come up with the following counterexample. Consider $F = \\left\\lbrace \\emptyset, \\left\\lbrace 1 \\right\\rbrace, \\left\\lbrace 2 \\right\\rbrace, \\left\\lbrace 3 \\right\\rbrace \\right\\rbrace$ and $G = \\left\\lbrace \\emptyset, \\left\\lbrace 4 \\right\\rbrace, \\left\\lbrace 5 \\right\\rbrace, \\left\\lbrace 6 \\right\\rbrace \\right\\rbrace$. Then $\\cup F = \\left\\lbrace 1,2,3 \\right\\rbrace$ and $\\cup G = \\left\\lbrace 4,5,6 \\right\\rbrace$. We see from here that $\\cup F \\cap \\cup G = \\emptyset$ but $F \\cap G = \\left\\lbrace \\emptyset \\right\\rbrace$ which clearly demonstrates that the fore mentioned theorem is not correct. So the proof must be erroneous as well. \nBut still, I cannot understand from here why the proof of the theorem is not valid. Everything seems fine to me, so it is clear that I am missing something.\nIf we look at the proof and choose $A = \\emptyset$ then the statement in the end of the theorem states that every element of $A$ is in both $\\cup F$ and $\\cup G$. But since $A$ has no elements, then the statement $\\forall x (x \\in A \\rightarrow (x \\in \\cup F \\wedge x \\in \\cup G))$ is true, isn't it?\n",
    "proof": "You're almost there.\n\nBut then every element of $A$ is in both $\\bigcup F$ and $\\bigcup G$\n\nis a correct conclusion from the premises, but is no contradiction; it just shows that $A=\\emptyset$, because no element can belong to it.\nThe correct version of the theorem is\n\nLet $F$ and $G$ be families of set such that $\\bigcup F$ and $\\bigcup G$ are disjoint. Then either $F$ and $G$ are disjoint or $F\\cap G=\\{\\emptyset\\}$.\n\n",
    "tags": [
      "elementary-set-theory",
      "proof-verification",
      "proof-writing"
    ],
    "score": 10,
    "answer_score": 13,
    "is_accepted": true,
    "question_id": 920381,
    "answer_id": 920391
  },
  {
    "theorem": "A proof using the contrapositive",
    "context": "I am trying to prove the following conjecture:\n\nProve that if $m$ and $n$ are integers and $mn$ is even, then $m$\n  is even or $n$ is even.\n\nProof by contraposition:\nAssume $m$ and $n$ are odd. Then $m = 2k + 1$ and $n = 2l + 1$. So\n$$mn = (2k + 1)(2l + 1) = 4kl + 2k + 2l + 1 = 2(2kl + k + l) + 1$$\nQED\nIs there anything else I need to do in order to prove this conjecture? Thank you!\n",
    "proof": "You did very well: you got to the \"meat\" of the proof.\nI'll simply add \"a side dish\": \nI would simply add, after demonstrating that, given $m$ and $n$ are both odd,  and hence, as odd,  there is an integer k such that $m= 2k+1$, and an integer $l$ such that $n = 2l+1$.  Thus it follows that  $$mn=  2(2kl + k + l) + 1 $$ by concluding that $$mn= 2(2kl + k + l) + 1 \\,\\text{ is odd. }$$\nHence, by the equivalence of the contrapositive, we have proven: \"If $mn$ is even, then either $m$ or $n$ (or both) is even.\"\n",
    "tags": [
      "proof-writing"
    ],
    "score": 10,
    "answer_score": 7,
    "is_accepted": true,
    "question_id": 2937103,
    "answer_id": 2937114
  },
  {
    "theorem": "Proof: Show there is set of $n+1$ points in $\\mathbb{R}^n$ such that distance between any two distinct points is $1$?",
    "context": "Argh, I hate to ask a question again so soon, especially one I feel like I should know. \nLinear algebra is taking its toll, and I am not quite used to the theory side of mathematics.\nAnyways, I want/need (more of a want) to show that there is a set of $n+1$ points in $\\mathbb{R}^n$ such that the distance between any two distinct points is $1$. \nI am also interested in proving that a set of $n+2$ points does not exist, but I imagine that should not be difficult to do after.\nThank you in advance.\nPS. I really do apologize if this is a duplicate, I spent a lot of time trying to figure this out to no avail.\n",
    "proof": "This approach is most reminiscent of your last question. In $\\mathbb R^{n+1},$ the standard unit axis vectors such as $(1,0,0,...,0)$ are all $\\sqrt 2$ apart. So, multiply by $(n+1),$  all the vectors $(n+1,0,0,...0)$ are $(n+1) \\sqrt 2$ apart. These lie in the hyperplane where the sum of the coordinates is $(n+1).$ This hyperplane can be translated at then rotated so that it coincides with $\\mathbb R^n,$ then shrunk by a common scalar factor so the distances are $1.$\nLet me do the translation anyway: subtract $1$ from all coordinates, so the $n+1$ entries of the first vector are $(n,-1,-1,...,-1)$ and the sum is now $0.$ A normal vector to this hyperplane is $(1,1,1,...,1).$ If you can figure out how to rotate that into $(0,0,0,...,\\sqrt {n+1})$ you will hhave successfully rotated all the points into the plane $x_{n+1}=0,$ which is  $\\mathbb R^n.$\n",
    "tags": [
      "linear-algebra",
      "proof-writing"
    ],
    "score": 10,
    "answer_score": 8,
    "is_accepted": true,
    "question_id": 768046,
    "answer_id": 768071
  },
  {
    "theorem": "Numerical method for finding the square-root.",
    "context": "I found a picture of Evan O'Dorney's winning project that gained him first place in the Intel Science talent search. He proposed a numerical method to find the square root, that gained him $100,000 USD.\nBelow are some links of pictures of the poster displaying the method.\n\nLink 1 \nLink 2 \nLink 3\n\nHow does this numerical method work and what is the proof?\nHis method makes use of Moebius Transformation.\n",
    "proof": "Essentially if you are interesting in evaluating $\\sqrt{a}$, the idea is to first find the greatest perfect square less than or equal to $a$. Say this is $b^2$ i.e. $b = \\lfloor \\sqrt{a} \\rfloor \\implies b^2 \\leq a < (b+1)^2$. Then consider the function\n$$f(x) = b + \\dfrac{a-b^2}{x+b}$$\n$$f(b) = b + \\underbrace{\\dfrac{a-b^2}{2b}}_{\\in [0,1]} \\in [b,b+1]$$\n$$f(f(b)) = b + \\underbrace{\\dfrac{a-b^2}{f(b) + b}}_{\\in [0,1]} \\in [b,b+1]$$\nIn general\n$$f^{(n)}(b) = \\underbrace{f \\circ f \\circ f \\circ \\cdots f}_{n \\text{times}}(b) = b + \\dfrac{a-b^2}{f^{(n-1)}(b)+b}$$\nHence, $f^{(n)}(b) \\in [b,b+1]$ always.\nIf $\\lim\\limits_{n \\to \\infty}f^{(n)}(b) = \\tilde{f}$ exists, then\n$$\\tilde{f} = b + \\dfrac{a-b^2}{\\tilde{f}+b}$$\nHence, $$\\tilde{f}^2 + b \\tilde{f} = b \\tilde{f} + b^2 + a - b^2 \\implies \\tilde{f}^2 = a$$\nTo prove the existence of the limit look at \n$$(f^{(n)}(b))^2 - a = \\left(b + \\dfrac{a-b^2}{f^{(n-1)}(b)+b} \\right)^2 - a = \\dfrac{(a-b^2)(a-(f^{(n-1)}(b))^2)}{(b+f^{(n-1)}(b))^2} = k_{n-1}(a,b)((f^{(n-1)}(b))^2-a) $$\nwhere $\\vert k_{n-1}(a,b) \\vert \\lt1$. Hence, convergence is also guaranteed.\nEDIT\nNote that $k_{n-1}(a,b) = \\dfrac{(a-b^2)}{(b+f^{(n-1)}(b))^2} \\leq \\dfrac{(b+1)^2 - 1 - b^2}{(b+b)^2} = \\dfrac{2b}{(2b)^2} = \\dfrac1{2b}$. This can be interpreted as larger the number, faster the convergence.\nComment: This method works only when you want to find the square of a number $\\geq 1$.\nEDIT\nTo complete the answer, I am adding @Hurkyl's comment. Functions of the form $$g(z) = \\dfrac{c_1z+c_2}{c_3z+c_4}$$are termed Möbius transformations. With each of these Möbius transformations, we can associate a matrix $$M = \\begin{bmatrix} c_1 & c_2\\\\ c_3 & c_4\\end{bmatrix}$$\nNote that the function, $$f(x) = b + \\dfrac{a-b^2}{x+b} = \\dfrac{bx + a}{x+b}$$ is a Möbius transformation.\nOf the many advantages of the associated matrix, one major advantage is that the associate matrix for the Möbius transformation\n$$g^{(n)}(z) = \\underbrace{g \\circ g \\circ \\cdots \\circ g}_{n \\text{ times}} = \\dfrac{c_1^{(n)} z + c_2^{(n)}}{c_3^{(n)} z + c_4^{(n)}}$$ is nothing but the matrix $$M^n = \\begin{bmatrix}c_1 & c_2\\\\ c_3 & c_4 \\end{bmatrix}^n = \\begin{bmatrix}c_1^{(n)} & c_2^{(n)}\\\\ c_3^{(n)} & c_4^{(n)} \\end{bmatrix}$$\n(Note that $c_k^{(n)}$ is to denote the coefficient $c_k$ at the $n^{th}$ level and is not the $n^{th}$ power of $c_k$.)\nHence, the function composition is nothing but raising the matrix $M$ to the appropriate power. This can be done in a fast way since $M^n$ can be computed in $\\mathcal{O}(\\log_2(n))$ operations. Thereby we can compute $g^{(2^n)}(b)$ in $\\mathcal{O}(n)$ operations.\n",
    "tags": [
      "numerical-methods",
      "recurrence-relations",
      "proof-writing",
      "contest-math",
      "recursive-algorithms"
    ],
    "score": 10,
    "answer_score": 8,
    "is_accepted": true,
    "question_id": 222364,
    "answer_id": 222389
  },
  {
    "theorem": "The Product of Subgroups of an Abelian Group",
    "context": "Reference: Fraleigh p. 58 Question 5.43 in A First Course in Abstract Algebra \nLet $G$ be an Abelian Group. Suppose $H$ and $K$ are subgroups of $G$, and $HK = \\{ xy: x \\in H \\text{ and } y \\in K \\}$. Prove that $HK$ is a subgroup of $G$.\nSince $H$ and $K$ are both subgroups, $e \\in H$ and $e \\in K$. $ee \\in HK$ which shows that $HK$ is not empty.\n(i). Let $ab, cd \\in HK$ such that $a, c \\in H$ and $b, d \\in K$. First I need to show that $abcd$ can be written as a product of elements of $H$ and elements of $K$. Since $G$ is Abelian, $abcd = acbd = (ac)(bd)$. We know that $(ac) \\in H$ and $(bd) \\in K$. \n(ii). Let $ab \\in HK$, then $a \\in H, b \\in K$. I need to show that $(ab)^{-1} \\in HK$ can be written as a product of elements of $H$ and $K$. Because $a \\in H$ and $H$ is a subgroup, $a^{-1} \\in H$. And since $b \\in K$ and $K$ is a subgroup, $b^{-1} \\in K$. $(ab)^{-1} = b^{-1}a^{-1}$.\nAm I interpreting the question correctly? And is my proof correct?\n\n@Arturo's Exercise:\nI'm a bit confused as to the definition of $HK = KH$ given. I thought that if two groups $A$ and $B$ were equal, every element of $A$ would be contained in $B$ and every element of $B$ would be contained in $A$.\n(i). Let $ab, cd \\in HK$, then $abcd \\in HK$. I need to show that $abcd \\in HK$. Using the definition above, $ab = b'a'$ and $ab = a''b''$ where $a', a'' \\in H$ and $b', b'' \\in K$. A similar statement can be made for $cd$.\n(ii). Let $ab \\in HK$, then $(ab)^{-1} \\in HK$\nCould I get a hint as to how to start part (i)? I've tried substituting to try and show that $abcd$ is a product of elements of $H$ and $K$, but I have not gotten anywhere.\n",
    "proof": "The last part, as Geoff notes, is perhaps a bit lacking: you did not show that $b^{-1}a^{-1}$ is an element of $HK$, because that requires you to show that it can be written as the product of something in $H$ times something in $K$, rather than \"something in $K$ times something in $H$\" (yes, they are the same because $G$ is abelian, but this needs to be said somewhere).\nNow, for further practice, you can try the usual problem:\n\nLet $G$ be a group, not necessarily abelian, and let $H$ and $K$ be subgroups. Prove that $HK=\\{ hk\\mid h\\in H,\\ k\\in K\\}$ is a subgroup of $G$ if and only if $HK$ and $KH$ are equal as sets.\n\n(Note that $HK=KH$ means that for each $h\\in H$ and $k\\in K$ there exist $h',h''\\in H$ and $k',k''$ in $K$ such that $hk=k'h'$ and $kh = h''k''$. We do not require $hk=kh$ for each $h\\in H$ and $k\\in K$.)\n\nLet $G$ be a group. If $A$ and $B$ are subsets of $G$, then we can form the subset $AB$,\n$$AB = \\{ab\\mid a\\in A, b\\in B\\}.$$\nFor example, take $G=S_3$, $A=\\{(1,2), (1,3)\\}$, $B=\\{(1,2,3),(1,3,2)\\}$. Then\n$$AB = \\{(1,2)(1,2,3), (1,2)(1,3,2), (1,3)(1,2,3), (1,3)(1,3,2)\\} = \\{(2,3), (1,3), (1,2)\\}.$$\nIf $H$ and $K$ are subgroups of $G$, then $HK$ may or may not be a subgroup (it's certainly a subset). For instance, if $G=S_3$, $H=\\{I,(1,2)\\}$, $K=\\{I,(1,3)\\}$, then\n$$\\begin{align*}\nHK &= \\{II, I(1,3), (1,2)I, (1,2)(1,3)\\}\\\\\n &= \\{I, (1,3), (1,2), (1,3,2)\\},\n\\end{align*}$$\nwhich is not a subgroup, since it is not closed under products or inverses. On the other hand, if $H=\\{I,(1,2)\\}$ and $K=\\{I, (1,2,3), (1,3,2)\\}$, then\n$$\\begin{align*}\nHK &= \\{II, I(1,2,3), I(1,3,2), (1,2)I, (1,2)(1,2,3), (1,2)(1,3,2)\\}\\\\\n &= \\{I, (1,2,3), (1,32), (1,2), (2,3), (1,3)\\}\\end{align*}$$\nwhich is a subgroup (in fact, it's the entire group).\nNow, in the first example above, with $H=\\{I, (1,2)\\}$ and $K=\\{I, (1,3)\\}$, we can also construct $KH$. We have:\n$$\\begin{align*}\nKH &= \\{II, I(1,2), (1,3)I, (1,3)(1,2)\\}\\\\\n &= \\{I, (1,2), (1,3), (1,2,3)\\}.\\end{align*}$$\nNotice that $KH$ is not equal to $HK$: $KH$ contains $(1,2,3)$, which is not in $HK$.\nNow look at the second example above, with $K=\\{I, (1,2,3), (1,3,2)\\}$ and $H=\\{I, (1,2)\\}$. We have:\n$$\\begin{align*}\nKH &= \\{ II, I(1,2), (1,2,3)I, (1,2,3)(1,2), (1,3,2)I, (1,3,2)(1,2)\\}\\\\\n &= \\{I, (1,2), (1,2,3), (1,3), (1,3,2), (2,3)\\}.\\end{align*}$$\nHere, $KH$ is equal to $HK$. However, also notice that it is not true that $hk=kh$ for each $h\\in H$ and $k\\in K$. $(1,3)$ is in both $HK$ and in $KH$, but $(1,3)$ appears in $HK$ as the product $(1,2)(1,3,2)$, whereas it appears in $KH$ as the product $(1,2,3)(1,2)$ (different $h$).\nThe proposition I suggest is asking you to prove that if the set $HK$ is equal to the set $KH$, then $HK$ is a subgroup; and conversely, that if $HK$ is a subgroup, then the set $HK$ must be equal to the set $KH$.\nNow, what are you assuming in (i)? That the sets are equal, or that $HK$ is a subgroup?\nIf we assume that $HK$ is a subgroup, that means that (i) it contains $e$; (ii) it is closed under products; and (iii) it is closed under inverses. Your objective is to show that $HK=KH$. In order to show that $HK=KH$, you need to show that $HK\\subseteq KH$ and that $KH\\subseteq HK$.\nSo, in order to show that $HK\\subseteq KH$, you need to show that if you have $x\\in HK$, then $x\\in KH$. So, let $x\\in HK$. That means that there exist $h\\in H$ and $k\\in K$ such that $x=hk$. What you need to show is that $x\\in KH$; that is, you need to show that there exists $k'\\in K$ and $h'\\in H$ (it's possible that $k=k'$, but then again it's possible that $k\\neq k'$; we don't know!) such that $x=k'h'$.\nWell... we know that $HK$ is a subgroup of $G$. Since $x\\in HK$, then we must have $x^{-1}\\in HK$; that means that there exist $h''\\in H$ and $k''\\in K$ such that $x^{-1}=h''k''$. Therefore...\nNow finish this part of the argument, and then show that $KH\\subseteq HK$.\nConversely, you need to show that if $HK=KH$, then $HK$ is a subgroup. You need to show that: (i) $e\\in HK$ (easy, since $H$ and $K$ are both subgroups); (ii) that if $x,y\\in HK$, then $xy\\in HK$; and (iii) that if $x\\in HK$, then $x^{-1}\\in HK$.\nWell, for (iii), for example: suppose that $x\\in HK$. Since $x\\in HK=KH$, then there exist $h\\in H$ and $k\\in K$ such that $x=kh$. What is $x^{-1}$? Is it in $HK$? Etc. Finish this off.\n",
    "tags": [
      "abstract-algebra",
      "group-theory",
      "proof-writing"
    ],
    "score": 10,
    "answer_score": 12,
    "is_accepted": true,
    "question_id": 57129,
    "answer_id": 57131
  },
  {
    "theorem": "Finite Sets, Equal Cardinality, Injective $\\iff$ Surjective.",
    "context": "This proof seems odd to me. I have come to the conclusion that I will use induction. I would like to see a smoother way or just some improvements on my technique.\n\nLet $f:A\\rightarrow B$ be a function between two finite sets of equal cardinality. Show that $f$ is surjective if and only if it  is injective.\n\nTo start, I will show that a surjection implies an injection using induction. I will dismiss the cases that both sets are empty or contain one element as being trivial (essentially vacuously true). \nAssume $|A| \\geq 2$, $|B| \\geq 2$, $|A| = n = |B|$, and $f:A \\rightarrow B$ is a surjection. For the base case, let $n = 2$. \nThere are two elements in both $A$ and $B$. Due to surjection, every element $b \\in B$ must be mapped to, through $f$, by at least one element $a \\in A$. If each of the two elements in $B$ were mapped to by the same element in $A$, the definition of function would be violated. Therefore, they are mapped to by unique elements in $A$. Thus, for $f(p), f(q) \\in B$, if $f(p) = f(q)$, it must be true that $p = q$ so $f$ is injective.\nNow assume that the surjection implies an injection for $n \\geq 2$. We must show this to be true for $|A| = n + 1 = |B|$. Since it is true for $|A| = n = |B|$, the $n + 1$ case represents the addition of one new element to both $A$ and $B$. The new element in $B$ cannot be mapped to any other element in $A$ except for the new one. If mapped to by an old one, the definition of function would be violated. It must be mapped to by something since $f$ is surjective, hence it must be the new element. Finally, the new element in $A$ cannot be mapped to an old element in $B$ because it is unique and the previous $B$ was shown to be injective.\n$$\\blacksquare$$\nThis is a very wordy and awkward proof in my opinion. I have been out of proofs for a long time. I would like to see one that is more clear or seek validation if there isn't. I know that I have only completed half of the proof and have yet to go the other way.\n",
    "proof": "If the sets have  cardinality $n$ and $f$ is injective, then the image of $f$ must be an $n$ element subset of $B$ and so equal to $B.$ If $f$ is surjective, then the preimage of each element of $B$ contains at least one element, and the preimages are disjoint. So the union of the preimages of the elements of $B$ has at least $n$ elements. Since $A$ has only $n$ elements, each preimage of an element of $B$ can contain only one element of $A.$ so that $f$ is injective.\n",
    "tags": [
      "functions",
      "proof-verification",
      "elementary-set-theory",
      "proof-writing"
    ],
    "score": 10,
    "answer_score": 6,
    "is_accepted": false,
    "question_id": 3323174,
    "answer_id": 3323179
  },
  {
    "theorem": "Integration trick $\\int^{2\\pi}_{0} f(a+ r \\cos \\theta, b+r\\sin \\theta)d\\theta=2\\pi f(a,b)$",
    "context": "While looking for some nice integrals that are not taught in school, I found this theorem:\n\nSuppose $f$ is a bivariate harmonic function; $(a,b)$ is a point in the plane; and $r$ is a positive real number. Then, \n  $$\n\\int^{2\\pi}_{0} f(a+ r \\cos \\theta, b+r\\sin \\theta)d\\theta=2\\pi f(a,b)\n$$ \n\nIs there a nice way to prove the theorem above? \nUnfortunately I have no idea how to start proving this, but I suspect it's related to Euler's formula since there is an example there that can be solved that way.\n\nSource $\\longrightarrow$ Integration Tricks | Brilliant Math & Science Wiki\n",
    "proof": "Start by writing $f$ as a function of a complex variable in the usual way, so the claimed result is\n$$\\int_0^{2\\pi} d\\theta \\,f\\bigl(z_0+re^{i\\theta}\\bigr)=2\\pi\\,f(z_0)$$\nwith $z_0:=a+ib$. Expand the integrand as a Taylor series, viz.\n$$\\sum_{n\\ge 0}\\frac{f^{(n)}(z_0)}{n!}r^n\\int_0^{2\\pi} d\\theta \\,e^{in\\theta}.$$\nThe required result follows from $\\int_0^{2\\pi} d\\theta\\, e^{in\\theta}=2\\pi\\delta_{n0}$.\n",
    "tags": [
      "integration",
      "proof-writing",
      "harmonic-functions"
    ],
    "score": 10,
    "answer_score": 8,
    "is_accepted": true,
    "question_id": 2694952,
    "answer_id": 2694959
  },
  {
    "theorem": "If $a \\equiv b\\pmod m$, then $\\gcd(a, m) = \\gcd(b, m)$",
    "context": "I was wondering if my proof makes any sense.\n$a \\equiv b \\pmod m$\n$ m \\mid a -b$\n$ ml = a - b$ for some integer $l$\nlet $d = \\gcd(a, m)$\nlet $c = \\gcd(b, m)$\n$$\\frac{a}{d} = \\frac{m}{d}l - \\frac{b}{d} \\Rightarrow - \\frac{a}{d} + \\frac{ml}{d} = \\frac{b}{d} \\Rightarrow d \\mid b \\Rightarrow d  \\leq c$$\nSimilarly, $c \\mid a$, which means $c \\leq d$\nThus, $c = d \\Rightarrow \\gcd(a, m) = \\gcd(b, m)$.\n",
    "proof": "Yes, that works. Quicker: if $\\rm\\: d\\mid m\\:$ then $\\rm\\:d\\mid a\\!\\iff\\! d\\mid a+mk.\\:$ So $\\rm\\ m,\\,a\\ $ and $\\rm\\ m,\\,a\\!+\\!mk\\ $ have the same set D of common divisors $\\rm\\,d,\\,$ so they have the same greatest common divisor (= max D).\n",
    "tags": [
      "modular-arithmetic",
      "proof-writing"
    ],
    "score": 10,
    "answer_score": 4,
    "is_accepted": true,
    "question_id": 321720,
    "answer_id": 321739
  },
  {
    "theorem": "How to determine the scope of a proof?",
    "context": "When given exercises for my coursework, I often encounter the problem of not knowing how pedantic to be in my proofs. Some seem to be statements so trivial that I'm forced to question whether or not the exercise is allowing me to use basic operations that I take for granted. For instance, I have the following exercise tonight:\n\nGiven $1 < b\\in\\mathbb{R}$, and $r = \\frac{m}{n} = \\frac{p}{q}$ prove that:\n  $$(b^m)^\\frac{1}{n} = (b^p)^\\frac{1}{q}$$\n\nI dont' know if I'm allowed to use a statement like $(x^a)^b = (x^{ab})$ in this context.\nI've had this situation go both ways. I've taken the option of writing a page long proof, and had the professor tell me that I expanded on it too much. I've also gone the route of using statements like the above to make it a few scant lines, only to have the professor tell me I wasn't allowed to use a statement like that.\nTo be clear, I don't want help with this particular exercise. I'd like to know if anybody has general guidelines for knowing which theorems one is allowed to use for a given exercise.\n",
    "proof": "This is a very interesting question and this answer might be a surprise to you: it depends on your other exercises. That is, if you answered correctly other exercises, the teacher will be inclined to accept a short proof, even if some intermediate steps are missing, as long as he or she is convinced that you master these intermediate steps.\nLet me explain this idea in the context of your example (in which I suppose that $m$ and $p$ are nonnegative integers and $n$ and $q$ are positive integers). Suppose you start your proof by writing \n\nIt is easy to prove by induction on $b \\in \\mathbb{N}$ that, for all $a \\in \\mathbb{N}$, $(x^a)^b= x^{ab}$.\n\nIf you just did correctly a few exercises involving induction, and if the rest of the argument is correct, this is likely to be accepted. However, if you previously did major mistakes on some induction argument, you may well receive a nasty comment like \"if this is easy to prove, why don't you do it?\"\nThat being said, when you detect that an exercise is an application of a theorem of the previous lesson, it is best to focus on this and explain how you want to apply the theorem. This involves carefully justify that the hypotheses are filled. Short proofs are usually better, but still need to be fully justified. Moreover, you have to understand the purpose of the exercise. For your example, it is likely that you are not allowed to use the formula $(x^a)^b= x^{ab}$, with $a, b \\in \\mathbb{Q}$, and that a more detailed proof is needed. But you can still write\n\nThe result would immediately follow from the fact that, for all $a, b \\in \\mathbb{Q}$, $(x^a)^b= x^{ab}$. \n\nand then proceed to the longer proof.\n",
    "tags": [
      "proof-writing",
      "convention"
    ],
    "score": 10,
    "answer_score": 8,
    "is_accepted": true,
    "question_id": 2900192,
    "answer_id": 2900271
  },
  {
    "theorem": "Why does this pattern repeat after 14 cycles instead of 256, can you give a proof?",
    "context": "I have 8 numbers in an array: the numbers are either 1 or 0 and are initially random.\nexample:\n[0,1,0,1,1,0,0,1]\nevery cycle, the array changes as follows: \n\nIf a cell has two adjacent values that are both equal, then the value becomes 1.\nOtherwise it becomes 0.\n\nExample after 1 iteration:\n[0, 1, 1, 0, 0, 0, 0, 0]\nafter 2:\n[0, 0, 0, 0, 1, 1, 1, 0]\netc...\nI simulated different starting conditions, and the cycle always repeats within 14 cycles. My intuition says it should repeat after 256 cycles (2^ArrayLength) or maybe even 64 (2^6 since the two edges will always be 0 after the first iteration).\nI have a feeling it has to do with how the 2nd value and the 7th value in the array are really only dependent on the 3rd and 6th value, since the other two next to these indexes never change after iteration 1, but I can't figure it out.\nCould anyone explain to me why this is 14?\n",
    "proof": "The main reason is that the values of the 1st, 3rd, 5th and 7th number in your array before an iteration completely determine the values of the 2nd, 4th, 6th and 8th number after the iteration. Similiarly, the valuse of the 2nd, 4th, 6th and 8th number before an iteration completely determine the values of the 1st, 3rd, 5th and 7th number after the iteration.\nTo see this, take as example the 5th number (odd). After an iteration its value will depend only on the before-iteration value of the 4th and 6th number (even).\nAs you correctly noted, after one iteration the 1st and 8th number are $0$ and stay that way. So after one iteration, your array looks like\n$$0x?y?z?0.$$\nAfter one more iteration the values of $x,y$ and $z$ (2nd, 4th and 6th array element) completely determine the non-$?$ values given here ($r,s$ and $t$):\n$$0?r?s?t0.$$\nAfter one more iteration, those values ($r,s$ and $t$) determine again the new (2nd, 4th and 6th array element):\n$$0x'?y'?z'?0.$$\nThat means the original $x,y$ and $z$ values, determine, after 2 iterations completely the values of $x',y'$ and $z'$.\nThe same can be said of course for the remaining 3 values (above shown as $?$,).\nSo from your original 8-bit data set, 2 bits immediately become and stay $0$, and then 2 sets of 3 bits of data remain that, after 2 iterations, determine the \"next\" value of the exact same 3 bits of data.\nSo those 2 sets of 3 bit data have a maximal cycle length of 8. Because we need two of our iterations for one step in the 3-bit cycle, that makes a maximal iteration count of 16.\nNow to get to your 14 iterations, what's needed is to actually write down those cycles of the 3bit data. It turns out that it's one cycle of 7 steps and one 1 cyle of 1 step.\nYou get the 1 step cycle if you start with \n$$00?0?1?0,$$\nthen the next iteration will be \n$$0?1?0?00$$ \nand the next one will be  (again)\n$$00?0?1?0.$$\nIf you start with any other configuration, you will find the 7-step cycle. As 1 step is two iterations, this explains your max 14 iterations.\n",
    "tags": [
      "proof-writing",
      "pattern-recognition"
    ],
    "score": 10,
    "answer_score": 6,
    "is_accepted": true,
    "question_id": 3311568,
    "answer_id": 3311639
  },
  {
    "theorem": "Proving $A \\cup (B \\cap C) = (A \\cup B) \\cap (A \\cup C)$.",
    "context": "\nProve the distributive property for sets:\n$A \\cup (B \\cap C) =  (A \\cup B) \\cap (A \\cup C)$\n\nI'm not good with proofs but my understanding is that I have to prove 2 things:\n(1)  $A \\cup (B \\cap C) \\subset (A \\cup B) \\cap (A \\cap C)$\n(2)  $A \\cap (B \\cap C) \\supset (A \\cup B) \\cap (A \\cup C)$\nThis is what I have done so far:\nPart (1)\nIf $x\\in A$, then $x \\in (A \\cup B)$ and $x \\in (A \\cup C)$.\n$\\therefore x \\in (A \\cup B) \\cap (A \\cap C)$\nIf $x \\in (B \\cap C)$ then $x \\in (A  \\cup B)$ and $x \\in (A \\cup C)$ because $x \\in B$ and $x \\in C$.\n$\\therefore x \\in (A \\cup B) \\cap (A \\cap C)$\n$\\therefore A \\cup (B \\cap C) \\subset (A \\cup B) \\cap (A \\cup C)$\nPart (2)\nNow we have to prove the reverse inequality:  $(A \\cup B) \\cap (A \\cap C)$.  Then $x \\in A \\cup B$ and $x \\in (A \\cup C)$\nIf $x \\in A$, then $x \\in A \\cup (B \\cap C)$\n\nThis is where I am up to.  I wanted to know whether my approach is correct and if I did part (1) correctly.  I'm stuck on part (2) and don't know how to proceed.  I'd appreciate any help.\nThank you!!\n",
    "proof": "You must first prove 2 cases:\n(1)  $A \\cap (B \\cup C) \\subset (A \\cap B) \\cup (A \\cap C)$\n(2)  $(A \\cap B) \\cup (A \\cap C) \\subset A \\cap (B \\cup C)$\nNote that in mathematics we use the following symbols:\n$\\cap=$ AND = $\\land$\n$\\cup=$ OR = $\\lor$\nCase 1:  $A \\cap (B \\cup C) \\subset (A \\cap B) \\cup (A \\cap C)$\nLet $x \\in A \\cap (B \\cup C) \\implies x \\in A \\land x \\in (B \\cup C)$\n$\\implies x \\in A \\land \\{ x \\in B \\lor x \\in C \\}$\n$\\implies \\{ x \\in A \\land x \\in B \\} \\lor\\{ x \\in A \\land x \\in C \\} $\n$\\implies x \\in (A \\cap B) \\lor x \\in (A \\cap C)$\n$\\implies x \\in (A \\cap B) \\cup (A \\cap C)$\n$\\therefore x \\in A \\cap (B \\cup C) \\implies x \\in (A \\cap B) \\cup (A \\cap C)$\n$\\therefore A \\cap (B \\cup C) \\subset (A \\cap B) \\cup (A \\cap C)$\nCase 2: $(A \\cap B) \\cup (A \\cap C) \\subset A \\cap (B \\cup C)$\nLet $x \\in (A \\cap B) \\cup (A \\cap C) \\implies x \\in (A \\cap B) \\lor x \\in (A \\cap C)$\n$\\implies \\{x \\in A \\land x \\in B \\} \\lor \\{ x \\in A \\land x \\in C \\}$\n$\\implies x \\in A \\land \\{ x \\in B \\lor x \\in C\\}$\n$\\implies x \\in A \\land \\{B \\cup C \\}$\n$\\implies x \\in A \\cap (B \\cup C)$\n$\\therefore x \\in (A \\cap B) \\cup (A \\cap C) \\implies x \\in A \\cap (B \\cup C)$\n$\\therefore (A \\cap B) \\cup (A \\cap C) \\subset A \\cap (B \\cup C)$\n\n$\\therefore A \\cup (B \\cap C) =  (A \\cup B) \\cap (A \\cup C)$\n",
    "tags": [
      "elementary-set-theory",
      "discrete-mathematics",
      "proof-writing"
    ],
    "score": 10,
    "answer_score": 5,
    "is_accepted": true,
    "question_id": 389689,
    "answer_id": 389738
  },
  {
    "theorem": "WLOG and &quot;by symmetry&quot; arguments and the foundations of mathematics",
    "context": "John Harrison's paper Without Loss of Generality raises the interesting point that although \"without loss of generality\"/\"by symmetry\" arguments are a common proof technique, there is no corresponding formal mathematical law, so that automated theorem provers must be extended to handle such arguments. \nIf ATPs cannot handle WLOG arguments in formal logic, can humans? Is it impossible to prove the WLOG argument in the general case in any of the conventional foundational logics of mathematics? If so, then does this indicate a deficit or shortfall or is the WLOG argument A) too trivial, or B) too metalogical?\nI will clarify what I mean by a WLOG argument. In mathematical logic, if a pair of objects $A$ and $B$ are known to be indistinguishable up to naming, it is immediate that a proof of any property $P$ of $A$ automatically implies $P(B)$ and, conversely, $P(B) \\to P(A)$. Such \"Without loss of generality\" or \"by symmetry\" arguments are used almost everywhere in mathematics and to say that the reasoning is obvious would be an enormous understatement. \n(To be precise, I want to emphasize that WLOG doesn't apply when some property of the objects is unknown and, when discovered, assigned to each of the objects arbitrarily. For example, although the roots of a polynomial are indistinguishable in the sense that the coefficients are invariant over permutation of the roots, it may be possible to discover the set of values and then make an arbitrary assignment of values to names. Indistinguishability is not the same as uncertainty.)\nIt could be argued that WLOG arguments are, like the cut rule, superfluous since they are only used to avoid needless repetition. However, since one may be referring to an uncountable or larger set of objects the WLOG argument may be necessary for the existence of a proof of finite length.\nPerhaps what is needed is some kind of an axiom to patch this hole up?\n",
    "proof": "The paper does not claim that automated theorem provers cannot handle the case of such arguments, and indeed shows how to formalize them. The fact is that humans do not write proofs in the same way that ATPs do - very much is left for the reader to do. Indeed, this often rises to the level of little lemmas that could have their own proofs, and the author may only hint at how one might prove them.\nThis is exactly the case when we say \"without loss of generality\" - we are alluding to an omitted proof. The lemma would generally be of the form:\n\nThere is some group of symmetries preserving the properties at hand in the problem. There exists a symmetry such that _____ properties hold.\n\nAnd this is more or less what the theorems that linked paper proves amount to. The proof should be obvious to the reader, but not to a machine.\nSo, on one hand, we cannot patch this hole, because there is no overarching proof of every such lemma we might want. But, on the other hand, there really is no hole to begin with - it's just another thing we need to translate for the machine.\n",
    "tags": [
      "logic",
      "proof-writing",
      "proof-explanation"
    ],
    "score": 10,
    "answer_score": 7,
    "is_accepted": false,
    "question_id": 1354612,
    "answer_id": 1354668
  },
  {
    "theorem": "Prove that $a &lt; b\\sqrt{3}$ under conditions given",
    "context": "There are integers $a$ and $b$ such that:\n1) $a > b > 1$\n2) $ab+1$ is divisible by $a+b$ and $ab-1$ is divisible by $a-b$. \nProve that $a < b\\sqrt{3}$.\nIt's really hard, do you see a solution?\n",
    "proof": "\nIf $a$ and $b$ share a common factor, then it is shared also by $a+b$ and $ab$, and therefore not by $ab+1$ and we could not have item 2).  Therefore $a$ and $b$ are relatively prime.\nItem 2) implies that $b^2-1$ is divisible by both $a+b$ and $a-b$, and hence their least common multiple $\\mathrm{lcm}(a+b,a-b) = (a+b)(a-b) / \\mathrm{gcd}(a+b,a-b)$.\nSince $b^2-1 > 0$, it follows that $b^2-1 \\geq \\mathrm{lcm}(a+b,a-b) = (a+b)(a-b) / \\mathrm{gcd}(a+b,a-b)$.\nWhat common factors can $a+b$ and $a-b$ have?  If they share an odd prime $p$ as a common factor, then $a$ and $b$ share that prime as a common factor; but $a$ and $b$ are relatively prime by the first entry on this list.  If $a+b$ and $a-b$ are both divisible by $4$, then $a$ and $b$ are both even, which we also ruled out.  Thus $\\mathrm{gcd}(a+b,a-b)$ is either $1$ or $2$.\nIn summary, $b^2 - 1 \\geq (a+b)(a-b)/2$.  Multiply through by $2$ to get $2b^2 - 2 \\geq a^2 - b^2$.  Add $b^2$ for $3b^2 - 2 \\geq a^2$, from which it follows that $3b^2 > a^2$.  Then take the square root.\n\n\nEdit: It was requested in the comments that I explain step 2.  To wit:\n2+. Since $ab+1$ is divisible by $a+b$, so is $(ab+1) - b(a+b) = -b^2 + 1 = -(b^2-1)$.\n2-. Since $ab-1$ is divisible by $a-b$, so is $(ab-1) - b(a-b) = b^2-1$.\n\nIt is perhaps useful to mention some words about how I found the solution.  The problem asked to prove the bound $a < b\\sqrt 3$.  I asked myself: rather than prove that specific bound, can we give some bound?  I.e. can we disprove the possibility that $a \\gg b$?\nWell, I said, if $a \\gg b$, then $a \\pm b \\approx a$.  So I know that $ab \\pm 1$ is divisible by something approximately $a$.  Well, what must the quotient be?  $ab \\pm 1 \\approx ab$, so $(ab \\pm 1) / (a \\pm b) \\approx b$.  Ok, so let's subtract off $b(a\\pm b)$ and see what's left.\nGreat, then $b^2 - 1$ is divisible by something approximately $a$.  Since $b>1$, this already gives some bound: it says that $b^2 \\gtrsim a$.  I wanted a bound of the form $b \\gtrsim a$, up to some fixed constant, and I'm not there yet, but I've at least ruled out, say, $a \\sim b^{1000}$.\nOk, I haven't used both conditions from line 2).  I know that something approximately $b^2$ is divisible by two different things that are approximately $a$.  I want $b^2 \\gtrsim a^2$ --- since I have two terms, I have $b^2 \\gtrsim \\mathrm{lcm}(\\approx a,\\approx a)$.  And lcms are usually products, because large numbers tend to have few large common factors.\nBut can I prove that?  Ah, I haven't used the stated conditions from 2), just their corollaries about $b^2-1$.  Those stated corollaries obviously imply that $a,b$ are linearly independent.  And that's almost what I want!  Indeed, I want to find a bound on $\\mathrm{gcd}(a+b,a-b)$, and I have a bound on $\\mathrm{gcd}(a,b)$ (in fact, I know the latter is $1$).  So perhaps that's all I need?  Yes: $\\mathrm{gcd}(a,b)=1$ implies $\\mathrm{gcd}(a+b,a-b)\\leq 2$.\n",
    "tags": [
      "proof-writing"
    ],
    "score": 10,
    "answer_score": 7,
    "is_accepted": false,
    "question_id": 938372,
    "answer_id": 938480
  },
  {
    "theorem": "Proof that maximizing a function is equivalent to minimizing its negative",
    "context": "The statement that maximizing a function over its argument is equivalent to minimizing that function over the same argument with a sign change seems to be accepted as trivial wherever I look (MSE, proofwiki, textbooks outside of optimization theory). \nIntuitively, if you have some function of a single variable that has a global maximum, and you \"flip it over\" by changing the sign, the global maximum is now a global minimum.\nHowever, it seems to me that math is all about a meticulous examination of surprising subtleties. Does anyone know of a good way to prove this statement?\n",
    "proof": "HINT: use the definition of global maximum\nGiven $f:X \\rightarrow R$, $x_0$ is a global maximum if $\\forall x \\in X, f(x)\\leq f(x_0)$\n",
    "tags": [
      "optimization",
      "proof-writing"
    ],
    "score": 10,
    "answer_score": 2,
    "is_accepted": true,
    "question_id": 333623,
    "answer_id": 333628
  },
  {
    "theorem": "It is easy to show that $S_m=\\sum_{n=1}^\\infty \\frac{n}{2^n + m}$ converges for any natural$\\ m$, but what is its value?",
    "context": "In fact the series would converge even if$\\ m$ were not natural, I just wanted to state that it is natural in my case. I have found the partial sum formula of$\\ S_0$,$\\displaystyle \\sum_{n=1}^k \\frac{n}{2^n} =\\frac{2^{k+1}-k-2}{2^k}$, thus easily obtaining$\\ S_0=2$. Then, since $\\displaystyle \\frac{1}{2^n+m}=\\frac{1}{2^n}-\\frac{m}{2^n\\left(2^n+m\\right)}$, I know $\\displaystyle S_m= 2-\\sum_{n=1}^\\infty \\frac{mn}{2^n\\left(2^n+m\\right)}$, though I'm not sure it is a convenient path to study the last series. \n",
    "proof": "I think your observation helps if you iterate it more.\n$$\n\\begin{align}\nS_m&=2-m\\sum_{n=0}^\\infty\\frac{n}{2^n(2^n+m)}\\\\\n&=2-m\\sum_{n=0}^\\infty \\frac{n}{2^n}\\left(\\frac{1}{2^n}-\\frac{m}{2^n(2^n+m)}\\right)\\\\\n&=2-m\\sum_{n=0}^\\infty \\frac{n}{4^n}+m^2\\sum_{n=0}^\\infty\\frac{n}{4^n(2^n+m)}\\\\\n&=2-\\frac49m+m^2\\sum_{n=0}^\\infty\\frac{n}{4^n(2^n+m)}\\\\\n\\end{align}$$\nRepeat this way, and brushing aside some convergence questions, we can write a power series in $m$:$$S_m=\\sum_{n=0}^{\\infty}(-1)^n\\frac{2^{n+1}}{(2^{n+1}-1)^2}m^n$$ I'm not sure where you go from here, but you have an alternating power series now. Because it's alternating, it may be a quicker thing to use to get decimal approximations.\nEDIT\nNope! This power series doesn't converge for $m\\geq2$. However, with $m=2$, it's partial sums oscillate between two values, the average of which appears to be $S_2$, so that's interesting.\nAs a power series, this converges for $m\\in(-2,2)$. You could at least use this to study $S_m$ for (the mostly non-integer) $m$ in $(-2,2)$. \nI examined a graph of the power series on $(-2,2)$, and it looked similar to functions of the form $\\frac{2\\cdot2^r}{(m+2)^r}$. Experimenting, using an $r$ in the neighborhood of $0.455$ gives $\\frac{2\\cdot2^r}{(m+2)^r}$ that matches the power series fairly well, and \nappears to give a decent approximation for $S_m$. So perhaps you can more rigorously find an $r\\approx0.455$ such that $S_m$ is asymptotically proportional to $\\frac{1}{(m+2)^r}$.\n",
    "tags": [
      "calculus",
      "real-analysis",
      "sequences-and-series",
      "limits",
      "proof-writing"
    ],
    "score": 10,
    "answer_score": 4,
    "is_accepted": true,
    "question_id": 979540,
    "answer_id": 990030
  },
  {
    "theorem": "Proof - Inverse of linear function is linear",
    "context": "This is my first proof related to linear functions.\nIt refers to the linear-algebra-$\\textit{linear}$ (not the calculus-$\\textit{linear}$).\nPlease comment.\nTheorem\nThe inverse of a linear bijection is linear.\nProof\nLet $X,Y$ be vector spaces over a common field.\nLet $f : X \\rightarrow Y$ be a linear bijection.\nWe denote by $f^{-1}$ the inverse of $f$.\nIt remains to prove that $f^{-1}$ is linear,\ni.e. both $\\textit{additive}$ and $\\textit{homogeneous}$.\nAdditivity\nLet $y_1, y_2 \\in Y$.\nWe prove that $$f^{-1}(y_1 + y_2) = f^{-1}(y_1) + f^{-1}(y_2).$$\n\\begin{equation*}\n\t\\begin{split}\n\t\tf^{-1}(y_1) + f^{-1}(y_2)\n\t\t\t&=\tf^{-1}\\Big( f\\big( f^{-1}(y_1) + f^{-1}(y_2) \\big) \\Big) && \\quad \\text{by bijectivity} \\\\\n\t\t\t&=\tf^{-1}\\Big(\tf\\big( f^{-1}(y_1) \\big) +\n\t\t\t\t\t\t\t\t\t\t\t\tf\\big( f^{-1}(y_2) \\big) \\Big) && \\quad \\text{by linearity of } f \\\\\n\t\t\t&=\tf^{-1}\\Big(\ty_1 + f\\big( f^{-1}(y_2) \\big) \\Big) && \\quad \\text{by bijectivity} \\\\\n\t\t\t&= f^{-1}(y_1 + y_2) && \\quad \\text{by bijectivity}\\phantom{\\Big(\\Big)} \\\\\n\t\\end{split}\n\\end{equation*}\nHomogeneity\nLet $y \\in Y$ and let $s$ be a scalar.\nWe prove that $$f^{-1}(sy) = sf^{-1}(y).$$\n\\begin{equation*}\n\t\\begin{split}\n\t\tsf^{-1}(y)\n\t\t\t&= f^{-1}\\Big( f\\big( sf^{-1}(y) \\big) \\Big) && \\quad \\text{by bijectivity} \\\\\n\t\t\t&= f^{-1}\\Big( sf\\big( f^{-1}(y) \\big) \\Big) && \\quad \\text{by linearity of } f \\\\\n\t\t\t&= f^{-1}(sy) && \\quad \\text{by bijectivity}\\phantom{\\Big(\\Big)} \\\\\n\t\\end{split}\n\\end{equation*}\nQED\n",
    "proof": "General principle: You can demonstrate two vectors in X are equal by showing that f maps them to the same vector in Y (since f is injective).  For example: Apply $f$ to $f^{-1}(y_1)+f^{-1}(y_2)$ and argue that the result is $y_1 + y_2$.  Do a similar calculation involving $f^{-1}(y_1+y_2)$.  You should then be able to see why the inverse of an additive injection is additive. Homogeneity can be approached the same way.  You may observe that surjectivity of $f$ is not really involved in what is going on here.\n",
    "tags": [
      "linear-algebra",
      "functions",
      "soft-question",
      "proof-verification",
      "proof-writing"
    ],
    "score": 10,
    "answer_score": 3,
    "is_accepted": false,
    "question_id": 914036,
    "answer_id": 919074
  },
  {
    "theorem": "Find all functions $F(x,y)$ such as $\\frac{\\sqrt{3}}{2}\\frac{\\partial{f}}{\\partial{x}}+\\frac{1}{2}\\frac{\\partial{f}}{\\partial{y}}=0$",
    "context": "How to find all possible functions $f(x,y)$ such as:\n$$ \\frac{\\sqrt{3}}{2}f_x+\\frac{1}{2}f_y=0$$\n(with $f_x = \\frac{\\partial{f}}{\\partial{x}}$ )\n\nHere's everything I tried:\n1) I can guess the most simple cases, $f_x=k$ and $f_y=-k\\sqrt{3}$ so the following is a solution (but not all possible functions):\n$$ f(x,y)= k(x-\\sqrt{3}y)+c$$\n2) The equation could be writen as:\n$$ cos (\\frac{\\pi}{6})f_x+sin(\\frac{\\pi}{6})f_y=0$$\nBut I culdn't see what can I do with that.\n3) By the implicit function theorem I could find that:\n$$\\frac{1}{\\sqrt{3}} = - \\frac{f_x}{f_y} = \\frac{dy}{dx} $$\nSo $f(x,y)$ has level curves in the form:\n$$y=\\frac{x}{\\sqrt{3}}+c $$\n4) I could also find the level curves by noticing that the equation says that the partial derivative of $f(x,y)$ in the direction of $\\langle\\sqrt{3},1\\rangle$ is zero.\n5) Or by solving the differential equation:\n$$ dx-\\sqrt{3}dy=0 $$\nBut 3), 4) and 5) couldn't help since I don't know how to find $f(x,y)$ based on its level curves.\n6) After some trial and error I could find (and prove) that for any $g:R\\rightarrow R$ the following function is ok:\n$$ f(x,y) = g(x-\\sqrt{3}y) $$\nI can also see that $\\langle\\sqrt{3},1\\rangle$ is perpendicular to $\\langle1,-\\sqrt{3}\\rangle$, so that's related to the fact that the partial derivative of $f(x,y)$ in the direction of $\\langle\\sqrt{3},1\\rangle$ is zero.\n\nI can see that I'm very close, but I can't find out:\n\nHow to prove that all possible functions are of that form?\nHow could I find that form by calculations?\n\nIf there are different ways to find the general form of $f(x,y)$ or to prove it, I would like to read about them all.\n",
    "proof": "Let´s use the method of characteristics.\nThe equation can be converted to:\n$$\\left(\\frac{\\sqrt{3}}{2}, \\frac{1}{2}\\right)\\cdot \\mathrm{grad}f=0$$\nThat means $f$ is constant along lines which direction is parallel to $\\left(\\frac{\\sqrt{3}}{2}, \\frac{1}{2}\\right)$.\nHence we can express those lines by:\n$$x-\\sqrt{3}y=k$$\nFor each value of $k$, $f$ assumes a different one. Hence $f$ depends on $k$, in other words $f$ is a function of $k$.\nTherefore:\n$$f(x,y)=g(x-\\sqrt{3}y)$$\n",
    "tags": [
      "multivariable-calculus",
      "proof-writing",
      "partial-derivative"
    ],
    "score": 10,
    "answer_score": 4,
    "is_accepted": false,
    "question_id": 376756,
    "answer_id": 435411
  },
  {
    "theorem": "Why does Rudin define $k = \\frac{y^n-x}{n y^{n-1}}$ or $h &lt; \\frac{x - y^n}{n(y+1)^{n-1}}$ when he tries to prove that every real x has a nth root?",
    "context": "tl;dr (Too Long To Read):\nWhat is the intuition/conceptual idea to why Rudin used the number:\n$$ k = \\frac{y^n-x}{n y^{n-1}} $$\nin his proof and not some other number? It seems that that number is not random, so how could he have come up with it? \n\nMy attempt:\nI was trying to prove theorem 1.21 from Rudin's real analysis book on my own without looking at Rudin's proof. The way I attempted to prove it was to try to prove what seemed true from my already collected intuition on the real numbers before I started to study real analysis. So I drew lots of picture which lead me to think of defining $E = \\{ \\bar y \\in R_{>0} : \\bar y < x^{1/n} \\}$:\nThus, I knew that $E = \\{ \\bar{y} \\in R_{>0} : \\bar y^n < x \\}$ is just the same as $E = \\{ \\bar y \\in R_{>0} : \\bar y < x^{1/n} \\}$. So it was obvious that what I had to show was that the $\\alpha = supE = y = x^{1/n}$ (also the reason for defining sets like that is cuz we probably need to use the Least Upper Bound (LUB) property cuz its one of the few things we are suppose to know about analysis so far). Thus, I proceed to show $E$ is bounded and non-empty so that I was guaranteed that the sup existed since we assumed the Least Upper Bound (LUB) property (a.k.a. the completeness axiom). \nThen I thought I want $y = \\alpha$ so one option was to try showing $y < \\alpha$ AND $y > \\alpha$ are false, so by trichotomy $y=\\alpha$. Since I didn't have direct access to $y$ I decided to take the same strategy except with $y^n = x$ instead of $y$ and $\\alpha^n$ instead of $\\alpha$. Intuitively I thought well lets assume $\\alpha^n < x$ and $ x < \\alpha^n $. The first one is too small so hopefully it should lead to some contradiction and perhaps show $\\alpha < y$ is false. Similarly the other one $ x < \\alpha^n $ should be too large somehow. Then maybe we can use trichotomy to get $\\alpha^n = x$ which completes the proof.\nI attempted the first one $\\alpha^n < x$. The only other thing I knew about $\\alpha$ was that $\\forall \\bar y \\in E, \\bar y^n < x$. Then I decided to combine both equation (since I was hinted to use $a^n - b^n$ cuz I saw that identity in the soln when I check for my solution that E was bounded and non-empty):\n$$ \\alpha^n - \\bar y^n < x - \\bar y ^n < 0$$\nthen because of the hint (that I probably wouldn't had realized I needed to use) I was extremely lucky and decided to subtract $\\bar y^n$ from both sides (notice that if I would have decided to subtract by $\\alpha^n$ it wouldn't have worked):\n$$  \\alpha^n - \\bar y^n = (\\alpha - \\bar y)\\left(\\sum_{0\\leq i+j \\leq n-1} \\bar y^i \\alpha^j \\right) = (\\alpha - \\bar y)K < 0$$\nwhere I noticed that $K > 0$ since every element of E is greater than zero and so is its least upper bound $\\alpha$. Thus $K > 0$ and getting rid of it gets me:\n$$ (\\alpha - \\bar y) < 0 \\implies \\alpha < \\bar y$$\nwhich is obviously false since that would imply that $ \\alpha$ is not an upper bound. Thus $\\alpha^n < x$ is false. \nNow assume $x < \\alpha^n $. One can't use the same argument as in my previous attempt because this time we are trying to create an element that is an upper bound smaller than $\\alpha$ and its not clear how elements form $E$ are useful.\nIts clear to me we have to choose an $h$ such that:\n$$ x < (\\alpha - h)^n < \\alpha^n$$ \nI have given it a proper attempt (see at the end of my question) but I am unable to prove the desired result no matter how much I play with the algebra and the known facts I have. \nThus, my question is how did Rudin come up with the following:\n$$k = \\frac{y^n-x}{n y^{n-1}}$$\nfrom his explanation it seems it just came out of a hat. I am sure if I plugged it in I would see that \"it works\" however, I wanted to know/see how to come up with it myself.\nSimilarly I don't see how/why he came up with this one:\n$$h < \\frac{x - y^n}{n(y+1)^{n-1}}$$\nit seems that its not even required considering my first proof/argument, but I assume it must use the same idea considering it seems it used the same identity $b^n - a^n = (b-a)(b^{n-1}+b^{n-2}a+ \\cdots + b a^{n-2} + a^{n-1})$.\nAnyone care to share what is the trick I missed? Is there some way to understand how one would have come up with using that? Is there some conceptual idea for the proof that he did not make explicit that I missed?\nI am hoping to get a more satisfying proof than just feeling I played around with symbols until I forced the paper to tell me the truth. It seems I missed some insights because even with the hints (like using the identity) didn't yield me a solution.\n\nWhat I tried:\nIf we have:\n$$ x < \\alpha^n$$\nthen at least intuitively, that must imply that there must be some element $y_{BAD}$ smaller than our supremum $\\alpha$ that is still an upper bound (this intuition is because we are going under the assumption that $\\alpha^n = x \\iff \\alpha = x^{1/n}$). Therefore it seems reasonable to try to decrease $\\alpha$ the right amount $h$ such that:\n$$ x < (\\alpha - h)^n < \\alpha^n $$\nthen since $h$ is some distance that we go down from $\\alpha$ we probably don't need to go down further than $\\alpha$ so it seems reasonable to require $0 < h < \\alpha$. With that we have using algebra:\n$$ x < (\\alpha - h)^n = (\\alpha - h)\\left( \\alpha^{n-1}+\\alpha^{n-2}h + \\dots + \\alpha h^{n-2} + h^{n-1} \\right) = (\\alpha - h) K < \\alpha^n $$\nthe reason we did that factorization is so that we can hopefully get some inequality for $h$ (intuitively, think that we are trying to make $h$ the subject so that we can choose the right one to get the contradiction we need). Therefore lets try to remove all the nasty exponents with $h$ by assuming $ h < \\alpha$ (otherwise $\\alpha$ decreases by too much):\n$$ K = \\alpha^{n-1}+\\alpha^{n-2}h + \\dots + \\alpha h^{n-2} + h^{n-1} < n \\alpha^{n-1}$$\nso lets plug it into:\n$$ x < (\\alpha - h) K < \\alpha^n $$\nafter plugging the inequality and leaving $h$ alone and some algebra I skipped I got:\n$$ \\alpha - \\frac{ \\alpha^n }{K} < h < \\alpha - \\frac{x}{n \\alpha^{n-1}}$$\nunfortunately I don't I didn't manage to plug in $K < n \\alpha^{n-1}$ successfully to both sides so I got stuck with the above (which is still in terms of $h^j, j>1$) which still unfortunately has high order terms for $h$...so close it feels... (note I've also tried more things but it would be ridiculous to put it all in here).\nso I feel got some of the main insights:\n\nRequire the constraint $x < (\\alpha - y)^n < \\alpha^n$ using $x<\\alpha^n$ and $h>0$.\nuse $(\\alpha - h)^n = (\\alpha - h)\\left( \\alpha^{n-1}+\\alpha^{n-2}h + \\dots + \\alpha h^{n-2} + h^{n-1} < n \\alpha^{n-1} \\right)$ to get $h$ alone.\nUse the upper bound on $K < n \\alpha^{n-1}$ to remove the higher order terms of $h$ that are annoying (since we are assuming we don't know how to take roots). i.e. (\\alpha - h)K < (\\alpha - h)n \\alpha^{n-1}\n\nthose seem the main ingredients but when I tried putting them together it seemed I was still missing something because playing with the algebra didn't lead to the answer. Someone know what it is?\n",
    "proof": "First, Rudin is using $n\\gt0$, and the inequality:\n$$b^n-a^n\\lt (b-a)nb^{n-1}$$\nonly holds for $n\\in\\mathbb{Z}, n\\gt1$.\nBut this isn't massively important here.\nWe are trying to find a contradiction to $y^n\\lt x$, where $x$ is given and $y=\\sup E$.\nA good place to begin is with a $y+h$, $h$ arbitrarily small and positive. In which case we get:\n$$(y+h)^n-y^n \\lt hn(y+h)^{n-1}$$\nby the above inequality (for $n\\gt1$!)\nAs $h$ is arbitrarily small, we can say $h\\lt 1$, and so we continue:\n$$(y+h)^n-y^n \\lt hn(y+h)^{n-1}\\lt hn(y+1)^{n-1}$$\nWe now use the fact that we are trying to find a contradiction, $(y+h)^n\\lt x$ for example, which gives us the contradiction Rudin then uses.\nSo we need:\n$$hn(y+1)^{n-1}\\lt x-y^n$$\nor:\n$$h\\lt \\frac{x-y^n}{n(y+1)^{n-1}}$$\nWe can see that:\n$$0\\lt \\frac{x-y^n}{n(y+1)^{n-1}}$$\nand so we are free to pick $0\\lt h\\lt 1$, as required.\n$k=\\dfrac{y^n-x}{ny^{n-1}}$ is similar, except for $k$ is already fixed, and $k\\lt y$ by the definitions given.\n",
    "tags": [
      "real-analysis",
      "proof-verification",
      "proof-writing",
      "proof-explanation",
      "alternative-proof"
    ],
    "score": 10,
    "answer_score": 2,
    "is_accepted": true,
    "question_id": 2281437,
    "answer_id": 2288370
  },
  {
    "theorem": "Is there some sort of trick to show naturality?",
    "context": "This is about natural transformations in category theory.\nAlmost always, I somewhat know why some defined maps or homomorphisms behave naturally, but I am almost never entirely sure (if things get sufficiently complicated). For quite some time now, I was wondering if there is some sort of magic trick (other than saying “it’s natural because we could define the transformation for all objects at the same time”) which automatically gives naturality of a transformation when defined only as a family of maps.\nFor example, Yoneda is beautifully natural in everything it can be natural in. It can be stated as:\n\nIf $\\mathcal C$ is a small category, then there is a natural isomorphism of functors $\\mathrm{Set}^{\\mathcal C} × \\mathcal C → \\mathrm{Set}$\n  $$\\mathrm{Ar}_{\\mathrm{Set}^{\\mathcal C}}(\\mathrm{Ar}_{\\mathcal C}(r,–),F) \\cong Fr,$$\n  which is “natural in both $F$ and $r$” for $F ∈ \\operatorname{Ob} \\mathrm{Set}^{\\mathcal C}$ and $r ∈ \\operatorname{Ob} \\mathcal C$, given by\n  $$\\mathrm{Ar}_{\\mathrm{Set}^{\\mathcal C}}(\\mathrm{Ar}_{\\mathcal C}(r,–),F) → Fr,~τ ↦ τ_r(\\mathrm{1}_r).$$\n\nOften only the bijection for fixed $F$ and $r$ is proved and nothing is said about the naturality. This is not only the case for Yoneda. Whenever I see something like this, I am trying to prove the naturality on my own, but very often I get tired and confused doing so and I give up.\n\nAre we just supposed to “see” the naturality or is it really tedious work left to the reader?\nAre there some principles behind showing naturality in such cases? What are these?\nHow can I express arguments like “since all maps are defined in the same way” in precise mathematical language? Is it even possible?\nMore generally, how should I think of naturality to just accept it whenever it’s stated and reasonable?\n\nI am asking this question because after one semester of Linear Algebra, I have been able to “see” linearity of maps, after two semesters of Analysis, I have been able to “see” continuity and differentiability (at least in the trivial cases), and so on. After struggling with category theory for about five years now, I still can’t “see” naturality. What am I missing?\n",
    "proof": "I don't know how much this helps, but in computer science the concept of parametric polymorphic function is quite useful.\nWhat the polymorphic function principle basically says is this:\n\nEvery function $f:F(a)\\rightarrow G(a)$ that is definable and polymorphic in $a$ is a natural transformation from $F$ to $G$.\n\nThe base language wrt which these functions need to be defined is usually any reasonable type-theoretic language, but the most well-known example is system F, from which the above principle follows from the parametricity theorem (the wikipedia article is quite poor, but it links the 2 main relevant papers by Reynolds and Wadler respectively).\nNote that the formal connection is a bit complicated, see this cstheory question: https://cstheory.stackexchange.com/questions/21516/natural-transformations-and-parametricity\n",
    "tags": [
      "category-theory",
      "proof-writing",
      "intuition"
    ],
    "score": 10,
    "answer_score": 2,
    "is_accepted": false,
    "question_id": 1521467,
    "answer_id": 1526532
  },
  {
    "theorem": "A very complete proof on the separability of $L^p$.",
    "context": "I have to prove the following important result.\n\nTheorem. Let $(X,\\mathcal{A},\\mu)$ be a measure space such that:\n$(1)\\;$ the measurable space $(X,\\mathcal{A})$ is separable.\n$(2)\\;$ $\\mu$ is sigma finite.\nThen $L^p(X,\\mathcal{A},\\mu)$ is separable for $p\\in [1,\\infty)$.\n\nWe remember that a measurable space $(X,\\mathcal{A})$ is said separable if exists a countable family $\\mathcal{C}\\subseteq \\mathcal{P}(X)$ sich that $\\mathcal{\\sigma}_0(\\mathcal{C})=\\mathcal{A}$, where $\\sigma_0$ denotes the generated sigma algebra.\nWe denote with $S(X,\\mathcal{A},\\mu)$ the set of all simple measurable function on $X$ a complex values.\nWe state two results that we will use.\n\nTheorem 1. Let $p\\in [1,\\infty)$, then le simple function $S(X,\\mathcal{A}, \\mu)\\cap L^p(X,\\mathcal{A},\\mu)$ are dense in $L^p$.\n\n\nLemma 2. Let $(X,\\mathcal{A},\\mu)$ be a finite measure space. Let $\\mathcal{C}\\subseteq\\mathcal{P}(X)$ a family such that $\\sigma_0(\\mathcal{C})=\\mathcal{A}$; let $\\mathcal{A}_0:=\\mathcal{A}_0(\\mathcal{C})$ the generated algebra of $\\mathcal{C}$. Then for all $F\\in \\mathcal{A}$ e for all $\\varepsilon >0$ exists $G\\in\\mathcal{A}_0$ such that $$\\mu(F\\setminus G)+\\mu(G\\setminus F)<\\varepsilon.$$\n\nFirst case $\\Large \\mu(X)<\\infty$\nWe introduce the collection $$S_{\\mathbb{Q}}(X,\\mathcal{A},\\mu)=\\{s\\in S(X,\\mathcal{A},\\mu)\\cap L^p\\;:\\; s(X)\\subseteq\\mathbb{Q}+i\\mathbb{Q}\\}$$\nFirst step $S_{\\mathbb{Q}}(X)$ is dense in $S(X)\\cap L^p$\nLet $\\varepsilon > 0 $ and $s\\in S\\cap L^p$ fixed. Let $$s=\\sum_{k=1}^n c_k \\chi_{E_k}$$ the standard representation of $s$.\nFor all $k=1,\\dots, n$ let $q_k$ be the complex number such that the real and imaginary parts are such that\n$$\\max_{k=1,\\dots, n}{|c_k-q_k|}<\\frac{\\varepsilon}{[n\\mu(X)]^{\\frac{1}{p}}}.$$ Then the simple function $$s_{\\mathbb{Q}}=\\sum_{k=1}^nq_k\\chi_{E_k}\\in S_{\\mathbb{Q}}(X)$$\nand results that\n\\begin{eqnarray*}\n\\lVert s-s_{\\mathbb{Q}} \\rVert_p^p &=& \\int_X \\left | \\sum_{k=1}^n(c_k-q_k)\\chi_{E_k}\\right |^p\\; d\\mu \\\\\n&\\color{red}{=}& \\int_X\\sum_{k=1}^n\\lvert c_k-q_k \\rvert^p\\chi_{E_k}\\;d\\mu \\\\\n&=&\\sum_{k=1}^n\\lvert c_k-q_k\\rvert^p\\mu(E_k) \\\\\n&\\le& \\sum_{k=1}^n\\frac{\\varepsilon^p}{n}\\frac{\\mu(E_k)}{\\mu(X)}\\\\\n&\\le& \\sum_{k=1}^n\\frac{\\varepsilon^p}{n}=\\varepsilon^p\n\\end{eqnarray*}\nThe red inequality arises from the fact that the $E_k$ are disjoint and the last inequality follows from the fact that $\\frac{\\mu(E_k)}{\\mu(X)}\\le 1$\nNow, since the space $(X,\\mathcal{A})$ is separable, exists a countable family $\\mathcal{C}\\subseteq \\mathcal{P}(X)$ such that $\\sigma_0(\\mathcal{C})=\\mathcal{A}$. Evidently the generate algebra $\\mathcal{A}_0:=\\mathcal{A}_0(\\mathcal{C})$ is countable. Now, we introduce the collection $$S_{\\mathbb{Q},\\mathcal{A}_0}(X,\\mathcal{A},\\mu)=\\left\\{s=\\sum_{k=1}^n d_k\\chi_{G_k}\\in S_{\\mathbb{Q}}\\;|\\; G_k\\in\\mathcal{A}_0, n\\in\\mathbb{N}\\right\\}.$$\nWe observe that this collection is also countable.\nSecond step $S_{\\mathbb{Q}, \\mathcal{A}_0}(X)$ is dense in $S_{\\mathbb{Q}}(X)$\nLet $t\\in S_{\\mathbb{Q}}(X)$, then $$t=\\sum_{k=1}^nc_k\\chi_{F_k},$$ where $F_k\\in\\mathcal{A}$ and $c_k\\in \\mathbb{Q}+i\\mathbb{Q}$ ($k=1,\\dots, n$). From the above lemma 2 for all $k=1,\\dots, n$ exists $G_k\\in\\mathcal{A}_0$ such that $$\\mu(F_k\\setminus G_k)+\\mu(G_k\\setminus F_k)<\\left(\\frac{\\varepsilon}{nM} \\right)^p,$$ where $M:=\\max _{k=1,\\dots, n}|c_k|$. Define $$s=\\sum_{k=1}^nc_k\\chi_{G_k},$$ then $s\\in S_{\\mathbb{Q},\\mathcal{A}_0}$ and results that\n\\begin{eqnarray*}\n\\lVert t-s \\rVert_p &=& \\left\\lVert \\sum_{k=1}^n c_k(\\chi_{F_k}-\\chi_{G_k})\\right\\rVert_p \\\\\n& \\stackrel{Minkowski}{\\leq}& \\sum_{k=1}^{n} \\left\\lVert c_k (\\chi_{F_k}-\\chi_{G_k})\\right \\rVert_p \\\\\n&=& \\sum_{k=1}^{n} \\lvert c_k \\rvert  \\left\\lVert \\chi_{F_k}-\\chi_{G_k}\\right\\rVert_p \\\\\n&=& \\sum_{k=1}^{n} \\lvert c_k \\rvert \\{\\mu(F_k\\setminus G_k)+\\mu(G_k\\setminus F_k)\\}^{1/p}< nM\\left(\\frac{\\varepsilon}{nM} \\right)=\\varepsilon.\n\\end{eqnarray*}\nNow, let $f\\in L^p$, then for Theorem 1. exists $s\\in S(X)\\cap L^p(X)$ such that $\\lVert f-s \\rVert_p<\\varepsilon$. For the frist step exists $s_{\\mathbb{Q}}\\in S_{\\mathbb{Q}}(X)$ such that $\\lVert s-s_{\\mathbb{Q}}\\rVert_p<\\varepsilon$, for the step 2 exists $t_{\\mathbb{Q},\\mathcal{A}_0}\\in S_{\\mathbb{Q},\\mathcal{A}_0}$ such that $$\\lVert s_{\\mathbb{Q}}-t_{\\mathbb{Q},\\mathcal{A}_0} \\rVert_p<\\varepsilon$$, then $$\\lVert f-t_{\\mathbb{Q},\\mathcal{A}_0} \\rVert_p<3\\varepsilon$$\nThis completely proves the theorem in the finite case.\nSecond case $\\mu(X)=\\infty$ Since $\\mu$ is sigma finite exists an increasing sequence $\\{E_n\\}\\subseteq\\mathcal{A}$ such that $$X=\\bigcup_{n=1}^\\infty E_n\\quad\\text{and}\\quad \\mu(E_n)<\\infty\\;\\forall n\\in\\mathbb{N}.$$ Let $s\\in S(X)\\cap L^p$, then $$\\infty > \\int_X \\lvert s\\rvert^p\\;d\\mu=\\lim_{n\\to\\infty}\\int_{E_n}\\lvert s \\rvert^p\\; d\\mu,$$ then $$(\\forall \\varepsilon>0)\\quad (\\exists n_0\\in\\mathbb{N})\\quad(\\forall n>n_0)\\quad \\int_{X\\setminus E_n} \\lvert s \\rvert^p\\;d\\mu < \\varepsilon$$\nWe define $t_n:=s\\chi_{E_n}$, then $\\{t_n\\}\\subseteq S(X)\\cap L^p(X)$. $$\\lVert t_n-s \\rVert_p=\\int_{X\\setminus E_n} \\lvert s \\rvert\\; d\\mu<\\varepsilon$$ for all $n>n_0$. We choose $$t:=t_{n_0+1},$$ then $$\\lVert t-s\\rVert_p<\\varepsilon.$$\nThe simple function $t$ is zero outside $Y:=E_{n_0+1}$, then can be seen defined only on $Y$, that is $t\\in S(Y)\\cap L^p$, observe that $\\mu(Y)<\\infty$. Thus we have that:\nLet $f\\in L^p$ be a function, then fro theorem 1 exists $s\\in S(X)\\cap L^p$ such that $\\lVert f - s \\rVert_p<\\varepsilon$. For above passage, exists $t\\in S(Y)\\cap L^p$ where $Y\\in\\mathcal{A}$ and $\\mu(Y)<\\infty$ such that $\\lvert t - s\\rVert_p<\\varepsilon$. For the first step applied to $Y$ exists $s_{\\mathbb{Q}}\\in S_{\\mathbb{Q}}(Y)$ such that\n$\\lVert t -s_{\\mathbb{Q}}\\rVert_{L^p(Y)}<\\varepsilon$ and for the second step exists $t_{\\mathbb{Q},\\mathcal{A}_0}\\in S_{\\mathbb{Q},\\mathcal{A}_0}(Y)$ such that\n$\\lVert s_{\\mathbb{Q}}-t_{\\mathbb{Q},\\mathcal{A}_0}\\rVert_{L^p(Y)}<\\varepsilon$.\nDefining $s_{\\mathbb{Q}}$ and $t_{\\mathbb{Q},\\mathcal{A}_0}$ ugual to zero in $X\\setminus Y$ we have that $$\\lVert f - t_{\\mathbb{Q},\\mathcal{A}_0} \\rVert_p<4\\varepsilon$$\n\nQuestion Is this a correct proof?\n\n",
    "proof": "I report here my answer give here\nA reference about this question is: \"Integration\" by Aadrian Cornelis Zaanen, §11, §20, §30.\nNotation and some preliminaries:\nLet $(X, \\Lambda, \\mu)$ measure space, Let $\\Lambda_1$ the collection of all finite measure of $X$.\nDef(1): $A, B \\, \\in \\Lambda_1$, $A \\sim B$ iff $\\mu(A-B)+\\mu(B-A)=0$\nWithout specifying it i will indicate with $\\Lambda_1$ both the 'real' collection and the quotient colletion $\\Lambda_1 / \\!\\sim$.\nProp(1): $(\\Lambda_1, \\rho_1)$ is a metric space, with $\\rho_1(A, B)=\\mu(A-B)+\\mu(B-A)$\nI don't report the proof of this proposition, it's easy to prove it, §11 for more details.\nIt's obvious that $\\forall A, B \\, \\in \\Lambda_1$, $\\rho_1(A, B) = \\| \\chi_A - \\chi_B\\|_1$ and $(\\Lambda_1, \\rho_1) \\hookrightarrow (L^1(X, \\Lambda, \\mu), \\| .\\|_1)$ is an isometric inclusion.\nLemma(1): Simple function are dense on $L^p$, $\\forall p \\in [1, +\\infty]$\nSeparability of $L^p$\nThm(1): $(L^1(X, \\Lambda, \\mu), \\| .\\|_1)$ is separable iff $(\\Lambda_1, \\rho_1)$ is separable.\nProof: $\\Longrightarrow$\n\nSince the isometric inclusion and the fact that, in metric space, every subset of separable is separable, these implication is trivial.\n\n$\\Longleftarrow$\n\nLet $Z \\subset (\\Lambda_1, \\rho_1)$ be countable dense, so define $S(Z)=\\text{Span}_{\\mathbb{Q}}(\\{\\chi_E\\}_{E \\in Z})$,\n\nLet be $f \\in L^1$ and $ \\epsilon > 0$,\n\nSince Lemma(1), $\\exists s_1(x)=\\sum_{n=1}^M \\alpha_n \\chi_{E_n}$, with $E_n \\in \\Lambda_1$, such that $\\|f-s_1\\|_1 \\leq \\epsilon / 3$\n\nSince density of rational in real number, $\\exists \\{\\gamma_n\\}_{n=1}^M \\in \\mathbb{Q}^M$ such that $\\sum_{n=1}^M |\\alpha_n-\\gamma_n| \\mu(E_n) \\leq \\epsilon / 3$, then we define $s_2(x)=\\sum_{n=1}^M \\gamma_n \\chi_{E_n}$ and by construction we have $\\|s_2-s_1\\|_1 \\leq \\epsilon / 3$,\n\nBy hp, $\\forall E_n \\, \\exists F_n \\in Z$ such that $\\|\\chi_{E_n} - \\chi_{F_n}\\|_1 \\leq \\epsilon / (3M |\\gamma_n|)$, so we define $s_3(x)=\\sum_{n=1}^M \\gamma_n \\chi_{F_n}$, then $s_3 \\in S(Z)$ and $\\|s_2-s_3\\|_1 \\leq \\epsilon / 3$,\n\nSo we found $\\|f-s_3\\|_1 \\leq \\|f-s_1\\|_1 + \\|s_2-s_1\\|_1 + \\|s_2-s_3\\|_1\\leq\\epsilon$. $\\square$\n\n\nThe generalization to other $p\\in [1, +\\infty)$ is immediate.\nWe observe that $(\\Lambda_1, \\rho_1) \\cong (\\Lambda_1, \\rho_p)$, where $\\rho_p(A, B)= \\| \\chi_A - \\chi_B\\|_p$, because both are metric space and trivially has the same convergent sequences then the same topology.\nThen $(\\Lambda_1, \\rho_p) \\hookrightarrow (L^p(X, \\Lambda, \\mu), \\| .\\|_p)$ is an isometric inclusion. Then we can view $(\\Lambda_1, \\rho_1)$ as subspace of $(L^p(X, \\Lambda, \\mu)$. The other implication is identical. Therefore:\nThm(2): $(L^p(X, \\Lambda, \\mu), \\| .\\|_1)$ is separable iff $(\\Lambda_1, \\rho_1)$ is separable.\nAddendum\nI read the link you added, i have two observation to do:\n\nMy notion of separability for measure space maybe is non equivalent with your, at the moment it is not clear to me.\nHere there is a question regarding relation between separability. And there using another, surely non-equivalent with 'mine', definition for separability of measure space.\n\nI think there is a bit of vagueness about the definition of separability for measure spaces.\nAnyway, $(X, \\Lambda, \\mu)$ where $\\Lambda=\\{X, \\emptyset, A, A^c\\}$ with $\\mu(X)=+\\infty, \\mu(A),\\mu(A^c) \\in (0, +\\infty)$ seem to me an example of separable and non $\\sigma$-finite space, then in order to thm(2) we found L^p are separable but the measure space is non $sigma$-finite.\n",
    "tags": [
      "real-analysis",
      "solution-verification",
      "proof-writing",
      "proof-explanation"
    ],
    "score": 10,
    "answer_score": 1,
    "is_accepted": false,
    "question_id": 4923301,
    "answer_id": 4925427
  },
  {
    "theorem": "What is an example of a proof by minimal counterexample?",
    "context": "I was reading about proof by infinite descent, and proof by minimal counterexample. My understanding of it is that we assume the existance of some smallest counterexample $A$ that disproves some proposition $P$, then go onto show that there is some smaller counterexample to this which to me seems like a mix of infinite descent and 'reverse proof by contradiction'. \nMy question is, how do we know that there might be some counterexample? Furthermore, are there any examples of this?\n",
    "proof": "Consider, for instance, the statement\n\nEvery $n\\in\\mathbb{N}\\setminus\\{1\\}$ can be written as a product of prime numbers (including the case in which there's a single prime number appearing only once).\n\nSuppose otherwise. Then there would be a smallest $n\\in\\mathbb{N}\\setminus\\{1\\}$ that could not be expressed as a product of prime numbers. In particular, this implies that $n$ cannot be a prime number. Since $n$ is also different from $1$, it can be written as $a\\times b$, where $a,b\\in\\{2,3,\\ldots,n-1\\}$. Since $n$ is the smallest counterexample, neither $a$ nor $b$ are counterexamples and therefore both of them can be written as a product of prime numbers. But then $n(=a\\times b)$ can be written in such a way too.\n",
    "tags": [
      "proof-writing",
      "examples-counterexamples",
      "infinite-descent"
    ],
    "score": 9,
    "answer_score": 29,
    "is_accepted": true,
    "question_id": 3002706,
    "answer_id": 3002716
  },
  {
    "theorem": "Proof that $6^n$ always has a last digit of $6$",
    "context": "Without being proficient in math at all, I have figured out, by looking at series of numbers, that $6$ in the $n$-th power always seems to end with the digit $6$.\nAnyone here willing to link me to a proof?\nI've been searching google, without luck, probably because I used the wrong keywords.\n",
    "proof": "We can prove it using mathematical induction.\nClaim: $6^n\\equiv 6\\bmod 10$ for all $n\\in\\mathbb{N}$ (the symbol $\\mathbb{N}$ denotes the natural numbers, and $\\bmod 10$ means we are using  modular arithmetic with a modulus of 10).\nBase case (i.e., showing it's true for $n=1$): $$6^1\\equiv 6\\bmod 10\\qquad\\checkmark$$\nInduction step (i.e., showing that, if it is true for $n=k$, then it is true for $n=k+1$):\n$$6^k\\equiv 6\\bmod 10\\implies 6^{k+1}\\equiv 6^k\\cdot 6\\equiv6\\cdot 6\\equiv 36\\equiv 6\\bmod 10\\qquad\\qquad\\checkmark$$\n",
    "tags": [
      "elementary-number-theory",
      "proof-writing",
      "exponentiation"
    ],
    "score": 9,
    "answer_score": 14,
    "is_accepted": true,
    "question_id": 62126,
    "answer_id": 62130
  },
  {
    "theorem": "Closure under matrix multiplication for 2x2 matrices",
    "context": "I need to show that this set is closed under matrix multiplication, is there a better way than doing it via a Cayley table? Or rather I assume there is and I just can't get my head around it. Any help would be greatly appreciated.\n$\\begin{gather*}\nM=\n\\left\\{\n\\begin{bmatrix} 0 & 1\\\\ 1 & 0 \\end{bmatrix},\n\\begin{bmatrix} 0 & -1\\\\ -1 & 0 \\end{bmatrix},\n\\begin{bmatrix} 1 & 0\\\\ 0 & 1 \\end{bmatrix},\n\\begin{bmatrix} -1 & 0\\\\ 0 & -1 \\end{bmatrix},\n\\begin{bmatrix} 0 & 1\\\\ -1 & 0 \\end{bmatrix},\n\\begin{bmatrix} 0 & -1\\\\ 1 & 0 \\end{bmatrix},\n\\begin{bmatrix} 1 & 0\\\\ 0 & -1 \\end{bmatrix},\n\\begin{bmatrix} -1 & 0\\\\ 0 & 1 \\end{bmatrix}\n\\right\\}\\\\\n\\end{gather*}$\n",
    "proof": "Call matrices of this $\\begin{pmatrix}*&0\\\\0&*\\end{pmatrix}$ form \"diagonal\" and matrices of this $\\begin{pmatrix}0&*\\\\*&0\\end{pmatrix}$ form \"skew diagonal\". First prove that the product of diagonal matrices is diagonal, the product of diagonal and skew diagonal is skew diagonal, the product of skew diagonal and diagonal is skew diagonal and that the product of skew diagonal and skew diagonal is diagonal.\nThen prove that every entry in the resulting matrices will always be $\\pm 1$. Finally note that every matrix which is diagonal or skew diagonal and has entries in $\\{\\pm 1\\}$ is one of your eight.\n",
    "tags": [
      "abstract-algebra",
      "matrices",
      "proof-writing",
      "cayley-table"
    ],
    "score": 9,
    "answer_score": 21,
    "is_accepted": true,
    "question_id": 2234038,
    "answer_id": 2234051
  },
  {
    "theorem": "Prove that $(a-b) \\mid (a^n-b^n)$",
    "context": "I'm trying to prove by induction that for all $a,b \\in \\mathbb{Z}$ and $n \\in \\mathbb{N}$, that $(a-b) \\mid (a^n-b^n)$. The base case was trivial, so I started by assuming that $(a-b) \\mid (a^n-b^n)$. But I found that:\n\\begin{align*}\n(a-b)(a^{n-1}+a^{n-2}b + a^{n-3}b^2+...+b^{n-1}) &= a^n-b^n.\n\\end{align*}\nDoesn't this imply that $(a-b) \\mid (a^n-b^n)$ as $a^{n-1}+a^{n-2}b+\\dots+b^{n-1}$ is clearly an integer? This obviously isn't a proof by induction, but is there anything wrong with taking this approach to prove this result, other than the fact that it isn't what is being asked? \nAside, I'm having trouble getting started with the proof by induction. I've tried making the above equivalence useful during the induction step, but it doesn't seem to be helpful, so any hints would be greatly appreciated!\n",
    "proof": "The induction step can be handled by observing that $a^{(k+1)} - b^{(k+1)} = aa^k - ab^k + ab^k - bb^k = a(a^k - b^k) + (a - b)b^k$.  Then by the inductive hypothesis, $a - b$ divides each summand.  \nWould say more but I must crash.  Perhaps I can add the details of the full inductive proof manana.  But it should be easy from here . . . \nG'night, fellow math-heads!\n",
    "tags": [
      "elementary-number-theory",
      "proof-writing",
      "induction",
      "divisibility"
    ],
    "score": 9,
    "answer_score": 14,
    "is_accepted": true,
    "question_id": 430645,
    "answer_id": 430656
  },
  {
    "theorem": "Short proof for the determinant of a $4$ by $4$ matrix",
    "context": "\nProve that $\\det \\begin{bmatrix}x&y&z&t\\\\-y&x&-t&z\\\\-z&t&x&-y\\\\-t&-z&y&x\\end{bmatrix} = (x^2+y^2+z^2+t^2)^2$\n\nI'm looking for an elegant proof that doesn't involve bruteforce.\nSince the answer is given, I'm thinking we can argue that the determinant here is a homogeneous polynomial $P(x,y,z,t)$ with degree $4$, that is invariant under $x\\to -x$ and permutations of $x,y,z,t$.\nAs a result, $P(x,y,z,t) = \\lambda (x^4+y^4+z^4+t^4) + \\delta (x^2y^2+x^2z^2 + x^2t^2+y^2z^2 + y^2t^2 + z^2t^2)$\n$\\lambda$ and $\\delta$ can be found by computing $P(0,0,0,1)$ or some such.\n\nThe problem is, it doesn't look easy to prove that $P$ doesn't change under permutation of $x,y,z,t$, neither that it's invariant when the variables are negated.\n\nCan you suggest another short proof, or prove the two claims above ?\n",
    "proof": "Hint:\n$$\n\\begin{pmatrix}\n   x &  y &  z &  t \\\\\n  -y &  x & -t &  z \\\\\n  -z &  t &  x & -y \\\\\n  -t & -z &  y &  x\n\\end{pmatrix}\n\\begin{pmatrix}\n   x & -y & -z & -t \\\\\n   y &  x &  t & -z \\\\\n   z & -t &  x &  y \\\\\n   t &  z & -y &  x\n\\end{pmatrix} = (x^2+y^2+z^2+t^2)\n\\begin{pmatrix}\n   1 & 0 & 0 & 0 \\\\\n   0 & 1 & 0 & 0 \\\\\n   0 & 0 & 1 & 0 \\\\\n   0 & 0 & 0 & 1\n\\end{pmatrix} \n$$\n",
    "tags": [
      "linear-algebra",
      "polynomials",
      "proof-writing",
      "determinant",
      "alternative-proof"
    ],
    "score": 9,
    "answer_score": 18,
    "is_accepted": true,
    "question_id": 1902343,
    "answer_id": 1902374
  },
  {
    "theorem": "Proof that the square of an odd number is of the form $8m+1$ for some integer $m$",
    "context": "\nShow that $n$ is odd $\\rightarrow \\exists m \\in \\mathbb Z,n^2  = 8m + 1$\n\nLet $k$ be an integer so that $n = 2k+1$. Then $n^2 = (2k+1)^2 = 4k^2 + 4k + 1 = 2\\left(2k^2+2k\\right) + 1$. \nBut this isn't equal to $8m+1$, so can I change that into $4(2m)+1$? \n",
    "proof": "$$n^2 = (2k+1)^2 = 4k^2+4k+1 = 4k(k+1)+1$$\nAt least one of $k$ and $k+1$ is divisible by $2$, so let $k(k+1)=2m$. Then, $n^2 = 4(2m)+1 = 8m+1$.\n",
    "tags": [
      "elementary-number-theory",
      "proof-writing",
      "modular-arithmetic"
    ],
    "score": 9,
    "answer_score": 22,
    "is_accepted": true,
    "question_id": 2453517,
    "answer_id": 2453519
  },
  {
    "theorem": "Why is the induction proof not sufficient? Topology...",
    "context": "I went to a exercise class and I got really confused. Consider the problem:\n\nLet $\\lbrace A_{n} \\rbrace$ be a sequence of connected subspaces of $X$, such that $A_{n}\\cap A_{n+1}\\neq \\emptyset$ for all $n$. Show that $\\cup A_{n}$ is connected.\n\nMy TA started by showing that for all $n\\in \\mathbb{N}$ $B_{n} = \\bigcup_{j=1}^{n} A_{j}$  is connected using induction. I understand this part. But I would finish the proof here, because if you now this holds for all natural numbers $n$ why can't you just deduce that it holds for $\\bigcup_{n=1}^{\\infty} A_{n}$? My TA didn't finished the proof here, but used the result to finish his proof in another way. \n",
    "proof": "There is a difference between knowing that each finite union is connected and knowing that the infinite union is connected. Let me give you an example, using a different property, that shows the difference very clearly.\nSuppose that $A_n=\\{n\\}$ for each $n\\in\\Bbb Z^+$. Then it’s certainly true that $\\bigcup_{k=1}^nA_k$ is finite for each $n\\in\\Bbb Z^+$, but it’s certainly not true that $\\bigcup_{k\\ge 1}A_k$ is finite. If you prefer a more topological example, think of each $A_n$ as a subset of $\\Bbb R$. Then $\\bigcup_{k=1}^nA_k$ is compact for each $n\\in\\Bbb Z^+$, but $\\bigcup_{k\\ge 1}A_k$ is not compact.\n",
    "tags": [
      "general-topology",
      "proof-writing",
      "induction"
    ],
    "score": 9,
    "answer_score": 20,
    "is_accepted": true,
    "question_id": 272891,
    "answer_id": 272894
  },
  {
    "theorem": "A finite graph with exactly two vertices with odd degree must have a path joining them.",
    "context": "I would really appreciate it if anyone would validate if my argument ( proof ? ) for the above statement is valid. I am aware of other proofs but the current argument is more of a task in rational/logical thinking and mathematical writing style. \nI am aware this is a open ended question but I would also like it if people would comment on my style so that I can improve on it.\n\nLet $G$ be a disconnected finite graph with exactly two vertices, $u$ and $v$, with odd degrees and two components. Suppose that $u$ and $v$ belong to different components of $G$. \nThen either of the two possibilities given below must hold such that number of edges in $G$ is a positive integer.\n1) There exists at least one other vertex with odd degree in each component of $G$ such that the number of edges in each component is a positive integer. A contradiction with our assumption that $G$ has exactly two vertices with odd degree.\n2) There exists an edge joining the two components. Another contradiction, as we have said previously that $G$ is disconnected.\n",
    "proof": "\"Let $G$ be a disconnected finite graph....\" Why? The proof should start with \"If $G$ is connected, then of course there's a path between any two vertices, so assume $G$ is a disconnected finite graph....\" \n\"...with exactly two vertices ... with odd degrees...\" Why? You are trying to prove something about graphs with at least two vertices of odd degree, not exactly two vertices of odd degree. And why two components? I think it should go, \"...with at least two vertices, $u$ and $v$, of odd degree.\" \n\"Then either of the two possibilities given below must hold such that [the] number of edges in $G$ is a positive integer.\" This is not English. The clause beginning \"such that\" doesn't make sense. What are you trying to say? \nYou give no reason why one of the two possibilities must hold. You write ungrammatical sentences (the two sentences with the word, \"contradiction\", are not grammatically correct). \nOn top of this, there is Didier's objection that what you are trying to prove is false. Maybe you really do want the hypothesis to state \"exactly\" instead of \"at least\". Or maybe you want the conclusion to be \"a path joining some pair of vertices of odd degree.\"  \n",
    "tags": [
      "graph-theory",
      "proof-writing"
    ],
    "score": 9,
    "answer_score": 4,
    "is_accepted": true,
    "question_id": 122006,
    "answer_id": 122009
  },
  {
    "theorem": "Prove compact subsets of metric spaces are closed",
    "context": "\nProve compact subsets of metric spaces are closed\n\n\nNote, this question is more of analyzing an incorrect proof of mine rather than supplying a correct proof. \n\nMy Attempted Proof\nSuppose $X$ is a metric space. Let $A \\subset X$ be a compact subset of $X$ and let $\\{V_{\\alpha}\\}$ be an open cover of $A$. Then there are finitely many indices $\\alpha_{i}$ such that $A \\subset V_{\\alpha_{1}} \\cup \\ ... \\ \\cup V_{\\alpha_{n}}$.\nNow let $x$ be a limit point of $A$. Assume $x \\not\\in A$. If $x \\not\\in A$ put $\\delta = \\inf \\ \\{\\ d(x, y) \\  | \\ y \\in A\\}$. Take $\\epsilon = \\frac{\\delta}{2}$, then $B_d(x, \\epsilon) \\cap A = \\emptyset$ so that a neighbourhood of $x$ does not intersect $A$ asserting that $x$ cannot be a limit point of $A$, hence $x \\in A$ so that $A$ is closed. $\\square$.\n\nNow there must be something critically wrong in my proof, as I don't even use the condition that $A$ is compact anywhere in the contradiction that I establish. The above proof would assert that every subset of a metric space is closed.\nI think my error must be in the following argument : $\\delta = \\inf \\ \\{\\ d(x, y) \\  | \\ y \\in A\\}$. For if we take $X = \\mathbb{R}$ and $A = (0, 1) \\subset \\mathbb{R}$, then $\\delta = 0$ if $x = 1$ or $x = 0$.\nAm I correct in analyzing this aspect of my proof?\n",
    "proof": "Suppose $A$ is compact. Then let $x$ be in $X \\setminus A$. We must find a neighbourhood of $x$ that is disjoint from $A$ to show closedness of $A$.\nFor every $a \\in A$ let $U_a = B(a, \\frac{d(a,x)}{2})$ and $V_a =B(x, \\frac{d(a,x)}{2})$. Then $U_a$ and $V_a$ are disjoint open neighbourhoods of $a$ and $x$ respectively (disjoint follows from the triangle inequality, check this).\nThen the $U_a ,a \\in A$ together form a cover of $A$, and now we use compactness of $A$ to get finitely many neighborhoods $U_{a_1}, \\ldots, U_{a_n}$, that also cover $A$. But then $V_{a_1} \\cap \\ldots V_{a_n}$ is an open neighbourhood of $x$ that misses $A$ (since a point in $A$ cannot be in all of $V_{a_1}, \\ldots, V_{a_n}$ because it belongs to some $U_{a_i}$ which is disjoint with $V_{a_i}$.).\nSo $A$ is closed.\nAn alternative with sequences: suppose $x \\in \\overline{A}$. Then being a point in the closure we find a sequence in $A$, $(a_n)$ that converges to $x$. The sequence $(a_n)$ from $A$ has a convergent subsequence in $A$ by compactness of $A$, so there is some $a \\in A$ and some subsequence $a_{n_k} \\rightarrow a$. But also $a_{n_k} \\rightarrow x$, and as limits of sequences in metric spaces are unique: $a =x$, but then $x \\in A$ as required: $\\overline{A} = A$. \n",
    "tags": [
      "real-analysis",
      "general-topology",
      "proof-verification",
      "metric-spaces",
      "proof-writing"
    ],
    "score": 9,
    "answer_score": 20,
    "is_accepted": true,
    "question_id": 2111662,
    "answer_id": 2112007
  },
  {
    "theorem": "Prove that $\\sqrt[3]{5} + \\sqrt{2}$ is irrational",
    "context": "I tried with both squaring and cubing the statement, it got messy, here's my  latest attempt: \nAssume for the sake of contradiction: $\\sqrt[3]{5} + \\sqrt{2}$ is rational\n$\\sqrt[3]{5} + \\sqrt{2}$ = $\\frac{a}{b}$ $a,b$ are odd integers $> 0$ and $ b\\neq 0$ \n${(\\sqrt[3]{5} + \\sqrt{2})}^3$ = $\\frac{a^3}{b^3}$\nby multiplying by $b^3$: \n${(\\sqrt[3]{5} + \\sqrt{2})}^3 \\times b^3 $ = ${a^3}$\nso: $a^3$ is divisible by ${(\\sqrt[3]{5} + \\sqrt{2})}^3$ which means $a$ is divisible by ${(\\sqrt[3]{5} + \\sqrt{2})}$\ndoing the same thing with $b$ i found : \n$\\frac{a^3}{{(\\sqrt[3]{5} + \\sqrt{2})}^3}   $ = ${b^3}$\nso: $b^3$ is divisible by ${(\\sqrt[3]{5} + \\sqrt{2})}^3$ which means $b$ is divisible by ${(\\sqrt[3]{5} + \\sqrt{2})}$  (wrong)\n${(\\sqrt[3]{5} + \\sqrt{2})}$ is a common divisor for both $a$ & $b$ which is a contradiction, thus $\\sqrt[3]{5} + \\sqrt{2}$ is irrational. (wrong)\n",
    "proof": "You can't do divisibility in irational and rational numbers. When you are operating with divisibility you have to have an integers. It is a relation defined on integer numbers.\n\nSuppose it is rational, then exist rational number $q$ such that $$\\sqrt[3]{5} + \\sqrt{2}= q$$ so $$ 5 = (q-\\sqrt{2})^3 = q^3-3q^2\\sqrt{2}+6q-2\\sqrt{2}$$\nSo we have $$\\sqrt{2}(\\underbrace{3q^2+2}_{\\in\\mathbb{Q}}) = \\underbrace{q^3+6q-5}_{\\in\\mathbb{Q}}$$\nso $$\\sqrt{2}=  \\underbrace{q^3+6q-5\\over 3q^2+2}_{\\in\\mathbb{Q}}$$\nA contradiction.\n",
    "tags": [
      "proof-writing",
      "irrational-numbers"
    ],
    "score": 9,
    "answer_score": 12,
    "is_accepted": true,
    "question_id": 2975313,
    "answer_id": 2975323
  },
  {
    "theorem": "Formalize a proof without words of the identity $(1 + 2 + \\cdots + n)^2 = 1^3 + 2^3 + \\cdots + n^3$",
    "context": "This website gives the following proof without words for the identity  $(1 + 2 + \\cdots + n)^2 = 1^3 + 2^3 + \\cdots + n^3$.\n\nI find it interesting but have trouble seeing the proof behind it.  Could anyone could give me a formal proof based on this animation?\n",
    "proof": "This gif is a nice visualization of the following algebraic manipulations (color to correspond to the colors in the gif):\n\\begin{align*}\n(1 + 2 + 3 + \\cdots + n)^2\n&= \\sum_{i=1}^n \\sum_{j=1}^n ij \\\\\n&= \\color{green}{\\sum_{1 \\le i < j \\le n} ij} + \\color{red}{\\sum_{1 \\le j < i \\le n} ji} + \\color{blue}{\\sum_{1 \\le j \\le n} j^2} \\\\\n&= \\color{green}{\\sum_{1 \\le i < j \\le n} ij} + \\color{red}{\\sum_{1 \\le i < j \\le n} ji} + \\color{blue}{\\sum_{1 \\le j \\le n} j^2} \\\\\n&= \\sum_{j=1}^n j \\left[ \\color{green}{\\sum_{i=1}^{j-1} i} + \\color{red}{\\sum_{i=1}^{j-1} i} + \\color{blue}{j} \\right] \\\\\n&= \\sum_{j=1}^n j \\left[ \\sum_{i=1}^{j-1} \\big(\\color{green}{(i)} + \\color{red}{(j-i)}\\big) + \\color{blue}{j} \\right] \\\\\n&= \\sum_{j=1}^n j \\left[ \\sum_{i=1}^j j\\right] \\\\\n&= \\sum_{j=1}^n j^3.\n\\end{align*}\n\n",
    "tags": [
      "summation",
      "proof-writing"
    ],
    "score": 9,
    "answer_score": 17,
    "is_accepted": true,
    "question_id": 1089990,
    "answer_id": 1090042
  },
  {
    "theorem": "Prove that if $\\mathcal P(A) \\cup \\mathcal P(B)= \\mathcal P(A\\cup B)$ then either $A \\subseteq B$ or $B \\subseteq A$.",
    "context": "Prove that for any sets $A$ or $B$, if $\\mathcal P(A) \\cup \\mathcal P(B)= \\mathcal P(A\\cup B)$ then either $A \\subseteq B$ or $B \\subseteq A$. ($\\mathcal P$ is the power set.)\nI'm having trouble making any progress with this proof at all. I've assumed that $\\mathcal P(A) \\cup \\mathcal P(B)= \\mathcal P(A\\cup B)$, and am trying to figure out some cases I can use to help me prove that either $A \\subseteq B$ or $B \\subseteq A$. \nThe statement that $\\mathcal P(A) \\cup \\mathcal P(B)= \\mathcal P(A\\cup B)$ seems to be somewhat useless though. I can't seem to make any inferences with it that yield any new information about any of the sets it applies to or elements therein. The only \"progress\" I seem to be able to make is that I can conclude that $A \\subseteq A \\cup B$, or that $B \\subseteq A \\cup B$, but I don't think this gives me anything I don't already know. I've tried going down the contradiction path as well but I haven't been able to find anything there either.\nI feel like I am missing something obvious here though...\n",
    "proof": "Hint: Try instead to prove the contrapositive: \n\nIf $A \\nsubseteq B$ and $B \\nsubseteq A$, then $\\mathcal{P} ( A ) \\cup \\mathcal{P} ( B ) \\neq \\mathcal{P} ( A \\cup B )$.\n\nRemember that $E \\nsubseteq F$ means that there is an element of $E$ which is not an element of $F$.\n",
    "tags": [
      "elementary-set-theory",
      "proof-writing"
    ],
    "score": 9,
    "answer_score": 8,
    "is_accepted": true,
    "question_id": 345978,
    "answer_id": 345979
  },
  {
    "theorem": "Prove $e^{\\ln{x}} = x$",
    "context": "Is it possible to prove $e^{\\ln{x}} = x$ for a student or can you only say that exponentiation is defined to be the inverse of natural logarithm and stop there?\n",
    "proof": "For a student. Let $x>0$. Then\n\\begin{align*}\ne^{ln x} &= x \\\\\n\\Leftrightarrow \\ln\\left(e^{ln x}\\right) &= \\ln x \\\\\n\\Leftrightarrow \\ln x \\cdot \\ln e &= \\ln x \\\\\n\\Leftrightarrow \\ln x \\cdot 1 &= \\ln x \\\\\n\\Leftrightarrow \\ln x &= \\ln x \\\\\n\\Leftrightarrow x &= x.\n\\end{align*}\n$ \\therefore e^{ln x} = x \\, \\forall \\, x>0.$\nAppealing to the fact that $\\log a^b=b \\cdot \\log a$ is perfectly acceptable, as it is a prerequisite for being in the club of people who are allowed to express logarithms.\nAs my commenters rightfully state, this identity proof is best shown in both directions,  with the caveat that $x>0$.\n",
    "tags": [
      "proof-writing",
      "logarithms"
    ],
    "score": 9,
    "answer_score": 14,
    "is_accepted": false,
    "question_id": 498598,
    "answer_id": 500071
  },
  {
    "theorem": "Pattern in twin primes",
    "context": "I recently noticed a pattern in twin primes. My questions are: does this pattern continue to hold indefinitely, and how would I prove it? Here's the pattern:\nFor the $n$th prime, there exists exactly $n-2$ twin prime pairs that can be created as follows:\n$p_n$ is the $n$th prime,\n$P_p=\\prod_{1<m<n}p_m$\n$(p_nP_p-4, p_nP_p-2)$\nHere's what I've worked up to:\n$n=3$ $(p_n=5)$ has $3-2=1$ twin prime pairs\n$P_p=3$ gives $(15-4,15-2)=(11,13)$\n$n=4$ has 2\n$P_p=3$ gives $(17,19)$\n$P_p=3\\cdot 5$ gives $(101,103)$\n$n=5$ has 3\n(29,31), (227,229), (1151,1153)\n$n=6$ has 4\n(191,193), (269,271), (2141,2143), (2999,3001)\n$n=7$ has 5\n(659,661), (2801,2803), (4637,4639), (23201,23203), (255251,255253)\nAgain, my questions are: does this pattern continue to hold indefinitely, and how would I prove it?\nP.S. How about a pic of a brute force Mathematica script up to like 20?\n",
    "proof": "This is an answer to your last demand:\nDo[\n q = Prime[n] Times@@@Rest[Subsets[Table[Prime[k], {k, 2, n - 1}]]];\n twin = Intersection[Select[q - 4, PrimeQ] + 2, Select[q - 2, PrimeQ]];\n Print[\"n = \", n, \" - \", twin, \" - \", Length[twin]],\n {n, 3, 20}]\n\nThe pattern breaks at $n=9$, since there are $8$ and not $7$ pairs of twin primes. For $n=20$ there are $2674$ pairs.\nI have included the possibility of $m=2$, since you use it (although in the question you say $2<m<n$.)\n",
    "tags": [
      "linear-algebra",
      "proof-writing",
      "solution-verification",
      "prime-numbers"
    ],
    "score": 9,
    "answer_score": 8,
    "is_accepted": true,
    "question_id": 1660283,
    "answer_id": 1660342
  },
  {
    "theorem": "Why Can&#39;t $\\delta$ depend on $x$ in $\\delta-\\epsilon$ Proofs",
    "context": "When proving $\\lim_{x\\to a} x^2 = a^2$ we need to show that for all $\\epsilon > 0$ there is some $\\delta > 0$ so that if $0<|x-a|<\\delta$ then $|x^2-a^2|< \\epsilon$. Now writing $|x^2-a^2|$ as $|x-a||x+a|$ why can't we just pick $\\delta = \\frac{\\epsilon}{|x+a|}$. Why is the reason that $\\delta$ must be independent of $x$?\n",
    "proof": "The definition of limit states:\n$$\\lim_{x \\to a}f(x) = L :\\iff\\forall \\epsilon > 0: \\exists \\delta > 0: \\forall x \\in dom(f) : (0 < |x-a| < \\delta \\implies|f(x)-L| < \\epsilon)$$\nFrom a logic point of view, $\\delta$ can only depend on the things that are introduced before the $\\delta$ is introduced. I.e., here $\\delta$ can only depend on $\\epsilon$. This is a rule that generally works for expressions with  nested quantifiers. \n",
    "tags": [
      "limits",
      "proof-writing",
      "epsilon-delta"
    ],
    "score": 9,
    "answer_score": 7,
    "is_accepted": true,
    "question_id": 2686159,
    "answer_id": 2686167
  },
  {
    "theorem": "Proving the Poincare Lemma for $1$ forms on $\\mathbb{R}^2$",
    "context": "I am trying to prove the Poincare Lemma for $1$ forms on $\\mathbb{R^2}$. So I said that if I doing this, I start of with\n$$\\omega = f_1(x_1,x_2) dx_1 + f_2(x_1,x_2)dx_2.$$\nFirst thing I want to prove is $d\\omega = 0$. So, I get\n$$d \\omega = df_1 \\wedge dx_1 + df_2\\wedge x_2$$\n$$ = \\left( \\frac{\\partial f_1}{\\partial x_1}dx_1 + \\frac{\\partial f_1}{\\partial x_2} dx_2 \\right) \\wedge dx_1 + \\left(\\frac{\\partial f_2}{\\partial x_1} dx_1 + \\frac{\\partial f_2}{\\partial x_2}dx_2 \\right) \\wedge dx_2. $$\n$dx_1 \\wedge dx_1 = 0$ and $dx_2 \\wedge dx_2 = 0$ and so we get\n$$\\frac{\\partial f_1}{\\partial x_2} dx_2 \\wedge dx_1 + \\frac{\\partial f_2}{\\partial x_1}dx_1 \\wedge dx_2.$$\nFrom the anti commutatitivty law we have $dx_1 \\wedge dx_2 = - dx_2 \\wedge dx_2$, and so we put this in and collect like terms and get\n$$\\left(\\frac{\\partial f_2}{\\partial x_1} - \\frac{\\partial f_1}{\\partial x_2} \\right) dx_i \\wedge dx_2,$$\nwhich tells me that $d \\omega = 0 \\iff \\frac{\\partial f_2}{\\partial x_1} = \\frac{\\partial f_1}{\\partial x_2}$, but I'm stuck how to use this to show that $\\omega = d \\eta$. Can someone help me please?\n",
    "proof": "Well if $ d\\omega = 0 $ then for any smooth closed curve $ \\gamma $ in $\\mathbb{R}^2 $ the area enclosed is a smooth compact manifold $M$ where $ \\partial M = \\gamma $. From stokes theorem we have for any closed curve $\\gamma $\n$$ \\int_\\gamma \\omega = \\int_{\\partial M} \\omega = \\int_M d\\omega = 0 $$\nThus if $\\gamma_1 $ and $\\gamma_2 $ are curves with same endpoints then $ \\int_{\\gamma_1}\\omega = \\int_{\\gamma_2}\\omega $. So let for $ x = (x_1,x_2) $ we define $\\gamma_x $ as any curve from origin to $x$ and define $ F : \\mathbb{R}^2 \\rightarrow \\mathbb{R} $ as \n$$ F(x) = \\int_{\\gamma_x}\\omega $$\nThus for any curve $\\alpha $ from $x$ to $x+he_k$ where $h>0 $ and $ k\\in\\{1,2\\} $ we have $ F(x+he_k) -F(x) = \\int_\\alpha \\omega $. So for $ f = (f_1,f_2) $ that are given, if we choose $\\alpha $ as the straight line i.e. $ \\alpha(t) = x + the_k $ then $ \\alpha (0) = x,\\ \\alpha(1) = x +he_k $ then we arrive at for $k\\in \\{1,2\\} $\n$$ F(x+he_k) -F(x) = \\int^1_0 (f\\circ\\alpha)(t).\\alpha'(t)dt = \\int^1_0 f(x+the_k).he_k dt = h\\int^1_0f_k(x+the_k)dt $$\nIf we define the function\n$$ g_k(t) = \\int^t_0f_k(x+ue_k)du $$ then $g_k(0) = 0 $ and $ g_k'(t) = f_k(x+te_k) $ hence $ g_k'(0) = f_k(x) $. Then from above we clearly have\n$$ \\frac{F(x+he_k)-F(x)}{h} = \\int^1_0f_k(x+the_k)dt = \\frac{1}{h}\\int^h_0 f_k(x+ue_k)du = \\frac{g_k(h)-g_k(0)}{h} $$\nHence taking $h \\rightarrow 0 $ we have $ \\partial F/\\partial x_k = g_k'(0) = f_k(x) $. Thus \n$$ \\omega = f_1dx_1 + f_2dx_2 = \\frac{\\partial F}{\\partial x_1}dx_1 + \\frac{\\partial F}{\\partial x_2}dx_2 = dF $$\n",
    "tags": [
      "differential-geometry",
      "proof-writing",
      "exterior-algebra"
    ],
    "score": 9,
    "answer_score": 16,
    "is_accepted": true,
    "question_id": 367343,
    "answer_id": 367404
  },
  {
    "theorem": "Stuck with proof for $\\forall A\\forall B(\\mathcal{P}(A)\\cup\\mathcal{P}(B)=\\mathcal{P}(A\\cup B)\\rightarrow A\\subseteq B \\vee B\\subseteq A)$",
    "context": "I came to point where I suppose for case 1 that $A\\subseteq B$ and conclusion is trivial. For case 2 I suppose that $A\\not\\subseteq B$ and try to prove $B\\subseteq A$, but that gets me nowhere. Any pointers here are most welcome.\n",
    "proof": "Here's a proof without contrapositive, if you prefer. \nSuppose that $P(A) \\cup P(B) = P(A \\cup B)$. Then $A \\cup B \\in P(A \\cup B) = P(A) \\cup P(B)$. This means that $A \\cup B$ is an element of either $P(A)$ or $P(B)$. Thus, either $B \\subseteq A \\cup B \\subseteq A$ or $A \\subseteq A \\cup B \\subseteq B$. \n",
    "tags": [
      "elementary-set-theory",
      "proof-writing"
    ],
    "score": 9,
    "answer_score": 7,
    "is_accepted": true,
    "question_id": 246491,
    "answer_id": 246529
  },
  {
    "theorem": "How does $\\tan 70^\\circ - \\sec 10^\\circ$ have the exact value of $\\sqrt{3}$?",
    "context": "Recently, while trying to solve a problem posed by the Youtube video here, I found the following relation:\n$$\\tan 70^\\circ - \\sec 10^\\circ = \\sqrt{3}$$\nThis relation is exact, and that can be proved by combining the purely geometric arguments presented in the video linked above and some basic trigonometry. \nBut as far as I know, the trigonometric ratios of $10^\\circ$ and $70^\\circ$ cannot be expressed in exact form (using only square roots). In fact, Gauss proved that the sine or cosine of any angle ${360^\\circ}\\over n$ cannot be expressed in terms of fractions and square roots unless $n = 2^m\\cdot\\prod p_i$ where every $p_i$ is a Fermat prime: neither $10^\\circ$ nor $70^\\circ$ is even constructible.\nSo how does the stated relation hold? I have been unable to prove why it should ... any insights appreciated! \n",
    "proof": "We need to prove that:\n$$\\tan70^{\\circ}-\\tan60^{\\circ}=\\frac{1}{\\cos10^{\\circ}}$$ or\n$$\\frac{\\sin10^{\\circ}}{\\cos70^{\\circ}\\cos60^{\\circ}}=\\frac{1}{\\cos10^{\\circ}}$$ or\n$$2\\sin10^{\\circ}\\cos10^{\\circ}=\\sin20^{\\circ},$$ which is true.\n\nThe first step follows from the sine addition subtraction rules and the tangent rules:\n$$\\sin(\\alpha\\pm\\beta)=\\sin\\alpha\\cos\\beta\\pm\\cos\\alpha\\sin\\beta\\\\=\\left(\\frac{\\sin\\alpha}{\\cos\\alpha}\\pm\\frac{\\sin\\beta}{\\cos\\beta}\\right)(\\cos\\alpha\\cos\\beta)=(\\tan\\alpha\\pm\\tan\\beta)(\\cos\\alpha\\cos\\beta)\\\\\\tan\\alpha\\pm\\tan\\beta=\\frac{\\sin(\\alpha\\pm\\beta)}{\\cos\\alpha\\cos\\beta}$$\nThe second step follows because $\\cos60^\\circ=\\frac12$ and $\\cos70^\\circ=\\sin20^\\circ$\n",
    "tags": [
      "trigonometry",
      "proof-writing"
    ],
    "score": 9,
    "answer_score": 13,
    "is_accepted": true,
    "question_id": 3502752,
    "answer_id": 3502759
  },
  {
    "theorem": "Logic of the implication in $ε$-$δ$ proofs",
    "context": "Im confused by why epsilon delta proofs logically work.\nAn example is \nProof: \nGiven $ε>0$, choose $δ = {ε\\over3}$. For all $x$, if $0<|x−2|<δ$ then $|(3x−1)−5| < ε$.\nThat last part  if $0<|x−2|<δ$ then $|(3x−1)−5| < ε$ LOOKS A LOT LIKE $P\\to Q$ because of the \"if then\" but yet the proof in the book solves it like as if its $Q\\to P$?:\n$$\\begin{align}|(3x−1)−5| &= |3x−6|\\\\ &= |3(x−2)|\\\\ &= 3|x−2|\\\\ &<3δ\\\\ &= 3\\left({ε\\over3}\\right) \\\\ &= ε\\end{align}$$\nSo my question is how come it looks like a $P\\to Q$ proof but yet we start with $Q$ to show $P$?\n",
    "proof": "The most basic way to prove a statement of the form \"If $P$ then $Q$\" is to assume $P$ and prove $Q$.\nIn this case $P$ is \"$0<|x-2|<\\delta$\", and $Q$ is $|(3x-1)-5|<\\epsilon$. Earlier in the proof we defined $\\delta = \\epsilon/3$.\nSo we assume $P$: Assume that the statement $0<|x-2|<\\delta$ is true, and set out to show that $Q$ holds. We accomplish this by the string of inequalities\n$$|(3x−1)−5| = |3x−6| = |3(x−2)| = 3|x−2|<3δ,$$\nthen we use $\\delta = \\epsilon/3$ to continue\n$$3\\delta = 3(\\epsilon/3) = \\epsilon.$$\nTherefore the first term is less than the last, which is precisely $Q$.\n",
    "tags": [
      "calculus",
      "limits",
      "proof-writing",
      "proof-explanation",
      "epsilon-delta"
    ],
    "score": 9,
    "answer_score": 5,
    "is_accepted": true,
    "question_id": 2463130,
    "answer_id": 2463140
  },
  {
    "theorem": "$A \\oplus B = A \\oplus C$ imply $B = C$?",
    "context": "I don't quite yet understand how $\\oplus$ (xor) works yet. I know that fundamentally in terms of truth tables it means only 1 value(p or q) can be true, but not both.\nBut when it comes to solving problems with them or proving equalities I have no idea how to use $\\oplus$.\nFor example: I'm trying to do a problem in which I have to prove or disprove with a counterexample whether or not $A \\oplus B = A \\oplus C$  implies $B = C$ is true. \nI know that the venn diagram of $\\oplus$ in this case includes the regions of A and B excluding the areas they overlap. And similarly it includes regions of A and C but not the areas they overlap. It would look something like this:\n\nI feel the statement above would be true just by looking at the venn diagram since the area ABC is included in the $\\oplus$, but I'm not sure if that's an adequate enough proof. \nOn the other hand, I could be completely wrong about my reasoning.  \nAlso just for clarity's sake: Would $A\\cup B = A \\cup C$ and $A \\cap B = A \\cap C$ be proven in a similar way to show whether or not the conditions imply $B = C$? A counterexample/ proof of this would be appreciated as well.\n",
    "proof": "Think of $\\oplus$ as $\\neq$. That is $A \\oplus B$ iff $A \\neq B$.\nNote that $A \\oplus A$ is always false, and $\\text{False}\\oplus A = A$.\nThen \n$A \\oplus (A \\oplus B) = (A \\oplus A) \\oplus B = \\text{False} \\oplus B = B $.\nSimilarly, $A \\oplus (A \\oplus C) = C$, hence $B=C$.\nAside: A 'cute' (as in amusing but not of any practical significance) use of $\\oplus$ is to swap the values of two bit variables in a programming language without using an intermediate variable:\n\\begin{eqnarray}\nx = y \\oplus x \\\\\ny = y \\oplus x \\\\\nx = y \\oplus x \\\\\n\\end{eqnarray}\nShow that the values of $x,y$ are swapped!\n",
    "tags": [
      "elementary-set-theory",
      "discrete-mathematics",
      "proof-writing"
    ],
    "score": 9,
    "answer_score": 10,
    "is_accepted": true,
    "question_id": 537172,
    "answer_id": 537188
  },
  {
    "theorem": "Is the set of all invertible $n \\times n$ matrices a vector space?",
    "context": "I'm studying Algebra and I'm asked to prove or disprove \"Is the set of all invertible $n \\times n$ matrices a vector space?\" I assume with respect to the usual matrix-sum and scalar multiplication.  I found that is true, but I'm not sure how to prove it. \nMy problem here is that this statement is too \"broad\", i.e. I cannot create a matrix with arbitrary values a,b,c,d,(...) considering that I don't know the size of the matrix.\nMy idea was to prove in a first time that the set of all invertible 2 x 2 matrices of real number is a vector space and then to show that this property could be extended to bigger matrices. However I don't know how to do it and it's precisely here that I need a little help ; how can I show that we can extend our statement?\nIs it enough to say that the matrix's size doesn't affect in any way properties we need to check? \nIs my overall strategy wrong?\nAny form of help would be very appreciated on this dubious answer.\n",
    "proof": "The set of all invertible $n\\times n$ matrices of real numbers is NOT a vector space.\nLet for example $I$, the unit matrix is invertible and so is $-I$. But their sum $I+(-I)=0$ is definitely not invertible!\n",
    "tags": [
      "linear-algebra",
      "matrices",
      "proof-writing",
      "matrix-calculus"
    ],
    "score": 9,
    "answer_score": 16,
    "is_accepted": false,
    "question_id": 641924,
    "answer_id": 641929
  },
  {
    "theorem": "Clarification on Without Loss of Generality",
    "context": "Suppose we have the following proposition:\nSuppose $a$ and $b$ are integers. If $a$ is odd or $b$ is odd, then $ab$ is odd or $a+b$ is odd.\nThe solution key I am looking at broke the proof into three cases: 1) if $a$ is odd, 2) if $b$ is odd, and 3) if $a$ and $b$ are odd. For each case, they prove that either $ab$ is odd or $a+b$ is odd. However, I feel as though this was unnecessary. In this context, $a$ and $b$ are indistinguishable because of the commutative properties of addition and multiplication. As such, the process to prove the first two cases is nearly identical.\nIn my proof, I broke the explanation into two cases: 1) if one integer ($a$ or $b$) is odd, and 2) if both integers are odd. For the first case, I stated, \"Without loss of generality suppose $a$ is odd and $b$ is even....\"\nI write all of this to say: is this a responsible use of WLOG? I currently understand WLOG as a simplification step that allows us to condense our proof if several cases employ the same logic, varying only in the literal representation of x's and o's (in this case the a's and b's). Is this the right way of understanding WLOG, or is there a better way?\nThis question has also brought up the nuance of WLOG. That is, the appropriate use of WLOG not only depends on the hypothesis but also on what we are trying to prove. Suppose, instead, that we were proving a statement related to division (like $\\frac{a}{b}$). Even if our hypothesis remained the same, the cases would no longer be interchangeable, since division is not commutative. This seems like a trivial realization, but, to this point, most of the examples that I had seen seemed to imply that WLOG seemed to apply in any circumstance where we had interchangeable elements in the premise.\n",
    "proof": "\nI write all of this to say: is this a responsible use of WLOG?\n\nYes, you are using WLOG here exactly as it is intended to be used.\n\nI currently understand WLOG as a simplification step that allows us to condense our proof if several cases employ the same logic, varying only in the literal representation of x's and o's (in this case the a's and b's). Is this the right way of understanding WLOG, or is there a better way?\n\nAlthough using WLOG can sometimes indeed result in the proof being more condensed, I think it's mostly a \"simplification\" in the sense that it makes the proof easier to read and understand (by a human who is versed in the conventions of mathematical writing, that is). Brevity is not always the goal, nor is it always the result, of using WLOG.\nOne nuance that often throws off less experienced readers of proofs when they encounter the WLOG idiom (and less experienced writers when they are considering using WLOG in their own proofs) is that the WLOG bit is often inserted without any explanation as to why there is no loss of generality. That part is left to the reader to figure out on their own. (And that might be one of the things that makes the proof more condensed - it's actually omitting some details, which obviously helps to make the proof shorter.) Usually it's quite easy to fill in that explanation if you are following the details of the proof, but sometimes this can require a bit of mental effort, and in any case one needs to acquire the particular mental habit of explaining to yourself why each WLOG assertion is justified. Students in the early phase of their mathematical training, particularly some of them who are used to detailed proofs that spoon-feed them with every little detail, are sometimes quite baffled about how to make sense of a \"WLOG\" assertion.\nIn your example, I think the reason for your doubt about whether you are using WLOG correctly is precisely because the convention in mathematical writing allows for making the WLOG claim without explaining why it is justified. If you were forced to add the explanation, you would do so (as you have done when writing this question), and then you would feel more confident that your explanation is correct. I have seen many of those same students who are baffled by WLOGs attempt to use WLOG when writing their own proofs, and, again probably because they have been trained through their reading of textbooks to just say \"without loss of generality\" with no further explanation, they sometimes end up peppering their writing with random-seeming \"WLOG\"s that make little sense and are often hilariously wrong.\nWhat this suggests is that it might be good practice to get in the habit of always including an explanation for why there is no loss of generality whenever you make a WLOG claim in your writing. Afterwards, you can go through the text and delete some or all of the explanations if they seem too obvious for the type of readers you are aiming your writing for. For example, I would say the explanation for the example in your question is pretty clearly not needed if you're writing for mature mathematicians. But it might still be helpful to write this explanation for yourself. Moreover, there are occasionally cases when the explanation would be needed, and when just saying \"Without loss of generality suppose $a$ is odd and $b$ is even\" could end up annoying and frustrating your readers, to the point where it would not necessarily be what I consider a \"responsible use\" of the WLOG convention.\n\nEDIT adding some more thoughts addressing a comment below by @Teepeemm:\n@Teepeemm quoted someone else's statement that WLOG means \"we prove the claim in a special case and the general case can be logically (and easily) reduced to this special case - hence our proof actually shows the claim in the general case!\"\nI agree with that description (especially in the context in which it was made where it was correcting someone's misguided idea of what WLOG is meant to be used for). But another way of thinking about WLOG is that it's a convention for organizing our choice of labels. If we have two numbers $a$ and $b$ and we know that one of them is odd and the other is even, and there is some claim about the two numbers that treats them symmetrically, then WLOG is a kind of notational device that says \"we are free to choose which of the labels we use for the odd one and which for the even one because from the point of view of the claim that doesn't make any difference, so let's choose the label $a$ for the odd one and $b$ for the even one\". So it's not so much that we are proving a special case, but rather we are proving the general case but presenting the argument in a way that's pleasant to read and understand.\nAs another example, a proof of a claim about a sequence of numbers $a_1,...,a_n$ might go: \"Assume without loss of generality that $a_1$ is the maximal number in the sequence. Then [some argument]\". An experienced mathematician will understand that this is shorthand for: \"Let $i$ denote the index for which $a_i$ is the maximal number in the sequence. Then [the same argument as before, replacing $i$ for each occurrence of 1].\" The latter version is more clunky because it adds an index $i$ that one needs to keep track of, which increases the cognitive load on the reader (and also adds the potential for error, for example one now needs to be careful not to use the same index 𝑖 anywhere else in the proof). So again, although at a formal level you could say we are proving the special case in which $i=1$, in this situation I would say it's more a kind of mental crutch that helps human readers digest the proof more easily through a careful choice of labels. It communicates the idea that \"the precise index of the maximal number in the sequence is immaterial to the claim, so we may as well assume it's 1\".\n",
    "tags": [
      "proof-writing"
    ],
    "score": 9,
    "answer_score": 9,
    "is_accepted": true,
    "question_id": 4948772,
    "answer_id": 4948925
  },
  {
    "theorem": "Existence of the transitive closure for sets",
    "context": "How can I prove that the transitive close of an arbitrary set $X$ exists?\n\nA set $X$ is called transitive if $\\in$ is transitiv on it. The transitive closure $\\mathrm{tcl}(X)$ of an arbitrary set $X$ is the smallest transitive set (w.r.t. inclusion) which is a superset of $X$.\n\nI guess it would suffice to show that any transitive superset of $X$ exists but I am failing right there.\n\nLet me show you my try (note: I know that it fails). Define a function $G(n), n\\in \\Bbb N$ with\n$$G(0)=X,\\quad G(n+1)=\\bigcup X.$$\nThis function exists because of the recursive function theorem. From there we can build the set $\\mathcal G=\\{G(n)\\mid n\\in \\Bbb N\\}$ by the axiom of replacement. We then have $\\mathrm{tcl}(X)=\\bigcup \\mathcal G$. This works, because $\\in$ is well-founded. Because if $\\bigcup \\mathcal G$ would not be transitive, this means ther would be an infinite descresing $\\in$-sequence (not okay because of axiom of foudnation).\nHowever, as I said, this fails.\n\nI cannot meaningully define $G$, because I do not realy have any clue what the codomain and domain could be. And I need them to define $G$ as a subset of $\\mathrm{Cod}(G)\\times \\mathrm{Dom}(G)$.\nThe part about why $\\bigcup\\mathcal G$ is transitive is a bit sloppy. But I might be able to make this rigorous. The main problem is the first item.\n\n",
    "proof": "I like your question, because you found a subtle flaw in the way that people sometimes talk about this theorem. \nLet's argue instead like this. We shall use the regularity axiom and argue by $\\in$-induction. If there is a set without a transitive closure, then there is an $\\in$-minimal such set. So we may assume without loss that every element $a\\in X$ has a transitive closure $tc(a)$. By the replacement axiom, therefore, we may form the set $\\{X\\}\\cup\\{tc(a)\\mid a\\in X\\}$. The union of this set is transitive and contains $X$ as a subset, and in fact, it is precisely the transitive closure of $X$. \nAn alternative argument would be to argue like this: $X$ is in some $V_\\theta$ in the cumulative hierarchy, and every $V_\\theta$ is transitive. So this provides a suitable co-domain for your funtion $G$.\nHere is third way to argue. For each natural number $n$, there is a unique sequence of length $n$ that iterates the union operation starting from $X$. This can be proved by induction on $n$, since there cannot be least number $n$ violating this. Thus, by the axiom of replacement, we can take the set of all such sequences. This amounts to having your function $G$, from which one can construct the transitive closure. \n",
    "tags": [
      "proof-writing",
      "set-theory",
      "relations"
    ],
    "score": 9,
    "answer_score": 7,
    "is_accepted": true,
    "question_id": 2284992,
    "answer_id": 2285017
  },
  {
    "theorem": "Show that boundary of a closed set is nowhere dense",
    "context": "Let $H$ be a closed set then, $Cl(H) =H$ and hence the $\\partial H \\subset H$.\nNow to show that the boundary is nowhere dense, it would suffice to show that $Int(Cl(\\partial H)) =\\emptyset$,\ni.e., $Int(\\partial H) = \\emptyset$, but how do I proceed further in order to show this?\n",
    "proof": "Let $U$ be an open set such that $U\\subset\\partial H$. We'll show that $U=\\emptyset$:\nSince $\\partial H\\subset H$ (since $H$ is closed), we must have $U\\subset H$. Since $U$ is open, this implies that $U\\subset\\operatorname{Int}(H)$.\nHence $U\\subset\\partial H\\cap\\operatorname{Int}(H)=\\emptyset$.\n",
    "tags": [
      "general-topology",
      "proof-writing"
    ],
    "score": 9,
    "answer_score": 12,
    "is_accepted": true,
    "question_id": 1143332,
    "answer_id": 1143342
  },
  {
    "theorem": "Is this proof of $H\\le G, [G:H] =2 \\implies a^2\\in H \\forall a\\in G$ correct?",
    "context": "\nIf $H$ is a subgroup of a group $G$ and the index (number of right cosets) of $H$ is $2$, then $a^2 \\in H$ for all $a\\in G$.\n\nMy attempt: if $a\\in H$ then $a^2\\in H$ directly. If $a\\notin H$ and $a^2\\notin H$ then $a(a^{-1})^{-1}  = a^2 \\notin H$ then $G = Ha \\cup Ha^{-1}$ (the union being disjoint). But $e\\notin Ha$ because $e=a^{-1}a$ and $a^{-1}\\notin H$. Also $e\\notin Ha^{-1}$ because $e = aa^{-1}$ and $a\\notin H$. Then we have a contradiction. So $a^2$ must be in $H$ in both cases.\nI think it's ok but I dont feel the arguments with the identity $e$ are right and don't see how to justify them more rigorously. \nThanks in advance \n",
    "proof": "Your proof looks fine, just a bit complicated. Here is another way. If $a\\in H$ then we are done. If $a\\notin H$ then $Ha\\ne H$. Since there are only $2$ right cosets we conclude that $H$ and $Ha$ are all the right cosets, and $a^2$ must be in one of them. But if we assume that $a^2\\in Ha$ then there is an element $h\\in H$ such that $a^2=ha$. We multiply by $a^{-1}$ from the right side and get $a=h\\in H$ which is a contradiction. So $a^2\\in H$. \n",
    "tags": [
      "abstract-algebra",
      "group-theory",
      "proof-writing"
    ],
    "score": 9,
    "answer_score": 11,
    "is_accepted": true,
    "question_id": 3190070,
    "answer_id": 3190077
  },
  {
    "theorem": "Prove that $\\sum_{k=0}^\\infty \\frac{1}{16^k} \\left(\\frac{120k^2 + 151k + 47}{512k^4 + 1024k^3 + 712k^2 + 194k + 15}\\right) = \\pi$",
    "context": "How to prove the following identity\n$$\\sum_{k=0}^\\infty \\frac{1}{16^k} \\left(\\frac{120k^2 + 151k + 47}{512k^4 + 1024k^3 + 712k^2 + 194k + 15}\\right) = \\pi$$\nI am totally clueless in this one. Would you help me, please? Any help would be appreciated. Thanks in advance.\n",
    "proof": "This is the now famous Bailey–Borwein–Plouffe formula, see\nhttp://en.wikipedia.org/wiki/Bailey%E2%80%93Borwein%E2%80%93Plouffe_formula,\nhttp://crd-legacy.lbl.gov/~dhbailey/dhbpapers/digits.pdf.\n",
    "tags": [
      "calculus",
      "sequences-and-series",
      "proof-writing",
      "pi"
    ],
    "score": 9,
    "answer_score": 7,
    "is_accepted": true,
    "question_id": 971574,
    "answer_id": 971582
  },
  {
    "theorem": "How to prove that a set is infinite iff it is Dedekind infinite?",
    "context": "I need to prove the following:\n\nA set $X$ is infinite if and only if it is equipotent to a proper subset of itself\n\nHere, $X$ is defined to be infinite if $|X|$ is not a non-negative integer or equivalently, there is no bijection $\\mathbb{N}_n \\rightarrow X$ for $n\\in \\mathbb{Z}^\\ge$.\nMy attempt:\nThe given statement is equivalent to:\n\n$X$ is equipotent to a proper subset of itself $\\Leftrightarrow$ $X$ is infinite\n\n\nCase '$\\Rightarrow$':\n$X$ is equipotent to a proper subset of itself means that if $A \\subset X$, then there exists a bijection $A\\rightarrow X$. Therefore, $|A|=|X|$.\nBut $A\\subset X$ implies $|A|<|X|$\nThis is a contradiction but I am not doing a proof by contradiction so I this outcome is strange to me.\nI know that $|A|<|X|$ then no bijection $A \\rightarrow X$ can exist. But the bijection must exist since we assumed $A$ and $X$ are equipotent. How do I resolve this and complete the proof?\n\nCase '$\\Leftarrow$'\nI am stumped by the solution provided:\n\nThe main problem I have is the inductive proof showing that $A$ is a denumerable subset. \nIn the base case, the set $A$ has only 1 element, namely $a_1$, so, I don't see how it is possible to build a bijection $\\mathbb{Z}^+\\rightarrow \\{x_1\\}$\nI don't even understand the inductive step as I don't know when the inductive hypothesis is used. In fact, I am not able to discern what the inductive hypothesis is. \nIt would be helpful if some one could explain how this proof is trying to show the existence of a denumberable subset for any given indefinite set as well as state what the $P(n), P(k)$ and $P(k+1)$ for this proof by induction are.\n",
    "proof": "I am going to replace some definitions by obviously equivalent ones. The benefit will be that the proofs are easier to understand. I am writing down very detailed proofs that you would not normally find in a college-level math textbook.\nDefinition: For $n \\in \\mathbb{N}$ define $[n] = \\{k \\in \\mathbb{N} \\mid k < n\\}$.\nDefinition: The image of a map $f : X \\to Y$ is the set $\\mathrm{im}(f) = \\{y \\in Y \\mid \\exists x \\in X \\,.\\, f(x) = y\\}$.\nDefinition: A set $X$ is infinite when for every $n \\in \\mathbb{N}$ and every map $f : [n] \\to X$ there is some $x \\in X$ which is not in the image of $f$.\nLemma 1: An infinite set $X$ contains an element.\nProof. Because $X$ is infinite there is $x \\in X$ which is not in the image of the unique map $[0] \\to X$. QED.\nLemma 2: If $X$ is infinite then there exists a map $c$ such that $c(n,f) \\in X \\setminus \\mathrm{im}(f)$ for every $n \\in \\mathbb{N}$ and every $f : [n] \\to X$.\nProof. For every $n \\in \\mathbb{N}$ and $f : [n] \\to X$ the set $X \\setminus \\mathrm{im}(f)$ contains an element because $X$ is infinite. By the axiom of choice there exists a choice map $c$ such that $c(n,f) \\in X \\setminus \\mathrm{im}(f)$ for all $n$ and $f$. QED.\nHere is an informal explanation of the map $c$. Given an infinite set $X$ and any elements $x_0, \\ldots, x_{n-1} \\in X$, we may view the tuple $(x_0, \\ldots, x_n)$ as a map $f : [n] \\to X$ where $f(k) = x_k$. By a slight abuse of notation we can then write $c(n,f)$ as $c(x_0, \\ldots, x_{n-1})$. Now we see that $c$ accepts finitely many elements of $X$ and returns an element in $X$ which is different from all of them.\nProposition 1: If $X$ is infinite then there exists an injective map $i : \\mathbb{N} \\to X$.\nProof. By Lemma 1 there is $x \\in X$ and let $c$ be the map from Lemma 2. Define $i$ by recursion:\n\\begin{align*}\ni(0) &= x \\\\\ni(n) &= c(i(0), \\ldots, i(n-1)) \\qquad\\qquad \\text{for $n \\geq 1$}\n\\end{align*}\nClearly, $i$ is injective because $i(n)$ is chosen so that it is different from $i(0), i(1), \\ldots, i(n-1)$. QED.\nProposition 2: If $X$ is infinite then it is in bijection with some proper subset $Y \\subseteq X$.\nProof. Suppose $X$ is infinite. We seek a proper subset $Y \\subseteq X$ and a bijection $f : X \\to Y$. Let $i$ be the map from Proposition 1. Define\n$$Y = X \\setminus \\{i(0)\\}$$\nand define $b : X \\to Y$ by\n$$b(x) = \\begin{cases}\nx & \\text{if $x \\not\\in \\mathrm{im}(i)$}\\\\\ni(n+1) & \\text{if $x = i(n)$ for a (unique) $n \\in \\mathbb{N}$}\n\\end{cases}\n$$\nin words, $b$ takes $i(0)$ to $i(1)$, $i(1)$ to $i(2)$, and so on, and does not move elements which are outside the image of $i$. Clearly, $b$ is injective because $i$ is. It is surjective because the only element not in the image of $b$ is $i(0)$. QED.\nProposition 3: If $X$ is a set and $b : X \\to Y$ a bijection from $X$ to a proper subset $Y \\subseteq X$, then there is an injective map $j : \\mathbb{N} \\to X$.\nProof.\nBecause $Y$ is a proper subset there exists $x \\in X \\setminus Y$. We define a map $j : \\mathbb{N} \\to X$ by recursion:\n\\begin{align*}\nj(0) &= x \\\\\nj(n) &= b(j(n-1)) \\qquad\\qquad\\text{for $n \\geq 1$}\n\\end{align*}\nTo prove that $j$ is injective, it suffices to verify that $j(m) \\neq j(n)$ for all $m < n$. We do this by induction on $m$:\n\nif $m = 0$ then $j(0) = x$ is different from $j(n) = b(j(n-1))$ because $b(j(n-1)) \\in Y$ and $x \\not\\in Y$.\nfor the induction step, suppose we had $j(m) = j(n)$ for some $0 < m < n$. Then we would have $b(j(m-1)) = j(m) = j(n) = b(j(n-1))$ and because $b$ is a bijection it would follow that $j(m-1) = j(n-1)$, which is impossible by the induction hypothesis for $m-1$. Therefore it must be the case that $j(m) \\neq j(n)$.\n\nQED.\nTheorem: A set is infinite if, and only if, it is equipotent with some proper subset.\nProof.\nIf $X$ is infinite then we apply Propostion 2 to get a proper subset $Y \\subseteq X$ which is equipotent with $X$.\nConversely, suppose we have a bijection $b : X \\to Y$ for a proper subset $Y \\subseteq X$. By Proposition 3 there is an injective map $j : \\mathbb{N} \\to X$. To see that $X$ is infinite, consider any $n \\in \\mathbb{N}$ and $f : [n] \\to X$. There are $n+1$ distinct elements $j(0), j(1), \\ldots, j(n) \\in X$, and so at least one of them is not in $\\mathrm{im}(f)$ because $\\mathrm{im}(f)$ has at most $n$ elements. QED.\n",
    "tags": [
      "functions",
      "set-theory",
      "proof-writing",
      "infinity"
    ],
    "score": 9,
    "answer_score": 6,
    "is_accepted": true,
    "question_id": 911370,
    "answer_id": 911426
  },
  {
    "theorem": "Show there exist three distinct positive real numbers a, b, c, none of which are integers, such that all of ab, ac, bc and abc are integers",
    "context": "This question is from \"Chartrand, Polimeni and Zhang Mathematical Proofs\": Chapter 8, Exercise 8.81.\nSince this is an existence question, a simple example suffices to complete a proof. The book provides\nProof Let $a = 6/5$, $b = 10/3$ and $c = 15/2$. Then $ab = 4$, $ac = 9$, $bc = 25$ and $abc = 30$\nI am interested however in specifying the general conditions for existence. While trying to solve the exercise I could not easily come up with an example, so I tried to find the general form for a, b and c so that an existence is possible.\nMy question is how would one go about this?\nFrom the solution one can see for example the following relationships:\n\\begin{align}\na=6/5=p/q \n\\end{align}\n\\begin{align}\nb = 2q/(p/2) \n\\end{align}\n\\begin{align}\nc = 3q/(p/3) \n\\end{align}\netc.\nAlso note that $ab = 4$, $ac = 9$, $bc = 25$ are all perfect squares. Now I wonder if that is a precondition as well but not sure how to go about showing this.\n",
    "proof": "Set $x:=ab$, $y:=bc$, $z:=ca$ and $p:=abc$. Note that\n$$\na=xz/p, \\quad \nb=xy/p, \\quad \\text{ and }\\quad \nc=yz/p.\n$$\nThen it is enough to find four values of $x,y,z,p$ such that $a,b,c$ are not integers. The relation between those integers is that\n$\np^2=xyz.\n$\nIf you want that $a,b,c\\notin \\mathbb{Z}$, then the integer $p$ does not divide any of the products $xy$, $yz$, $zx$.\nA class of examples: Pick three pairwise coprime integers $r,s,t \\ge 2$ and set\n$$\nx=r^2, y=s^2, z=t^2 \\,\\,\\text{ and }\\,\\,p:=rst.\n$$\nThen\n$$\na=rt/s, \\quad \nb=rs/t, \\quad \\text{ and }\\quad \nc=st/r\n$$\nare not integers.\n",
    "tags": [
      "elementary-number-theory",
      "proof-writing"
    ],
    "score": 9,
    "answer_score": 7,
    "is_accepted": true,
    "question_id": 4747416,
    "answer_id": 4747426
  },
  {
    "theorem": "Proving a palindromic integer with an even number of digits is divisible by 11",
    "context": "I'm in an introductory course for discrete math so I'm a novice at English proofs. I'm not sure if my reasoning here is valid or if I'm using modular arithmetic correctly. Specifically the line I marked with $(**)$. I would appreciate any feedback. Sorry if any part of the proof seems obvious we are generally expected to spell everything out.\n\nObjective: Prove every palindromic integer with an even number of digits is divisible by $11$.\nProof:\nConsider a palindromic integer $p$ in the form of $x_{1} x_{2} …. x_{n-1}x_{n}x_{n}x_{n-1}...x_{2}x_{1}$ where $p$ has $2n$ digits.\nThis can be expanded as:\n$$x_{1} + x_{2}\\cdot10 + … + x_{n}\\cdot10^{n} + x_{n}\\cdot10^{n+1} + … + x_{2}\\cdot10^{2n} + x_{1}\\cdot10^{2n+1}$$\n$(**)$ $10 \\equiv -1 \\pmod{11}$ so if we take $\\pmod{11}$ of the expression we can replace $10\\equiv -1$.\n$$x_{1} + x_{2}\\cdot(-1) + … + x_{n}\\cdot(-1)^{n} + x_{n}\\cdot(-1)^{n+1} + … + x_{2}\\cdot(-1)_{2n} + x_{1}\\cdot(-1)^{2n+1}$$\nSince we know $2n$ is even (and therefore $2n + 1$ is odd), and $(-1)^{a} = 1$ when $a$ is even and $= -1$ when $a$ is odd we can rewrite the expression as:\n$$x_{1} - x_{2} + … + x_{n} - x_{n} + … + x_{2} - x_{1} = 0$$\nTherefore since $p \\equiv 0 \\pmod{11}$, we have that $11$ divides $p$.\n",
    "proof": "You have some off-by-one errors, but you have the right idea. Note that:\n\\begin{align*}\np\n&= (x_1 + x_2 10 + \\cdots + x_n 10^{n-1}) + (x_{n} 10^n + x_{n-1} 10^{n+1} + \\cdots + x_1 10^{2n - 1}) \\\\\n&= \\sum_{k=1}^n x_k 10^{k-1} + \\sum_{k=1}^n x_k 10^{2n-k} \\\\\n&= \\sum_{k=1}^n x_k10^{k-1}(1 + 10^{2n - 2k - 1}) \\\\\n&\\equiv \\sum_{k=1}^n x_k10^{k-1}(1 + (-1)^{2n - 2k - 1}) \\pmod {11} \\qquad\\qquad\\text{since $10 \\equiv -1 \\pmod{11}$} \\\\\n&\\equiv \\sum_{k=1}^n x_k10^{k-1}(1 -1) \\pmod {11} ~~~~~~~~~~~~~~~~~~~\\qquad\\qquad\\text{since $2n - 2k - 1$ is odd} \\\\\n&\\equiv 0 \\pmod {11}\n\\end{align*}\n",
    "tags": [
      "number-theory",
      "proof-writing",
      "modular-arithmetic",
      "solution-verification",
      "palindrome"
    ],
    "score": 9,
    "answer_score": 7,
    "is_accepted": true,
    "question_id": 985345,
    "answer_id": 985360
  },
  {
    "theorem": "Suppose $f:[0,1] \\Rightarrow \\mathbb{R}$ is continuous and $\\int_0^x f(x)dx = \\int_x^1 f(x)dx$. Prove that $f(x) = 0$ for all $x$",
    "context": "Suppose $f:[0,1] \\Rightarrow \\mathbb{R}$ is continuous and $\\int_0^x f(x)dx = \\int_x^1 f(x)dx$. Prove that $f(x) = 0$ for all $x$.\nSo, I can intuitively see that this is true. My proof mostly makes sense, I think, but I'm not sure if it covers the case where there are negative and positive values in each segment, resulting in a mean value of 0, but still having nonzero values. Can someone tell me how to cover that, or how this does?\nSuppose $f(x) \\neq 0$ for all x $\\in$ [0,1]. By the mean value theorem, there exist some c and d for which $f(c)(x) = f(d)(1-x) = \\int_0^x f(x)dx$. Since x $\\neq$ (1-x) for all x $\\in$ [0,1], it follows that $f(c) = f(d) = 0$. Therefore, $f(x) = 0$ for all $x \\in [0,1]$. \n",
    "proof": "The Fundamental Theorem of Calculus proof suggested in a comment by Peter Tamaroff is one short line, and one cannot do better.\nHere is a more awkward proof that does not use the FTC. Suppose that $f(x)\\ne 0$ for some $x$. Say for example that $f(a)=c\\gt 0$ for some $a$. By continuity we can assume that $a$ is not $0$ or $1$. Then there is an interval $(a-\\epsilon,a+\\epsilon)$ contained in $(0,1)$ such that $f(x)\\gt c/2$ in this interval. \nNote that $\\int_{a-\\epsilon}^{a+\\epsilon}\\gt c\\epsilon\\gt 0$. \nLet $x_1=a-\\epsilon$, and $x_2=a+\\epsilon$. Then if $\\int_0^{x_1} f(t)\\,dt=\\int_{x_1}^1 f(t)\\,dt$, we must have $\\int_{0}^{x_2} f(t)\\,dt \\gt \\int_{x_2}^1 f(t)\\,dt$. \n",
    "tags": [
      "real-analysis",
      "proof-writing"
    ],
    "score": 9,
    "answer_score": 4,
    "is_accepted": true,
    "question_id": 546535,
    "answer_id": 546553
  },
  {
    "theorem": "Prove by Hilbert deduction: $\\vdash _{HFOL} \\forall x (\\neg(A \\to \\neg B))\\to \\neg(\\forall xA \\to \\neg(\\forall xB))$",
    "context": "I'd really like your help proving:\n$\\vdash_{HFOL} \\forall x (\\neg(A \\to \\neg B))\\to \\neg(\\forall xA \\to \\neg(\\forall xB))$\nWhere $HFOL$ is the proof system which contains the Hilbert relevant axioms:\n\n$A\\rightarrow(B \\rightarrow A)$\n$(A\\rightarrow(B\\rightarrow C))\\to((A\\rightarrow B)\\rightarrow(A\\rightarrow C))$\n$(A\\rightarrow B)\\rightarrow ((A\\rightarrow\\bar{B})\\rightarrow \\bar{A})$\n$\\bar{\\bar{A}} \\rightarrow A$\n\nPlus the following two axioms:\n$\\forall x(B\\to A)\\to (\\exists xB \\to A)$\n$\\forall x(A \\to B) \\to (A \\to \\forall x B)$\nRestricted to use only while $x$ is not a free variable in $A$.\nAnd other two axioms: $\\forall xA \\to A \\{ t/x \\}$, $A\\{t/x\\} \\to \\exists xA$ where $t$ is free for assignment instead of $x$ in $A$.\nFinally theres the $MP$ rule: From $A, A\\to B$ we conclude $B$ and the Gen rule which from $A$ we conclude $\\forall xA$.\nI tried couple of different ways but didn't come with any smart result.\nAny suggestions?\n",
    "proof": "Jozef, forgive the clumsiness of my notation, since I'm not used to HFOL,I enjoyed the challenge.\nI also provided a second proof in Dijsktra's Style, since I found it an useful aid to my intuition, since you mentioned that you tried many things without luck. It may prove useful.\n\nProof in HFOL:\n$\\forall x(\\neg (A\\rightarrow  \\neg B))\\vdash\\neg (\\forall xA\\rightarrow \\neg \\forall xB)$\nKnowing $(A\\rightarrow B)\\rightarrow ((A\\rightarrow \\neg B)\\rightarrow \\neg A)$, we could try:\n$\\forall x(\\neg (A\\rightarrow \\neg B)), \\forall xA\\rightarrow \\neg \\forall xB\\vdash \\neg (A\\rightarrow \\neg B), (A\\rightarrow \\neg B)$\nI picked  $\\neg (A\\rightarrow \\neg B), (A\\rightarrow \\neg B)$ because we can get $\\neg (A\\rightarrow \\neg B)$ it easily.\nWe get $\\neg (A\\rightarrow \\neg B)$ from $\\forall x(\\neg (A\\rightarrow \\neg B))$\nand $ \\forall xA\\rightarrow A{t/x}$.\nNow for the tricky part. $A\\rightarrow \\neg B$\n$\\forall x(\\neg (A\\rightarrow \\neg B)), \\forall xA\\rightarrow \\neg \\forall xB\\vdash A\\rightarrow \\neg B$\nKnowing the MP rule we could try:\n$\\forall x(\\neg (A\\rightarrow \\neg B)), \\forall xA\\rightarrow \\neg \\forall xB,A\\vdash \\neg B$\nWe get $\\forall xA$ from $A$ and the Gen rule.\nWe get $\\neg \\forall xB$ from $\\forall xA$, $\\forall xA\\rightarrow \\neg \\forall xB$ and the MP rule.\nWe get $\\neg B$ from $\\neg \\forall xB$ and $\\forall xA\\rightarrow A\\lbrace{ t/x\\rbrace}$.\nDone.\n\nNow in Dijkstra's Style.\n$\\forall x(\\neg (A \\rightarrow  \\neg B))\\rightarrow \\neg (\\forall xA \\rightarrow  \\neg \\forall xB)$\n$=\\lbrace p\\rightarrow q = \\neg p\\vee q \\rbrace$\n$\\forall x(\\neg (\\neg A \\vee \\neg B)) \\rightarrow  \\neg (\\neg \\forall xA \\vee \\neg \\forall xB)$\n$=\\lbrace de Morgan\\rbrace$\n$\\forall x(A \\wedge B)\\rightarrow (\\forall xA \\wedge \\forall xB)$\n$=\\lbrace Distributivity\\,\\, of \\,\\, \\forall\\,\\, over\\,\\, \\wedge \\rbrace$\n$(\\forall xA \\wedge \\forall xB)\\rightarrow (\\forall xA \\wedge \\forall xB)$\nDone.\nEdit: To clarify why I found the second proof useful for aiding my intuition. As you can see this proof is considerately shorter. This is because the style allows more general laws (instead of a minimal set of axioms) and because it prefers rules that avoid case analysis. This means that it is far easier to prove something in this style.\nNow, to me, the hardest step in my first proof was:\n$\\forall x(\\neg (A\\rightarrow \\neg B)), \\forall xA\\rightarrow \\neg \\forall xB\\vdash \\neg (A\\rightarrow \\neg B), (A\\rightarrow \\neg B)$\nI knew I had to apply $(A\\rightarrow B)\\rightarrow ((A\\rightarrow \\neg B)\\rightarrow \\neg A)$ But I had to decide, what formula to use as $B$.\nGoing back to my second proof, $\\forall x(\\neg (A \\rightarrow  \\neg B))$ and $\\neg (\\forall xA \\rightarrow  \\neg \\forall xB)$ are equal.\nThat meant that whatever I could prove with one, I could prove with the other.\nSo, when I notice I could prove $\\neg (A \\rightarrow  \\neg B)$ from $\\forall x(\\neg (A \\rightarrow  \\neg B))$, I knew I could prove the opposite from $(\\forall xA \\rightarrow  \\neg \\forall xB)$. This reduced the amount of things to try immensely.\n",
    "tags": [
      "logic",
      "proof-writing",
      "proof-theory"
    ],
    "score": 9,
    "answer_score": 3,
    "is_accepted": true,
    "question_id": 401910,
    "answer_id": 409683
  },
  {
    "theorem": "If $g$ is Riemann-integrable in a closed interval and $f$ is a increasing function in a closed interval, is $g\\circ f$ Riemann-integrable?",
    "context": "\nIf $g$ is Riemann-integrable in a closed interval and $f$ is a increasing function in a closed interval, is $g\\circ f$ Riemann-integrable?\n\nTo clarify: the problem stated that the composition is well defined. I think that the statement of the question is true but Im having trouble to write/concrete a proof.\nI know that a monotone function defined in a closed interval is Riemann-integrable by a previous result. And I know that the reverse statement ($g$ being monotone and $f$ integrable) is not true in general.\nAfter thinking some moment my idea for the proof is to show that $g\\circ f$ have, at most, countable discontinuities. To do this I was thinking how to show some kind of order-correspondence between the points of the domain of a monotone function and it image.\nCan you help me? I get stuck and it is very possible that Im wrong in my assumption about that the statement is true. Some hint will be appreciated.\nEDIT: I get a new idea, design a partition of $f$ that take any possible discontinuity into a closed interval of known length, and after see what happen in the composition with this closed intervals with discontinuities, and by the other hand see what happen with the parts of continuous mapping.\n\nAfter reading this answer I get the idea to provide a proof for the validity of the statement.\nFirst I will characterize the different kind of images of closed intervals that a monotonic function can create. For an increasing function we have that $x<y$ implies that $f(x)\\le f(y)$, and that at most a monotone function can have a countable number of discontinuities in any closed interval.\nNo discontinuities on the image: if the image of $f([x_1,x_2])$ is continuous then it can be at most of three types: \n\nA constant function $f([x_1,x_2])=\\{a\\}$. Then $(g\\circ f)([x_1,x_2])=g(\\{a\\})=\\{c\\}$, then the function $(g\\circ f)$ is constant in $[x_1,x_2]$ so is Riemann-integrable\nA strictly increasing function i.e. $f([x_1,x_2])=[a,b]$. Then $(g\\circ f)([x_1,x_2])=g([a,b])=[c,d]$. If $g$ is Riemann integrable in $[a,b]$ then exists a sequence of partitions $(P_n)$ such that\n\n$$\\lim_{n\\to\\infty}U(g,P_n)-L(g,P_n)=0$$\nThen because $f$ is bijective in $[a,b]$ then for every $P_n$ exists a partition $P'_n$ in $[x_1,x_2]$ such that\n$$\\lim_{n\\to\\infty}U(g\\circ f,P'_n)-L(g\\circ f,P'_n)=0$$\nso $g\\circ f$ is Riemann-integrable in $[x_1,x_2]$\n\nA mix of both previous cases. Then the interval $[x_1,x_2]$ can be partitioned on types of subintervals discussed previously (constant and strictly monotonic images), so $g\\circ f$ is Riemann-integrable here too.\n\nDiscontinuity in the image: a jump discontinuity in $[x_1,x_2]$ is mapped into two types of the previously discussed intervals with the difference that can be the case that in the image exists an interval with an open boundary of the kind $[a,b)$ or $(a,b]$.\nBut cause $f$ and $g$ are Riemann integrable in closed intervals this mean that they are bounded, and if $g$ is integrable in any subinterval $[a-\\varepsilon,b]$ then is integrable in $(a,b]$ (this is known by a previous proof).\nMy question: it is this proof correct? It lacks something essential? Can you help me to write it better? Thank you in advance.\nEDIT 2: as the user @ParamanandSingh pointed in the comments it is possible that the discontinuities cannot be isolated, one by one, inside some interval. So I will search further for a correct proof of the statement.\n",
    "proof": "I will try to explain the counterexample provided in ParamanandSingh's math overflow link. We will need a few prerequisites but they're hopefully not too advanced.\n\nNull Set/Measure Zero Set: A null set $N \\subset \\mathbb{R}$, also called a measure zero set, is a set that can be convered by a countable union of intervals with arbitrarily small total length. See this wikipedia link.\n\nMeasure zero here anticipates the Lebesgue measure for the reals. The Lebesgue measure assigns 'length' to sets of the real line in the natural/expected way. There's technicalities involved, but all we'll really need from it is that the measure of an interval with endpoints $a<b$ is $(b-a)$.\n\nLebesgue Criterion (for Riemann integrability): A bounded function on a compact interval is Riemann-integrable if and only if its set of discontinuities has measure zero. See this wikipedia link.\n\nThis criterion characterizes how 'small' a function's set of discontinuities needs to be for that function to be Riemann-integrable. It is a standard (though not trivial) result with many proofs lying around; no measure theory is needed!\n\nCantor and Fat Cantor Sets: The standard, ternary Cantor set has measure zero; see this wikipedia link. There are Cantor sets with positive measure; for instance, the Smith-Volterra-Cantor set.\n\nWe can deduce these using lenghts of intervals! Let $C$ be the ternary Cantor set.\nAt the first step, we delete an interval of length $1/3$ from $C_0=[0,1]$, obtaining $C_1$. At the second step, we delete two intervals, each of which has length $1/9$, from $C_1$, obtaining $C_2$. More generally, at the $n$-th step we delete $2^{n-1}$ intervals, each of which has length $3^{-n}$, from $C_{n-1}$, obtaining $C_n$.\nThus, the total length deleted this way is $$\\sum_{n=1}^{\\infty}\\frac{1}{2}{\\left(\\frac{2}{3}\\right)}^n=\\frac{1}{2}\\frac{\\frac23}{1-\\frac23}=1$$\nwhich is the length of $[0,1]$, the interval we began with. It follows that the ternary Cantor set $C$ -- what remains after all deletions -- has measure zero.\nCalculations for the Smith-Volterra-Cantor set $S$ are similar. At the $n$-th step, $2^{n-1}$ intervals, each of which has length $2^{-2n}$, are deleted from $S_{n-1}$, obtaining $S_n$. Therefore, the total length deleted this way is $$\\sum_{n=1}^{\\infty}\\frac{1}{2}\\frac{2^n}{2^{2n}}=\\frac{1}{2}\\sum_{n=1}^{\\infty}\\frac{1}{2^n}=\\frac{1}{2}\\frac{\\frac12}{1-\\frac12}=\\frac12$$\nThe measure of $S$ is simply $1$ (the measure of $[0,1]$, the interval we began with) minus $\\frac12$ (the total measure of what we discarded), that is, $S$ has measure $\\frac12$. In particular, it has positive measure and is not a null set.\n\nNow that we have the prerequisites, let's dive onto the counterexample. First, let $f:[0,1]\\longrightarrow \\mathbb{R}$ be the characteristic function of the ternarcy Cantor set $C$, that is:\n\\begin{equation}\nf(x)=\\left\\{\\begin{array}{ll}1&\\text{if $x \\in C$}\\\\0&\\text{if $x \\notin C$}\\\\\\end{array}\\right.\n\\end{equation}\nNotice that $f$ has discontinuities precisely on $C$; a good way to see this is to notice that at each step in the construction of $C$, $f$ is defined to be $0$ on the deleted intervals (so it is clearly continuous in their interior) It follows by the Lebesgue Criterion that $f$ is Riemann-integrable.\nFinally, we will define a monotone increasing function $g:[0,1] \\longrightarrow \\mathbb{R}$ that maps the Smith-Volterra-Cantor set $S$ to $C$. We will obtain $g$ as the limit of a sequence of functions.\nLet $S_0=C_0=[0,1]$, let $S_n$ be what remains in the construction of $S$ after $n$ steps, and similarly for $C_n$. Notice that each of $S_n$ and $C_n$ is a disjoint union of $2^n$ closed non-degenerate intervals; we may describe each of them by its collection of $2^{n+1}$ endpoints. For instance, in the obvious notation $C_0=S_0\\sim\\{0,1\\}$, $C_1\\sim\\left\\{0,\\frac13,\\frac23,1\\right\\}$ and $S_1\\sim\\left\\{0,\\frac38,\\frac58,1\\right\\}$.\nDefine $g_n:[0,1]\\longrightarrow\\mathbb{R}$ as follows. Let $S_n\\sim\\{s_k\\}$, where $k$ ranges from $1$ to $2^{n+1}$ and the $s_k$ are increasing. Similarly, let $C_n\\sim \\{c_k\\}$. For all $k=1,\\dots,2^{n+1}$ put $g_n(s_k)=c_k$, and extend it to all of $[0,1]$ linearly. In other words, $g_n$ fits the $i$-th interval in $S_n$ into the $i$-th interval in $C_n$, and fills in the blanks with linear pieces. It's easy to see that $g_n$ is increasing (and continuous!) for all $n$.\nIt's not too hard to see that $g_n$ converges pointwise for all $x \\in [0,1]$; we define $g$ to be this limit function. It follows immediately that $g$ is increasing and maps $S$ to $C$. (It's a bit of work, but one can show that the convergence $g_n\\to g$ is actually uniform, so that $g$ is indeed continuous!)\nAt last, the composition $f\\circ g$ has $S$ for its set of discontinuities. Since $S$ has positive measure, it follows by the Lebesgue criterion that $f\\circ g$ is not Riemann-integrable, which concludes the proof.\n",
    "tags": [
      "real-analysis",
      "integration",
      "proof-verification",
      "proof-writing"
    ],
    "score": 9,
    "answer_score": 9,
    "is_accepted": true,
    "question_id": 1833028,
    "answer_id": 1834357
  },
  {
    "theorem": "Catalan numbers - algebraic proof of the recurrence relation",
    "context": "\nI would like to prove following recursive relation for the Catalan numbers:\n  $$\\tag{1}\nC_0=1,\\quad C_n=\\sum_{i=0}^{n-1}C_iC_{n-i-1}\\text{, for }n\\ge 1\n$$\nwithout combinatoric arguments, only algebraically; and no generating function.\n\nStarting point:\n$$\\tag{2}\nC_n:=\\frac{1}{n+1}\\binom{2n}{n}.\n$$\nThe following recursion can be also used (already proved):\n$$\\tag{3}\nC_0=1,\\quad C_n=\\frac{2(2n-1)}{n+1}C_{n-1}\\text{, for }n\\ge 1\n$$\nMaybe the identities for Binomial coefficients (wikipedia) are useful. In particular the Chu–Vandermonde identity, \n$$\\tag{4a}\n\\sum _{j=0}^{k}{\\binom {m}{j}}{\\binom {n-m}{k-j}}={\\binom {n}{k}}\n$$\nor\n$$\\tag{4b}\n\\sum _{m=0}^{n}{\\binom {m}{j}}{\\binom {n-m}{k-j}}={\\binom {n+1}{k+1}}\n$$\ncould be useful.\nWhat I have tried? I tried to substitute the definition (2) in the r.h.s. of (1) to obtain the l.h.s. of (1). Another attempt was to take $C_{n-1}$ from (1) (known by induction assumption) and try with (3) to recover $C_n$. In both cases, although I can smell that every thing is more or less related I can't find the technical steps to do the job. \nA combinatoric proof with Dyck paths can be found here, but this is not the way I'm trying to follow.\nEDIT\nThe answer by \"Robert Z\" is very good and nice and I'll accept it; if someone could find a direct proof without generalised binomial coefficient, I'will accept his answer instead.\n",
    "proof": "First proof.\nNote that for $0\\leq i\\leq  n-1$,\n$$C_iC_{n-i-1}=a(n,i+1)-a(n,i)$$\nwhere \n$$a(n,j):=\\frac{(2j-n)}{2n(n+1)}\\binom{2j}{j}\\binom{2(n-j)}{n-j}.$$\nHence, for $n\\geq 1$,\n$$\\begin{align}\\sum_{i=0}^{n-1}C_iC_{n-i-1}&=\n\\sum_{i=0}^{n-1}(a(n,i+1)-a(n,i))\\\\&=a(n,n)-a(n,0)=C_n.\n\\end{align}$$\nSecond proof.\nWe have that\n$$C_n=\\frac{2(2n-1)\\cdots (n+1)n}{(n+1)!}=2(-4)^n\\binom{1/2}{n+1}$$\nwhere we use the notion of generalized binomial coefficient.\nHence, for $n\\geq 1$,\n$$\\begin{align}\\sum_{i=0}^{n-1}C_iC_{n-i-1}&=4\\sum_{i=0}^{n-1}(-4)^i\\binom{1/2}{i+1}(-4)^{n-i-1}\\binom{1/2}{n-i}\\\\\n&=-(-4)^n\\sum_{j=1}^{n}\\binom{1/2}{j}\\binom{1/2}{n+1-j}\\\\\n&=-(-4)^n\\sum_{j=0}^{n+1}\\binom{1/2}{j}\\binom{1/2}{n+1-j}+2(-4)^{n}\\binom{1/2}{n+1}\\\\\n&=0+C_n=C_n\\end{align}$$\nwhere we applied the generalized Chu–Vandermonde identity.\n",
    "tags": [
      "combinatorics",
      "proof-writing",
      "binomial-coefficients",
      "catalan-numbers"
    ],
    "score": 9,
    "answer_score": 8,
    "is_accepted": true,
    "question_id": 3304415,
    "answer_id": 3304420
  },
  {
    "theorem": "A problem about the differential mean value theorem $2ηf(1)+(c^2-1)f&#39;(η)=f(ξ)$",
    "context": "\nAssume that the function $f : \\left[0, 1\\right] \\to \\mathbb{R}$ is continuous on $\\left[0,1\\right]$ and is differentiable on $\\left(0,1\\right)$. Let $c \\in \\left(0,1\\right)$. Prove that there exist $\\xi, \\eta \\in \\left(0, 1\\right)$ such that\n  \\begin{align}\n2 \\eta f\\left(1\\right) + \\left(c^2 - 1\\right) f^\\prime\\left(\\eta\\right) = f\\left(\\xi\\right) .\n\\end{align}\n\nI tried to use the Lagrange mean value theorem and the Rolle mean value theorem on $[0,1]$, but failed.\n",
    "proof": "Hints:\n1) First take $c=\\frac{\\sqrt{3}}{2}$, $f(x)=(x-\\frac{1}{2})^2$, and show that the hypothesis $\\xi \\in (0,1)$ cannot be satisfied. I suppose that you want $\\xi \\in [0,1]$.\n2) a) Put $u = c^2(f(1)-f(0))+f(0)$. Show that we have $f(0)<u<f(1)$ (if $f(1)>f(0)$) or $f(1)<u<f(0)$ (if $f(1)<f(0)$), and deduce in all cases (the case $f(0)=f(1)$ is left to you) there exists an $\\xi$ such that $u=f(\\xi)$.\nb) Put $g(x)=(c^2-1)f(x)+x^2f(1)$, compute $g(1)-g(0)$ and finish the proof.\n",
    "tags": [
      "real-analysis",
      "analysis",
      "proof-writing",
      "proof-explanation"
    ],
    "score": 9,
    "answer_score": 1,
    "is_accepted": true,
    "question_id": 2912006,
    "answer_id": 3007710
  },
  {
    "theorem": "Evaluate the general infinite square root",
    "context": "$$x = \\sqrt{n\\sqrt{n\\sqrt{n}} \\cdots}$$\nI see that:\n$$x = \\sqrt{nx}$$\n$$x^2 -nx = 0$$\nThem:\n$$x(x - n) = 0 \\implies x \\in \\{0, n\\}$$\nHow should I reject the $x = 0$ solution?  (any level proof is fine, analysis, calculus etc...)\n",
    "proof": "Define\n$$x_m:=\\overbrace{\\sqrt{n\\sqrt {n\\ldots\\sqrt n}}}^{m\\;\\text{times}}\\;\\;,\\;\\;\\;\\;m,n\\in\\Bbb N$$\nthen\n$$x_m\\le x_{m+1}\\iff x_m\\le\\sqrt{nx_m}\\iff x_m^2\\le nx_m\\iff x_m(x_m-n)\\le0\\iff x_m\\le n$$\nThe last inequality can be proved by induction:\n$$x_1=\\sqrt n\\le n\\;\\;\\;\\color{green}\\checkmark$$\n$$x_m=\\sqrt{nx_{m-1}}\\stackrel{\\text{Ind. Hyp.}}\\le\\sqrt{nn}=n$$\nWe get thus that the sequence $\\;\\{x_m\\}_{m\\in\\Bbb N}\\;$ is monotonic increasing , and thus it converges (in the wide sense) to its supremum $\\;\\alpha\\;$ (and we can already forget of zero), which is finite by the above, and then using arithmetic of limits:\n$$\\alpha\\xleftarrow[m\\to\\infty]{} x_m=\\sqrt{nx_{m-1}}\\xrightarrow[m\\to\\infty]{}\\sqrt{n\\alpha}\\implies \\alpha^2=n\\alpha\\implies\\alpha=n$$\n",
    "tags": [
      "calculus",
      "real-analysis",
      "analysis",
      "derivatives",
      "proof-writing"
    ],
    "score": 9,
    "answer_score": 8,
    "is_accepted": false,
    "question_id": 1136748,
    "answer_id": 1136774
  },
  {
    "theorem": "Proof that arithmetic and geometric mean converge",
    "context": "I need some help with understanding a part of this proof and also writing it up correctly. Given $a_n\\geq a_{n+1}\\geq b_{n+1} \\geq b_n$ with $a_1=a$ and $b_1=b$. I am also given that  $$a_{n+1}=\\frac{a_n+b_n}{2}$$ and $$b_{n+1}=\\sqrt{a_nb_n}$$\nI need to show that sequences ${a_n}$ and ${b_n}$ converges and that ${a_n}$ and ${b_n}$ have the same limit.  \nI am told to use the monotonic convergence theorem to prove that both sequences converges and I have the following proof:\nNotice that {$a_n$} is monotonically decreasing while {$b_n$}  is monotonically increasing.  Since {$a_n$}  is bounded above by supremum $a_1$ below by its infimum $b_1$, {$a_n$} according to the monotonic convergence theorem has to converge.  \nSimilarly, notice that {$b_n$} is bounded below by infimum $b$ and supremum $a$.  By monotonic convergence theorem {$b_n$} must also converge as well.  \nNext, I am told to show that {$a_n$} and {$b_n$} have the same limit.  In other words, if [$a_n-b_n$] as n tends to infinity must be 0.  For this part, it seems to be the case that one can prove it by just showing that  $a_{n+1} - b_{n+1} \\leq (1/2) (a_n - b_n) $.  And I know you can just show this by using the definition of the arithmetic mean, which is  $a_{n+1} - b_{n+1} \\leq a_{n+1} - b_n = (1/2) (a_n - b_n)$.  Why is that?  It seems incompletely and not so obvious to me.  An explanation here would help.\nPlease help me edit my proof (what I have already) and clarify my understanding\n",
    "proof": "Note that from your first inequality: $$a_n > a_{n+1} > b_{n+1} > b_n$$ we know that each sequence $\\{ b_n \\}$ and $\\{ a_n \\}$ is monotonic and bounded. Therefore they both converge.\nLet $L = \\lim_{n\\to\\infty} a_n$ and $M = \\lim_{n\\to\\infty} b_n$.\nNow we also know that $$L = \\lim_{n\\to \\infty} a_{n+1} = \\lim_{n\\to \\infty} \\frac{a_n + b_n}{2} = \\frac{L+M}{2}$$ and $$M = \\lim_{n\\to\\infty} b_{n+1} = \\lim_{n\\to\\infty} \\sqrt{a_n b_n} = \\sqrt{LM}.$$\nEither equation leads to your solution. $$L = \\frac{L+M}{2} \\implies 2L = L+M \\implies L=M$$ and $$M=\\sqrt{LM} \\implies M^2 = LM \\implies M=L$$ provided $M\\neq 0$ for the second equation. However, provided that we assume the initial condition $a>b\\neq 0$, we have $M > 0$ by monotonicity.\n",
    "tags": [
      "real-analysis",
      "analysis",
      "proof-verification",
      "proof-writing"
    ],
    "score": 9,
    "answer_score": 6,
    "is_accepted": false,
    "question_id": 932684,
    "answer_id": 1340559
  },
  {
    "theorem": "How to prove that, among any &#119899;+1 distinct odd integers from {1,…,3&#119899;}, at least one will divide another?",
    "context": "This was one of the exercises in my textbook and I've been working on it for well over 10 hours over the span of 3 days without much progress. I don't think that it's even supposed to be a hard problem so I just give up at this point. (If it were a hard problem then I would have been more motivated knowing it's supposed to be a challenge). It asks\n\nAssume that $n$ is a positive integer. Prove that if one chooses $n+1$ distinct odd integers from $\\{1, 2, 3, \\dots, 3n\\}$ then at least one of these numbers will divide another.\n\nThis was on a section on the pigeonhole principle so I've used that in my failed attempts below.\nMy goal was to find a formula for the $m$th box such every box included all the primes and composite numbers in the set. Earlier in the book, there was an example of a problem that asked something similar. It was about choosing 101 numbers from the set of numbers between 1 and 200. And there it was easy to consider the primes, after all, there were a known number of them. But in this problem, there is no way to accurately determine the number of primes between $0$ and $3n$, at least with my current knowledge.\nAttempt 1\nI first thought that you could simply put $n$ and $3n$ in the same box. But where do prime numbers greater than $n$ go? Where do multiples of $5$ go? They can't be a multiple of $3$, so this won't work.\nAttempt 2\nWhat if we considered the first $n$ odd numbers in the list? Every number greater than $n$ must be a product of these numbers. Oh, but what about the primes greater than $n$ again?\nAttempt 3\nAt this point I realized something. Formulas for groups of numbers in the $m$th box can't have prime numbers. So what if the $m$th box contained the $m$th prime number? Then the rest of the numbers in the box could be multiples of this prime. Or they could be this prime times some other number to the $k$th power ($p_m \\cdot c^k$). But where do numbers like $15$ and $25$ go? If $15$ goes into the box, then $25$ can't go in the same box because $15$ doesn't divide $25$. If $25$ goes in the box, then where does $15$ go?\nDetails I noticed while working on the exercise\nSome of these may be repeats of what I've mentioned throughout the post.\n\nConjecture (could be wrong): One must choose at least one prime number. Then if this were true if we had the boxes containing the $m$th prime number then all we would have to do is prove that one other number had this prime number in its prime factorization.\nNotice that $abc$ divides $abcd$. What if we used this property to our advantage? If some box had some number, then the other boxes could be a multiple of this number.\nAgain, since we don't know how many primes are in the set, then the boxes must be based on the primes. Right?\nAll odd numbers in $[2n-1, 3n]$ must be expressed as the products of numbers in $[1, 2n-1]$.\n\n",
    "proof": "Your attempt  1 is actually a correct approach.\nAssume that for an odd integer $k$ not divisible by 3, box $k$ contains all the odd numbers from $\\{1,2,\\ldots, 3n\\}$ that are of the form $3^ik$, where $i$ is a non-negative integer. Then among any two numbers in a box $k$, one divides the other.\nFor example box 1 contains all the numbers of form $3^i$ and box 5 all the numbers of form $5\\cdot 3^i$\nNow, you have box 1, box 5, box 7, box 11, box 13, box 17, etc, so the number of boxes is the number of odd integers in $\\{1,\\ldots, 3n\\}$ that are not divisible by 3.\nIn case when $n$ is even, the number of odd integers is $3n/2$ and $n/2$ of them are divisible by 3, so the number of boxes is $n$.\nIn case when $n$ is odd the number of odd integers is $(3n+1)/2$ and $(n+1)/2$ of them are divisible by 3, so the number of boxes is again $n$.\nSo among your $n+1$ numbers at least two belong to the same box, say box $k$. Then one of the numbers is $3^ik$, another is $3^jk$, so clearly one divides another.\n",
    "tags": [
      "proof-writing",
      "divisibility",
      "integers",
      "pigeonhole-principle"
    ],
    "score": 9,
    "answer_score": 8,
    "is_accepted": true,
    "question_id": 4667024,
    "answer_id": 4667061
  },
  {
    "theorem": "Idea is correct, proof lacks rigor, coefficient of $t$ in $\\det(I+tA)$",
    "context": "The goal of this exercise is to find the coefficient of $t$ in the polynomial $\\det(I+tA)$ where $I, A \\in Mat_n(\\mathbb R)$.\nI've thought about this long and hard as I am combinatorically challenged (it's the most difficult branch of mathematics for certain) and I'm pretty sure I have a convincing argument, but it feels handwavey and lacks rigor. I'd like someone to review it and possibly suggest way to make it less verbal and more rigorous.\nMy argument uses Laplace expansion, initially using the first column of $I+tA$.\n$\\det(I+tA) = (1+ta_{1,1})\\begin{vmatrix}1+ta_{2,2} & ta_{2,3} & \\dots & ta_{2,n}\\\\ta_{3,2} & 1+ta_{3, 3} & \\dots & ta_{3,n}\\\\ \\vdots & \\vdots & \\dots & \\vdots \\\\ ta_{n, 2} & ta_{n, 3} & \\dots & 1+ta_{n,n}\\end{vmatrix} -ta_{2,1}\\det(B_2) + ta_{3,1}\\det(B_3)-\\dots+(-1)^{n+1}ta_{n, 1}\\det(B_n)$\nWhere $B_i$ are some sub matrices with accordance to Laplace.\nNotice that since we used the first column in our original expansion, the first row of every $B_i$ is just $\\begin{pmatrix}ta_{1,2} \\dots ta_{1,n}\\end{pmatrix} = t\\begin{pmatrix}a_{1,2} \\dots a_{1,n}\\end{pmatrix} $\nif we define $B_i'$ to be exactly the same as $B_i$ except the first row is divided by $t$, then by determinant rules $\\det(B_i) = t\\det(B_i')$, and so:\n$-ta_{2,1}\\det(B_2) + ta_{3,1}\\det(B_3)-\\dots+(-1)^{n+1}ta_{n, 1}\\det(B_n) = -t^2a_{2,1}\\det(B_2') + t^2a_{3,1}\\det(B_3')-\\dots+(-1)^{n+1}t^2a_{n, 1}\\det(B_n')$\nand they only contain non linear terms of $t$. So they don't matter to us at all. We can just focus on the first determinant.\nBut now we can just use the exact same argument on $\\begin{vmatrix}1+ta_{2,2} & ta_{2,3} & \\dots & ta_{2,n}\\\\ta_{3,2} & 1+ta_{3, 3} & \\dots & ta_{3,n}\\\\ \\vdots & \\vdots & \\dots & \\vdots \\\\ ta_{n, 2} & ta_{n, 3} & \\dots & 1+ta_{n,n}\\end{vmatrix}$! The only thing that will matter is the first element and its leading minor. The others can be discarded as they are higher order of $t$. \nWe will use it again and again until at the end we are only left with $\\prod_{i=1}^{n}(1+ta_{i,i})$. The answer is hidden somewhere in this product.\nSince we are only interested in linear terms, we are not allowed to multiply $ta_{i,i}$ with $ta_{j,j}$. So every $ta_{i,i}$ can only be multiplied by $1$. So overall we should have $ta_{1,1} + ta_{2,2} + \\dots ta_{n,n} = tTrace(A)$.\nIs this result correct? is this \"proof\" valid? It feels too wishy washy and verbal.\n",
    "proof": "The result is correct. I think your argument is fine, although it is not very amenable to be written nicely. \nHere is a shorter proof. \nIf $\\lambda_1,\\ldots,\\lambda_n$ are the eigenvalues of $A$ counting multiplicities, then the eigenvalues of $I+tA$ are $1+t\\lambda_1,\\ldots,1+t\\lambda_n$. Thus\n$$\n\\det(I+tA)=(1+t\\lambda_1)\\cdots(1+t\\lambda_n)=t^n\\lambda_1\\cdots\\lambda_n+\\cdots+t(\\lambda_1+\\cdots+\\lambda_n)+1.\n$$\nSo the coefficient of $t$ is \n$$\n\\lambda_1+\\cdots+\\lambda_n=\\operatorname{Tr}(A). \n$$\nWe also see from above that the coefficient of $t^n$ is $\\det A$. \n",
    "tags": [
      "linear-algebra",
      "proof-verification",
      "proof-writing",
      "determinant",
      "trace"
    ],
    "score": 9,
    "answer_score": 5,
    "is_accepted": true,
    "question_id": 2666862,
    "answer_id": 2666865
  },
  {
    "theorem": "Proving an if and only if statement",
    "context": "Suppose I am trying to prove a statement in the form A if and only if B. I know I need to prove that \n\nIf A, then B\nIf B, then A\n\nI know that 1 is equivalent to proving \"If not B, then not A\". \nMy question is: When proving A if and only if B, is it permissible to prove \"if not B, then not A\" and then \"if B, then A.\" \nI have seen many people prove A iff B by showing \"If not A, then not B\" and then \"If not B, then not A,\" but never the way I described, which is why I am asking if it is okay.\n",
    "proof": "Yes that is perfectly fine. A implies B is logically equivalent to \"not B, then not A\".\n",
    "tags": [
      "logic",
      "proof-writing"
    ],
    "score": 9,
    "answer_score": 7,
    "is_accepted": true,
    "question_id": 1676135,
    "answer_id": 1676139
  },
  {
    "theorem": "Understanding a proof about nested nonempty connected compact subsets",
    "context": "I know this question has asked to death here on MSE but I have not found a satisfactory solution. A solution found online is extremely elegant but I do not quite understand it!\n\nGiven nested sequence of closed nonempty connected subsets of\n  a compact metric space $X$. Prove that $\\bigcap_{i=1} X_i$ is nonempty\n  and connected.\n\nIt is a standard fact that arbitrary intersection of compact set is nonempty and compact. \nI wish to understand the bit of the proof that this is also connected\nProof: https://math.berkeley.edu/sites/default/files/pages/f10solutions.pdf\n\nSuppose that $\\bigcap_{i=1} X_i$ is not connected. Let $A$ and $B$ be two\n  disjoint nonempty closed sets so that $\\bigcap_{i=1} X_i = A \\cup B$.\n  Find disjoint open sets $U$ and $V$ so that $A \\subset U$ and $B \\subset V$.\nPut $F_i = X_i − (U \\cup V )$. Then $\\{Fi\\}$ is a nested sequence of\n  compact sets, whose intersection is empty. Thus $F_i = ∅$ for some\n  $i$. That is, $X_i \\subset U \\cup V$.\nHowever, $X_i$ intersects both $U$ and $V$ , since $X_i  \\cap A\\neq ∅$ and $X_i \\cap B \\neq ∅$. This contradicts the assumption that $X_i$\n  is connected.\n\nCan someone please elaborate on some of the details of the proof? \n1) Why bother finding open sets containing $A,B$ and how do we know that they even exist? This move seems sort of unnatural.\n2) What is so special about $U \\cup V$ especially, why does $F_i = X_i − (U \\cup V )$ imply $\\{F_i\\}$ has empty intersection? Why does intersection being empty imply the existence of $F_i = \\varnothing$?\n3) Can someone please justify  $F_i = \\varnothing \\Leftrightarrow X_i \\subset U \\cup V$. \n4) Can someone please justify the contradiction?\nI know this is the pretty much the entire proof, but I have seriously tried to crack it and failed to understand it. Perhaps the proof is too difficult.  \n",
    "proof": "1) We have some fiddley-ness at the start to get $U, V$ disjoint. It's a bit long, so see Disjoint compact sets in a Hausdorff space can be separated\nThis is possible because the space is Hausdorff (given that it is metrizable). There's a nice phrase by Willard, who says that this is an example of compact sets behaving like points - as $A, B$ are closed in a compact space, they are also compact, and because the space is Hausdorff, there are disjoint open sets containing $A$ and $B$. Neat!\nThe reason we want such sets will become apparent as we wade on.\n2) $U\\cup V$ is special because it covers $\\bigcap_{i=1}X_i$, and because it's open. So defining $F_i\\equiv X_i\\setminus (U\\cup V)$ is a closed subset of $X$, hence compact, and doesn't contain the intersection $\\bigcap_{i=1}X_i$; finally note the $F_i$ also form a nested sequence (draw a diagram!). So if all $F_i$ were non-empty, their intersection would be non-empty - but that's not true! \nAs $$\\bigcap_{i=1}F_i=\\bigcap_{i=1}(X_i\\setminus (U\\cup V))=(\\bigcap_{i=1}X_i)\\setminus (U\\cup V)=\\emptyset$$ because we already knew $U\\cup V$ covers that intersection.\nSo, there is some specific $F_k$ that is empty!\n3) Nearly there now. As this $F_k$ is defined as $X_k\\setminus (U\\cup V)$, then if $F_k=\\emptyset$, that implies every element of $X_k$ was also in $U\\cup V$. i.e. $X_k\\subset (U\\cup V)$.\n4) So we have that there is a pair of disjoint open sets covering this connected $X_k$. But surely they provide a separation of $X_k$? Just taking the complements of the open sets $U, V$ (namely our original $A$ and $B$...) and then intersecting with $X_k$ to get a separation. The only way this wouldn't be true is if $X_k$ had trivial intersection with one of those complements, i.e. if $X_k$ was entirely contained by one of the open sets. But this can't be true, because we know $$X_k\\cap A\\supseteq (\\bigcap_{i=1}X_i)\\cap A\\neq\\emptyset$$\nThe same holds for $B$, and so $X_k$ really has a separation. This is a contradiction because it was supposed to be connected!\nPhew. I hope that makes sense, please ask for clarifications if anything is too terse. As often seems to be the case, a short looking proof is actually an example of economy of writing, rather than a simple idea. That's not to say this isn't a nice proof, it's just not one you can 'see' in a flash without having spent some time thinking topologically.\n",
    "tags": [
      "general-topology",
      "proof-verification",
      "proof-writing",
      "connectedness",
      "proof-explanation"
    ],
    "score": 9,
    "answer_score": 7,
    "is_accepted": true,
    "question_id": 1631126,
    "answer_id": 1631322
  },
  {
    "theorem": "Prove A is an open set if and only if $A \\cap Bd(A) = \\emptyset $",
    "context": "Prove A  is an open set if and only if $A \\cap Bd(A) = \\emptyset $\nHere is my start:\nSuppose A is an open set.  We know $X-A$ is closed. Need to show $A \\cap Bd(A) = \\emptyset$\nLet $ x \\in A$. \nGoing the other direction, Suppose $ A \\cap Bd(A) = \\emptyset$. Need to show $A$ is an open set.\nLet $x \\in A$.  since $x \\in A$ $x \\notin Bd(A)$\nAm I headed in the right direction? I feel more confident about where I have started for the second part then the 1st. \n",
    "proof": "You can knock out the first direction with a little set algebra. \n$(\\implies)$ Suppose $A$ is open, meaning $A = A^\\circ$. Then we have $$\\begin{align} A \\cap \\text{Bd}(A) = A \\cap (\\overline{A}\\setminus A^\\circ) \\\\ =  A \\cap (\\overline{A}\\setminus A) \\\\ = A \\cap (\\overline{A}\\cap A^c) \\\\ = (A\\cap A^c)\\cap \\overline{A} \\\\ = \\emptyset \\cap \\overline{A} \\\\ = \\emptyset\\end{align}$$\n$(\\impliedby)$ Now suppose $A \\cap \\text{Bd}(A) = \\emptyset $ but for the sake of contradiction we will additionally assume that $A$ is not open. Then there is an element $x \\in A$ such that no open set containing $x$ is a subset of $A$. But $A^\\circ$ is always open and $A^\\circ \\subset A$ so $x\\notin A^\\circ$. This means $x \\in \\overline{A}\\setminus A^\\circ$, or that $x$ is a boundary point of $A$. Since $x$ is an element of $A$ and an element of the boundary, then $x\\in A\\cap\\text{Bd}(A) \\neq \\emptyset$, a contradiction.\n",
    "tags": [
      "general-topology",
      "proof-writing"
    ],
    "score": 9,
    "answer_score": 6,
    "is_accepted": true,
    "question_id": 1245343,
    "answer_id": 1273480
  },
  {
    "theorem": "Learning Proofs (for Computer Science)",
    "context": "Harvard's math curriculum, for freshmen, is divided into 4 classes beyond the BC Calculus level, Math 21, 23, 25 and 55. Math 21 is your classic plug-and-chug multivariable calculus and linear algebra course. The rest of the courses teach multivariable calculus and linear algebra in the context of proofs, along with some real analysis. I decided to take Math 21 this semester (don't ask me why, it was a horrible decision and it is why I am writing this post).\nAnyways, I now realize that I need to learn proofs in order to use them in higher-level computer science classes. Plus, I might have an interest in higher mathematics in concepts such as real analysis, probability theory, optimization, etc. How should I go about learning proofs, specifically in the context of discrete math, linear algebra, and real analysis, so that I can apply my knowledge to computer science and the higher-level math in which I might be interested?\nThanks for your advice in advance. This is my first question on Math Stack Exchange, and I'm curious to see what the community is like!\n",
    "proof": "Your question really resonated with me. I wish I could beam back in a time machine to give myself the following advice. Since I can't, I'll pass it along to you.\n\nBecome conscious of your own problem-solving processes. Read Polya's book How to Solve It carefully. Do all the exercises; this is very important. This is a book of great power hiding under great modesty: it presents itself as a trivial high-school algebra book. Nothing could be further from the truth; it's had a huge influence on Artificial Intelligence, for example. See the Wikipedia entry for a quick summary of what you'll get. After reading the book, try to observe your mind as you attempt to solve some problems. This brings a power of its own. If you can study Artificial Intelligence (AI), do. I know this sounds strange, but AI deals quite explicitly with these kinds of problems. Imagine trying to write a program to do proofs. How would you do it? Merely working on this problem, not necessarily succeeding, will help you with your own proofs.\nStudy logic. Work on proofs in propositional logic first. This gives you nice practice with proofs in and of themselves, but better yet gives you a firm intuitive grasp of what real rigor is like and when a proof is really a proof and when it's done or not. It gets rid of that queasy uncertain feeling one has had before exposure to logic. If you can over the long run take a class on mathematical logic, do so. Mathematical induction proofs before exposure to mathematical logic seem like magic or hand-waving hocus-pocus. Afterward, after seeing the induction axiom in all its rigorous glory, one knows exactly what is required for an induction proof and why it's needed.\nDo examples. Everyone will tell you this and they're right. Finding the right mother lode of examples is tough. The best book I've ever seen for this by a long shot is Polya's book Mathematical Discovery. How To Solve It was just the warm-up, the first draft. Mathematical Discovery is the culmination. Try just the first two chapters. It's essential to try the problems! Polya is an amazing teacher; clearly these problems are the gems collected over a long lifetime. Just reading the chapter is liking eating a teaspoon of ice cream: merely a taste. Doing the problems is like eating the whole five-gallon drum of ice cream: much more filling and substantial, and when you absorb it as part of you, you've changed.\n\nFinally, I want to echo Peter Smith's advice to study as many textbooks as you need to in order to get a clear explanation of the concept you want. I remember trying to understand a concept and getting something like 6 books from the library once, and only one of them did a decent job of explaining the concept. So lots of books is a good thing, not a bad thing.\n",
    "tags": [
      "real-analysis",
      "linear-algebra",
      "proof-writing",
      "computer-science"
    ],
    "score": 9,
    "answer_score": 4,
    "is_accepted": false,
    "question_id": 530664,
    "answer_id": 536587
  },
  {
    "theorem": "general formula for the nth derivative of $f(x)=\\frac{1}{1+e^{x}}$",
    "context": "consider the function :$$f(x)=\\frac{1}{1+e^{x}}$$\nthe nth derivative of the function is given by the following formula:\n$$f^{(n)} (x)=\\sum_{k=1}^{n+1}a_{n,k}\\frac{1}{\\left(1+e^{x}\\right)^{k}}$$ where\n$$a_{n,k}=\\left(-1\\right)^{n}\\sum_{j=0}^{k-1}\\left(-1\\right)^{j}{{k-1}\\choose{j}}\\left(j+1\\right)^{n}$$\nmy question is that:how the formula can be derived without using induction? I have no idea about that, so any hint or full proof would be highly appreciated.\n",
    "proof": "A calculation without recurrence relation and without induction.\nLet’s use the following formula, both sides come from the mixed bivariate generating function $~\\displaystyle e^{z(e^x-1)}~$ for the Stirling numbers of the second kind:\n\n$$e^{-z}\\sum\\limits_{k=0}^\\infty\\frac{z^k}{k!}k^n = \\sum\\limits_{k=0}^n z^k S(n,k)$$\n\n$S(n,k)~$ are the Stirling numbers of the second kind .\nSetting $~z:=at~$, multiplicating by $~e^{-t}~$ and integrating from $~t=0~$ to $~\\infty~$ we get: \n\n$$\\sum\\limits_{k=0}^\\infty k^n\\left(\\frac{a}{1+a}\\right)^k = (a+1)\\sum\\limits_{k=0}^n a^k k!S(n,k)$$\n\nSetting $~\\displaystyle\\frac{a}{1+a} = -e^{-x}~$, substracting $~S_{n,0}=0^n~$ and \nusing $~S(n+1,k+1)=(k+1)S(n,k+1)+S(n,k)~$ we get:\n\n$$\\sum\\limits_{k=1}^\\infty k^n\\left(-e^{-x}\\right)^k = \\sum\\limits_{k=0}^n \\frac{(-1)^{k+1} k!S(n+1,k+1)}{(1+e^x)^{k+1}}$$\n\nIt follows:\n$\\hspace{1cm}~\\displaystyle\\frac{d^n}{dx^n}\\frac{1}{1+e^x} = (-1)^{n+1}\\sum\\limits_{k=1}^\\infty k^n\\left(-e^{-x}\\right)^k = \\sum\\limits_{k=0}^n \\frac{a_{n,k+1}}{(1+e^x)^{k+1}}~$ \nwith $~~~\\displaystyle a_{n,k+1} = (-1)^{n+k} k!S(n+1,k+1) = \\left(-1\\right)^{n}\\sum_{j=0}^{k}\\left(-1\\right)^{j}{{k}\\choose{j}}\\left(j+1\\right)^{n}$\n\nNotes:\n$\\displaystyle e^{z(e^x-1)}=e^{-z}e^{ze^x}=e^{-z}\\sum\\limits_{k=0}^\\infty\\frac{z^k}{k!}e^{xk}=e^{-z}\\sum\\limits_{k=0}^\\infty\\frac{z^k}{k!}\\sum\\limits_{n=0}^\\infty\\frac{x^n k^n}{n!}=\\sum\\limits_{n=0}^\\infty\\frac{x^n }{n!}\\left(e^{-z}\\sum\\limits_{k=0}^\\infty\\frac{z^k k^n}{k!}\\right)$\n$\\displaystyle e^{z(e^x-1)}=\\sum\\limits_{k=0}^\\infty\\frac{z^k}{k!}(e^x-1)^k=\\sum\\limits_{k=0}^\\infty\\frac{z^k}{k!}\\sum\\limits_{j=0}^k(-1)^{k-j}{\\binom k j}e^{xj}=$\n$\\displaystyle\\hspace{1.5cm}=\\sum\\limits_{k=0}^\\infty\\frac{z^k}{k!}\\sum\\limits_{j=0}^k(-1)^{k-j}{\\binom k j}\\sum\\limits_{n=0}^\\infty\\frac{x^n}{n!}j^n=\\sum\\limits_{n=0}^\\infty\\frac{x^n}{n!}\\left(\\sum\\limits_{k=0}^\\infty\\frac{z^k}{k!}\\sum\\limits_{j=0}^k(-1)^{k-j}{\\binom k j}j^n\\right)$\n$\\displaystyle\\hspace{1.5cm}=\\sum\\limits_{n=0}^\\infty\\frac{x^n}{n!}\\left(\\sum\\limits_{k=0}^\\infty z^k S(n,k)\\right)=\\sum\\limits_{n=0}^\\infty\\frac{x^n}{n!}\\left(\\sum\\limits_{k=0}^n z^k S(n,k)\\right)$ \n$\\hspace{1.8cm}$ because of $~S(n,k)=0~$ for $~k>n$\nComparing the coefficients of $~x^n~$ we get the first formula.\nAnd the second formula comes from integrating by $~\\int\\limits_0^\\infty ... dt~$ ; \nwith $~c>0~$ it's $\\displaystyle~\\int\\limits_0^\\infty\\frac{t^n}{e^{ct}}dt=\\frac{1}{c^{n+1}}\\int\\limits_0^\\infty\\frac{(ct)^n}{e^{ct}}d(ct)=\\frac{n!}{c^{n+1}}~$ :\n$\\displaystyle\\sum\\limits_{k=0}^\\infty\\frac{(at)^k}{e^{at}k!}k^n = \\sum\\limits_{k=0}^n (at)^k S(n,k) ~~~~ |\\cdot e^{-t} ~~~~ |\\int\\limits_0^\\infty ... dt$\n$\\displaystyle\\sum\\limits_{k=0}^\\infty\\frac{a^k k^n}{k!}\\int\\limits_0^\\infty\\frac{t^k}{e^{(a+1)t}}dt = \\sum\\limits_{k=0}^n a^k S(n,k)\\int\\limits_0^\\infty\\frac{t^k}{e^t}dt$\n$\\displaystyle\\sum\\limits_{k=0}^\\infty \\frac{a^k k^n}{(1+a)^{k+1}} = \\sum\\limits_{k=0}^n a^k k!S(n,k) ~~~~ |\\cdot (1+a)$\nFrom there to the third formula: \n$\\displaystyle\\sum\\limits_{k=0}^\\infty k^n\\left(\\frac{a}{1+a}\\right)^k = (a+1)\\sum\\limits_{k=0}^n a^k k!S(n,k)$\n$\\displaystyle = \\sum\\limits_{k=0}^n a^{k+1} k!S(n,k) + \\sum\\limits_{k=0}^n a^k k!S(n,k) = \\sum\\limits_{k=1}^{n+1} a^k (k -1)!S(n,k-1) + \\sum\\limits_{k=0}^n a^k k!S(n,k) $\n$\\displaystyle = a^{n+1} n!S(n,n) + \\sum\\limits_{k=1}^{n} a^k (k -1)!(S(n,k-1) + k S(n,k)) + a^0 0!S(n,0) $\n$\\displaystyle = a^{n+1} n!S(n,n) + \\sum\\limits_{k=1}^{n} a^k (k -1)!S(n+1,k) + S(n,0) $\n$\\displaystyle = \\sum\\limits_{k=1}^{n+1} a^k (k -1)!S(n+1,k) + S(n,0) = \\sum\\limits_{k=0}^{n} a^{k+1} k!S(n+1,k+1) + S(n,0) $\nSubstracting $~0^n=S(n,0)~$ leads to $\\displaystyle \\sum\\limits_{k=1}^\\infty k^n\\left(\\frac{a}{1+a}\\right)^k = \\sum\\limits_{k=0}^{n} a^{k+1} k!S(n+1,k+1)~$ .  \nWith setting $\\displaystyle ~\\frac{a}{1+a}=-e^{-x}~$ we get $\\displaystyle ~a=-\\frac{1}{1+e^x}~$ and therefore the last formula. \n",
    "tags": [
      "derivatives",
      "proof-writing"
    ],
    "score": 9,
    "answer_score": 5,
    "is_accepted": true,
    "question_id": 3416127,
    "answer_id": 3416698
  },
  {
    "theorem": "Are linear transformations precisely those that keep lines straight and the origin fixed?",
    "context": "It's easy to show that given a linear transformation $T:\\mathbb{R}^n \\rightarrow \\mathbb{R}^m$ lines are mapped to lines and the origin stays fixed (as long as its rank $=n$).\nYet is the converse true?\nMore precisely, if $T:\\mathbb{R}^n \\rightarrow \\mathbb{R}^m$ is a function that maps lines to lines in the sense that for any pair of vectors $a, b$ there exists vectors $c, d$ such that $T(a+tb)=c+td$ & $T(0)=0$ can we deduce that $T(x+y)=T(x)+T(y)$ for all vectors $x, y$?\nWould appreciate any help.\n",
    "proof": "Edit: We have to clarify: Do you actually mean \"Lines are mapped to lines\", i.e. the image of a line under $T$ is again a line (what I assumed), or do you mean actually mean that $T(a + tb) = c + td$ for all $t\\in [0,1]$?\n\nNot if $m=1$! \nTake for example \n$T: \\mathbb{R}^n \\rightarrow \\mathbb{R}$\n$T(x) = 2x_1$ if $x_1>0$ \n$T(x) = x_1$ if $x_1\\leq0$\nIt projects the line to one dimension and stretches the line on the right half plane, but not on the left half plane. Its non linear around 0, but will still always project lines to lines.\nFrom \"(as long as its rank $=n$)\" I assume you'd add the condition that $T$ has to have full rank, and than we could assume $m=n > 2$ and there my counter example obviously does not work any more. \n",
    "tags": [
      "linear-algebra",
      "proof-writing",
      "linear-transformations"
    ],
    "score": 9,
    "answer_score": 3,
    "is_accepted": false,
    "question_id": 3361835,
    "answer_id": 3361929
  },
  {
    "theorem": "How do I close the gap between intuitively knowing something is true vs being able to prove it?",
    "context": "For example, one of my review problems is: Let $S_k$ be the kernel of $T^k$. Show there is a $K$ such that $S_K = S_{K+1} = \\cdots$\nSomewhere in the back of my brain there's an intuition that told me, \"Well duh, $K = \\dim(T)$.\" Obviously (to me anyway) once $K$ gets bigger than $\\dim(T)$, you're either cycling around in some $T$-invariant subspace, or you're in the null space. My brain got there by envisioning a matrix in my head and thinking about what would happen to each vector in the domain until it was satisfied that $K=\\dim(T)$ satisfies the prompt. At this point that part of my brain was content that a solution exists and moved on to something more interesting. But if I sit down and try to prove it with only the properties of transformations and subspaces, I don't even know where to start. Do I take a basis? Do I count dimensions? Nothing I try seems to get me anywhere.\nI think my brain is thinking about it the wrong way. Lower division math is about manipulating formulas and calculating. I got pretty good at doing that, and now my brain seems to attack every problem that way. I get the sense from talking to other (smarter) people that proofs are different. When my instructors come up with proofs they seem to be doing something completely different in their minds than I'm doing. To me it seems more akin to solving a puzzle than to manipulating equations. I don't see what they're doing that makes it so clear to them, in the way that lower division stuff is clear to me.\nI've heard many tips, including \"Write the first line and the last line of your proof, and then try to fill in the gaps.\" And also, \"Write statements for everything you know is true in one place.\" And also, \"Write as many statements as you can until you see something that can help you make the conclusion you need.\" And so on. Those are good tips that help simplify the problem, but I feel like the real solution is rewiring my brain to think in a different way. Sitting here and looking at and doing dozens of proofs hasn't gotten me anywhere, so I'm hoping for some insight from some people smarter than I am.\n",
    "proof": "Your professors are not thinking much differently than you can.  But the proofs they are supplying are (most of the time) the results of somebody thinking about the problem \"intuitively\", seeing why the proposition \"has to\" be true, then putting each step in that intuition into a justifiable statement.  That last step is the one you are having trouble with.\nSo there are two tough issues.   The first is to be able to express steps in your intuitive reasoning as steps in a proof.  For example, when you think \"$J(K)$ is cycling around in some subspace\" you have to realize that means that if you have the set of previous values of $J(n)$, then $J(K)$ can be expressed as a linear combination of the previous values.  So at some point you be saying something like \"Let $W(n)$ be the space spanned by ...\" and then, since your intuition is guiding you correctly, you will have to say that some new vector(s) must be expressible as a combination of the basis vectors of $W(n)$ if the dimension is at least $K$.\nThe second tough issue is that in a proof, you have to fill in the boundary cases and making sure each step is airtight.  And if you are doing real math, and not just a homework or test problem, there will be some times where in filling in these boundary cases, you will need to add restrictions to (weaken) your original proposition.\nThe last step in a really nice proof is to see how you can change your reasoning so as to make the proof clearer, shorter, or more elegant. That last part is truly an art, and may be a place where you don't have as much talent as some better mathematician. \n",
    "tags": [
      "linear-algebra",
      "proof-writing",
      "intuition"
    ],
    "score": 9,
    "answer_score": 4,
    "is_accepted": true,
    "question_id": 1065373,
    "answer_id": 1065397
  },
  {
    "theorem": "If a graph has no cycles of odd length, then it is bipartite: is my proof correct?",
    "context": "I came up with a proof of \n\nGraph $G$ has no cycles of odd length $\\implies$ $G$ is bipartite.\n\nlike this:\nWithout loss of generality, let's only consider a connected component, because if every connected component of a graph is bipartite, then the whole graph is bipartite.\nPick up a random vertex $v$ in $G$, calculate the length of the shortest simple path from $v$ to any other node, call this value distance from $v$, and divide nodes into 2 groups according to the parity of their distance to $v$.  If we can prove that nodes belong to the same group can not be adjacent, then we know that we actually get a partition of the $G$ that fulfill the definition of bipartite graph.\nNow, to introduce contradiction, assume two nodes $x, y$ with both even or odd distance from $v$ are adjacent, then the shortest simple path $\\langle v, x \\rangle$, $\\langle v, y \\rangle$ and edge $\\{x, y\\}$ contains a cycle with odd length, which is contradictory to that $G$ has no cycles of odd length. In other words, nodes both with even or odd distance from $v$ can not be adjacent, which is exactly what we need.\nSo my question is, is my proof correct?  And is there simpler method to prove the proposition?\nEdit:\n(to address comment from Srivatsan Narayanan)\nTo prove that $\\langle v, x \\rangle$ and $\\langle v, y \\rangle$, together with $\\langle x, y \\rangle$ contains a cycle with odd length is obvious when $\\langle v, x \\rangle$ and $\\langle v, y\\rangle$ are disjoint.  When that's not the case, let's give the last node shared by $\\langle v, x\\rangle$ and $\\langle v, y\\rangle$ the name $v'$.  So the three nodes $v', x, y$ forms a cycle with length $\\newcommand{\\len}{\\operatorname{len}}$\n$$ L = \\len(\\langle v', x \\rangle) + \\len(\\langle v', y \\rangle) + 1 = \\len(\\langle v, x \\rangle) + \\len( \\langle v, y \\rangle) - 2 \\cdot \\len(\\langle v, v'\\rangle) + 1 .$$\nwhere $\\len()$ means the length of the shortest path.\nAs $\\len(\\langle v, x \\rangle)$ and $\\len(\\langle v , y \\rangle)$ are both even or odd, then $L$ must be odd. Therefore, in both cases, disjoint or not, $\\langle v, x \\rangle$, $\\langle v, y \\rangle$ and $\\langle x, y\\rangle$ contains a cycle with odd length.\nEdit2\nTo see $\\len(\\langle v, x \\rangle) = \\len(\\langle v, v'\\rangle) + \\len(\\langle v', x \\rangle)$, we can simply prove that both $\\langle v, v' \\rangle$ and $\\langle v', x \\rangle$ are both shortest path.  And that's obvious, because if it's not the case, there exist a path shorter than $\\langle v, v' \\rangle$ from $v$ to $v'$, or there exist a path shorter than $\\langle v', x\\rangle$ from $v'$ to $x$, then $\\langle v, x \\rangle$ can not be a shortest path.\n",
    "proof": "I believe the question is resolved to the satisfaction of the OP. See the comments and the revisions to the question for the relevant discussions.$\\newcommand{\\len}{\\operatorname{len}}$ \n\nHere I present a different, and--in my mind--conceptually cleaner proof of the same fact. \nAssume $G$ is a connected graph such that all of whose cycles are of even length. We generalize this slightly to the following\n\nProposition. Any closed walk in $G$ has even length. \n\nProof. Towards a contradiction, suppose not. Let $W$ be a closed walk of odd length such that the length of $W$ is as small as possible. By hypothesis, $W$ cannot be a cycle; i.e., $W$ visits some intermediate vertex at least twice. Hence we can write $W$ as the \"concatenation\" of two non-trivial closed walks $W_1$ and $W_2$, each of which is shorter than $W$. Further, $\\len W_1 + \\len W_2 = \\len W$, which is odd. Thus at least one of $W_1$ and $W_2$ is of odd length, contradicting the minimality of $W$. Thus there cannot be any closed walk in $G$ of odd length. $\\quad\\quad \\Box$\nPartitioning the graph into even and odd vertices. Now, fix a vertex $v$, and define $E$ (resp. $O$) be the set of vertices $x$ in $G$ such that there is an even-length (resp. odd-length) walk from $v$ to $x$. The sets $E$ and $O$ partition $V$:\n\nAssuming $G$ is connected, then clearly $E \\cup O = V$. \nWe now show that $E \\cap O = \\emptyset$. To the contrary, suppose $x$ is in both $E$ and $O$. Then there is a $v$-$x$ walk $W_1$ of even length and another one $W_2$ of odd length. Then the walk $W_1 \\circ \\operatorname{reverse} (W_2)$ is a closed walk in $G$ of odd length, a contradiction.\n\nFinally, we show that every edge crosses the cut $(E, O)$:\n\nAssume $x \\in E$ and $xy$ is an edge. Then there exists a $v$-$x$ walk $W$ of even length. Therefore, $W \\circ xy$ is a $v$-$y$ walk and it has odd length. Therefore, $y \\in O$. \nSimilarly, if $x \\in O$ and $xy$ is an edge, we can show that $y$ is in $E$. This proof is similar to the above case. \n\nThis establishes that $G$ is bipartite, as desired.\n",
    "tags": [
      "graph-theory",
      "proof-writing"
    ],
    "score": 9,
    "answer_score": 4,
    "is_accepted": true,
    "question_id": 61920,
    "answer_id": 85430
  },
  {
    "theorem": "Let $G$ be a group, and $H$ a subgroup of $G$. Let $a, b \\in G$. Prove $Ha=Hb$ iff $ab^{-1} \\in H$.",
    "context": "Let $G$ be a group, and $H$ a subgroup of $G$. Let $a, b \\in G$.\nProve $Ha=Hb$ iff $ab^{-1} \\in H$.\n$\\rightarrow$ If $Ha=Hb$, then $h_1a=h_2b$ for some $h_1, h_2 \\in H$.\nSo, $ab^{-1} = h_1^{-1}h_2$.\nTherefore, $ab^{-1} \\in H$.\n$\\leftarrow$ If $ab^{-1} \\in H$, then $ab^{-1} = h_3$ for some $h_3 \\in H$.\nSo, $a=h_3b$, and thus $a \\in Hb$.\nTherefore, $a \\in Hb$ implies $Ha=Hb$.\nIs this proof correct? I am unsure about the first step (If $Ha=Hb$, then $h_1a=h_2b$ for some $h_1, h_2 \\in H$.).\n",
    "proof": "Your argument is correct (including the first step), but I think your last assertion requires a bit more justification. That is, you should explain why $a \\in Hb$ implies $Ha = Hb$; just to be clear, it is true, but I don't think you've clearly explained why it is true. As $a = h_3b$, $ha = hh_3b \\in Hb$, so the fact that $Ha \\subseteq Hb$ follows fairly easily (but may still be worth pointing out). The reverse inclusion is not as trivial.\nAdded Later: As Sayantan Kolgy mentions in his answer, you may be using the fact that distinct cosets are disjoint so as $a \\in Ha$ and $a \\in Hb$, we must have $Ha = Hb$. While this is a much more direct way at arriving at the conclusion, it isn't immediately clear that this is the fact you are using (as my answer demonstrates).\n",
    "tags": [
      "abstract-algebra",
      "group-theory",
      "proof-verification",
      "proof-writing"
    ],
    "score": 9,
    "answer_score": 2,
    "is_accepted": true,
    "question_id": 1008182,
    "answer_id": 1108571
  },
  {
    "theorem": "For every $b$ in the power $a^{b}$, does there exist an $a$ such that the digit sum of this power is equal to $a$?",
    "context": "$1^0 = 1\\to 1 =1$\n$x^1=x\\to x=x\\;\\forall x$.\n$9^2 = 81\\to 8+1=9$\n$8^3=512\\to 5+1+2=8$.\n$7^4=2401\\to 2+4+0+1=7$\n$46^5 = 205962976\\to 2+0+5+9+6+2+9+7+6=46$\n$64^6 = 68719476736\\to 6+8+7+1+9+4+7+6+7+3+6 = 64$\n$68^7= 6722988818432\\to 6+7+2+2+9+8+8+8+1+8+4+3+2 = 68$\n$54^8 = 72301961339136\\to 7+2+3+0+1+9+6+1+3+3+9+1+3+6=54$\n$71^9 = 45848500718449031$  $\\downarrow$ $4+5+8+4+8+5+0+0+7+1+8+4+4+9+0+3+1 = 71$\n\n\n\nConjecture:\n\nGiven a positive integer $b$, there exists a positive integer $a$ such that the digit sum of $a^b$ is equal to $a$.\n\nCan this be proven? I don't know how to prove it; it was about 3:45am in the morning and I couldn't go to sleep, so I just went on my calculator and messed around because I was bored. That's when I noticed this cool property and decided to share it here.\nIt is now 4:35am so... I gotta go to bed. I'll see you in some hours and hopefully gather the time to work on this. Sorry about that!\nOh, incidentally, I also noticed that the digit sum of $29^5$ is $23$ and the digit sum of $23^5$ is $29$, so... there are cycles here. Same for $31$ and $34$. Also, the digit sum of $13^2$ is $16$ and the digit sum of $16^2$ is $13$. These cycles seem to only have two numbers involved, but I think regarding the seventh power, there is more than two involved (start with $72^7$ I think), however there is also a cycle between two numbers of seventh powers (between $44$ and $62$). Does this help? I don't know.\nI have to go to bed. Good night!\n\nThank you in advance.\n",
    "proof": "Sorry for having to put this in the answer section (as I don’t have the reputation to add comments yet) but this seems like a really interesting problem. I noticed with b=2, the value you get when you square it and add the digits seems to end up being 9 more frequently than average (the values I got were 1,4,9,7,7,9,13,10,9,1,4,9,16,16, 9 etc). This may be the first step. Alternatively you could plot the values of a and b, so plot (1,1), (2,4), (3,9), (4,7) etc and where these points fall onto the line y=x, you get a solution (for b=2, (9,9)).\n",
    "tags": [
      "number-theory",
      "proof-writing",
      "exponentiation",
      "integers",
      "conjectures"
    ],
    "score": 9,
    "answer_score": 2,
    "is_accepted": false,
    "question_id": 3193902,
    "answer_id": 3193953
  },
  {
    "theorem": "Why so many &#39;multi-part&#39; definitions, as opposed to &#39;unified&#39; ones?",
    "context": "Many definitions consist of multiple parts: an equivalence relation is symmetric AND reflexive AND transitive; a topology is closed over finite intersections AND over arbitrary unions; etc.  However, I've seen a number of cases where it seems simpler to combine the parts into a single definition: the result is often shorter and easier to calculate with.$\n\\newcommand{\\ref}[1]{\\text{(#1)}}\n\\newcommand{\\inf}[1]{\\text{inf}(#1)}\n\\newcommand{\\sup}[1]{\\text{sup}(#1)}\n\\newcommand{\\then}{\\Rightarrow}\n\\newcommand{\\when}{\\Leftarrow}\n\\newcommand{\\true}{\\text{true}}\n\\newcommand{\\false}{\\text{false}}\n$\n\n$\\bullet\\;$ As my most recent example, I discovered (through questions here on MSE) that $\\;\\inf{\\cdots}\\;$ can simply be defined by postulating $$\nz \\leq \\inf{A} \\;\\equiv\\; \\langle \\forall a : a \\in A : z \\leq a \\rangle\n$$ for any $\\;z\\;$ and lower-bounded $\\;A\\;$.  Contrast this with \\begin{align}\n& z \\in A \\;\\then\\; \\inf{A} \\leq z \\\\\n& \\langle \\forall a : a \\in A : z \\leq a \\rangle \\;\\then\\; z \\leq \\inf{A} \\\\\n\\end{align} or even \\begin{align}\n& z \\in A \\;\\then\\; \\inf{A} \\leq z \\\\\n& \\langle \\forall \\epsilon : \\epsilon > 0 : \\langle \\exists a : a \\in A : a < \\inf{A} + \\epsilon \\rangle \\rangle \\\\\n\\end{align}\n$\\bullet\\;$ For sets, the symmetric difference is often defined as $$\nA \\triangle B \\;=\\; (A \\setminus B) \\cup (B \\setminus A)\n$$ or $$\nA \\triangle B \\;=\\; (A \\cup B) \\setminus (A \\cap B)\n$$ while in practical proofs I find it much easier to work with $$\nx \\in A \\triangle B \\;\\equiv\\; x \\in A \\;\\not\\equiv\\; x \\in B\n$$ for all $\\;x\\;$, since $\\;\\not\\equiv\\;$ is the logic-level equivalent of $\\;\\triangle\\;$.\n$\\bullet\\;$ The textbook definition of '$\\;\\mathscr T\\text{ is a topology on }X\\;$' is that \\begin{align}\n& \\mathscr T \\subseteq \\mathscr P(X) \\\\\n& \\emptyset \\in \\mathscr T \\\\\n& X \\in \\mathscr T \\\\\n& \\mathscr T\\text{ is closed under }\\cdots \\cap \\cdots \\\\\n& \\mathscr T\\text{ is closed under }\\bigcup \\\\\n\\end{align}  However, given closure under $\\;\\bigcup\\;$, the first three conditions can be unified to just $$\n\\bigcup \\mathscr T = X\n$$ which has the very intuitive reading '$\\;\\mathscr T\\;$ covers $\\;X\\;$'.\n$\\bullet\\;$ In logic, I almost aways see the 'uniqueness quantifier' $\\langle \\exists! x :: P(x) \\rangle$ ('there exists exactly one') defined as $$\n\\langle \\exists x :: P(x) \\rangle \\;\\land\\; \\langle \\forall x,y : P(x) \\land P(y) : x=y \\rangle\n$$ where $$\n\\langle \\exists y :: \\langle \\forall x :: P(x) \\;\\equiv\\; x = y \\rangle \\rangle\n$$ is shorter and often seems much easier to work with.  And it has a nice symmetry: the $\\;\\then\\;$ direction of the equivalence is uniqueness, which the $\\;\\when\\;$ direction is existence.\n$\\bullet\\;$ Finally, as an example from various domains, a statement of the form $\\;P \\equiv Q\\;$ is very often seen as an invitation to give separate proofs for $\\;P \\then Q\\;$ and $\\;Q \\then P\\;$; and similarly for mutual inclusion for sets, and for proving equality of numbers using $\\;\\le\\;$ and $\\;\\ge\\;$, or even $\\;\\lt,=,\\gt\\;$.\n\nThe common pattern in all of the above, is that people seem to prefer 'multi-part' definitions over 'unified' definitions.  And I'm wondering why this is.\nDoes a proof which is split in parts perhaps have a proof-practical advantage?  As a kind of counterexample, a while ago I discovered that a relation $\\;R\\;$ on $\\;A\\;$ is an equivalence relation exactly when $$\naRb \\:\\equiv\\: \\langle \\forall x :: aRx \\equiv bRx\\rangle\n$$ holds for all $\\;a,b\\;$ (where $\\;a,b,x\\;$ range over $\\;A\\;$).  However, when I tried to actually use this definition to prove some relation to be an equivalence relation, then almost always the resulting proof was more complex than a proof of the three parts (reflexivity, symmetry, transitivity).  So in this specific example, the 'unified' definition did not really help me.  But in my experience, this has been the exception: 'unified' definitions almost always really work in practice for me.\nDo the parts perhaps have an educational value?  Perhaps, at least initially, it is easier to build an intuition using separate parts, and then both those proofs and also later proofs are structured around that 'multi-part' intuition.\nIs there perhaps an 'implicational bias'?  In other words, is it perhaps that I've been brought up in the 'school' of Dijkstra-Feijen, Gries-Schneider, et al., where there is an emphasis on equality and equivalence and symmetry, while most people approach proofs 'sequentially' based on inferences?\nOr is something else at work here?\n",
    "proof": "Regarding binary relations, there are many important different types. An equivalence relation is symmetric, reflexive, and transitive. A linear order < is anti-symmetric, irreflexive, transitive, and satisfies trichotomy. A well-order is a linear order with an additional condition. A poset (the kind used in the set-theoretic topic called Forcing) is reflexive and transitive. And there are of course many others.  Instead of trying to compress the definitions, it is often more useful to list the parts, as it can then be seen how varying the parts results  in  other structures. \nChess players say \"To win you must use all your pieces\". When attempting a proof, a list of properties, even if logically redundant, can help you to see some important property that you haven't used. \nSometimes a defining list is easier to use because it incorporates more data: Let $\\times$ be an associative binary operation on a set $G\\ne \\phi$ such that $\\forall x,y\\in G\\;[\\;(\\exists! z\\in G\\;(x\\times z=y)\\land (\\exists!z'\\in G\\; (z'\\times x=y) \\;].$ It takes some work to show that this meets all the \"usual list\" of conditions for a group. The usual def'n mentions an identity and unique two-sided inverses.\nOn the other hand, some writers do present def'ns that are much longer than most people would deem necessary. \n",
    "tags": [
      "soft-question",
      "proof-writing",
      "definition"
    ],
    "score": 9,
    "answer_score": 2,
    "is_accepted": false,
    "question_id": 1656343,
    "answer_id": 2352008
  },
  {
    "theorem": "Why is $a^{x+y}=a^xa^y$",
    "context": "After reading chapter 1 of Rudin's Principles of mathematical analysis, I started working on the exercises. Exercise 6 was a big surprise since exercises 1-5 had all been very simple, whereas 6 had me stumped for a ver long time until I read solutions online. The problem is:\n\nFor integers $m,n,p,q$ let $r=m/n=p/q$, prove that for positive $b$\n$$(b^m)^{1/n}=(b^p)^{1/q}$$\nThen show that for rationals $r,s$\n$$b^{r+s}=b^rb^s$$\nIf $$b^x=\\sup \\{ b^t : t\\le x,t\\in \\mathbb{Q}\\}$$\nShow that for reals $x,y$\n$$b^{x+y}=b^xb^y$$\nAfter reading the solutions I think I could have solved the first and second parts of the question, but for the last part, the solution made use of several propositions, corollaries and had to prove a lemma before starting the demonstration. \n\nThis made me a little suspicious, since exercise 7 asked for the proof of several of these facts. So I was wondering if there are alternate proofs, and if there are, what is the simplest you know?\n\nThis is the link to the solutions I am talking about\n",
    "proof": "Although it takes up a lot of space, the argument is not particularly complex or even long. I think it's presented in a misleadingly way. The lemma evoked is extremely straightforward and admits a much simpler proof:\n$$r < \\frac{r+(x+y)}{2} = \\frac{2x+r}{4}+\\frac{2y+r}{4} < x + y$$\nNow to finish the problem you can just iteratively apply this to obtain a monotonic sequence of rationals that approach $x+y$ from bellow, and take a sup, as the function $f(x)=k^x$ is monotonic and order preserving.\n",
    "tags": [
      "real-analysis",
      "proof-writing"
    ],
    "score": 9,
    "answer_score": 1,
    "is_accepted": false,
    "question_id": 2078187,
    "answer_id": 2078203
  },
  {
    "theorem": "Prove that $\\sqrt{2} + \\sqrt[3]{3}$ is irrational",
    "context": "$\\sqrt{2} + \\sqrt[3]{3}$ is irrational ?\nThese are my steps -\n$\\sqrt{2} + \\sqrt[3]{3} = a$\n$3 = (a-\\sqrt{2})^{3}$\n$3 = a^{3} -3a^{2}\\sqrt{2} + 6a -2\\sqrt{2}$\n$3a^{2}\\sqrt{2}+2\\sqrt{2} = a^{3}+6a-3$\n$\\sqrt{2}(3a^{2}+2) = a^{3}+6a-3$\nThen, $\\sqrt{2}$ in the left side is irrational , and mulitply irratinal with rational is irrational.\nThe right side is rational.\nSo,  $irrational \\neq rational$.\nThis is a good proof ?\n",
    "proof": "Taking powers of $\\alpha=\\sqrt2+\\sqrt[\\large3]{3}$ and putting them into matrix form, we get\n$$\n\\begin{bmatrix}\n\\alpha^0\\\\\\alpha^1\\\\\\alpha^2\\\\\\alpha^3\\\\\\alpha^4\\\\\\alpha^5\\\\\\alpha^6\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n1&0&0&0&0&0\\\\\n0&1&1&0&0&0\\\\\n2&0&0&2&1&0\\\\\n3&2&6&0&0&3\\\\\n4&12&3&8&12&0\\\\\n60&4&20&15&3&20\\\\\n17&120&90&24&60&18\n\\end{bmatrix}\n\\begin{bmatrix}\n1\\\\2^{1/2}\\\\3^{1/3}\\\\2^{1/2}3^{1/3}\\\\3^{2/3}\\\\2^{1/2}3^{2/3}\n\\end{bmatrix}\\tag{1}\n$$\nWe can use the method from this answer to get a vector perpendicular to all the columns in the matrix above:\n$$\n\\begin{bmatrix}\n1\\\\-36\\\\12\\\\-6\\\\-6\\\\0\\\\1\n\\end{bmatrix}^{\\large T}\n\\begin{bmatrix}\n1&0&0&0&0&0\\\\\n0&1&1&0&0&0\\\\\n2&0&0&2&1&0\\\\\n3&2&6&0&0&3\\\\\n4&12&3&8&12&0\\\\\n60&4&20&15&3&20\\\\\n17&120&90&24&60&18\n\\end{bmatrix}=0\\tag{2}\n$$\n$(1)$ and $(2)$ imply that\n$$\n\\alpha^6-6\\alpha^4-6\\alpha^3+12\\alpha^2-36\\alpha+1=0\\tag{3}\n$$\n$(3)$ says that $\\alpha$ is an algebraic integer. A rational algebraic integer must be an integer. However, $1\\lt\\sqrt2\\lt\\frac32$ and $1\\lt\\sqrt[\\large3]3\\lt\\frac32$, thus $2\\lt\\alpha\\lt3$. Therefore, $\\alpha$ must be irrational.\n",
    "tags": [
      "algebra-precalculus",
      "proof-verification",
      "proof-writing",
      "irrational-numbers",
      "rational-numbers"
    ],
    "score": 9,
    "answer_score": 1,
    "is_accepted": false,
    "question_id": 1571961,
    "answer_id": 1575238
  },
  {
    "theorem": "Proof that $\\log_23 +\\log_52$ is irrational number",
    "context": "Problem is to prove that\n$$\\log_23 +\\log_52$$\nis irrational number.\nMy attempt: \nI try to write number like $$\\log_23 +\\frac{1}{\\log_25}$$ but I didn't get anything(proof by contradiction). I also try to find polynomial such that given number is zero point but also without success.\n",
    "proof": "Refer to Qiaochu Yuan's answer from (Question 986227), which states that, contingent on the currently unproven Schanuel's conjecture, the logarithms of the primes, $\\ln 2,\\, \\ln3\\, \\,\\ln 5\\,\\ldots$, are algebraically (not just linearly) independent over $\\mathbb{Q}$. The stronger statement would follow that $\\log_2 3+\\log_5 2$ is transcendental and consequentially irrational.\n\nProof. By unique prime factorization, the logarithms of the primes are linearly independent over $\\mathbb{Q}$. If $p_1, p_2, \\dots$ is an enumeration of the primes, then by Schanuel's conjecture it follows that $\\mathbb{Q}(\\log p_1, \\log p_2, \\dots \\log p_k)$ has transcendence degree at least $k$, hence exactly $k$, for all $k$. $\\Box$\n\nYour problem will likely remain open until Schanuel's conjecture is resolved. To quote GH from MO's answer to (Math Overflow: Question 185540),\n\n...[that $\\log_35+\\log_25$ is irrational] is probably true, but proving it might be out of reach at the moment.\n\n",
    "tags": [
      "proof-writing",
      "logarithms",
      "irrational-numbers",
      "rationality-testing",
      "algebraic-numbers"
    ],
    "score": 9,
    "answer_score": 1,
    "is_accepted": false,
    "question_id": 3521921,
    "answer_id": 3531622
  },
  {
    "theorem": "Controlled natural language for mathematics",
    "context": "I am a French student very inspired by Bourbaki's but I can no longer stand to write approximate proofs.\nI was wondering if there was a language between formal and natural language that was both non-binding for the reader (he or she had nothing to learn to read the demonstration) and mechanical enough to be understood by a computer.\nI found two or three projects but no clear documentation.\nTo be precise, I don't mind that the project is abandoned as long as there is an exhaustive (and high level) description of the syntax.\n",
    "proof": "I recommend reading \"Book of Proof\" $2009$ by Richard H. Hammack. Hardcover is only about $25$ USD and it describes the language (mostly natural) to be used in proofs.  It shows how to address the reader as though in a conversation: \"We can see that...\". It also shows some mathematical symbols used in proofs but mostly, it's how to approach a proof by induction or whatever with examples.\nI don't know how a proof would fit in a computer but it appears that PYTHON is the preferred language for doing math stuff. On the other hand, there is a movement to have machines both write and validate proofs. I don't know the language but it is described in \"Mathematics without Apologies: Portrait of a Problematic Vocation (Science Essentials)\" $2015$ by Michael Harris.\nIf you find what you are looking for, give us an update. I'm sure that many people would be interested.\n",
    "tags": [
      "proof-writing"
    ],
    "score": 9,
    "answer_score": 0,
    "is_accepted": false,
    "question_id": 3375732,
    "answer_id": 3380659
  },
  {
    "theorem": "Prove that $x^3 + x^2 = 1$ has no rational solutions?",
    "context": "Is this enough for a proof?:\n$$x^3+x^2 = 1$$\nI would factor and get: $x^2(x+1) = 1$\nI would show that $x = \\sqrt1$, which is rational but then what else would I have to show? $x+1=1$ which gives me $x=0$ and since $x$ cannot equal to $0$ as this would make the statement false ($0$ times anything is $0$). Is it enough to simply state this falsity or is there another way to express it?\nThanks!\n",
    "proof": "By the rational root theorem, a rational root would have to be $x=1$ or $x=-1$, but neither works.\n",
    "tags": [
      "polynomials",
      "proof-writing",
      "proof-verification",
      "rational-numbers"
    ],
    "score": 8,
    "answer_score": 48,
    "is_accepted": true,
    "question_id": 529227,
    "answer_id": 529228
  },
  {
    "theorem": "Can we prove &quot;If $A = B$ then $C=D$&quot; by assuming $C=D$ and showing $A=B$?",
    "context": "Consider a statement If $A = B$ then $C = D.$ If one were to prove it, the first thing that he/she might do is to use the fact that $A = B$ and, with some algebraic manipulation, reach the conclusion $C = D$. \nNow, I proved it by using some other route. \nI assumed that $C = D$ and used it to reach the conclusion that $A = B.$ Since the fact that $A = B$ is true, our assumption was right and thus it is proved that $C$ is actually equal to $D$. \n\nMy question is: Is this way of proving something correct?\n\n",
    "proof": "No, what you are describing is the logical fallacy of affirming the consequent.\nIf you have $P\\implies Q$, that doesn't mean that $P$ is true just because $Q$ is. Example in propositional logic\n\nIt rains, therefore the road is wet.\n\nThis is perfectly valid, however just because you see the road wet doesn't mean it has rained. After all I can grab my hose and spray it all over the road for an hour without it having rained or be raining.\n",
    "tags": [
      "algebra-precalculus",
      "proof-writing"
    ],
    "score": 8,
    "answer_score": 35,
    "is_accepted": true,
    "question_id": 1984240,
    "answer_id": 1984276
  },
  {
    "theorem": "Show that if $r$ is an nth root of $1$ and $r\\ne1$, then $1 + r + r^2 + ... + r^{n-1} = 0$.",
    "context": "Show that if $r$ is an nth root of $1$ and $r\\ne1$, then $1 + r + r^2 + ... + r^{n-1} = 0$.\nI think I can represent all the roots of 1 as follows:\n$r = 1^{\\frac{1}{n}} ( \\frac{\\cos{2\\pi k}}{n} +  i\\frac{\\sin{2\\pi k}}{n} )$\nFrom there I am not sure how to get to $1 + r + r^2 + ... + r^{n-1} = 0$.\n",
    "proof": "$1 + r + r^2 + r^3 + ... + r^{n-1}$ is the sum of the first $n$ terms of a geometric series.  The formula for this summation, as long as $r \\ne 1$, is\n$$\\frac{r^n - 1}{r - 1}$$\nSince $r$ is an nth root of unity, $r^n - 1 = 0$.  Since the denominator is nonzero, this quantity is equal to zero.\nHope this helps!\n",
    "tags": [
      "complex-numbers",
      "proof-writing",
      "power-series"
    ],
    "score": 8,
    "answer_score": 13,
    "is_accepted": true,
    "question_id": 517966,
    "answer_id": 518010
  },
  {
    "theorem": "Show $8\\mid n^2-1$ if $n$ is an odd positive integer.",
    "context": "Show that $n^2-1$ is divisible by $8$, if $n$ is an odd positive integer.\nPlease help me to prove whether this statement is true or false.\n",
    "proof": "If $n$ is an odd positive integer, then $n=2m+1$ for some non-negative integer $m$. Can you see how to finish? Hint- Do two cases , one where $m$ is odd and one where $m$ is even.\n",
    "tags": [
      "elementary-number-theory",
      "proof-writing"
    ],
    "score": 8,
    "answer_score": 12,
    "is_accepted": true,
    "question_id": 199185,
    "answer_id": 199187
  },
  {
    "theorem": "Is it valid to use operations on both sides before inequality is proven?",
    "context": "As part of a bigger proof, I am trying to prove the inequality:\n$$\\frac{ab}{a^2 + b^2}< \\frac{1}{2}$$\nIs the following proving method correct?\n$$\\frac{ab}{(a^2)+(b^2)} <\\frac{1}{2}$$\n$$2ab < (a^2 + b^2)$$\n$$0 <(a-b)^2$$\n$a \\neq b$ thus $a - b \\neq 0$ and every number squared is not negative thus $(a-b)^2$ is positive thus bigger than zero\nMy concern with this proof is that I am not sure if I am allowed to do operations on both sides before inequality is proven. \n",
    "proof": "As Nelver's answer highlights, you've technically proven the converse of what we want (i.e. the opposite implication). That said, this is something that is done a lot of the time by many people and in some sense, it's a more natural approach, but you have to be careful about how you write it up. One thing you can do is to say that all of the steps you've taken are reversible.\nThe other way to write this, which is more common, is to use equivalences (if and only ifs). So you could write \\begin{align*} \n\\frac{ab}{a^2+b^2}<\\frac{1}{2} \\iff 2ab<a^2+b^2 \\iff 0<(a-b)^2\n\\end{align*} which is true since $a\\neq b$. The $\\iff$ is saying that the statement holds if and only if the statement on the right holds.\n",
    "tags": [
      "algebra-precalculus",
      "inequality",
      "proof-writing",
      "solution-verification"
    ],
    "score": 8,
    "answer_score": 17,
    "is_accepted": true,
    "question_id": 3628601,
    "answer_id": 3628652
  },
  {
    "theorem": "How does $-(-1)^n$ become $(-1)^{n+1}$",
    "context": "I am currently studying how to prove the Fibonacci Identity by Simple Induction, shown here, however I do not understand how $-(-1)^n$ becomes $(-1)^{n+1}$.  Can anybody explain to me the logic behind this?\n",
    "proof": "$-(-1)^n=(-1)^1(-1)^n=(-1)^{n+1}$\n",
    "tags": [
      "proof-writing",
      "arithmetic"
    ],
    "score": 8,
    "answer_score": 23,
    "is_accepted": true,
    "question_id": 1440106,
    "answer_id": 1440110
  },
  {
    "theorem": "How can you prove that $1.05^{50} &lt; 100$ without a calculator?",
    "context": "Is there a way to prove that $1.05^{50} < 100$ without a calculator?\nI have tried this...\n$$1.05^{50}<10^2$$\n$$(\\frac{105}{100})^{50}<10^2$$\n$$(\\frac{21}{20})^{50}<10^2$$\n$$\\frac{21^{50}}{20^{50}}<10^2$$\n$$\\frac{21^{50}}{2^{50}*10^{50}}<10^2$$\n$$\\frac{21^{50}}{2^{50}}<10^{52}$$\n$$10.5^{50}<10^{52}$$\n...but I don't know where to go. Can someone assist me (alternate methods are fine)?\nEDIT:\n Can anyone help me prove that $2^{1000}<10^{302}$ without a calculator?\n",
    "proof": "$$\n1.05^{50} = \\left(1+\\frac{5}{100}\\right)^{50}\n= \\sqrt{\\left(1+\\frac{5}{100}\\right)^{100}}\n< \\sqrt{e^5} = \\sqrt{e}^5 < 2^5 = 32\n$$\nEdit: for you second inequality, $2^{1000}<10^{302}$ is equivalent to $(2^{10})^{100}<10^2(10^3)^{100}$, which is equivalent to\n$$\n\\left(\\frac{2^{10}}{10^3}\\right)^{100} = \\left(1+\\frac{24}{1000}\\right)^{100} < 100.\n$$\nFrom $a\\log(1+t)\\leq at$ for $a>0$ we deduce $(1+t)^a\\leq e^{at}$. Therefore\n$$\n\\left(1+\\frac{24}{1000}\\right)^{100} < e^{\\frac{24}{1000}\\cdot100}\n<e^{5/2} < 32 < 100.\n$$\n\nAddendum. What is trickier, is proving that $2^{1000}>10^{301}$. Can you do that?\nHere is how I go about it. Maybe someone else can find a simpler derivation.\nDefine\n$$\n\\exp_n(x) = \\sum_{k=0}^n \\frac{x^k}{k!} < \\exp(x) .\n$$\nWe want $\\left(\\frac{2^{10}}{10^3}\\right)^{100}>10$. From $(1-t)^a\\leq e^{-at}$ you deduce $\\left(\\frac1{1-t}\\right)^a\\geq e^{at}$. So\n$$\n\\begin{split}\n\\left(\\frac{2^{10}}{10^3}\\right)^{100}\n&= \\left(\\frac{1024}{1000}\\right)^{100}\n= \\left(\\frac{1}{1-\\frac{24}{1024}}\\right)^{100}\n\\geq e^{100\\cdot\\frac{24}{1024}} = e^{75/32} \\\\\n&= e^{2+11/32} > e^{2+11/33}\n= \\exp(2)\\exp(1/3) > \\exp_5(2)\\exp_2(1/3) \\\\\n&= \\left(1+2+2+\\frac43+\\frac23+\\frac4{15}\\right)\\left(1+\\frac13+\\frac1{18}\\right) \\\\\n&= \\frac{109}{15} \\cdot \\frac{25}{18} = \\frac{545}{54} > 10.\n\\end{split}\n$$\n",
    "tags": [
      "inequality",
      "proof-writing",
      "number-comparison"
    ],
    "score": 8,
    "answer_score": 17,
    "is_accepted": true,
    "question_id": 2990427,
    "answer_id": 2990434
  },
  {
    "theorem": "Proof that $\\sqrt{5}$ is irrational",
    "context": "In my textbook the following proof is given for the fact that $\\sqrt{5}$ is irrational:\n$ x = \\frac{p}{q}$ and $x^2 = 5$. We choose $p$ and $q$ so that the have no common factors, so we know that $p$ and $q$ aren't both divisible by $5$.\n$$\\left(\\dfrac{p}{q}\\right)^2 = 5\\\\ \\text{ so } p^2=5q^2$$\nThis means that $p^2$ is divisble by 5. But this also means that $p$ is divisible by 5. \n$p=5k$, so $p^2=25k^2$ and so $q^2=5k^2$. This means that both $q$ and $p$ are divisible by 5, and since that can't be the case, we've proven that $\\sqrt{5}$ is irrational.\nWhat bothers me with this proof is the beginning, in which we choose a $p$ and $q$ so that they haven't got a common factor. How can we know for sure that there exists a $p$ and $q$ with no common factors such that $x=\\dfrac{p}{q} = \\sqrt{5}$? Because it seems that step could be used for every number\nEdit:\nI found out what started my confusion: I thought that any fraction with numerator 1 had a common factor, since every integer can be divided by 1. This has given me another question: Are confusions like this the reason why 1 is not considered a prime number? \n",
    "proof": "We know that by the definition of rational numbers, essentially: rationals can be written as $\\frac{p}{q}$ for integers $p,q$, $q \\neq 0$. \nIf for some choice they would have a common factor, we could divide it out, with the same quotient (our rational number) remaining, and they would have one factor less in common. As the number of common factors is finite, we have to repeat this at most finitely many times to have a choice of $p$ and $q$ with no common factors.\nThe proof starts by assuming we have done this step already.\nMore formally, you could consider the set $A:= \\{(n,m) \\in \\mathbb{N} \\times \\mathbb{N}: n^2 = 5m^2\\}$, and  if $\\sqrt{5}$ were rational, $A$ would be non-empty (as $\\sqrt{5} > 0$ we can pick two positive integers WLOG.)\nBut $\\mathbb{N} \\times \\mathbb{N}$ is well-ordered in the order $(a,b) < (c,d)$ iff $c < d$ or $c=d$ and $a<b$. (This is just the product of the ordinal $\\omega$ with itself in set theory.). Let $(n,m)$ be the minimum of this non-empty set. If $a|n$ and $a|m$ would both hold for some $a > 1$, then $(n',m'):=(\\frac{n}{a}, \\frac{m}{a}) \\in A$ and  $(n',m') < (n,m)$, contradicting minimality. This agument can easily be adapted to show any fraction has a unique (modulo the sign of $p,q$, or demand $q>0$) equivalent representation $\\frac{p}{q}$ where there is no $a>1$ such that $a|p$ and $a|q$.\n",
    "tags": [
      "proof-writing"
    ],
    "score": 8,
    "answer_score": 9,
    "is_accepted": true,
    "question_id": 263864,
    "answer_id": 263871
  },
  {
    "theorem": "How can I derive what is $1\\cdot 2\\cdot 3\\cdot 4 + 2\\cdot 3\\cdot 4\\cdot 5+ 3\\cdot 4\\cdot 5\\cdot 6+\\cdots + (n-3)(n-2)(n-1)(n)$ ??",
    "context": "I'd like to evaluate the series $$1\\cdot 2\\cdot 3\\cdot 4 + 2\\cdot 3\\cdot 4\\cdot 5+ 3\\cdot 4\\cdot 5\\cdot 6+\\cdots + (n-3)(n-2)(n-1)(n)$$\nSince I am a high school student, I only know how to prove such formula's (By principal of mathematical induction). I don't know how to find result of such series.\nPlease help. I shall be thankful if you guys can provide me general solution (Since I have been told that there exist a general solution by my friend who gave me this question).\n",
    "proof": "In general, we have\n$$\\sum_{k=1}^nk(k+1)(k+2)\\dots(k+p)=\\frac{n(n+1)(n+2)\\dots(n+p+1)}{p+2}$$\nProve by induction,\n$$\\sum_{k=1}^{n+1}k(k+1)(k+2)\\dots(k+p)\\\\=(n+1)(n+2)\\dots(n+p+1)+\\sum_{k=1}^nk(k+1)(k+2)\\dots(k+p)\\\\=(n+1)(n+2)\\dots(n+p+1)+\\frac{n(n+1)(n+2)\\dots(n+p+1)}{p+2}\\\\=\\frac{\\color{#034da3}{(p+2)}\\color{#ee8844}{(n+1)(n+2)\\dots(n+p+1)}+\\color{#034da3}n\\color{#ee8844}{(n+1)(n+2)\\dots(n+p+1)}}{p+2}\\\\=\\frac{\\color{#ee8844}{(n+1)(n+2)\\dots(n+p+1)}\\color{#034da3}{(n+p+2)}}{p+2}$$\nAnd easy enough to check for $n=1$ to see it is true.\n$$1\\times2\\times3\\times\\ldots\\times(1+p)=\\frac{1\\times2\\times3\\times\\ldots\\times(1+p)\\times\\require{cancel}\\cancel{(2+p)}}{\\cancel{p+2}}$$\n\nThis is slightly off, since my sum ends at $n(n+1)(n+2)(n+3)$, while you end off at $(n-3)(n-2)(n-1)n$.  To readjust, have $n=p-3$ in my sum and it will become yours.\n",
    "tags": [
      "calculus",
      "sequences-and-series",
      "summation",
      "proof-writing",
      "binomial-coefficients"
    ],
    "score": 8,
    "answer_score": 8,
    "is_accepted": true,
    "question_id": 2021231,
    "answer_id": 2021244
  },
  {
    "theorem": "Prove that there are no integers $x$ and $y$ such that $3x^2=13+4y^2$",
    "context": "\nProve that there are no integers $x$ and $y$ such that $3x^2=13+4y^2$.\n\nFrom the equation, I know that $3x^2$ must be odd and therefore equal $2k + 1$ for some integer $k$. \nBut I am unsure what to do after that. \nI have also worked out that $k = 2(y^2 + 3)$, but I don't know if that helps at all.\nMy instructor noted that I should look at whether $x^2$ is even or odd, but I am at a loss.\n",
    "proof": "$3x^2=\\underbrace{13+4y^2}_\\text{odd}$\nWe know that $x$ is odd so we can substitute $x=2k+1$\n\\begin{align}3(2k+1)^2=13+4y^2\\\\\n3(4k^2+4k+1)=13+4y^2\\\\\n3(4k^2+4k)=10+4y^2\\\\\n12(k^2+k)=10+4y^2\\\\\n\\underbrace{6(k^2+k)}_\\text{even}=\\underbrace{5+2y^2}_\\text{odd}\n\\end{align}\nbut $5+2y^2$ is odd not even which contradict the factor $6$ on the left side of the equation.\n",
    "tags": [
      "elementary-number-theory",
      "proof-writing",
      "diophantine-equations"
    ],
    "score": 8,
    "answer_score": 14,
    "is_accepted": true,
    "question_id": 2435185,
    "answer_id": 2435208
  },
  {
    "theorem": "Proof that squares are divisible by 3 when their sum is",
    "context": "In this proof, they write $3|a^2+b^2 \\implies 3|a$, $3|b$. I tried using the same proof used to prove $3|a^2 \\implies 3|a$, where $3$ being prime and writing $a^2 = a\\cdot a$ suggests that $a$ is divisible by $3$. I'm not sure how to prove the $3|a^2+b^2$ case, though.\n\nE9. There is no quadruple of positive integers $(x, y, z, u)$ satisfying $$x^2 + y^2 = 3(z^2 + u^2).$$\nSolution. Suppose there is such a quadruple. We choose the solution with the smallest $x^2 + y^2$. Let $(a, b, c, d)$ be the chosen solution. Then $$a^2 + b^2 = 3(c^2 + d^2) \\implies 3|a^2 + b^2 \\implies 3|a, 3|b \\implies a = 3a_1, b = 3b_1,\\\\a^2 + b^2 = 9(a^2_1 + b^2_1) = 3(c^2 + d^2) \\implies c^2 + d^2 = 3(a^2_1 + b^2_1).$$\nWe have found a new solution $(c, d, a_1, b_1)$ with $c^2 + d^2 \\lt a^2 + b^2$. Contradiction.\nWe have used the fact that $3|a^2 + b^2 \\implies 3|a, 3|b$. Show this yourself. We will return to similar examples when treating infinite descent.\n\n",
    "proof": "For a number $n$ we have\n$n\\equiv 0,1,2 \\mod 3$ so we get $$n^2\\equiv 0,1\\mod 3$$ For $$a^2+b^2$$ we have\n$$a^2+b^2\\equiv 0 \\mod 3$$ The only possibility is $$a^2=b^2\\equiv 0 \\mod 3$$\n",
    "tags": [
      "elementary-number-theory",
      "proof-writing",
      "contest-math"
    ],
    "score": 8,
    "answer_score": 14,
    "is_accepted": true,
    "question_id": 2833519,
    "answer_id": 2833535
  },
  {
    "theorem": "Use of $\\implies$ and $=$ when stricter conditions are true",
    "context": "I am about to begin studying as an undergraduate. Previously, the mathematical exams I have studied have been incredibly lax in terms of notation or neatness of proof - it would not be penalised, for instance, to write $f'(x)$ instead of $\\frac{dy}{dx}$ when the function given was in terms of $y$. It was also considered equally valid when given an identity to prove, to work from the LHS and then from the RHS and join the two together in the middle (rather than a coherent working from one side to the other).\nAnyway, as it is not something featured in my previous syllabus, I am looking to improve my understanding of notation to use when proving a statement. I have come across two inconsistencies in teaching materials so far:\n1. Use of $=$ when $\\equiv$ is valid.\nNow, I am comfortable with the meaning of these symbols. But my question is when the use of $\\equiv$ or $=$ is mandatory, and when it is permissible. An obvious case to me would be something like: $$\\frac{3x^2+34x+6}{x^2+2x-24} \\equiv \\frac{3x+7}{x-4} + \\frac{9}{x+6}$$ Here, use of $=$ would imply it is an equation to solve, rather than something true $\\forall x \\in \\mathbb{R}$ ($x \\ne -6, 4$). All materials I have come across would use $\\equiv$ here.\nBut why is it common to write $3+2=5$, when clearly this is not an equation to solve, but an equivalency? Would $3+2 \\equiv 5$ not be more accurate? In a more borderline case, if part of the answer to a question includes the simplification of $3(x-4)+2$ to $3x-10$, which symbol is most appropriate? I have seen both $ 3(x-4)+2 = 3x-10$ and $3(x-4)+2 \\equiv 3x-10$.\nThe definition of the $\\equiv$ symbol (two things are equal for all values of any variables used) seems to contradict the notation I have seen in common use, based on its lack of use outside of proving identities.\n2. Use of $\\implies$ or $\\impliedby$ when $\\iff$ is possible.\nIf I am writing a proof and we progress from one stage to the next, it seems that $\\implies$ would demonstrate the flow of logic more accurately, even if $\\impliedby$ is also true and thus $\\iff$ could be used. For example, if a question asked us to find all possible values of $x$ in some trigonometric equation, and we had simplified down to:\n$$\\sin(x) = 0 \\implies x = n\\pi\\; (n \\in \\mathbb{Z})$$\nHere, I am conflicted, because $\\implies$ seems to be more coherent, as we are interested in progressing from the left to the right but have no use for the information that progressing from the right to the left is valid. However, if $\\implies$ contains within it the implicit notion that $\\require{cancel} \\cancel{\\impliedby}$, then the use is incorrect and should be replaced by $\\iff$. Which symbol should be used, or are both permissible (and the question down to personal preference)?\n",
    "proof": "It is absolutely fantastic that you are thinking about these issues as you start your undergraduate career. If only more students would appreciate these subtleties in mathematics. One nice thing in mathematics is that everything can be made precise.\nA couple of remarks though:\n\nOften we will define precise use of notation and then immediately violate the convention. Insisting on precise notation everywhere is not helpful because everything becomes too cumbersome. It can become easy to bury what you are trying to communicate in notation. Abuse of notation is very common and accepted. Also, mathematics is about more than a game of notation. While it strictly speaking might be true at the root, mathematics is also about ideas.\nRemember the context. Saying that $x^2 + x  + 1=0$ might mean an invitation to solve the equation. It might mean that $x$ is an elsewhere defined number and that the number $x^2 + x + 1$ is equal to $0$.\nAsk your teacher. The teacher will probably have certain preferences when it comes to notation. Don't then get mad at your teacher because he/she doesn't follow notation that you have used before. Instead, get used to the change of preferences.\n\nNow, typically $=$ is used to say that two elements in a set are the same element. Saying \"solve $x^2 = 2$\" can then mean find all elements $x$ in the set $\\mathbb{R}$ whose square is the same as the element $2$ in the set of real numbers. Saying that $2(x-3) = 2x -6$ might say that the polynomial $2(x-3)$ is the same as the polynomial $2x - 6$. It might mean that the function $f:\\mathbb{R} \\to \\mathbb{R}$ given by $f(x) = 2(x-3)$ is the same (as element in a set of functions) as the function $g:\\mathbb{R} \\to \\mathbb{R}$ given by $g(x) = 2x - 6$.\nSaying that $\\frac{x^2}{x} = x$ might again be about an equality of functions. But what it the domain of these functions? both functions would have the same domain, namely $\\mathbb{R}\\setminus\\{0\\}$.\nYou say that \"if $\\implies$ contains within it the implicit notion that $\\require{cancel} \\cancel{\\impliedby}$ ...\" This is simply not true. You can, in fact take the definition of $A\\iff B$ as ($A\\implies B$ and $B\\implies A$). So indeed both of the following is correct\n$$\nx^2 = 1 \\implies x\\in \\{\\pm1\\} \\\\\nx^2 = 1 \\iff x\\in \\{\\pm1\\} \n$$\nBoth are therefore permissible.\nHere is the thing. When you are writing a proof you want to be careful and precise. Getting into the habit of writing $\\iff$ everywhere you can will likely lead you to use it wrongly at some point. Being careful is to prove that $A\\iff B$ by first showing $A\\implies B$ and then $B\\implies A$ even if you could do both at the same time.\nThe symbol $\\equiv$ is often used in different ways. It will often depend on the definition. I think that most sources would not use $\\equiv$ in the place of $=$.\n",
    "tags": [
      "soft-question",
      "proof-writing",
      "notation"
    ],
    "score": 8,
    "answer_score": 8,
    "is_accepted": true,
    "question_id": 2441782,
    "answer_id": 2441889
  },
  {
    "theorem": "Show that $\\gcd(a,b)=\\operatorname{lcm}(a,b)$ if and only if $a=b$.",
    "context": "I know how to prove $a=b$ only if $\\gcd(a,b)=\\operatorname{lcm}(a,b)$, but I don't know how to prove the \"if part\". Can anyone help me?\n",
    "proof": "Hint: $\\gcd(a,b)\\le a,b$ and ${\\rm lcm} (a,b)\\ge a,b$.\n",
    "tags": [
      "elementary-number-theory",
      "proof-writing"
    ],
    "score": 8,
    "answer_score": 8,
    "is_accepted": false,
    "question_id": 502571,
    "answer_id": 502584
  },
  {
    "theorem": "Theorem about two real numbers",
    "context": "My question is:\n$a,b$ are two positive real numbers such that their product is constant,equal to $k$ say. Prove: the sum $a+b$ is minimum if and only if $a = b= \\sqrt k$.\nCan this be solved using $A.M.-\\;G.M.$ inequality? If yes,then I would like to know it that way too.\n",
    "proof": "We have \n$$(a+b)^2=(a-b)^2+4ab=(a-b)^2+4k.$$\nTo minimize $a+b$, we minimize $(a+b)^2$. To do this, we minimize $(a-b)^2$, by setting $a=b$.\n",
    "tags": [
      "algebra-precalculus",
      "proof-writing",
      "a.m.-g.m.-inequality"
    ],
    "score": 8,
    "answer_score": 15,
    "is_accepted": false,
    "question_id": 155366,
    "answer_id": 155369
  },
  {
    "theorem": "How to write mathematical induction?",
    "context": "Reading the literature about mathematical induction, I have learnt that there are between 4 and 3 steps in reasoning and writing the proof. I say between 3 and 4 because actually I see that texts and authors of proofs diverge slightly in their explanation and in the style in which the proof is written. This is my impression, at least.\n(1) In general, we all agree that the first component is the statement we want to prove and we write it. (2) Then, it follows the base case, when we check the formula for n=1, and we see if the statement holds. So far, there is no difference between proof writers. (3) Afterwards, it follows the assumption, when we write k = n and we state that k is a generic number which appartains to N. Can be whatever natural number. (4) Finally, there is the inductive step, when we write n = k+1, and we proceed with the algebraic manipulation.\nI see that some authors concentrate the steps (3) and (4) in an unique step and actually I like it more, as I don't see the need to write one passage exactly the same as the previous one, but with k in place of n! I am asking for your advice as I would like to set once for all my style in writing this kind of proof, grouping steps (3) and (4) in a unique one. However, since that I like to introduce verbally the steps, I am thinking about the right way to introduce the assumption/inductive step. Is it correct the following form, in your opinion? Assume that there exists a general number k+1, with k=n, such that... etc... I am indeed not sure about how to write this sentence, this is why I ask here.\nPlease, let me know if, where and why I am wrong and also you can point me out if the words I choose are the proper ones - for instance, about \"general number\". By the way, I am not English mother tongue speaker. \nPS. I did not use logical symbols only because I don't know how to edit them but, obviously, I would write the sentence in bold using logical connectives.\n",
    "proof": "I will answer this question with four typical examples, all different in nature:\nExample 1:\nProve by induction that $n^3-n+3$ is divisible by $3$ $\\forall \\space n \\in \\mathbb{N^+}$.\nFor proof by induction; these are the $\\color{red}{\\mathrm{three}}$ steps to carry out: \nStep 1: Basis Case: For $n=1 \\implies n^3-n+3= 1^3-1+3=3$ which is divisible by $3$. So statement holds for $n=1$.\nStep 2: Inductive Assumption: Assume statement is true for $n=k$ such that $n^3-n+3=\\color{blue}{(k^3-k+3)}=\\color{blue}{3p} \\tag{1}$ Where $p,k \\in \\mathbb{N^+}$.\nStep 3: Prove Statement holds for $n=k+1$ such that $$n^3-n+3=(k+1)^3-(k+1)+3$$\n$$=k^3+3k^2+3k+1-k-1+3=3k^2+3k+\\color{blue}{(k^3-k+3)}= 3(k^2+k)+\\color{blue}{3p}=\\color{#180}{3}(k^2+k+p)$$ using our inductive assumption $(1)$ the resulting expression clearly is divisible by $3$.  \nHence $n^3-n+3$ is divisible by $3$ $\\forall \\space n \\in \\mathbb{N^+}$ \nQED. \n(QED is an abbreviation of the Latin words \"Quod Erat Demonstrandum\" which loosely translated means \"that which was to be demonstrated\". It is usually placed at the end of a mathematical proof to indicate that the proof is complete). Alternatively, you can use $\\fbox{}$\nExample 2:\nUse trigonometric identities and induction to prove that \n$\\left(\\begin{array}{cc}\n \\cos \\theta & -\\sin \\theta \\\\\n \\sin \\theta & \\cos \\theta\n\\end{array} \\right)^{n} =\n\\left(\\begin{array}{cc}\n \\cos (n \\theta) & -\\sin (n\\theta)\\\\\n \\sin (n \\theta) & \\cos (n \\theta)\n\\end{array} \\right)$ \nAs before, for proof by induction; these are the $\\color{red}{\\mathrm{three}}$ steps to carry out: \nStep 1: Basis Case: For $n=1 \\implies$ LHS \n$=\\left(\\begin{array}{cc}\n \\cos \\theta & -\\sin \\theta \\\\\n \\sin \\theta & \\cos \\theta\n\\end{array} \\right)^{n}$ \n$=\\left(\\begin{array}{cc}\n \\cos \\theta & -\\sin \\theta \\\\\n \\sin \\theta & \\cos \\theta\n\\end{array} \\right)^{1}$\n$=\\left(\\begin{array}{cc}\n \\cos (1 \\theta) & -\\sin (1\\theta)\\\\\n \\sin (1 \\theta) & \\cos (1 \\theta)\n\\end{array} \\right)$\n$=\\left(\\begin{array}{cc}\n \\cos (\\theta) & -\\sin (\\theta)\\\\\n \\sin ( \\theta) & \\cos ( \\theta)\n\\end{array} \\right)=$ RHS. So statement holds for $n=1$.\nStep 2: Inductive Assumption: Assume statement is true for $n=k$ such that\n$\\left(\\begin{array}{cc}\n \\cos \\theta & -\\sin \\theta \\\\\n \\sin \\theta & \\cos \\theta\n\\end{array} \\right)^{n}$\n$=\\left(\\begin{array}{cc}\n \\cos \\theta & -\\sin \\theta \\\\\n \\sin \\theta & \\cos \\theta\n\\end{array} \\right)^{k}$\n$=\\left(\\begin{array}{cc}\n \\cos (k \\theta) & -\\sin (k\\theta)\\\\\n \\sin (k \\theta) & \\cos (k \\theta)\n\\end{array} \\right)\\tag{1}$\nStep 3: Prove Statement holds for $n=k+1$ such that \n$\\left(\\begin{array}{cc}\n \\cos \\theta & -\\sin \\theta \\\\\n \\sin \\theta & \\cos \\theta\n\\end{array} \\right)^{n}$ \n$=\\left(\\begin{array}{cc}\n \\cos \\theta & -\\sin \\theta \\\\\n \\sin \\theta & \\cos \\theta\n\\end{array} \\right)^{k+1}$ \n$=\\left(\\begin{array}{cc}\n \\cos \\theta & -\\sin \\theta \\\\\n \\sin \\theta & \\cos \\theta\n\\end{array} \\right)^{k}$ $\\times \n\\left(\\begin{array}{cc}\n \\cos \\theta & -\\sin \\theta \\\\\n \\sin \\theta & \\cos \\theta\n\\end{array} \\right)^{1}$\n$=\\left(\\begin{array}{cc}\n \\cos (k \\theta) & -\\sin (k\\theta)\\\\\n \\sin (k \\theta) & \\cos (k \\theta)\n\\end{array} \\right)\n\\times \n\\left(\\begin{array}{cc}\n \\cos \\theta & -\\sin \\theta \\\\\n \\sin \\theta & \\cos \\theta\n\\end{array} \\right)$\n[using our inductive assumption $(1)$]\n$=\\left(\\begin{array}{cc}\n \\cos\\theta\\cos (k \\theta)-\\sin\\theta \\sin(k\\theta) & -\\left(\\sin \\theta\\cos (k \\theta)+\\sin (k \\theta)\\cos \\theta\\right)\\\\\n \\cos\\theta\\sin (k \\theta)+\\sin\\theta \\cos(k\\theta) & \\cos\\theta\\cos (k \\theta)-\\sin\\theta \\sin(k\\theta)\n\\end{array} \\right)$\n$=\\color{blue}{\\left(\\begin{array}{cc}\n \\cos (\\theta(k+1)) & -\\sin (\\theta(k+1))\\\\\n \\sin (\\theta(k+1)) & \\cos(\\theta(k+1))\n\\end{array} \\right)}$ \nWhere in the last step I used the fact that $$\\cos(A \\mp B)=\\cos A \\cos B \\pm \\sin A \\sin B$$ and $$\\sin(A \\pm B)=\\sin A \\cos B \\pm \\cos A \\sin B$$ and $\\color{blue}{\\mathrm{this}}$ is the same result if $n=k+1$ is substituted into the RHS of your original disposition. \nHence statement is true for all $n \\in \\mathbb{N}.$\nQED.\nExample 3:\nProve by induction that $3^{(3n+4)} + 7^{(2n+1)}$ is divisible by $11$ for all natural numbers $n$:\nStep 1: Basis Case: for $n=1$: $P(1)= 3^{(3(1)+4)} + 7^{(2(1)+1)} = 2530$, which is divisible by $11$ so statement holds for $n=1$.\nStep 2: Inductive Assumption: Assume statement is true for $n=k$: $P(k) =3^{(3k+4)} + 7^{(2k+1)} = 11a \\implies 3^{3k+4} = \\color{blue}{11a - 7^{2k+1}}$, where $a \\in \\mathbb{N}$\nStep 3: Prove Statement holds for $n=k+1$: \n$P(k+1)= 3^{3k+7} + 7^{2k+3} = 27 \\cdot 3^{3k+4} + 49\\cdot 7^{2k+1}\\tag{1}$\nUsing the inductive assumption shown in $\\color{blue}{\\mathrm{blue}}$, $(1)$ becomes: \n$27(\\color{blue}{11a - 7^{2k+1}})+49\\cdot 7^{2k+1}=27\\cdot 11a -27\\cdot 7^{2k+1}+ 49\\cdot 7^{2k+1}=27\\cdot 11a +2\\cdot 11\\cdot 7^{2k+1}=\\color{red}{11}(27a+2\\cdot 7^{2k+1})$ \nHence $3^{3n+4} + 7^{2n+1}$ is a multiple of $\\color{red}{11} \\space\\forall \\space n\\in \\mathbb{N}$ \nQED.\nExample 4:\nProve by induction that $$\\sum_{i=1}^n i^2 = \\frac{n(n+1)(2n+1)}{6} \\quad \\forall \\space n \\in \\mathbb{N}$$\nStep 1: Basis Case: For $i=1$: $$\\sum^{i=k}_{i=1} i^2=\\frac{1(1+1)(2\\times 1+1)}{6}= \\frac{2\\times 3}{6}=1$$ So statement holds for $i=1$.\nStep 2: Inductive Assumption: Assume statement is true for $i=k$: \n$$\\sum^{i=k}_{i=1} i^2=\\frac{k(k+1)(2k+1)}{6} $$\nStep 3: Prove Statement holds for $i=k+1$. You need to prove that for $i=k+1$: $$\\sum^{i=k+1}_{i=1} i^2=\\color{blue}{\\frac{(k+1)(k+2)(2k+3)}{6}}$$\nTo do this you cannot use: $$\\sum^{i=k}_{i=n} i^2=\\color{red}{\\frac{n(n+1)(2n+1)}{6}} $$ as this is what you are trying to prove.\nSo what you do instead is notice that:\n$$\\sum^{i=k+1}_{i=1} i^2= \\underbrace{\\frac{k(k+1)(2k+1)}{6}}_{\\text{sum of k terms}} + \\underbrace{(k+1)^2}_{\\text{(k+1)th term}}$$\n$$\\sum^{i=k+1}_{i=1} i^2= \\frac{k(k+1)(2k+1)}{6}+\\frac{6(k+1)^2}{6}$$\n$$\\sum^{i=k+1}_{i=1} i^2= \\frac{(k+1)\\left(k(2k+1)+6(k+1)\\right)}{6}$$\n$$\\sum^{i=k+1}_{i=1} i^2= \\frac{(k+1)(2k^2+\\color{green}{7k}+6)}{6}=\\frac{(k+1)(2k^2+\\color{green}{4k+3k}+6)}{6}=\\frac{(k+1)\\left(2k(k+2)+3(k+2)\\right)}{6}=\\color{blue}{\\frac{(k+1)(k+2)(2k+3)}{6}}\\quad \\forall \\space k,n \\in \\mathbb{N} \\quad\\fbox{}$$\nWhich is the relation we set out to prove. So the method is to substitute $i=k+1$ into the formula you are trying to prove and then use the inductive assumption to recover the $\\color{blue}{\\mathrm{blue}}$ equation at the end. \nObserve that in the part marked $\\color{green}{\\mathrm{green}}$ $7k$ has simply been rewritten as $4k+3k$. From then on you simply take out common factors. \nNote that this method is only valid when you have two numbers whose product is $12$ and sum is $7$. \nOr, put in another way for the general quadratic $ax^2 +bx +c$, this inspection method is only valid iff you can find two numbers whose product is $ac$ and sum is $b$.\n",
    "tags": [
      "algebra-precalculus",
      "proof-writing",
      "induction"
    ],
    "score": 8,
    "answer_score": 11,
    "is_accepted": true,
    "question_id": 1498974,
    "answer_id": 1498978
  },
  {
    "theorem": "Sum of Rows of Vandermonde Matrix",
    "context": "Question \nConsider some Vandermonde matrix defined by\n$$\nV = \\begin{bmatrix}\n1 & x_1 &x_1^2& ... & x_1^{M-1}\\\\\n1 & x_2 &x_2^2& ... & x_2^{M-1}\\\\\n\\vdots & \\vdots & \\vdots && \\vdots\\\\\n1 & x_M &x_M^2& ... & x_M^{M-1}\\\\\n\\end{bmatrix}\n$$\nfor some $\\{x_i :i = 1, 2, ..., M \\land x_i \\in \\mathbb{R}\\}$.\nNumerically, I have observed that the sum of the rows of the inverse may have an interesting property:\n$$\n\\sum_k (V^{-1})_{i,k} = \\delta_{i,0}.\n$$\nThat is, this sum is equal to one for the first row of $V^{-1}$ and zero for all others.\nIs there any way for me to demonstrate this property?\nAttempts\n\nI began with elementwise analytical expressions for the inverse of the Vandermonde matrix but was unable to simplify my expressions.\nInspired by this question, I used expressions for the sums of a truncated geometric series and obtained the expression:\n$$\n\\sum_k V^{-1}_{i,k}\\left(\\frac{1 - x_k^M}{1-x_k} \\right) = 1\n$$\nBut am unable to see a way forward.\nFinally, I have found one proof which is necessary for $\\sum_k (V^{-1})_{i,k} = \\delta_{i,0}$ but certainly not sufficient.\n\n",
    "proof": "By direct calculation, one has\n$$ \\begin{pmatrix} 1 \\\\ 1  \\\\ \\vdots \\\\ 1\\end{pmatrix} = V \\times \\begin{pmatrix} 1 \\\\ 0  \\\\ \\vdots \\\\ 0\\end{pmatrix}$$\nSo multiplying on the left by $V^{-1}$, one gets\n$$  V^{-1} \\times \\begin{pmatrix} 1 \\\\ 1  \\\\ \\vdots \\\\ 1\\end{pmatrix} =\\begin{pmatrix} 1 \\\\ 0  \\\\ \\vdots \\\\ 0\\end{pmatrix}$$\nThis means exactly that the sum of the coefficients of the first row of $V^{-1}$ is $1$, and the sums of the other rows are $0$.\n",
    "tags": [
      "linear-algebra",
      "matrices",
      "summation",
      "proof-writing"
    ],
    "score": 8,
    "answer_score": 19,
    "is_accepted": true,
    "question_id": 4801894,
    "answer_id": 4801901
  },
  {
    "theorem": "Prove that $\\frac{k^7}{7}+\\frac{k^5}{5}+\\frac{2k^3}{3}-\\frac{k}{105}$ is an integer.",
    "context": "Prove that $$\\frac{k^7}{7}+\\frac{k^5}{5}+\\frac{2k^3}{3}-\\frac{k}{105}$$ is an integer using mathematical induction.\nI tried using mathematical induction but using binomial formula also it becomes little bit complicated.\nPlease show me your proof.\nSorry if this question was already asked. Actually i did not found it. In that case only sharing the link will be enough.\n",
    "proof": "@I like Serena has a great answer but since the OP asked for a proof by induction, I'll show what that would look like. Define\n$$f(k)=\\frac{k^7}{7}+\\frac{k^5}{5}+\\frac{2k^3}{3}-\\frac{k}{105}=\\frac{15k^7 + 21k^5+70k^3-k}{105}$$\nFor our base case, let $k=1$. Then we have\n$$f(1)=\\frac{15+21+70-1}{105}=1$$\nwhich is an integer. Now suppose $f(k)$ is an integer for some $k\\geq 1$. We want to prove that $f(k+1)$ is also an integer. To that end, observe that\n\\begin{align}\nf(k+1)&=\\frac{15(k+1)^7 + 21(k+1)^5+70(k+1)^3-(k+1)}{105}\\\\\n&=\\frac{15k^7 + 105k^6+336k^5+630k^4 + 805k^3+735k^2+419k+105}{105}\n\\end{align}\nTherefore \n\\begin{align}\nf(k+1)-f(k)&=\\frac{105k^6+315k^5+630k^4+735k^3+735k^2+420k+105}{105}\\\\\n&=\\frac{105(k^6+3k^5+6k^4+7k^3+7k^2+4k+1)}{105}\\\\\n&= k^6+3k^5+6k^4+7k^3+7k^2+4k+1\n\\end{align}\nWhich is an integer, say $N$. Rearranging this gives $f(k+1)=f(k)+N$ and since $f(k)$ is assumed to be an integer from the induction hypothesis, $f(k+1)$ is the sum of two integers, hence an integer.\n",
    "tags": [
      "algebra-precalculus",
      "proof-writing",
      "induction"
    ],
    "score": 8,
    "answer_score": 5,
    "is_accepted": true,
    "question_id": 3075979,
    "answer_id": 3076032
  },
  {
    "theorem": "True for all $n$ implies true as $n$ tends to $\\infty$?",
    "context": "I'm doing some exercises and came across one that has two parts, as follows:\nGiven a transition matrix for a Markov Chain, $\\mathbf{P}$, and a vector $\\mathbf{f}$, $\\mathbf{f}$ is harmonic if\n$$ \\mathbf{f} = \\mathbf{P}\\mathbf{f}$$\n$(a)$ Show that if $\\mathbf{f}$ is harmonic, then\n$$ \\mathbf{f}=\\mathbf{P}^n\\mathbf{f} $$\nfor all $n$\n$(b)$ Using $(a)$, show that if $\\mathbf{f}$ is harmonic,\n$$ \\mathbf{f} = \\mathbf{P}^\\infty \\mathbf{f} $$\nAm I incorrect in assuming that if $(a)$ holds, then $(b)$ holds by necessity? Are there any cases where proving that something holds for all $n$ does not prove that it holds as $n$ tends to infinity?\n",
    "proof": "Simple counterexample\n\n$$\\frac{1}{n}>0\\text{ for all }n\\in\\Bbb N$$\n\n",
    "tags": [
      "proof-writing"
    ],
    "score": 8,
    "answer_score": 13,
    "is_accepted": true,
    "question_id": 192048,
    "answer_id": 192050
  },
  {
    "theorem": "How does proof by contradiction work?",
    "context": "I just had a thought from looking at this answer in this thread: For every $\\epsilon >0$ , if $a+\\epsilon >b$ , can we conclude $a>b$? \nI understand Brian's proof, but I just thought of something. If you prove a hypothesis is untrue, then the logical negation of that hypothesis must be true via proof by contradiction. But why? Can't it be the case that if a hypothesis is false, the negation is also false?\n",
    "proof": "If a hypothesis is false, then the negation of the hypothesis is true. The negation of any false statement is always true, and the negation of any true statement is always false.\nIt's because a statement can only ever be true or false, there's nothing in between.\n\nThe idea behind proof of contradiction is that you basically prove that a hypothesis \"cannot be untrue\". I.e., you prove that if the hypothesis is false, then $1=0$. You then conclude that it is therefore not true that the hypothesis is false, and in standard logic, that means the hypothesis is true.\n\nMore strictly, if you have a hypothesis $H$, and we use the label $\\top$ for the trivial true statement and $\\bot$ as the trivial false statement, then in a proof of contradiction, you are proving the statement\n$$\\neg H\\implies \\bot$$\nand you then use the fact that the statement $A\\implies B$ is equivalent to $\\neg B\\implies \\neg A$. Using this on your statement, that means that you have proven the statement\n$$\\neg(\\bot)\\implies \\neg(\\neg H)$$\nwhich is the same as\n$$\\top \\implies H.$$\nNow, you use the fact that if $A\\implies B$ and $A$ are both true, then $B$ is true. So, since $\\top\\implies H$ is true, and $\\top$ is true (by definition), then $H$ must be true.\n",
    "tags": [
      "proof-writing"
    ],
    "score": 8,
    "answer_score": 13,
    "is_accepted": true,
    "question_id": 2263478,
    "answer_id": 2263483
  },
  {
    "theorem": "Prove that if $x$ is odd, then $x^2$ is odd",
    "context": "\nProve that if $x$ is odd, then $x^2$ is odd\n\nSuppose $x$ is odd. Dividing $x^2$ by 2, we get:\n$$\\frac{x^2}{2} = x \\cdot \\frac{x}{2}$$\n$\\frac{x}{2}$ can be rewritten as $\\frac{x}{2} = a + 0.5$ where $a \\in \\mathbb Z$. Now, $x\\cdot\\frac{x}{2}$ can be rewritten as:\n$$x\\cdot\\frac{x}{2} = x(a+0.5) = xa + \\frac{x}{2}$$\n$xa \\in \\mathbb Z$ and $\\frac{x}{2} \\notin \\mathbb Z$, hence $xa + \\frac{x}{2}$ is not a integer. And since $xa + \\frac{x}{2} = \\frac{x^2}{2}$, it follows that $x^2$ is not divisible by two, and thus $x^2$ is odd.\nIs it correct?\n",
    "proof": "Your written proof is correct. FYI, here is another proof technique which, although it's far more than you need in your case as Bill's comment indicates, is somewhat shorter and, perhaps, of some use to you, such as for other more complicated related problems.\nSince $x$ is odd, this means it has no factors of $2$. By the Fundamental theorem of arithmetic, $x^2$ has the same prime factors as $x$, just twice as many of each of them and, thus, also no factor of $2$. As such, it is also odd.\n",
    "tags": [
      "elementary-number-theory",
      "proof-verification",
      "proof-writing"
    ],
    "score": 8,
    "answer_score": 5,
    "is_accepted": true,
    "question_id": 3308841,
    "answer_id": 3308878
  },
  {
    "theorem": "Prove that for any integer $n, n^2+4$ is not divisible by $7$.",
    "context": "The question tells you to use the Division Theorem, here is my attempt:\nEvery integer can be expressed in the form $7q+r$ where $r$ is one of $0,1,2,3,4,5$ or $6$ and $q$ is an integer $\\geq0$.\n$n=7q+r$\n$n^2=(7q+r)^2=49q^2+14rq+r^2$\n$n^2=7(7q^2+2rq)+r^2$\n$n^2+4=7(7q^2+2rq)+r^2+4$\n$7(7q^2+2rq)$ is either divisible by $7$, or it is $0$ (when $q=0$), so it is $r^2+4$ we are concerned with.\nAssume that $r^2+4$ is divisible by 7. Then $r^2+4=7k$ for some integer $k$.\nThis is the original problem we were faced with, except whereas $n$ could be any integer, $r$ is constrained to be one of $0,1,2,3,4,5$ or $6$.\nThrough trial and error, we see that no valid value of $r$ satisfies $r^2+4=7k$ so we have proved our theorem by contradiction.\nI'm pretty sure this is either wrong somewhere or at the very least not the proof that the question intended. Any help would be appreciated.\n",
    "proof": "since we have $$n\\equiv 0,1,2,3,4,5,6\\mod 7$$ we get $$n^2\\equiv 0,1,2,4\\mod 7$$\ntherefore $$n^2+4\\equiv 1,4,5,6\\mod 7$$\n",
    "tags": [
      "elementary-number-theory",
      "proof-writing",
      "divisibility"
    ],
    "score": 8,
    "answer_score": 8,
    "is_accepted": true,
    "question_id": 2422879,
    "answer_id": 2422891
  },
  {
    "theorem": "Looking for Proofs Of Basic Properties Of Real Numbers",
    "context": "I have just begun my study of complex numbers and I learned where imaginary numbers came from and their importance. However there's one thing that I need to clarify and that is the properties of real numbers and their proofs.\n\nClosure Laws\nFor all $a,b \\in \\mathbb{R}$, $a+b$, $a-b$, $ab$, $a/b$ are real numbers. Thus $\\mathbb{R}$ is closed under four fundamental operations.\nCommutative Laws\nFor all $a,b \\in \\mathbb{R}$ $a+b = b+a$ and $ab = ba$.\nAssociative Laws\nFor all $a,b,c \\in \\mathbb{R}$ $a+(b+c) = (a+b)+c$ and $a(bc) = (ab)c$.\nAdditive Identity\nFor all $a \\in \\mathbb{R}$ there exists $0\\in \\mathbb{R}$ such that $a+0 = 0+a = a$.\nAdditive inverse\nFor all $a \\in \\mathbb{R}$ there exists a $b \\in \\mathbb{R}$ such that $a+b = b+a = 0$, the additive identity $b = -a$ is called the additive inverse or the negative of $a$.\n\nand similarly Multiplicative Identity, Multiplicative inverse, Distributive Law, Trichotomy Law, Transitivity of order, Monotone Law of Addition, Monotone law of multiplication.\nI understand that the above laws hold good throughout mathematics. Should these laws be accepted as being true \"on faith\" or are there proofs?\nIf yes, I am curious to know the proofs. As per my understanding no textbook has ever talked about proofs for these.\n",
    "proof": "If some book states them like that, you should NOT take them on faith, NOR believe that they can get proven.  The set of all real numbers is NOT closed under the operation of division, as the above statement of the closure laws indicates, since there does not exist division by 0 on the set of all real numbers.\n",
    "tags": [
      "real-analysis",
      "soft-question",
      "proof-writing"
    ],
    "score": 8,
    "answer_score": 1,
    "is_accepted": true,
    "question_id": 61377,
    "answer_id": 61447
  },
  {
    "theorem": "What can the writer assume in a proof?",
    "context": "When writing a proof, what level of mathematical understanding can I assume my reader has?\nFor example, can I assume they know all odd integers can be represented by $2q+1$? (Right?)\nOr that all even integers can be represented by $2k$?\nWhen do I have the power to draw on a theorem?\nCan I always assume that my previous problem was right and cite it to help me with my current proof?\n",
    "proof": "In mathematical writing, as in all writing, you need to think carefully about who your readers will be (and who you want them to be).  It is rarely possible to write in a way which will be equally satisfactory to every conceivable person.\nIn this case, although you don't say so explicitly, it sounds like you are writing up problem sets for a course, so your intended reader is the grader of the problem set and/or the course instructor (perhaps the same person).  With such a small audience it is feasible to simply ask them about their preferences, and while you probably don't want to do this before you turn in every single problem set, some questions in the beginning will probably make things go smoothly.  In general though, this type of reader is someone who knows the material well -- probably better than you do -- and in most cases broadly knows what you are trying to say even before you say it.  However, they are also looking for gaps and mistakes in your arguments.  So in this case I would start by erring on the side of including more details / supporting reasoning, while understanding that if you do not express any given idea / argument in the best possible way you are more likely to be understood anyway than with a reader who really doesn't know what you're trying to tell her.\nWith regard to your specific questions:\n\ncan I assume they know all odd integers can be represented by $4q+1$[?]\n\nGosh, I hope not, since this is not true: e.g. $3 \\neq 4q + 1$.\n[Added: It seems that the \"$4$\" was just a typo which has since been corrected to $2$.  In this case there may well be something to prove.  It is relatively common to define an integer to be odd if it isn't even, and then one has to justify that an odd number is of the form $2k+1$.  In fact, in an honors course for future math  majors that I am currently teaching this came up in the first week. I defined an integer to be even if it is of the form $2k$ for some integer $k$ and odd if it is of the form $2k+1$, but there was still something to show: every integer is either even or odd and not both.  The \"not both\" is easy, but the first part requires something: a few days later I proved it by mathematical induction, and then later stated it as a special case of the theorem about division with remainder. So no, for my intended audience I did not want to just assume that familiar facts about even and odd numbers are true, although I probably would do so in a course pitched either at a higher or a lower level.]\n\nOr that all even integers can be represented by $2k$?\n\nThis is a standard definition of an even number (in fact, I can't think of any other standard definition at the moment). So this may well have come up before in class or in the course text.  If not, and you are not working with any other definition of even, then you can just say something like \"if $x$ is even -- that is, $x$ is of the form $2k$ for some integer $k$ -- ...\" and move on.  \n\nCan I always assume that my previous problem was right and cite it to help me with my current proof?\n\nI don't see why not.  But if you are unsure, ask your grader/instructor.\n",
    "tags": [
      "proof-writing"
    ],
    "score": 8,
    "answer_score": 16,
    "is_accepted": true,
    "question_id": 62298,
    "answer_id": 62304
  },
  {
    "theorem": "Construct an A4 paper.",
    "context": "The A4 European paper format is designed such that if you cut in halve the longest side you would obtain two papers where the ratio of the longest side of each is the same as the original paper.\nIs it possible to build an A4 paper that satisfies this objective with a ruler and  a pair of scissors?\nI don't understand what is meant by ratio of the longest side of each. Can someone also give me a hint on what to do to prove this mathematically?\n",
    "proof": "As @LordSharktheUnknown notes in his comment, the question is essentially asking you to, given a length $x$, compute a length $\\frac x{\\sqrt2}$. This is because if the long side of the paper is $x$, and the short side $\\frac x{\\sqrt2}$, we have the ratio as $\\sqrt2$, and the ratio of the cut rectangle as $$\\frac{\\frac x{\\sqrt2}}{\\frac x2}=\\sqrt2$$So, we can construct this length by drawing length $x$ on the side of a large piece of paper on the corner, and then folding the corner such that the folded paper just hits the end of the drawn length. After unfolding, the folded line should be length $\\sqrt2 x$. We can easily halve this length to get the desired length for the short side.\n",
    "tags": [
      "discrete-mathematics",
      "proof-writing"
    ],
    "score": 8,
    "answer_score": 6,
    "is_accepted": false,
    "question_id": 3379682,
    "answer_id": 3379700
  },
  {
    "theorem": "$\\epsilon$-$\\delta$ limits of functions question",
    "context": "I'm studying how to write epsilon-delta proofs for limits of sequences, limits of functions, continuity, and differentiability and I'm having trouble with the general methodological procedure used in some of the proofs in the text as opposed to some of the proofs I have come up with. I'm going to post an example question with the proof I came up with and the proof given by the book. If possible, could you comment on how my proof is insufficient to prove the result, because I don't understand why it isn't.\nI understand the definition of the limit of a function to be the following:\n\n$\\lim_{x \\rightarrow a} f(x) = L$ means, $\\forall \\epsilon >0, \\exists \\delta >0; 0 < |x-a|< \\delta \\Rightarrow |f(x) - L|< \\epsilon$, with $x, a, L \\in \\mathbb{R}$.\n\nQuestion:\nGive an $\\epsilon$-$\\delta$ proof that $\\lim_{x \\rightarrow 2} x^3 =8$.\nMy Proof:\n\nLet $\\epsilon > 0$ be arbitrary and for $\\delta = (\\epsilon +8)^\\frac{1}{3} -2$ assume that $ 0 < |x-2| < \\delta$, with $x \\in \\mathbb{R}$. Then, \n  $$|x-2| < (\\epsilon +8)^\\frac{1}{3} -2$$\n  $$x-2 < (\\epsilon + 8)^\\frac{1}{3} -2 $$\n  $$x< (\\epsilon +8)^\\frac{1}{3}$$\n  $$x^3 < \\epsilon +8$$\n  $$x^3 - 8 < \\epsilon$$\n  $$|x^3 - 8| < \\epsilon$$\n  Therefore, for $f(x) = x^3$ and $L = 8$, we have shown $|f(x) -L| < \\epsilon$ and resultantly $\\lim_{n \\rightarrow 2} x^3 = 8$.\n\nBook's Proof:\n\nLet $\\epsilon >0$ be given and choose $\\delta = \\min(1, \\frac{\\epsilon}{19})$. Let $x \\in \\mathbb{R}$ such that $0 < |x-2| < \\delta = \\min(1,\\frac{\\epsilon}{19})$. Since $|x-2|<1$, it follows that $ -1 < x-2 < 1$ and so $1 <x<3$. Thus $|x^2 + 2x +4| < 19$. Because $|x-2| < \\frac{\\epsilon}{19}$, it follows that $|x^3 - 8| = |x-2||x^2 + 2x +4| < 19|x-2|< 19(\\frac{\\epsilon}{19})=\\epsilon$.\n\nAren't we trying to show that given our assumption that $0 < |x-a|< \\delta$ we need to show that $|f(x) - L|< \\epsilon$. That is all that is required right? We pick some $\\delta$ for which this works? I'm able to follow to proof in the book and understand that it is correct, I just don't understand why all of the additional details are necessary.\nThank you for any responses.\n",
    "proof": "I think you have a fairly decent idea, but the implementation is quite seriously flawed. As I pointed out in my comment, one of the mistakes in your work is that you cannot conclude that $|x^3 - 8| \\lt \\varepsilon$ given only $x^3 - 8 \\lt \\varepsilon$. (Exercise: Do you see why this is wrong? The trouble arises when $x$ is smaller than $2$, so that $x^3-8$ is a -- possibly large -- negative number.) \nOne way to remedy the proof is to fork into two cases: $x < 2$ and $x > 2$. That is, we define two different thresholds $\\delta_+$ and $\\delta_-$ that work separately for $x > 2$ and $x < 2$ respectively; then the overall $\\delta$ is defined to be the smaller of the two.  \nLet's now see the above idea in action. Fix an $\\varepsilon > 0$. Define $\\delta_+ = (8 + \\varepsilon)^{1/3} - 2$ and $\\delta_- = 2 - (8 - \\varepsilon)^{1/3}$; also define $\\delta = \\min \\{ \\delta_-, \\delta_+ \\}$.  Note that $\\delta_+, \\delta_-$, and $\\delta$ are all strictly positive; the proof would be incomplete without this observation. Now \n\nwhen $2 < x < (2 + \\delta_+)$, we have $0 < x^3 - 8 < \\varepsilon$; and \nwhen $(2 - \\delta_-) < x < 2$, we have $- \\varepsilon < x^3 - 8 < 0$. \n\nCombining these two statements, we can write that whenever $2 - \\delta_- < x < 2 + \\delta_+$ and $x \\ne 2$, we have $- \\varepsilon < x^3 - 8 < + \\varepsilon$. In particular, for $x \\in (2 - \\delta, 2 + \\delta) \\smallsetminus \\{ 2 \\}$, we have $|x^3 - 8| \\lt \\varepsilon$. We have thus showed that the limit of $f(x)$ as $x \\to 2$ is $8$. $\\qquad \\diamond$ \n\nAlthough the above proof is correct, it is quite unsatisfactory because of many reasons. [This list is admittedly subjective and vague, so I recommend that you do not worry if something is unclear here.] \n\nThis style of argument relies in some sense on the fact that $f$ is monotonic. The monotonicity allowed us to “invert” the $\\varepsilon$-$\\delta$ condition in a straightforward way. (Moreover, it was useful that $f$ had a “nice” inverse.) For many functions, such a simple strategy does not work; so we often resort to establishing “bounds”. The textbook proof gives a good example of the latter approach. \nThe trick of considering the left and right sides separately works only in one dimension, i.e., the real line. It wouldn't work in more general spaces like $\\mathbf R^2$, for instance. Once again, the textbook proof would generalise more easily. \nIn our proof, we were able to find a suitable $\\delta$ without expending too much effort. In contrast, the textbook proof proceeds via a nontrivial estimate. Nevertheless, this effort does not totally go waste because the author manages to find a $\\delta$ having a much simpler form; specifically, it is proportional to $\\varepsilon$. This significance of this point will become more evident once you learn about derivatives, because the derivative of a function at a given point essentially tries to quantify the ratio $\\varepsilon / \\delta$ for small values of $\\delta$. \n\nLet us see the textbook proof now.\n\nThe textbook proof done “backwards”. For any polynomial $f$ and for any real number $a$, the difference $f(x) - f(a)$ is divisible by $x-a$. Therefore we can factor an $x-a$ out, and write $f(x) - f(a)$ as the product of $x-a$ and some other polynomial. Already this suggests that when $x-a$ is “small”, then the difference $f(x) - f(a)$ must also be small. However, to make this intuition precise, we proceed as follows.  \nIn our example, $f(x) = x^3$ and $a=2$, so\n$$\r\nf(x) - 8 = (x-2) \\cdot (x^2 + 2x + 4).\r\n$$\nAs mentioned before, the $(x-2)$ factor is responsible for making the difference $(f(x) - 8)$ go to $0$ as $x \\to 2$. On the other hand, the second factor $x^2 + 2x + 4$ approaches $2^2 + 2 \\cdot 2 + 4 = 12$ as $x \\to 2$. Inspired by this observation, we want to write that for $x$ close to $2$, \n$$\nf(x) - 8 \\approx 12 (x-2). \\tag{$\\dagger$}\n$$\nUnfortunately, as intuitive as it might seem, this statement is neither precise nor correct, because we cannot selectively evaluate just one of the factors at the point $x=2$. Nevertheless this can be fixed because we only care about establishing an upper bound on the second factor when $x$ is close to $2$. \nMore precisely, for all $x \\in (1, 3)$, we have \n$$\r\n|x^2 + 2x + 4| = x^2 + 2x+4 \\leqslant 3^2 + 2 \\cdot 3 + 4 = 19,\r\n$$ \nwhich implies that\n$$\n|x^3 - 8| \\leqslant 19|x-2| \\tag{$\\ddagger$}\n$$\nfor all $x \\in (1, 2)$. Comparing $(\\dagger)$ and $(\\ddagger)$, note that the right hand side slightly worsened from $12 |x-2|$ to $19 |x-2|$, but this is not of much consequence to us for the purposes of calculating the limit. All we want is some bound that goes to $0$, and $(\\ddagger)$ works just fine. \n[[EDIT: There is a close connection to derivatives here. Note that even though $(\\dagger)$ doesn't make precise sense, the expression $12(x-2)$ feels like the “right” approximation to $(f(x)-8)$. In particular, the $19$ in $(\\ddagger)$ is plainly arbitrary; we could have replaced it by any constant bigger than $12$ (for $x$ sufficiently close to $2$). In fact, we can think of $f(x) - 8$ as essentially $12(x-2)$, plus a “lower-order” correction term; derivatives formalise this idea nicely.]] \nFinally, given $\\varepsilon > 0$, we pick our $\\delta$ such that both of the following conditions hold simultaneously:\n\nFirst, for our bound $(\\ddagger)$ to apply, we want our $x$ to lie in the interval $(1, 3)$, which requires $\\delta$ to be smaller than $1$.\n$(\\ddagger)$ gives an upper bound of $19 |x-2|$ on $|f(x) - 8|$, so we want this upper bound to be at most $\\varepsilon$. This forces the constraint $19\\delta \\leqslant \\varepsilon$. \n\nOf course, we could satisfy both these inequalities by picking $\\delta = \\min \\{ 1 , \\frac{\\varepsilon}{19} \\}$, which is exactly the choice made by the author. Now it is a matter of carefully doing the proof  “forwards” to ensure that the whole argument works fine. I leave this as an exercise. \n",
    "tags": [
      "calculus",
      "limits",
      "proof-writing"
    ],
    "score": 8,
    "answer_score": 12,
    "is_accepted": true,
    "question_id": 95521,
    "answer_id": 95528
  },
  {
    "theorem": "How to prove l&#39;Hospital&#39;s rule for $\\infty/\\infty$",
    "context": "I'm having trouble with this l'Hospital's rule wiki page(the proof of l'Hospital's rule):\nhttp://en.wikipedia.org/wiki/LHospital%27s_rule\nWell, in the case where the limit looks like $0/0$, it's quite easy to understand.\nOn the contrary, the other case( $\\infty/\\infty$ case) is puzzling. Notations like 'liminf', 'limsup', etc. confuse me further.(Can't understand)\nIt would be of great help if the proof can be explained step by step.\n",
    "proof": "The case $\\frac{0}{0}$ is an immediate consequence of Cauchy's Mean value Theorem.\n$\\frac{\\infty}{\\infty}$ can also be proven the same way, but it is a little more technical since you have to be careful with the interval where you apply this Theorem. \nLets see if I can remember it:\nProof for $\\frac{\\infty}{\\infty}$\nLet $\\lim_{x \\to c} f(x) =\\lim_{x \\to c} g(x) =+\\infty$ (the other cases can be obtained from this by replaceing $f,g$ by $\\pm f, \\pm g$. \nAssume $\\lim_{x \\to c} \\frac{f'(x)}{g'(x)}=l$ and that $g'$ doesn't vanish near $c$.\nI will prove that $\\lim_{x \\to c^-} \\frac{f(x)}{g(x)}=l$, the other one side limit is identical.\nLet $\\epsilon >0$. Then, there exists a $\\delta>0$ such that \n$$\\left| \\frac{f'(x)}{g'(x)}- l \\right| < \\epsilon $$\nfor all $c- \\delta < x <c$.\nBy Cauchy Mean Value Theorem, for each $c- \\delta < x <c$ there exists some $y_x \\in (c_\\epsilon, x)$ such that\n$$\\frac{f(x)-f(c- \\delta)}{g(x)-g(c-\\delta)} = \\frac{f'(y_x)}{g'(y_x)}$$\nTherefore, for each $\\epsilon >0$, there exists some $\\delta$ such that for all $c-\\delta < x <c$ we have\n$$ \\left| \\frac{f(x)-f(c- \\delta)}{g(x)-g(c-\\delta)}- l \\right| < \\epsilon $$\nNow, use the fact that $\\lim_{x \\to c} f(x) =\\lim_{x \\to c} g(x) =+\\infty$ to prove that\n$$\\lim_{x \\to c} \\left( \\frac{f(x)-f(c- \\delta)}{g(x)-g(c-\\delta)} -\\frac{f(x)}{g(x)} \\right) =0$$\nTherefore, there exists some $\\delta' < \\delta$ so that for all $c- \\delta' < x <c$ we have\n$$  \\left| \\frac{f(x)-f(c- \\delta)}{g(x)-g(c-\\delta)} -\\frac{f(x)}{g(x)} \\right| < \\epsilon \\,.$$\nCombining the two inequalities you get for $c- \\delta' < x <c$:\n$$ \\left| \\frac{f(x)}{g(x)} - l \\right| < 2\\epsilon \\,.$$\nAdded: To prove\n$$\\lim_{x \\to c} \\left( \\frac{f(x)-f(c- \\delta)}{g(x)-g(c-\\delta)} -\\frac{f(x)}{g(x)} \\right) =0$$\nNote that \n$$ \\frac{f(x)-f(c- \\delta)}{g(x)-g(c-\\delta)} -\\frac{f(x)}{g(x)} = \\frac{f(x)g(c-\\delta)-g(x)f(c-\\delta)}{g(x)\\left( g(x)-g(c-\\delta)\\right)}\\\\\n=\\frac{f(x)g(c-\\delta)}{g(x)\\left( g(x)-g(c-\\delta)\\right)}-\\frac{f(c-\\delta)}{g(x)-g(c-\\delta)}$$\nIt is clear that the second fraction goes to $0$, the first fraction requires just a bit of effort.\nNote that at this point in the argument, $\\epsilon$ and $\\delta$ are fixed.\nNow, for all $c-\\delta < x <c$ we have\n$$ \\left| \\frac{f(x)-f(c- \\delta)}{g(x)-g(c-\\delta)}- l \\right| < \\epsilon$$\ntherefore, for all $c-\\delta < x <c$ we have\n$$|f(x)| \\leq |f(x)-f(c- \\delta)| +|f(c- \\delta)| < (\\epsilon+ |l|) |g(x)-g(c-\\delta)|+|f(c- \\delta)| \\,.$$\nUsing this inequality, you get immediately that the first fraction also goes to $0$.\n",
    "tags": [
      "calculus",
      "limits",
      "derivatives",
      "proof-writing"
    ],
    "score": 8,
    "answer_score": 14,
    "is_accepted": true,
    "question_id": 793364,
    "answer_id": 793385
  },
  {
    "theorem": "The harmonic sum of coprime integers is not an integer.",
    "context": "As stated, I tried to prove the following:\nThe theorem seems to be very incompletely phrased, since one can obtain non integer sums of the form\n$$\\frac{1}{4} + \\frac{1}{4} + \\frac{1}{4} + \\frac{1}{7} + \\frac{1}{9}$$\nor\n$$\\frac{1}{2} + \\frac{1}{4} + \\frac{1}{4} + \\frac{1}{{18}} + \\frac{1}{{20}}$$\nso further detail is needed. Maybe this should be closed as no longer relevant until I come up with a better phrasing and I consider more initial conditions. The big question would be\n\nGiven the set $S$ of $n$ integers\n$$S=\\{x_1,x_2,\\dots,x_n\\}$$ what are sufficient conditions on $x_1,x_2,\\dots,x_n$ so that $$\\eta = \\sum_{k \\leq n }x_k^{-1}$$ is not an integer?\n\nThough I don't know if this is an important/relevant question to be asking.\n\nTHEOREM If $x_1,\\dots,x_n $ are pairwise coprime, $x_i\\neq 1$, let\n$$\\mu =\\sum_{k=1}^n \\frac 1 x_k $$\nThen $\\mu$ can't be an integer.\n\nPROOF By induction on $n$. Asume the theorem is true for $2, \\dots, n-1$. I'll analize the case $k=n$.\n$(1)$ It is true for $n=2$. If $$(x_1,x_2)=1 \\Rightarrow (x_1 x_2,x_1+x_2)=1$$\nThe proof is simple. We have that $(x_1,x_2)=1$. Let $d \\mid x_1+x_2 , d \\mid x_1x_2$. Then\n$$d\\mid x_1(x_1+x_2)-x_1x_2 \\Rightarrow d\\mid x_1^2$$\n$$d\\mid x_2(x_1+x_2)-x_1x_2 \\Rightarrow d\\mid x_2^2$$\nSo $$d \\mid (x_1^2,x_2^2)=(x_1,x_2)=1  \\Rightarrow d=1$$\nThis means $$\\frac{1}{x_1}+\\frac{1}{x_2}=\\frac{x_1+x_2}{x_1 x_2}=\\phi$$\nis not an integer.\n$(2)$ Let\n$$\\mu = \\frac{1}{x_1}+ \\frac{1}{x_2}+\\cdots+ \\frac{1}{x_{n-1}}+ \\frac{1}{x_n}$$\nThen\n$$x_n \\mu-1 = x_n\\left(\\frac{1}{x_1}+ \\frac{1}{x_2}+\\cdots+ \\frac{1}{x_{n-1}}\\right) =x_n \\omega$$\nBy hypothesis, $(x_1,\\dots,x_{n-1})=1$ so $\\omega$ is not an integer. Thus, if $x_n \\mu-1$ were an integer, it must be the case:\n$$ x_n\\left(\\frac{1}{x_1}+ \\frac{1}{x_2}+\\cdots+ \\frac{1}{x_{n-1}}\\right) =k \\text{ ; } k \\text{ an integer }$$\n$$ x_n \\frac{\\tau}{x_1 x_2 \\cdots x_{n-1}} =k \\text{ ; } k \\text{ an integer }$$\n$\\tau$ is the numerator obtained upon taking a common denominator.\nBut since $\\omega$ is not an integer, then it must be the case\n$$x_1 x_2 \\cdots x_{n-1} \\mid x_n$$\nwhich is imposible. Then $x_n \\mu -1$ is not an integer. But since $x_n$ and $1$ are, this means $\\mu$ isn't an integer, this is,\n$$\\mu =\\sum_{k=1}^n \\frac 1 x_k $$\nis not an integer. $\\blacktriangle$\nNOTE The hypothesis that $x_k \\neq 1$ is necessary to avoid sums like\n$$\\frac{1}{1}+\\overbrace{\\frac{1}{n}+\\cdots +\\frac{1}{n}}^{n }=1+n\\frac{1}{n}=2$$\nhowever, if $(x_1,\\dots,x_n)=1$, the sum\n$$\\nu =\\sum_{k=1}^n \\frac 1 x_k +1 $$\nwill clearly not be an integer.\n",
    "proof": "Suppose that $(a_i,b_i)=1$ for $1\\le i\\le n$, and that $(b_i,b_j)=1$ for $1\\le i< \nj\\le n$. Then the denominator, in lowest terms, of\n$$\n\\sum_{i=1}^n\\frac{a_i}{b_i}\\tag{1}\n$$\nis\n$$\n\\prod_{i=1}^nb_i\\tag{2}\n$$\nThis follows by induction from the case $n=2$, and that is true because\n$$\n(a_1b_2+a_2b_1,b_1b_2)=1\\tag{3}\n$$\nSuppose not, and there is some prime $p$ that divides both $b_1b_2$ and $a_1b_2+a_2b_1$. If $p\\,|\\,b_1$, then $p\\,|\\,a_1b_2$, but since $(a_1,b_1)=1$ and $(b_1,b_2)=1$, $p$ can divide neither $a_1$ nor $b_2$. If $p\\,|\\,b_2$, then $p\\,|\\,a_2b_1$, but since $(a_2,b_2)=1$ and $(b_1,b_2)=1$, $p$ can divide neither $a_2$ nor $b_1$. Therefore, $(3)$ must be true.\nTherefore, since harmonic sums with pairwise coprime denominators fall under this case, no harmonic sum with pairwise coprime denominators (other than the singleton sum $1$) can be an integer.\n",
    "tags": [
      "elementary-number-theory",
      "proof-writing"
    ],
    "score": 8,
    "answer_score": 4,
    "is_accepted": true,
    "question_id": 157939,
    "answer_id": 159250
  },
  {
    "theorem": "What are some strategies to write a proof that can be easily comprehended?",
    "context": "There are already a lot of articles on how to correctly write a proof. In contrast, assuming that I already have a very hard proof written, I am interested in rewriting the proof to make it easy to read, such that a second year undergrad can easily understand.\nI expect the students to follow each step of the proof without difficulty. I am not expecting the students to be able to prove similar theorems by themselves.\nI tried to come-up-with some common strategies:\n\nDivide the theorem proof into a few lemmata, and then divide the proof of a lemma into a few steps. Try to make each step as easy as possible.\n\nUse a lot of plots and examples to help the reader to understand.\n\n\nDo you have any tricks to make your proof easily understood by readers? Any ideas or references will help!\n",
    "proof": "Disclaimer: Writing is a holistic art, not an algorithm. Any answer offering suggestions on how to write is necessarily subjective. The items below are scattered observations and opinions, possibly debatable or provocative. Originally this was to be community-wiki. As the list took shape, I decided it's sufficiently personal that non-CW is preferable.\n\"Writing well\" is a practice, not a destination. Compiling and following a list of \"good habits\" does not of itself result in \"good writing.\" An author must also study existing writing to analyze its effectiveness. Most importantly, an author must write, and rewrite, and candidly critique their own writing.\nThanks: Multiple references at the end are included thanks to Jair Taylor's link to Tao's blog, which itself mentions other references.\n\nOrganization\n\nMathematics is a metaphorical landscape. Statements are destinations, and proofs are trails. Some trails are short and steep, others are longer but paved. (Also, some are long and rocky, others are short and paved!) As an author, you are the reader's guide to the terrain. Enjoy sharing the spectacular views, but remember also to point out the protruding rock where people are prone to trip, the place where four identical-looking trails meet, the slippery spot where the stream crosses the trail. Use extra care when traversing the ledge with no railing.\nHuman readers are not efficient at cognitive task-switching. Separate the steps, ideas, and techniques of a proof to allow the reader to focus on one item at a time. Use sentences and paragraphs to effect this level of organization.\nHuman readers are not machines. We get tired, we forget, we skim when we should scan. Write accordingly. Highlight essential, subtle hypotheses. Emphasize the flow of reasoning. Include a judicious amount of expository error-correction.\nTo the extent possible, organize proof logic to follow the same order as the prose.\nEspecially in a long proof, give the reader short, orienting cues.\n\n\n\"We first establish existence. [Three paragraphs later.] This completes the proof of existence. [Paragraph break.] To prove uniqueness ....\"\n\n\nIf some calculation or logical idiom is used repeatedly in a proof, factor it out as a lemma. A lemma is to a proof what a subroutine is to an algorithm.\nClearly separate examples from abstract reasoning.\n\nStyle\n\nKnow your readership and write accordingly. (Similarly, know your audience and speak accordingly.)\nInvite the reader into a beautiful, compelling story without ingratiating. Develop material in a way that a reader can easily perceive the embodied beauty.\nBe direct. Say what is needed as simply as possible. Prefer shorter sentences to longer sentences.\nPrefer the active voice.\n\n\n\"We will prove every continuous function is integrable.\" (Active and direct.)\n\n\n\"It will be shown in the following theorem that every element of the set of continuous functions is an element of the set of integrable functions.\" (Passive and indirect.)\n\n\nIn the active voice, referring to the author and the reader as \"we\" can create a sense of camaraderie.\nThere is no harm in making the reader's life easier.\nInversely, there is no benefit in making the reader's life harder. Mathematical prose should not be an access barrier a reader must overcome to prove their worthiness.\nWrite parallel constructions serially. Avoid parentheticals and \"respectively.\"\nIntroduce notation before it is used.\n\n\n\"Let $f:M \\to N$ be a local isometry, where $M$ (resp. $N$) is a Riemannian manifold with metric $g$ (resp. $h$) and we assume $g$ is complete (resp. $h$ has non-positive curvature).\"\n\n\n\"Let $(M, g)$ be a complete Riemannian manifold, $(N, h)$ a Riemannian manifold with non-positive curvature, and let $f:(M, g) \\to (N, h)$ be a local isometry.\"\n\n\nBe accurate even when being imprecise. The first example below is morally true ... to someone who already understands the technical details. The second is more literal and therefore less liable to cause confusion.\n\n\n\"A function $f$ is continuous at $x$ if at every nearby point $x'$, the value $f(x')$ is close to $f(x)$.\"\n\n\n\"A function $f$ is continuous at $x$ if we can make $f(x')$ as close to $f(x)$ as we like by taking $x'$ sufficiently close to $x$.\"\n\n\nWrite, wait, re-read, and revise. We all sometimes write sentences that mis-convey intentions, paragraphs that over-complicate, proofs with jumbled structure.\n\nDo not say some piece of mathematics is beautiful and move on. Substantively convey the beauty to the reader, ideally in a compelling manner the reader can directly apprehend so that authorly exhortations of beauty are superfluous.\n\nAvoid writing \"obviously,\" \"clearly,\" \"everyone knows,\" and \"as you should have learned in calculus.\" Each phrase is micro-aggressive, implicitly judging readers for whom the point was not obvious or clear or known. Cumulatively, this excludes and eventually alienates valuable prospective contributors to the subject. (I'm dispelling this ingrained habit only now, after nearly four decades of university/college teaching.)\n\n\n\n\"The function $f$ is obviously integrable because it is continuous.\"\n\n\n\"The function $f$ is integrable because it is continuous.\"\n\n\nAvoid pronouns. If you aren't sure what noun \"it\" refers to, there is an important detail you don't understand.\n\n\n\"If $f$ is continuous and $F(x) = \\int_{a}^{x} f(t)\\, dt$, it is integrable because it is continuous.\"\n\n\nAvoid making wry quips about specific disciplines, people who work in them, and how they conceptualize or what standards are used. Over time these cultivate disdain for \"others.\"\nThat said, a good-natured joke can be illustrative and memorable. Some favorite punch lines:\n\n\nThe mathematician drips water on the fire's edge, says \"A solution exists,\" and goes back to bed.\n\n\nThe mathematician says, \"No, we know one of these hundred sheep is black on one side.\"\n\n\nThe mathematician muses, \"If one person goes into the house, it will be empty again.\"\n\nNotation\n\nChoose notation to enhance clarity and readability, in parallel with concepts.\nChoose notation according to conventions of the field and the reader's expectations.\nLaTeX grants bewildering typographical power. Do not use this power heedlessly.\nIn ordinary prose, write out \"for every,\" \"there exist,\" \"implies,\" \"therefore,\" \"if and only if,\" etc., instead of using symbols or abbreviations. Generally, do not make a goal of being terse.\nRelation symbols are verb phrases, not prepositions. For example:\n$$\n\\begin{array}{cl|cl}\n  = &\\text{equals/is equal to} & \\neq & \\text{is not equal to} \\\\\n  < &\\text{is less than} & > & \\text{is greater than} \\\\\n  \\in & \\text{is an element of} & \\subset & \\text{is a subset of/is contained in}\n\\end{array}\n$$\nRead mathematical prose with the verbs expanded to check that it sounds grammatical.\n\n\nMany people write $\\in$ for in, but strictly \"for all $x \\in A$\" reads \"for all $x$ is an element of $A$.\"\n\n\nIt's common to see, e.g., \"let $x < y$ be real numbers,\" which reads \"let $x$ is less than $y$ be real numbers.\" In a final draft, write out, \"Let $x$ and $y$ be real numbers such that $x < y$....\"\n\n\nUse Latin abbreviations with care:\n$$\n\\begin{array}{cll}\n  \\text{Abbreviation} & \\text{Means} & \\text{Latin} \\\\\n  \\hline\n  \\text{i.e.} & \\text{that is} & \\textit{id est} \\\\\n  \\text{e.g.} & \\text{for example} & \\textit{exempli gratia} \\\\\n  \\text{cf.} & \\text{compare} & \\textit{confer}\n\\end{array}\n$$\nParticularly, confer is one word (not c.f.), and it does not originally mean see.\nIn light of the preceding two points, this is as good a place as any to note the tension between Obeying the Rules of Grammar and Usage, and Letting Language Evolve as it Naturally Does such as by letting common usage (\"$\\in$\" for in, or \"cf.\" for see) guide meaning. On the one hand, \"rules\" establish useful conventions, and mathematical prose has a particularly high need for precision. On the other, language and culture change over time, and what was once clear becomes difficult to read and understand. As already noted, human authors and readers are not machines. Meaning is not conveyed solely by symbols or words or sentences, but by subtle emotional associations and expectations. All authors strike a balance in the moment, and somehow communication happens.\n\nLaTeX Usage\n\nCentralize presentational code, which describes how the document will appear, in the preamble. Use macros\n\nTo code for flexibility.\nTo match code syntax to semantics.\n\n\n\n\n  \\newcommand{\\Number}[1]{\\mathbf{#1}}\n  \\newcommand{\\Reals}{\\Number{R}}\n  \\newcommand{\\Ratio}{\\Number{Q}}\n  \\newcommand{\\Vec}[1]{\\mathbf{#1}} % Same typographical effect, different meaning, e.g.,\n                                    % Let $\\Vec{v}$ be an element of $\\Reals^{n}$.\n\n\nStudy and use the amsmath package.\nDo not hard-code widths to achieve alignment. If the amsmath package cannot easily align some construct, use a macro that measures its contents, e.g.\n\n\n  \\newlength{\\TmpLen}\n  \\newcommand{\\PadTo}[3][c]{%\n    \\settowidth{\\TmpLen}{$#2$}%\n    \\makebox[\\TmpLen][#1]{$#3$}%\n  }\n  % Usage: \\PadTo[l]{a_{k} \\cos(2\\pi kx)}{a_{0}} typesets $a_{0}$\n  % left-aligned in a box as wide as $a_{k} \\cos(2\\pi kx)$\n\n\nA document body should contain no commands that explicitly set the font (except possibly individual instances), and very few that explicitly introduce spacing.\nUse braces around macro arguments, even if (La)TeX does not require them. Your future self, who needs to do regular-expression search-and-replace or otherwise parse source files, will thank you.\nUse line breaks and indentation in a source file to improve human readability.\n\n\nWould you rather find and fix typos in this:\n\n\n\\ba\\lim_{n\\to\\infty}\\left(1+\\frac xn\\right)^n&=\\lim_{h\\to0^+}\\left(1+\nxh\\right)^{1/h} \\\\&=\\lim_{h\\to0^+}\\exp\\left[\\frac1h\\log(1+xh)\\right] \\\\\n\n\nor this:\n\n\n  \\begin{align*}\n  \\lim_{n \\to \\infty} \\left(1 + \\frac{x}{n} \\right)^{n}\n    &= \\lim_{h \\to 0^{+}} (1 + xh)^{1/h} \\\\\n    &= \\lim_{h \\to 0^{+}} \\exp\\left[\\frac{1}{h} \\log(1 + xh)\\right] \\\\\n\nFurther Reading\n\nP. Halmos On Writing Mathematics\nN. J. Higham Handbook of Writing for the Mathematical Sciences, Third Edition\nA. D. Hwang Writing in the Age of LaTeX\nD. E. Knuth, T. Larrabee, P. M. Roberts Mathematical Writing\nN. E. Steenrod, P. R. Halmos, M. M. Schiffer, J. A. Dieudonné How to Write Mathematics\nT. Tao On Writing\n\n",
    "tags": [
      "proof-writing",
      "soft-question"
    ],
    "score": 8,
    "answer_score": 9,
    "is_accepted": true,
    "question_id": 4457318,
    "answer_id": 4457779
  },
  {
    "theorem": "Axiom of Choice needed to &quot;categorify&quot; the cardinals?",
    "context": "I was playing around in $\\mathsf{Set},$ trying to reduce it modulo isomorphisms to make a category $\\mathsf{Card},$ letting the objects of $\\mathsf{Card}$ be the isomorphism classes of $\\mathsf{Set}$ and let the morphisms of $\\mathsf{Card}$ be the isomorphism classes of $\\mathsf{Set}^\\to$ (the arrow category). Unfortunately, I got stuck trying to define composition in the obvious way.\nHere's the approach I was taking. Given $a\\in\\mathsf{Set}$ or $f\\in\\mathsf{Set}^\\to,$ we denote their respective isomorphism classes by $|a|$ and $\\bar f.$ I've shown that each $\\bar f$ uniquely determines a source and a target $|a|$ and $|b|,$ by taking any $f\\in\\bar f,$ and letting $a,b$ the source and target of $f$ (this is independent of our choice of $f$). If we have $\\bar f:|a|\\to|b|$ and $\\bar g:|b|\\to|c|,$ then there exist $a\\in|a|,b\\in|b|,c\\in|c|,f\\in\\bar f,$ and $g\\in\\bar g$ such that $f:a\\to b$ and $g:b\\to c.$ It seems natural to define $\\bar g\\bar f:=\\overline{gf},$ but I'm having trouble showing independence from the choices of $a,b,c,f,$ and $g.$\nI started by taking $f_j:a_j\\to b_j$ and $g_j:b_j\\to c_j$ for $j=1,2,$ and taking isomorphisms $\\langle u_1,v_1\\rangle:f_1\\to f_2$ and $\\langle u_2,v_2\\rangle:g_1\\to g_2.$ So, $f_2u_1=v_1f_1$ and $g_2u_2=v_2g_1.$ Now, if I could find some iso $u:a_1\\to a_2$ such that $\\langle u,u_2\\rangle:f_1\\to f_2,$ then I'd be done. Likewise if I could find an iso $v:c_1\\to c_2$ such that $\\langle v_1,v\\rangle:g_1\\to g_2.$ Now, the latter doesn't seem feasible, since there's no guarantee that $v_2$ should map fibers of $g_1$ bijectively to fibers of $g_2.$ I've not had any success demonstrating the former, either.\n\nIf I use the Axiom of Choice, then I can show that if $f:A\\to B$ and $g:X\\to Y$ are isomorphic objects in $\\mathsf{Set}^\\to$, then for any iso $v:B\\to Y,$ there is an iso $u:A\\to X$ such that $\\langle u,v\\rangle:f\\to g.$ From there, I can finish the proof that the operation is well (and uniquely) defined. In fact, this result seems to imply the Axiom of Choice, as well, which makes me suspect that it is necessary. If so, would someone be able to outline a proof or provide a reference?\nIf not, then could someone help me get \"unstuck\"?\n\nNow, if we stick to isomorphism classes of injective functions, we can categorify the cardinals as a partial order (a well-order iff Choice holds), but I'd like to include more isomorphism classes than that, if possible.\n\nAdded: As Eric points out, my difficulties are only to be expected. It would seem, then, that my desires might be fruitless, and that only isomorphism classes of injective functions allow such composition to be well-defined. Am I correct?\n",
    "proof": "The construction you're looking for is called taking a skeleton of a category. In general, doing this requires picking a representative of each isomorphism class of object, and this requires even more than the axiom of choice for a large category like $\\text{Set}$: it requires the axiom of global choice. (In particular there is no way of producing skeletons by simply quotienting the set of objects by isomorphism, because as Eric Wofsey points out in the comments, there's no way to make this compatible with composition of morphisms.) \nBut in some sense all of this is against the spirit of categorification. You should regard $\\text{Set}$ itself as already being a natural categorification of the cardinals. \n",
    "tags": [
      "category-theory",
      "proof-writing",
      "axiom-of-choice"
    ],
    "score": 8,
    "answer_score": 8,
    "is_accepted": true,
    "question_id": 1583722,
    "answer_id": 1583763
  },
  {
    "theorem": "How do I know when to use &quot;let&quot; and &quot;suppose&quot; in a proof?",
    "context": "When the goal is $∀n\\in\\Bbb N ∀m\\in\\Bbb N (n \\ge m \\rightarrow H_n-H_m \\ge {n-m \\over n})$, I can begin the proof with \"Let n and m be arbitrary. Suppose n and m are natural numbers\" or \"Let n and m be arbitrary natural numbers.\"\nThe boundary between \"let\" and \"suppose\" feels blurry. When do I use \"let\" and \"suppose\" in a math proof?\n",
    "proof": "\"Let $n$ and $m$ be arbitrary natural numbers\" assigns a meaning to $n$ and $m$ whereas \"Suppose $n$ and $m$ are natural numbers\" makes an assumption on the meaning of $n$ and $m$.  In terms of proof writing, the difference between the two is fairly arbitrary since if one makes a supposition on an as of yet defined variable it's assumed to be definitive.  The biggest distinction between the two is that \"suppose\" does not necessarily assume the concept exists, as in the case of proof by contradiction.\n",
    "tags": [
      "proof-writing"
    ],
    "score": 8,
    "answer_score": 8,
    "is_accepted": true,
    "question_id": 1402485,
    "answer_id": 1402498
  },
  {
    "theorem": "Difference between proof and plausible argument.",
    "context": "I wanted to ask about the difference between a proof and a plausible argument. What is difference in proving a statement and providing a plausibility argument for it. The example on which I am basing it is:\n\nWe know that in numerical analysis, the error of composite trapezoidal rule is $\\frac{nh^2}{12}{\\times}f^{\\prime\\prime}(\\epsilon)$.Now this can be obtained by using Taylor's series for every division of the the integral $\\int_a^bf(x)dx$ and adding the individual errors. \n\nBut this is not a proof ( at least I have been told so ). This is ( what they call ) just an argument to give us a formula to work. Then what is a proof?\nIf anyone of you can think of a better description of this problem, then please edit this post accordingly. \n",
    "proof": "Before I begin, note that mathematics is an exercise in thought. Mathematics says nothing on what carbon atoms on another sheet of carbon atoms represent - merely how certain ideas like numbers and functions correlate.\nStrictly speaking, what mathematicians mean by an actual proof is a formal proof. To quote wikipedia:\n\nA formal proof or derivation is a finite sequence of sentences (called well-formed formulas in the case of a formal language) each of which is an axiom or follows from the preceding sentences in the sequence by a rule of inference.\n\nFor example, when working with integers, it can be an axiom that $1\\in\\mathbb N$, and another axiom can be that if $a,b\\in\\mathbb N$ then $a+b\\in\\mathbb N$. From this we can deduce that $1+1\\in\\mathbb N$.\nThe mathematical field of logic has a lot of tools to notate these sequences formally, and also offers various rules of inference (most notably modus ponens), but ultimately no piece of physical paper can be - ahem - proven to be a proof.\nHowever, hardly any real-life proofs are written in this formal system, because it's quite a hassle. For real analysis, this was done in the famous work Principia mathematica, but the formal proof that $1+1=2$ takes two days, a graduate course in set theory and the sacrifice of a goat to understand.\nWhat mathematicians practically mean by \"proof\" is then anything which gives a clear hint as to how to write a formal proof. This means that the \"usual steps\" of simplifications and expansions are abbreviated as one step or skipped altogether. Indeed, there is no fine line between a proof and a proof sketch, because every proof is effectively just a sketch, strictly speaking.\nThere is no way to distinguish a proof sketch from an invalid argument. Sometimes the best thing you can do is convince the author of the proof sketch that he is incorrect, and mathematicians do that by asking about the details of a proof. \"Why can you make this simplification?\" \"Does this always hold?\" Sometimes you know that the theory you're working in, such as real analysis, does not have inconsistencies (ie. everything that is provable is actually true), and in that case you can show a proof is incorrect by giving counterexamples. This is not always as easy as it sounds.\n",
    "tags": [
      "proof-writing"
    ],
    "score": 8,
    "answer_score": 5,
    "is_accepted": true,
    "question_id": 162564,
    "answer_id": 176212
  },
  {
    "theorem": "Prove that if two polynomial functions have equal values over a closed interval, they are equal.",
    "context": "I want to prove that two polynomial functions that are equal over a specific interval, $(a, b) \\in \\mathbb R$ (closed interval with more than one point if that condition is necessary) are equal over $\\mathbb R$. I had an attempt that intuitively makes sense but I want to make the argument more formal.\nThese were my attempts:\nI have tried expanding the general equation of a polynomial as a product of its roots but got stuck.\nI then simplified the question. I examined the case of two linear functions equivalent at a point. I figured if it is equivalent at another point (not necessarily over an interval), it would be trivial to prove that the two functions are the same.\nI moved to extend it to quadratic functions. I figured all we need is three points to pinpoint the expression of the polynomial. Given that the functions are equal over an interval, we have more than enough.\nI figured that, intuitively, this should continue for any two polynomials of arbitrary degree. Therefore, if two polynomials of $n$-degree are equal over $n + 1$ points on the plane, then the two polynomials are the same. However, something feels very wishy-washy with this proof. How can I make this proof more formal and are there any alternate proofs?\n",
    "proof": "If the closed interval comprises a single point, then the result is false. Otherwise the closed interval contains infinitely many points and this reduces to the fact that a polynomial of degree $n$ can have at most $n$ distinct roots. This is a standard result that is usually proved using polynomial division. I phrase the proof below using the real numbers for definiteness, but it works over any field or any integral domain.\nIf $f(x)$ is a polynomial over $\\Bbb{R}$ of degree $n$ and $f(x_1) = 0$ for some real number $x_0$, then if we divide $f(x)$ by $(x - x_1)$ we get a polynomial $g_1(x)$ of degree $n-1$ and a real number $r$ such that:\n$$\nf(x) = (x - x_1) g_1(x) + r\n$$\nwhere, as $f(x_1) = 0$, we must have $r = 0$. I.e., the polynomial $(x - x_1)$ of degree $1$ divides $f(x)$.\nIf $f(x)$ has another root $x_2 \\neq x_1$, then $x_2$ must also be a root of $g(x)$, so $(x - x_2)$ divides $g(x)$. Repeating this process, if $f(x)$ has $n$ distinct roots, $x_1, \\ldots, x_n$, we will get:\n$$\nf(x) = (x - x_1)(x - x_2) \\cdots (x - x_n)g_n(x)\n$$\nwhere $g_n(x)$ has degree zero, i.e., it is a constant. So if $f(x)$ has another root $x_{n+1}$ (differing from each of $x_1, \\ldots, x_n$), we have $(x_{n+1} - x_1)(x_{n+1} - x_2) \\cdots (x_{n+1} - x_n) \\neq 0$ and the only possibility is that $g_n(x) = 0$ for all $x$, implying that $f(x) = 0$ for all $x$.\n",
    "tags": [
      "real-analysis",
      "functions",
      "polynomials",
      "solution-verification",
      "proof-writing"
    ],
    "score": 8,
    "answer_score": 6,
    "is_accepted": true,
    "question_id": 4852894,
    "answer_id": 4852908
  },
  {
    "theorem": "Is my proof correct on how $k$ must be a power of $2$? Are there other proofs?",
    "context": "So I was looking at the Fermat Primes. These are primes of the form $2^k+1$ for a natural number $k$, such that I define by $\\mathbb{N}:=\\big\\{1,2,3,\\ldots\\big\\}$ and $0\\notin \\mathbb{N}$. We denote by $F_1$ the first Fermat Prime; $F_2$ the second Fermat Prime; et cetera up to $F_k$.\nThus far, however, only $5$ Fermat Primes are known. They are the following, respectively: $$3,5,17,257,65537,\\ldots$$ I write $(\\ldots)$ because I assume that there exist more Fermat Primes, but it is a conjecture that the above Fermat primes are the only ones.\n\nI introduce this to you in the event that you do not know of them, because my question very much relates to these Fermat Primes.\n\n\nIf $2^k+1$ is prime, then must it be true that $k$ is strictly a power of $2$?\n\n\nI noticed that, $$\\begin{align}3&=2^{2^0}+1\\\\ 5&=2^{2^1}+1 \\\\ 17&=2^{2^2}+1 \\\\ 257&=2^{2^3}+1 \\\\ 65537&=2^{2^4}+1.\\end{align}$$\n\nAttempt at Proof:\nI proved that $k$ is even, because supposing otherwise implies that $k=2j+1\\,\\exists j\\in\\mathbb{N}$ and $$2^k+1=2^{2j+1}+1=2^{2j+1}-(-1)^{2j+1}\\stackrel{\\centerdot}{:}2-(-1)=3.\\tag*{$\\bigg(\\begin{align}&\\text{$a\\stackrel{\\centerdot}{:}b$ is read as $a$} \\\\ &\\text{is divisible by $b$.}\\end{align}\\bigg)$}$$ Ergo, $2^k+1$ is not prime if $k$ is odd; $k$ must be even. $\\;\\bigcirc$\nNow if $k$ is even, then $k=2j$ and $$2^k+1=2^{2j}+1=4^j+1.$$ If $j$ is odd, then likewise, as similarly demonstrated before, $$4^j+1=4^j-(-1)^j\\stackrel{\\centerdot}{:}4-(-1)=5.$$ Ergo, $l\\nmid k\\,\\exists l$ odd; $k$ must be a power of $2$ as it will only then have no odd divisors. $\\;\\bigcirc$\n\nIs my proof correct? I don't see anything wrong with it, but if it is correct, are there other (better) proofs? Also, is there anything new on Fermat Primes; particularly, are there any other general constraints on $k$? How far has the conjecture been tested for? I checked the first $250$ Fermat Primes and it seems like the conjecture is true.\n(If you want, you can go here, type x=2;x=2x-1;c<=250;x and then wait a minute before the first $250$ Fermat Primes appear. I still have to thank one of the users on the MSE who introduced me to the site accessible via the link.)\nThank you in advance.\n",
    "proof": "A simple proof is based on the factorization of $x^n+1$ when $n$ is odd:\n$$\nx^n+1 = (x+1)(x^{n-1}-x^{n-2}+\\cdots+1)\n$$\nTherefore, if $m=nd$ with $n$ odd, then $x^d+1$ divides $x^m+1=(x^d)^n+1$.\nIn particular, $2^m+1$ is divisible by $2^d+1>1$ and so is not prime.\nThus, if $2^m+1$ is prime, then $m$ has no odd factor and so is a power of $2$.\n",
    "tags": [
      "proof-verification",
      "proof-writing",
      "prime-numbers",
      "conjectures",
      "fermat-numbers"
    ],
    "score": 8,
    "answer_score": 11,
    "is_accepted": true,
    "question_id": 2794208,
    "answer_id": 2794244
  },
  {
    "theorem": "Is &quot;A and B imply C&quot; equivalent to &quot;For all A such that B, C&quot;?",
    "context": "So I mostly study PDE, harmonic analysis, image processing, and so on, but for whatever reason I decided to be a TA for an undergraduate \"introduction to proofs\" course this semester.  I suppose I wanted a challenge.  The experience has been a rewarding one so far, but I've come across a couple things that I've found difficult to explain in a clear and concise manner.  For example, consider the standard definition of uniform continuity:\n\"For all $\\epsilon>0$ there is a $\\delta>0$ such that whenever $x,y\\in S$ satisfy $\\vert x-y\\vert<\\delta$, we have $\\vert f(x)-f(y)\\vert<\\epsilon$\"\nStudents are asked to \"convert this to logical symbols\", and I have seen the following two approaches: \n\"$\\forall \\epsilon>0$ $\\exists\\delta>0$ s.t. $(x,y\\in S \\wedge \\vert x-y\\vert<\\delta)\\Rightarrow\\vert f(x)-f(y)\\vert<\\epsilon$\" \nand what I consider the more standard version: \n\"$\\forall\\epsilon>0$ $\\exists\\delta>0$ s.t. $\\forall x,y\\in S$ s.t. $\\vert x-y\\vert<\\delta$, $\\vert f(x)-f(y)\\vert<\\epsilon$.\"\nSo my question is:  Are they 100% logically equivalent...?  In particular, it seems that \"for all A such that B, C\" is equivalent to \"A and B imply C\".  Is there a way to \"prove\" this equivalence, or is it just a \"functional\" equivalence (i.e. they tell you to do the same thing)?  I've been struggling to explain the two approaches/advocate for one over the other, so bonus points if you have simple examples for the two.\nThanks ahead!\n",
    "proof": "Consider the \"statement\" $(\\exists k\\in \\mathbb{N})(a=2k)$. It doesn't make sense because $a$ isn't quantified (there's no $\\forall$ or $\\exists$ associated with it). You can't tell wether that \"statement\" is true or false because it isn't a statement. The following formula is a statement: $(\\forall a\\in \\mathbb{R})(\\exists k\\in \\mathbb{N})(a=2k)$, you can tell wether it's true or false. \nRegarding your question, $x$ and $y$ aren't quantified. I could very well assume the $x,y$ in $$(\\forall \\epsilon>0) (\\exists\\delta>0) \\bigl((x,y\\in S \\wedge \\vert x-y\\vert<\\delta)\\Rightarrow\\vert f(x)-f(y)\\vert<\\epsilon\\bigr) \\tag 1$$\nare existencial, meaning I could interpret the above pseudo-statement as meaning $$(\\forall \\epsilon>0) (\\exists\\delta>0) (\\exists x,y)\\bigl((x,y\\in S \\wedge \\vert x-y\\vert<\\delta)\\Rightarrow\\vert f(x)-f(y)\\vert<\\epsilon\\bigr) \\tag 2$$\nwhich isn't what you want at all.\nPlease note that what I said above was merely an intuitive approach to the issue at hand. I simply cannot decide that $(1)$ means $(2)$, I cannot decide it means anything because it isn't a statement.\nThe question rises: what is a statement? The answer to that question isn't appropiate to post here, I believe. However @dtldarek has already provided some insight into that.\nI know that in the literature you'll find such \"definitions\" of continuity. Well, they're badly written. I know it's hard to believe, but that's the case.\nIf any of the variables in a formula isn't quantified, then that formula isn't a statement and has no meaning.\nEdit: The following problem is analogous to your question, the difference is the formulae at hand are statements. Hopefully this will be helpful to you:\n$$(\\forall \\epsilon>0) (\\exists\\delta>0) (\\forall x,y)\\bigl((x,y\\in S \\wedge \\vert x-y\\vert<\\delta)\\Rightarrow\\vert f(x)-f(y)\\vert<\\epsilon\\bigr) \\tag i$$\n$$(\\forall \\epsilon>0) (\\exists\\delta>0) (\\forall x,y\\in S)\\bigl((\\vert x-y\\vert<\\delta)\\Rightarrow\\vert f(x)-f(y)\\vert<\\epsilon\\bigr) \\tag {ii})$$\nAre $(i)$ and $(ii)$ equivalent? In fact they are and that equivalence is \"very strong\". It's more than the fact that you can conclude one from the other. The statement $(ii)$ is just short hand notation for the formally correct $(i)$.\n(In the above paragraph I chose to ignore the quantification regarding $\\delta$ and $\\epsilon$ for the sake of simplicity. The quantification regarding $\\delta$ and $\\epsilon$ has to be dealt with in a similar manner).\nAllow me illustrate this with a simpler version of this problem.\nThe statement $(\\forall x\\in \\mathbb{R})(x\\ge 0)$, (which is obviously false, but that's of no interest to us) is just short for the formally correct $(\\forall x)(x\\in \\mathbb{R} \\Longrightarrow x\\ge 0)$. The existential quantifier $\\exists$ is a bit different. Consider the statement $(\\exists n\\in \\mathbb{N})(n=1337)$. This of course, is just short for $(\\exists n)(n\\in \\mathbb{N} \\wedge n=1337)$ and not short for the silly $(\\exists n)(n\\in \\mathbb{N} \\Longrightarrow n=1337)$. Well, it's not silly, just not what one would expect $(\\exists n\\in \\mathbb{N})(n=1337)$ to mean.\n",
    "tags": [
      "logic",
      "proof-writing",
      "education"
    ],
    "score": 8,
    "answer_score": 11,
    "is_accepted": true,
    "question_id": 291682,
    "answer_id": 291710
  },
  {
    "theorem": "How to show that $f(x) = x^2$ is continuous using topological definition?",
    "context": "\nI am trying to show that simple continuous functions satisfy\n  topological definition of continuity\n\nRecall given $(X, \\mathcal{T}), (Y, \\mathcal{J}), f$ is continuous if $f^{-1}(V) \\in \\mathcal{T}, \\forall V \\in \\mathcal{J}$\nThen given $f:\\mathbb{R} \\to \\mathbb{R}$ equipped with the usual topology $\\mathcal{T}$, we wish to show that $f(x) = x^2$ is continuous $\\Leftrightarrow$ show that $f^{-1}(V)$ is open  $\\forall V \\in \\mathcal{T}$\n\nAttempt:\nGiven $f: \\mathbb{R} \\to \\mathbb{R}, x \\mapsto x^2$\nBy $\\epsilon-\\delta$ definition of continuity, we know that $\\forall x \\in \\mathbb{R}, \\forall \\epsilon > 0, \\exists \\delta > 0$, such that $\\forall x_o \\in \\mathbb{R}$ whenever $x \\in \\mathcal{B}_{\\delta}(x_o)  \\implies f(x) \\in \\mathcal{B}_{\\epsilon}(f(x_o))$\nThen given $x_o \\in \\mathbb{R}, \\epsilon >0$, let $V = \\mathcal{B}_{\\epsilon}(f(x_o))$. Then $x \\in f^{-1}(V) = f^{-1}(\\mathcal{B}_{\\epsilon}(f(x_o)))$\nHowever, since $f$ satisfies $\\epsilon-\\delta$ version of continuity, $\\exists \\delta >0$ such that $x \\in \\mathcal{B}_{\\delta}(x_o) \\subseteq f^{-1}(V) = f^{-1}(\\mathcal{B}_{\\epsilon}(f(x_o)))$. This shows $f^{-1}(V)$ is open by definition of open set in $\\mathbb{R}$. \nEnd of proof.\n\ncan anyone check if this is correct? my main concern is that not all $V$ is of the form $\\mathcal{B}_{\\epsilon}(f(x_o))$...\n",
    "proof": "Open intervals $(a,b), a < b$ form a base for the topology of $\\mathbb{R}$.\nWhat is $f^{-1}[(a,b)] = \\left\\{x \\in \\mathbb{R}: x^2 \\in (a,b) \\right\\}$? \nIf $b \\le 0$, then no square is in $(a,b)$ so then $f^{-1}[(a,b)] = \\emptyset$, which is open. So assume $b > 0$. Then $x^2 < b$ iff $x \\in (-\\sqrt{b},\\sqrt{b})$.\nIf $a < 0$, the $a$ does not impose an extra condition, as all $x^2 \\ge 0 > a$ in that case, so \n$f^{-1}[(a,b)] = (-\\sqrt{b},\\sqrt{b})$ if $b > 0, a < 0$, which is an open interval in $\\mathbb{R}$ so open.\nOtherwise we also need $x^2 > a \\ge 0$, so $x < -\\sqrt{a}$ or $x > \\sqrt{a}$ respectively.\nSo then $f^{-1}[(a,b)] = (-\\sqrt{b}, -\\sqrt{a}) \\cup (\\sqrt{a},\\sqrt{b})$ if $b > a \\ge 0$, which is open as the union of two open intervals.\nThis covers all cases, so $f^{-1}[(a,b)]$ is open for all intervals.\nNow if $O$ is open, we can write $O = \\bigcup_{i \\in I} (a_i,b_i)$, for some family of open intervals $(a_i,b_i), i \\in I$, as the intervals form a base. But then\n$$f^{-1}[O] = f^{-1}[\\bigcup_{i \\in I} (a_i,b_i)] = \\bigcup_{i \\in I} f^{-1}[(a_i,b_i)]$$\nby standard properties of $f^{-1}$ and the last set is open as unions of open sets are open, and we have shown that the inverse images of the base sets are open.\n",
    "tags": [
      "general-topology",
      "proof-verification",
      "continuity",
      "proof-writing",
      "epsilon-delta"
    ],
    "score": 8,
    "answer_score": 6,
    "is_accepted": true,
    "question_id": 1811530,
    "answer_id": 1811930
  },
  {
    "theorem": "Proof that the Period of $\\sin(x)$ is $2\\pi$.",
    "context": "As I was walking through campus today, I had an interesting question pop into my head: How can we prove that the period of $\\tan(x)$ is $\\pi$ rather than $2\\pi$? The answer to this was extremely straightforward: We start off with $$\\tan(x) = \\tan(x + T) = {\\tan(x) + \\tan(T) \\over 1 - \\tan(x) \\tan(T)}$$ to give us $$-\\tan^2(x)\\tan(T) = \\tan(T)$$ $$0 = \\tan(T) + \\tan^2(x)\\tan(T)$$ $$0 = \\tan(T)[1 + \\tan^2(x)]$$ $$\\implies \\tan(T) = 0\\;\\;\\;\\;\\;\\text{and}\\;\\;\\;\\;1 + \\tan^2(x) = 0 \\implies \\text{No real solution for any $x\\in\\mathbb{R}$}$$ Which for $\\tan(T) = 0 \\implies {\\sin(T) \\over \\cos(T)} = 0 \\implies \\sin(T) = 0$, we have $T = 0, \\pi  \\implies T = \\pi$ to show that the period of $\\tan(x)$ is $\\pi$ if we desire a nontrivial answer.\nBut I got stuck trying to do the same with $\\sin(x)$. I tried:\n$$\\sin(x) = \\sin(x + T) = \\sin(x)\\cos(T) + \\sin(T)\\cos(x)$$ $$\\implies \\sin(x)[1 - \\cos(T)] = \\sin(T)\\cos(x)$$ $$\\implies \\tan(x) = {\\sin(T) \\over 1 - \\cos(T)}$$\nBut I got stuck here. I'm not sure how to isolate a single trig function in terms of $T$.\nI Googled this proof, but everyone either uses Taylor Expansions, Euler's Formula, or calculus. But I'm looking for an argument I could present to someone with knowledge of trigonometry and no more. Any ideas?\n",
    "proof": "If $T$ is such that for all $x\\in \\mathbb{R} $ we have $\\sin(x+T) =\\sin(x) $, then in particular, setting $x=0$, we have \n$$\\sin T =0$$\nSo $T=k\\pi$ with $k \\in \\mathbb{Z} $. To conclude, we then need to check that $\\sin(x+2\\pi) =\\sin(x)$ using your formula above (and that $\\pi$ is not a period, by plugging $x=-\\pi/2$ for example).\n",
    "tags": [
      "algebra-precalculus",
      "trigonometry",
      "proof-writing"
    ],
    "score": 8,
    "answer_score": 5,
    "is_accepted": true,
    "question_id": 1781733,
    "answer_id": 1781743
  },
  {
    "theorem": "How to select the right modulus to prove that there do not exist integers $a$ and $b$ such that $a^2+b^2=1234567$?",
    "context": "\nExample 19.2.6 Prove that there do not exist integers $a$ and $b$ such that $\\left.a^{2}+b^{2}=1234567 \\text { . [Problems IV, Question } 1\\right]$\nSolution The set of remainders modulo $4, R_{4},$ is $\\{0,1,2,3\\}$ and so working modulo 4 we have four possibilities to consider. From modular arithmetic,\n\n\\begin{array}{l}\na \\equiv 0 \\bmod 4 \\Rightarrow a^{2} \\equiv 0 \\bmod 4 \\\\\na \\equiv 1 \\bmod 4 \\Rightarrow a^{2} \\equiv 1 \\bmod 4 \\\\\na \\equiv 2 \\bmod 4 \\Rightarrow a^{2} \\equiv 4 \\equiv 0 \\bmod 4 \\\\\na \\equiv 3 \\bmod 4 \\Rightarrow a^{2} \\equiv 9 \\equiv 1 \\bmod 4\n\\end{array}\n\nThus for any $a \\in \\mathbb{Z}, a^{2} \\equiv 0$ or 1 mod $4 .$ Therefore given two integers $a$ and $b$, $a^{2}+b^{2} \\equiv 0$ or 1 or 2 mod $4,$ i.e. $a^{2}+b^{2} \\neq 3$ mod 4\nSuppose now for contradiction that $a$ and $b$ are integers such that $a^{2}+b^{2}=1234567 .$ Then, since $1234567 \\equiv 3$ mod $4,$ we have $a^{2}+b^{2} \\equiv$ 3 mod 4 giving the required contradiction. Hence such integers cannot exist.\n\nI understand the solution but I don't know how the author decided to start with modulo 4 instead of something else? What is it about the expression $a^2+b^2=1234567$ that would trigger us to select modulo 4 instead of something else.\nI tried to solve the question in a similar manner using modulo 2 but eventually got stuck. This leads me to believe that this question can only be solved using modulo 4. Is this true?\n",
    "proof": "Considering in mod $4$ is a good 'tool' in such a question just because we have for any integer $a$\n$$a^2\\equiv 0,1\\pmod 4$$\nas the author says. This fact is useful in this case just because\n$$a^2+b^2\\not\\equiv 3\\pmod 4$$\n$$1234567\\equiv 3\\pmod4.$$\nNote that this works just because $1234567\\equiv 3\\pmod 4$. \nP.S. Considering in mod $3,8,16$ is also useful.\nI'll give you an example. In the following question, considering in mod $3$ may be the first choice (try, and you'll see why) :\nQuestion : Find all positive integers $(n,x,y)$ such that\n$$y^2=x^n-x+2.$$\nThe answer is $(n,x,y)=(2m,2,2^m)$ for all positive integers $m$.\nConsidering in mod $3$ works because $y^2\\equiv 0,1\\pmod 3$.\n",
    "tags": [
      "elementary-number-theory",
      "proof-writing",
      "modular-arithmetic"
    ],
    "score": 8,
    "answer_score": 6,
    "is_accepted": true,
    "question_id": 966517,
    "answer_id": 966555
  },
  {
    "theorem": "Proof of the eyeball theorem.",
    "context": "$\\underline{\\text{Introduction}:-}$\nI think the eyeball theorem is not really that popular and it also doesn't have a Wikipedia page. I have first found it on Quora, however I don't currently remember what the question was, so I cannot link it.\nI have also found a question in MO\nSearching the name on google also gives some proofs, but those proofs didn't really satisfied me. So I tried proving it on my own.\n$\\underline{\\text{Eyeball Theorem}:-}$\n$■\\text{Statement}:$\n\n$|GH|=|EF|$\n$■\\text{My Proof}:-$\n\n(I couldn't find a similar picture, so I just drew it quickly with a sketch pen. If you have problem understanding this picture, then please ask me)\nFrom my picture,\n$\\angle AI_2P=\\angle BI_1R=\\angle ACB=\\angle ADB=90°$\nSo,\n$\\begin{cases}\\sin{\\alpha}=\\frac{RI_1}{BR}=\\frac{AC}{AB}\\\\\\sin{\\theta}=\\frac{PI_2}{AP}=\\frac{BD}{AB}\\end{cases}$\n$\\implies\\begin{cases}\\frac{RI_1}{r_1}=\\frac{r_2}{AB}\\\\\\frac{PI_2}{r_2}=\\frac{r_1}{AB}\\end{cases}$\n$\\implies\\begin{cases}AB\\cdot RI_1=r_1r_2\\\\AB\\cdot PI_2=r_1r_2\\end{cases}$\n$\\implies AB\\cdot RI_1=AB\\cdot PI_2$\n$\\implies 2RI_2=2PI_2$\n$\\implies PQ=RS$. $\\blacksquare$\n$《\\bigstar》\\text{Generalization}:-$\nI think that the $2D$ case implies the $N-D$ case.($N\\geq 2$)\nSince, the $N$-cones from the centres of the two $N$-balls will create an $N-1$-ball in the places of intersection and since their radii are the same then their $N-1$ volume would be same too which is our concern here.\nCorrect me if I'm wrong.\n$\\underline{\\text{My Question}:-}$\nI would like to have my proof reviewed and I would also like to have more elegant proofs of this theorem.\n",
    "proof": "Your proof seems to be absolutely correct.\nBut, since you asked for elegancy, I think, it's nicer to not use trignometry in a geometry problem, especially when you can avoid it. Note that the $\\sin \\theta$ and the $\\sin \\alpha$ expressions that you invoked were mainly used to find\n$$\\frac{RI_1}{r_1}=\\frac{r_2}{AB}\\\\ \\frac{PI_2}{r_2}=\\frac{r_1}{AB}$$\nwhich can be very easily and more elegantly derived from noticing the fact that\n$$\\Delta I_2AP\\sim \\Delta DAB\\\\ \\Delta I_1BR\\sim \\Delta CBA$$\nAlso, here's one more proof-\n$\\underline{\\text{My Approach}:-}$\nLet $CB$ and $DA$ intersect at $O$ and let $AQ$ and $BS$ extended intersect at $O^\\prime$. Notice that\n$$\\Delta COA\\sim \\Delta DOB$$\nAlso, because of symmetry,\n$$AB\\perp OO^\\prime$$\nUsing this, we can see that\n$$\\Delta APQ\\sim \\Delta AOO^\\prime\\\\ \\Delta BRS\\sim \\Delta BOO^\\prime$$\nThe rest is trivial and only requires some algebra with the sides of the similar triangles.\n",
    "tags": [
      "geometry",
      "proof-writing",
      "solution-verification",
      "circles",
      "alternative-proof"
    ],
    "score": 8,
    "answer_score": 5,
    "is_accepted": true,
    "question_id": 4208142,
    "answer_id": 4228738
  },
  {
    "theorem": "Proof: A tangent space of the manifold of SPD matrices is the set of symmetric matrices",
    "context": "The set of SPD matrices, $\\mathbb{P}_n := \\{X \\in \\mathbb{R}^{n \\times n} | X=X^T, X \\succ 0 \\}   $, forms a differentiable manifold.\nClaim: The tangent space at a point $A$, $T_A\\mathbb{P}_n$, is the space of symmetric matrices.\nInfo: I have seen this mentioned in a paper (https://www.ncbi.nlm.nih.gov/pubmed/27845666), but (with my limited understanding of differential geometry) can't see how this occurs (although it may actualy be obvious to someone with a more rigorous maths education). I have also checked out the book they referenced when making this claim (http://www.cmat.edu.uy/~lessa/tesis/Positive%20Definite%20Matrices.pdf) in order to understand it. Sadly the section that I think they are referencing (Start of chapter 6) is far too formal for me. For completeness I will write out what is written there, and if it is the proof I am looking for then an explanation of some of the jumps in the steps would answer my question.\n\nThe space $\\mathbb{M}_n$ is a Hilbert space with inner product $ \\langle A,B \\rangle = tr A^*B $ and the associated norm $\\vert \\vert A \\vert \\vert _2 = (tr A^*A)^{\\frac{1}{2}}$\n\nThe set of Hermitian matrices constitutes a real vector space $\\mathbb{H}_n$ in $\\mathbb{M}_n$.\n\nThe subset $\\mathbb{P}_n$ consisting of strictly positive matrices is an open subset in $\\mathbb{H}_n$. Hence it is a differentiable manifold.\n\nThe tangent space to $\\mathbb{P}_n$ at any of its points A is the space $T_A\\mathbb{P}_n = \\{A\\} \\times \\mathbb{H}_n$\n\n\nIf this is indeed the proof that I seek then these are the steps that I don't understand:\n\nI know that open subsets of manifolds are manifolds, and I am also presuming that \"stricly positive\" means positive definite. But how do I know that this manifold is differentiable, ie has differentiable transistion maps?\n\nThis is my main confusion - $\\mathbb{H}_n$ is the set of symmetric matrices, but why is the tangent space at any point defined like this? I understand the tangent space to be the collection of velocity vectors through that point. Why can I represent the gradients of all the geodesics that pass through a point like this?\n\n\nEDIT - UPDATE:\nHaving been browsing the stack I also came across tangent space clarification  where someone is asking why tangent spaces aren't just defined in the same way (ish) as they were in 4) - does anyone know where this definition/way of writing has come from?\n",
    "proof": "3) Any open subset of a smooth manifold is again a smooth manifold (all the charts are simply restrictions of smooth charts to open subsets). \n4) The tangent space $T_a V$ to a vector space $V$ (in this case $V=\\mathbb{H}(n)$) can be identified with the vector space itself (via the isomorphism which takes an element $v\\in V$ to the directional derivative $D_v\\vert_a$).\nMoreover, the tangent space to an open subset of a manifold is isomorphic to the tangent space of the manifold itself. Hence in our case we have \n $$ T_A\\mathbb{P}(n) \\cong T_A \\mathbb{H}(n) \\cong \\mathbb{H}(n)\\,.$$\nFor reference I suggest you read the beginning of the chapter \"Tangent vectors\" in the book \"Introduction to Smooth Manifolds\" by John M. Lee. \n",
    "tags": [
      "differential-geometry",
      "proof-writing",
      "manifolds",
      "proof-explanation",
      "positive-definite"
    ],
    "score": 8,
    "answer_score": 8,
    "is_accepted": false,
    "question_id": 3037846,
    "answer_id": 3560825
  },
  {
    "theorem": "Prove that stabilizer subgroups of G are conjugate to each other",
    "context": "\nSuppose that a group $G$ acts on a set $X$. Show that if $x_1$ and $x_2$ in X are in\n  the same $G$-orbit, then their stabilizer subgroups of $G$ are conjugate\n  to each other.\n\nMy proof:\nAssume $x_1 = g_1x$ and $x_2 = g_2 x$ for some $g_1, g_2 \\in G$. Let $h \\in G_{x_1}$. We claim that $g_2g_1^{-1}hg_1g_2^{-1}$ is in $G_{x_2}$, thus proving that the two stabilizer subgroups are conjugate to each other.\nIndeed, \n$$\\begin{align}\nx_1&=g_1x\\\\\ng_2g_1^{-1}x_1&=g_2x\\\\\ng_2g_1^{-1}hx_1&=g_2x\\\\\ng_2g_1^{-1}hg_1x&=g_2x\\\\\n(g_2g_1^{-1}hg_1g_2^{-1})x_2&=x_2\\\\\n\\end{align}$$ as desired.\nI think it is a bit messy. Can you please comment on my proof and leave your own proof so that I can learn in a better way? Thanks in advance.\n",
    "proof": "Suppose $x$ and $y$ are in the same orbit, so $x=gy$ for some $g$. If $h$ stabilizes $y$, then $ghg^{-1}x=ghy=gy=x$. This gives an injective homomorphism from $Stab(y)$ to $Stab(x)$. You can verify that $h\\mapsto g^{-1}hg$ is its inverse, so the two groups are conjugate.\n",
    "tags": [
      "abstract-algebra",
      "group-theory",
      "proof-verification",
      "proof-writing"
    ],
    "score": 8,
    "answer_score": 8,
    "is_accepted": true,
    "question_id": 1015219,
    "answer_id": 1015252
  },
  {
    "theorem": "Proving that $\\displaystyle \\int_0^x \\frac{\\sin t}{t+1}dt &gt; 0$ for all $x &gt;0$",
    "context": "I devised this proof that\n$$\\tag{1} \\int_0^x \\frac{\\sin t}{t+1}dt > 0 \\text{ ; } \\forall x >0$$\nThe idea is to prove that the area from $(0,\\pi)$ is greater than the absolute value of the negative area in $(\\pi, 2\\pi)$, and so on, so that the final area is always positive.\n$f(x) = \\dfrac{\\sin x}{x+1}$ is positive if $\\sin x >0$ and negative if $\\sin x <0$. This is to say\n$$f(x) >0 \\Leftrightarrow x \\in \\bigcup_{k=0}^{\\infty}(2k\\pi,(2k+1)\\pi)$$\n$$f(x) <0 \\Leftrightarrow x \\in \\bigcup_{k=1}^{\\infty}((2k-1)\\pi,2k\\pi)$$\nIf we prove that $$\\tag{2} |f(x)| > |f(x+\\pi)|$$ for every $x$ then we prove $(1)$.\nBut,\n$|f(x)| =\\left| \\dfrac{\\sin x}{x+1} \\right|$\n$|f(x+\\pi)| =\\left| \\dfrac{\\sin (x+\\pi)}{x+\\pi+1} \\right|=\\left| \\dfrac{\\sin x}{x+\\pi+1} \\right|$\nThus $(2)$ is proven, and we then have that in general,\n$$ |f(x+n \\pi)| > |f(x+(n+1) \\pi)|$$, thus\n$$\\int\\limits_{\\left( {2k} \\right)\\pi }^{\\left( {2k + 1} \\right)\\pi } {\\frac{{\\sin t}}{{t + 1}}dt}  + \\int\\limits_{\\left( {2k + 1} \\right)\\pi }^{\\left( {2k + 2} \\right)\\pi } {\\frac{{\\sin t}}{{t + 1}}dt}  > 0$$\nand then\n$$ \\int_0^x \\frac{\\sin t}{t+1}dt > 0 \\text{ ; } \\forall x >0$$\nIs it right? And if it is right - is it understandable?\n",
    "proof": "You're on the right track.. you also have to show that ${\\displaystyle \\int_{2k\\pi}^x {\\sin(t) \\over t + 1}\\,dt > 0}$ for all $2k\\pi < x < 2(k+1)\\pi$, since you have \n$$ \\int_{0}^x {\\sin(t) \\over t + 1}\\,dt = \\sum_{i = 0}^{k-1} \\bigg(\\int_{2i\\pi}^{(2i + 1)\\pi} {\\sin(t) \\over t + 1}\\,dt + \\int_{(2i+1)\\pi}^{(2i + 2)\\pi} {\\sin(t) \\over t + 1}\\,dt\\bigg) + \\int_{2k\\pi}^x {\\sin(t) \\over t + 1}\\,dt  $$\nYou've shown the first sum is positive, but you still have to show the last term is positive too. For that part, I suggest showing that ${\\displaystyle \\int_{2k\\pi}^x {\\sin(t) \\over t + 1}\\,dt}$ increases as $x$ goes from $2k\\pi$ to $(2k + 1)\\pi$, and then decreases as $x$ goes from $(2k + 1)\\pi$ to $(2k + 2)\\pi$. From what you've done already, you know it will not decrease all the way to zero.\n\nAnd now for the \"slick trick\" solution: Note that the derivative of $1 - \\cos(x)$ is $\\sin(x)$, and the derivative of ${\\displaystyle {1 \\over t + 1}}$ is ${\\displaystyle -{1 \\over (t + 1)^2}}$. So integrating by parts you have\n$$\\int_{0}^x {\\sin(t) \\over t + 1}\\,dt  = {1 - \\cos(t) \\over t + 1}\\bigg|_{t = 0}^{t = x} + \r\n\\int_0^x {1 - \\cos(t) \\over (t + 1)^2}\\,dt$$\n$$= {1 - \\cos(x) \\over x + 1} + \\int_0^x {1 - \\cos(t) \\over (t + 1)^2}\\,dt$$\nSince $1 - \\cos(t) \\geq 0$ for all $t$, the first term is nonnegative. Similarly, the integrand of the second term is nonnegative and thus the resulting integral is positive for $x > 0$.\n",
    "tags": [
      "calculus",
      "proof-writing"
    ],
    "score": 8,
    "answer_score": 5,
    "is_accepted": true,
    "question_id": 113672,
    "answer_id": 113683
  },
  {
    "theorem": "How can I make this kind of proof rigorous?",
    "context": "I have transformed a Diophantine equation into the form\n$$\n\\frac{a-b-3}{a-b-1} = \\frac{2(b-1)(a+1)}{a^2+b^2},  \\tag{$\\star$}\n$$\nwhere $a > b \\ge 1$ are integers. I want to prove that $a-b=3$ [thus forcing $b=1$]. It’s easy to show that $1 ≤ a-b ≤ 3$ has only the one desired solution. Now assuming $a-b ≥ 4$, I can say\n\\begin{align}\n  \\frac{a-b-3}{a-b-1} \\simeq 1,\n\\end{align}\nand thus\n\\begin{align}\n  1 &\\simeq \\frac{2(b-1)(a+1)}{a^2+b^2}  \\\\\n  a^2+b^2 &\\simeq 2(b-1)(a+1) \\\\\n       &= 2ab+2(b-a-1)  \\\\\n  \\therefore\\quad (a-b)^2 &\\simeq 2(b-a-1) ≤ 2(-4-1)=-10.\n\\end{align}\nWith a strict equality, this would clearly be impossible; QED. The proof might also be valid if I can qualify precisely what is meant by “$\\simeq$”, such that the contradiction holds.\n\nQUESTION: How can I make this method of proof rigorous?\n\nEDIT: In case it helps, I have proven that\n$$ab = 4(256n^3-640n^2+533n-148) \\qquad\\text{and}\\qquad a-b=16n-13$$\nfor some $n \\ge 1$, and need to prove that $n=1$ is the only solution. Combining those two equations yields\n$$a^2+b^2=(8n-7)(256n^2-384n+145),$$\nwhich has certain implications I might be able to leverage…\n",
    "proof": "Claim: For $C > 2$, and arbitrary constants $D, E, F$, then the inequality\n$$ a^2 + b^ 2 - Cab - Da - Eb - F \\leq 0. $$\nalways has integer solutions.\nProof: We want to find\n$$ (a-b)^2 \\leq (C - 2 ) ab + Da + Eb + F. $$\nSince $ C - 2 > 0$, we can take $ b = a+1, a  \\rightarrow \\infty$.\nThen, the RHS is dominated by $a^2$ hence tends to $ + \\infty$, so is $ \\geq 1$ for some large enough $a$. So, (integer) solutions exist.\nCorollary: With the restriction of $ a - b \\geq N$, since $\\frac{a-b-3}{a-b-1 } \\geq \\frac{N-3}{N-1}$, in OP's solution, we have\n$$a^2 + b^2 - \\frac{2(N-1)}{N-3} (b-1)(a+1) \\leq 0.$$\nThis has integer roots, hence we cannot just use this approach to reach a contradiction.\nSince this proof approach doesn't work, we cannot make it rigorous (in it's current form).\nFor the more general case, to show that $ (a-b)^2 < 0$\n\nIf $ C = 2$, then it greatly depends on the constants (and should be pretty obvious.).\nIf $ |C| < 2 $, then the RHS is eventually negative, so we can restrict to considering \"small\" cases.\n\n\nTo solve the problem, cross multiply and factor to get\n$$ (a-b)^3 - (a+b)^2 - 2 = 0. $$\nThe problem is then equivalent to showing that $ x^3 - y^2 = 2 $ only has one solution, namely $ (3, 5)$.\nSee solution here, which uses ideas of $\\mathbb{Z} [ \\sqrt{-2} ] $.\nIt suggests that you need much more advanced tools than what you just listed out.\n",
    "tags": [
      "elementary-number-theory",
      "inequality",
      "proof-writing",
      "diophantine-equations"
    ],
    "score": 8,
    "answer_score": 7,
    "is_accepted": true,
    "question_id": 4173006,
    "answer_id": 4173024
  },
  {
    "theorem": "Prove that the number of common tangents to two circles exterior to one another is 4",
    "context": "Given two circles outside each other, the maximum number of common tangents to the two circles is 4 according to Wikipedia. How can this be proven?\n\nThat's a picture above showing what I'm talking about. How can we prove that no other common tangents can be drawn aside those four?\n",
    "proof": "Hint Let the equation of the tangent be $a x + b y =  c$ with $a^2+b^2=1$, and let $(x_i, y_i), r_i$ be the centers and radius of the circles. The following system must hold\n\\begin{equation}\n\\begin{array}\\cr\na x_1 + b y_1 - c = \\pm r_1\\cr\na x_2 + b y_2 - c = \\pm r_2\\cr\na^2 + b^2 = 1\\cr\n\\end{array}\n\\end{equation}\nThe two first equations with unknowns $(a, b, c)$ form a linear system with rank 2 if the centers are different, hence for each of the 4 right hand sides, there is a straight line of solutions $(a, b, c)$. Two points at most on this straight line satisfy $a^2 + b^2 = 1$. It gives at most 8 solutions but these solutions come in identical pairs because  $(-a, -b, -c)$ defines the same line as $(a, b, c)$, hence there are at most 4 tangent lines.\nWhere does the system come from ?\nThe equation of a line $L$ in the plane can be written $a x + b y = c$ where $(a, b)\\not = (0,0)$. Multiplying $a, b, c$ by a non zero factor does not change the line defined by this equation, so we can as well multiply by $1/\\sqrt{a^2+b^2}$ to obtain an equation where finally $a^2 + b^2=1$. If $P = (x_i, y_i)$ is any point in the plane, the distance between $P$ and the line $L$ is\n\\begin{equation}\nd(P, L) = |a x_i + b y_i - c|\n\\end{equation}\nIndeed for any $t\\in {\\mathbb R}$, the point $(x_i + t a, y_i + t b)$ belongs to the line perpendicular to $L$ passing through $P$, because the vector $(a, b)$ is orthogonal to $L$. This point belongs to $L$ iff\n\\begin{equation}\na (x_i + t a) + b(y_i+ t b) = c\n\\end{equation}\nwhich implies $t = -(a x_i + b y_i - c)$. The distance from $P$ to the intersection point is then $|t| = |a x_i + b y_i - c|$.\nThus, the two first equations of the system simply say that the distance of the center of each circle to the line $L$ is $r_i$. This property characterizes the tangent lines of the circles.\n",
    "tags": [
      "geometry",
      "proof-writing",
      "analytic-geometry",
      "circles",
      "tangent-line"
    ],
    "score": 8,
    "answer_score": 4,
    "is_accepted": false,
    "question_id": 3942747,
    "answer_id": 3942773
  },
  {
    "theorem": "What is a heuristic proof?",
    "context": "I just wonder what does a heuristic proof approach really means. I keep finding it in books and from teachers. I got to the next analogy: \nA heuristic proof of a mathematical proof is like pseudo-code for an algorithm.\nNot a technical math question but I would like to know. \nThanks in advance.\n",
    "proof": "First, I object to the term \"heuristic proof.\"  That phrase is a contradiction in terms.  Heuristics are not (generally speaking) proof, and proofs generally require more detail and nuance than heuristics.  It would be better to use the phrase \"heuristic argument.\"\nWith that bit of pedantry addressed, it is often (though not always) helpful to look at what lexicographers have determined that word means.  In this case, Merriam-Webster suggests that a heuristic is something\n\ninvolving or serving as an aid to learning, discovery, or problem-solving by experimental and especially trial-and-error methods.\n\nSo heuristics are learning or problem-solving tools.  In mathematics, heuristics are generally informal arguments that are meant to convince someone that a result is true without necessarily getting into the nitty-gritty of a proof, which might be rather involved.  For example, in an elementary calculus class, you will often see an argument for the chain rule that looks something like\n\nThe derivative of $f$ with respect to $g$ is denoted by $\\frac{\\mathrm{d}f}{\\mathrm{d}g}$, and the derivative of $g$ with respect to $x$ is denoted by $\\frac{\\mathrm{d}g}{\\mathrm{d}x}$.  Thus we have\n  $$\\require{cancel} \n\\frac{\\mathrm{d}f}{\\cancel{\\mathrm{d}g}} \\cdot \\frac{\\cancel{\\mathrm{d}g}}{\\mathrm{d}x}\n=\\frac{\\mathrm{d}f}{\\mathrm{d}x}. $$\n  Therefore $\\frac{\\mathrm{d}}{\\mathrm{d}x} (f\\circ g)(x) = f'(g(x)) g'(x)$.\n\nThis argument is not rigorous, and cannot really be made rigorous without a great deal of explanation (either by way of non-standard analysis, or a very careful $\\varepsilon$-$\\delta$ argument).  However, the notation is suggestive, and the basic result is correct, so it is a useful aid to learning.\nAnother example is the heuristic argument for the Prime Number Theorem, which (roughly) states that the number of prime numbers less than $N$ is on the order of $N/\\log(N)$.  An actual proof of the Prime Number Theorem is rather involved, but the following heuristic argument is generally enough to convince someone that it is true:\n\nSuppose that the primes are uniformly randomly distributed among all of the positive integers.  Let $P(N)$ denote the probability that $N$ is prime, where $N$ is some sufficiently large number.  If $N$ is prime and $M$ is larger than $N$, then $N$ divides $M$ with probability $1/N$ (2 divides every other number, 3 divides every third number, 5 divides every fifth number, and so on).  In particular, if is prime, then $N$ divides $N+1$ with probability $1/N$.  By the Law of Total Probability\n  $$ P(N+1) = \\underbrace{P(N)\\left[P(N)\\left( 1- \\frac{1}{N}\\right)\\right]}_{(1)} + \\underbrace{(1-P(N))P(N)}_{(2)}$$\n  where (1) if $N$ is prime, then (assuming independence) $N+1$ is prime with probability $P(N+1) \\approx P(N)$ and also $N \\nmid N+1$, which happens with probability $1-\\frac{1}{N}$, and (2) $N$ is not prime with probability $1-P(N)$, but for $N$ large enough, both $N$ and $N+1$ have about the same chance of being prime.  Thus $P(N+1) \\approx P(N)$.  By some basic algebra\n  $$ \\frac{P(N+1)}{P(N)}\n= P(N) \\left(1-\\frac{1}{N}\\right) + (1-P(N))\n= 1 - \\frac{P(N)}{N}.$$\n  But we can approximate $P(N+1)$ by $P(N) + P'(N)$ (i.e. $P(x+\\Delta x) \\approx P(x) + P'(x)$), so this becomes\n  $$ 1 + \\frac{P'(N)}{P(N)} = 1 - \\frac{P(N)}{N}\n\\implies \\frac{P'(N)}{P(N)} = - \\frac{P(N)}{N}. $$\n  This is a first order PDE with general solution\n  $$ P(N) = \\frac{1}{C + \\log(N)}. $$\n  In other words, the probability that a randomly chosen integer $N$ is prime is approximately $\\frac{1}{\\log(N)}$.  If $N$ is any positive integer, then there are $N$ positive integers less than or equal to $N$, and so\n  $$ (\\text{number of primes up to $N$}) \\approx N \\cdot \\frac{1}{\\log(N)}. $$\n\nThere are a lot of holes in this argument (there are infinitely many positive integers, yet we have assumed that the probability that a given number is prime is uniform, which is a problem; we have not been very careful with the errors in a number of approximations; etc).  In this case, lulu's comment that a heuristic argument is \"an informal argument which assumes plausible (but unproven) results\" is quite apt---there is a long way to go before we have a real proof.\nIndeed, the usual proofs of the Prime Number Theorem are not probabilistic proofs, and attack the problem along other lines.  However, most of the authentic proofs are pretty technical, and don't really give any insight into why we might even suspect that the Prime Number Theorem is true in the first place.  This heuristic argument should convince us that the theorem might be true, and give us reason to pursue a better proof.\n",
    "tags": [
      "proof-writing",
      "proof-explanation",
      "proof-theory"
    ],
    "score": 8,
    "answer_score": 7,
    "is_accepted": true,
    "question_id": 2823024,
    "answer_id": 2823109
  },
  {
    "theorem": "valid proof of series $\\sum \\limits_{v=1}^n v$",
    "context": "$$\\sum \\limits_{v=1}^n v=\\frac{n^2+n}{2}$$\nplease don't downvote if this proof is stupid, it is my first proof, and i am only in grade 5, so i haven't a teacher for any of this 'big sums'\nproof:\nif we look at $\\sum \\limits_{v=1}^3 v=1+2+3,\\sum \\limits_{v=1}^4 v=1+2+3+4,\\sum \\limits_{v=1}^5 v=1+2+3+4+5$\ni learnt rainbow numbers in class three years ago, so i use that knowlege here:\n$n=3,1+3=4$ and $2$.\n$n=4,1+4$ and $2+3$\n$n=5,1+5$ and $2+4$ and $3$\nand more that i have done on paper that i don't wanna type.\nwe can see from this for the odd case that we have $(n+1)$ added together moving in from the outside, so we get to add $(n+1)$ to the total $\\frac{(n-1)}2$ times plus the center number, which is $\\frac{n+1}2$.. giving $\\frac{n-1}2(n+1)+\\frac{n+1}2=\\frac{(n+1)(n-1)}{2}+\\frac{n+1}{2}$ and i can get $\\frac{n^2-1}2+\\frac{n+1}2=\\frac{n^2+n}2$ which is what we want.\nso odd are proven.\nfor even we have a simplier problem: we have $n+1$ on each pair of numbers going in. since we are even numbers, we have $1+n=n+1$ , with $n$ even, $2+(n-1)=n+1$ and we can see this is good for all numbers since we increase one side by one and lower the other by 1. so we get $\\frac{n}2$ times $n+1$ gives $\\frac{n^2+n}{2}$\nthus is proven for all cases. thus is is proven\n",
    "proof": "While most of the proofs that you will see are algebraic, sometimes it is useful to get a geometric view of the problem. I've always preferred getting multiple perspectives to give me deeper understanding of the problem at hand.\nIn the image, there are 5 different views of the problem. The first one has $(n+1)^2 - (n+1)$ cookies arranged in a square, with the diagonal removed. The second one arranges $n^2$ pizza into a square and then cuts the square in half. The third view arranges two sets of cookies into triangles to form a single rectangle. The fourth view we arrange squares into $n$ Ls that fit together to form a rectangle. Lastly, we have $n+1$ computers on a network that connects every computer directly to every other computer.  \nAs an exercise, try cutting the middle row of pizzas in half horizontally, and rearrange the triangle of pizzas into a rectangle.\n\n",
    "tags": [
      "algebra-precalculus",
      "proof-verification",
      "proof-writing",
      "alternative-proof"
    ],
    "score": 8,
    "answer_score": 1,
    "is_accepted": true,
    "question_id": 1037736,
    "answer_id": 1038449
  },
  {
    "theorem": "UPDATED: If $f(x + y) \\leq yf(x) + f(f(x))$ for all real numbers $x$ and $y$, prove that $f(0) = 0.$",
    "context": "$\\large\\text{UPDATED:}$ (with completely correct arguments)\n\nLet $f : \\mathbb R \\to \\mathbb R$ be a real-valued function defined on the set of real numbers that satisfies \n  $$f(x + y) \\leq yf(x) + f(f(x))$$ for all real numbers $x$ and $y$.  Prove that $f(x) = 0$ for all $x ≤ 0$.  (IMO $2011$)\n\nThe purpose of my question is only proof verification. (not knowing the correct solution)\nHere, I focus only on the case of $f(0) = 0.$ Because this is the main part of the problem and this is very easy to show that, $f(0) = 0$ follows $f(x) = 0$ for all $x ≤ 0.$ I want to prove only the $f(0)=0$.\nHere are my steps:\n\nCase $1.$ $f(0)\\in \\mathbb R^+$\nWe have,\n$$f(0)\\leq-xf(x)+f(f(x))$$\n$$f(x)\\leq xf(0)+f(f(0))$$\nApplying $x \\longrightarrow -\\infty$ we get from $f(x)\\leq xf(0)+f(f(0))$, $\\lim_{x\\to -\\infty}f(x) = -\\infty $.  \nThen applying again $x \\longrightarrow -\\infty$, from $f(0)\\leq-xf(x)+f(f(x))$ we get $f(0) \\longrightarrow-\\infty \\not \\in\\mathbb R^+$, which gives a contradiction.\n\nCase $2.$ $f(0)<0$ (with the wrong argument, e.g. $\\lambda=0$)\n$\\require{enclose} \\enclose{horizontalstrike}{ \\text{We have, from}}$ $\\require{enclose} \\enclose{horizontalstrike}{f(x)\\leq xf(0)+f(f(0))}$ $\\require{enclose} \\enclose{horizontalstrike}{\\text{we deduce}}$ $\\require{enclose} \\enclose{horizontalstrike}{\\lim_{x\\to +\\infty}f(x)=-\\infty.}$ $ \\require{enclose} \\enclose{horizontalstrike}{\\text{Suppose that,}}$ $\\require{enclose} \\enclose{horizontalstrike}{ \\lim_{x\\to -\\infty}f(x)=+\\infty.}$ $\\require{enclose} \\enclose{horizontalstrike}{\\text{Applying}}$ $\\require{enclose} \\enclose{horizontalstrike}{x\\to-\\infty}$ $\\require{enclose} \\enclose{horizontalstrike}{\\text{from}}$ $\\require{enclose} \\enclose{horizontalstrike}{f(x-1) ≤ -f(x) + f(f(x))}$ $\\require{enclose} \\enclose{horizontalstrike}{\\text{we have}}$ $\\require{enclose} \\enclose{horizontalstrike}{\\lim_{x\\to -\\infty}f(x+(-1)) \\longrightarrow +\\infty}$. $\\require{enclose} \\enclose{horizontalstrike}{\\text{But,}}$ $\\require{enclose} \\enclose{horizontalstrike}{\\lim_{x\\to -\\infty} (-f(x) + f(f(x)))=-\\infty}$. $\\require{enclose} \\enclose{horizontalstrike}{\\text{According our assumption, we applied}}$ $\\require{enclose} \\enclose{horizontalstrike}{\\lim_{x\\to +\\infty}f(x)=-\\infty.}$ $\\require{enclose} \\enclose{horizontalstrike}{\\text{So, this is a contradiction.}}$ $\\require{enclose} \\enclose{horizontalstrike}{ \\text {Suppose that}}$ , $\\require{enclose} \\enclose{horizontalstrike}{ \\lim \\inf_{x\\to -\\infty}f(x)=a}$ $\\require{enclose} \\enclose{horizontalstrike}{ \\text{and}}$ $\\require{enclose} \\enclose{horizontalstrike}{ \\lim \\sup_{x\\to -\\infty}f(x)=b}$, $\\require{enclose} \\enclose{horizontalstrike}{ \\text{where}}$ $\\require{enclose} \\enclose{horizontalstrike}{ a,b\\in\\mathbb{R}}$ $\\require{enclose} \\enclose{horizontalstrike}{ \\text{and for any}}$ $\\require{enclose} \\enclose{horizontalstrike}{ \\lambda \\in [a,b]}$ $\\require{enclose} \\enclose{horizontalstrike}{ \\text{we have}}$ $\\require{enclose} \\enclose{horizontalstrike}{ \\lambda\\leq y\\lambda+f(\\lambda)}$. $\\require{enclose} \\enclose{horizontalstrike}{ \\text{For any}}$ $\\require{enclose} \\enclose{horizontalstrike}{ \\lambda}$ $\\require{enclose} \\enclose{horizontalstrike}{ \\text{we can always choose a finite}}$ $\\require{enclose} \\enclose{horizontalstrike}{y}$ $\\require{enclose} \\enclose{horizontalstrike}{ \\text{such that, where we get}}$ $\\require{enclose} \\enclose{horizontalstrike}{ \\lambda\\ > y\\lambda+f(\\lambda)}$ $\\require{enclose} \\enclose{horizontalstrike}{ \\text{which gives a contradiction.}}$ $\\require{enclose} \\enclose{horizontalstrike}{ \\text{So, we deduce that}}$ $\\require{enclose} \\enclose{horizontalstrike}{ \\lim_{x\\to -\\infty}f(x)=-\\infty}$. $\\require{enclose} \\enclose{horizontalstrike}{\\text{Then, applying}}$ $\\require{enclose} \\enclose{horizontalstrike}{x\\to-\\infty}$ $\\require{enclose} \\enclose{horizontalstrike}{\\text{from}}$ $\\require{enclose} \\enclose{horizontalstrike}{f(0)\\leq-xf(x)+f(f(x))}$ $\\require{enclose} \\enclose{horizontalstrike}{\\text{we get}}$ $\\require{enclose} \\enclose{horizontalstrike}{f(0)\\longrightarrow -\\infty}$.\n\nCase $2.$ $f(0) \\in \\mathbb {R^-}$ (with the correct argument)\nWe have, from $f(x)\\leq xf(0)+f(f(0))$ we deduce $\\lim_{x\\to +\\infty}f(x)=-\\infty.$ From $f(x + y) \\leq yf(x) + f(f(x))$ we have:\n$\\begin{cases} f(x)\\leq f(f(x)) \\\\ f(x) \\leq xf(0)+ f(f(0)) \\end{cases} \\Longrightarrow f(x)\\leq f(x)f(0)+f(f(0)) \\Longrightarrow f(x)(1-f(0))\\leq f(f(0))$. \nThen applying $x=f(0)$, we get $f(f(0))\\leq 0$, which imply $f(x)\\leq 0$, which gives $f(f(x))\\leq 0$. In this case, we have $f(x)<0.$ Because, if $f(x)=0$, from $f(x)\\leq f(f(x))$, we get $f(0)\\geq 0$, which gives a contradiction. \n  Then, from  $f(x + y) \\leq yf(x) + f(f(x))$  we have:\n$f(z)\\leq(z-x)f(x)+f(f(x)) \\Longrightarrow f(x) \\leq (x-y)f(y)+f(f(y))\\Longrightarrow 0\\leq(f(y)-y)f(y) \\Longrightarrow f(x)(f(x)-x)\\geq 0 \\Longrightarrow f(x) \\leq x $\nApplying $x\\to-\\infty$ from $f(0)\\leq-xf(x)+f(f(x))$, we get $f(0)\\longrightarrow -\\infty \\not \\in \\mathbb{R^-}$, which gives again a contradiction.\nSo, we can deduce that $f(0)=0$.\nQ.E.D.\n\nCan you verify the new solution?\nI just want to make sure that I got $ f (0) = 0 $ correctly.\nThank you!\n",
    "proof": "I will do some comments on your redaction.\nCase 1. $f(0)>0$\n\nWe have,\n$$f(0)\\leq-xf(x)+f(f(x))$$\n$$f(x)\\leq xf(0)+f(f(0))$$\nLet $x\\to -\\infty$ we get from $f(x)\\leq xf(0)+f(f(0)) \\Longrightarrow \\lim_{x\\to -\\infty}f(x)=-\\infty$.\n\nFor a good redaction, don't mix $\\Rightarrow$ with a french sentence.\n\nThen applying $\\lim_{x\\to -\\infty}f(x)=-\\infty$, from $f(0)\\leq-xf(x)+f(f(x))$ we get $f(0) \\longrightarrow-\\infty.$ So, this is a contradiction.\n\nEdit : OK. Precise clearly that the two terms tends to $- \\infty$ to avoid fastidious verifications to the reader.\n\nCase 2. $f(0)<0$\nWe have, from $f(x)\\leq xf(0)+f(f(0))$ we deduce $\\lim_{x\\to +\\infty}f(x)=-\\infty.$ Suppose that, $\\lim_{x\\to -\\infty}f(x)=+\\infty.$ Applying $x\\to-\\infty$ from $f(x-1) ≤ -f(x) + f(f(x))$ we have  $\\lim_{x\\to -\\infty}f(x+(-1)) \\longrightarrow +\\infty$.\n\nYou meant : $\\lim_{x\\to -\\infty}f(x+(-1)) \\rightarrow +\\infty$ according to your assumption.\n\nBut, $\\lim_{x\\to -\\infty} (-f(x) + f(f(x)))=-\\infty$. According our assumption, we applied $\\lim_{x\\to +\\infty}f(x)=-\\infty.$ So, this is a contradiction.\n\nOk since $\\lim_{x\\to -\\infty} f(f(x)) = - \\infty$.\n\nSuppose that, $\\lim \\inf_{x\\to -\\infty}f(x)=a$ and $\\lim \\sup_{x\\to -\\infty}f(x)=b$, where $a,b\\in\\mathbb{R}$\n\nOk. (A priori, $a, b \\in \\mathbb{R} \\cup \\{ - \\infty \\}$ but you deal with this after ) EDIT : to be more precise, $a \\in \\mathbb{R} \\cup \\{ - \\infty \\}$ and $b\\in \\mathbb{R} \\cup \\{ - \\infty, +\\infty \\} $ ; you forgot the case $b = +\\infty$ in your reasoning.\n\nand for any $\\lambda \\in [a,b]$ we have $\\lambda\\leq y\\lambda+f(\\lambda)$.\n\nThis argument is interesting but problematic. I believe you have taken a sequence $x_n \\rightarrow - \\infty$ such that $f(x_n)$ tends to $\\lambda$. This kind of argument is possible only if $f$ is supposed continuous (intuitively its graphe oscillates continuously between a and b). Furthermore, you cannot have a control on $f(x_n +y)$ while doing this (it might be improved by replacing $y$ by $y_n$). Finally, since again f is not supposed continuous, the behaviour of $f(f(x_n))$ might be chaotic and not converge at all to $f(\\lambda)$.\nIf $f$ is supposed continuous, it is possible to make a (rigorous) proof. (X)\n[[ EDIT : I said you needed the continuity for the first step because you have taken \"any $\\lambda \\in [a, b]$\". I think things will be clearer if I present the argument.\nIf you have continuity. You have two sequences $(a_n)$ and $(b_n)$ tending to $-\\infty$ such that $$\\lim_{n\\rightarrow \\infty} f(a_n) = a, \\lim_{n \\rightarrow \\infty} f(b_n) = b$$\nBy the intermediate value theorem, you can find a sequence $(x_n)$ tending to $-\\infty$, such that $f(x_n) \\rightarrow \\lambda$, and also (a little more technical) a sequence $(y_n)$ with $sup (y_n) = +\\infty$, $inf (y_n) = -\\infty$ such that $f(x_n + y_n) \\rightarrow \\lambda$.\nLet us suppose $\\lambda \\neq 0$.\nLooking at the inequality :\n$$f(x_n + y_n) \\leq y_n f(x_n) + f(f(x_n))$$\nYou have a limit for the left term, but the right term cannot be minorated : contradiction.\nRemark : If $\\lambda = 0$ the argument not applies. So you have a problem if $a = b = 0$.\nIf you don't have continuity.\nI recall some properties of the lim inf :\n$$\\lim \\inf_{x \\rightarrow - \\infty} f(x+y) = \\lim \\inf_{x \\rightarrow - \\infty} f(x)$$\n$$ \\lim \\inf_{x \\rightarrow - \\infty} a f(x) =  a \\lim \\inf_{x \\rightarrow - \\infty} f(x) \\text{ if } a \\geq 0 $$\n$$ \\lim \\inf_{x \\rightarrow - \\infty} a f(x) =  a \\lim \\sup_{x \\rightarrow - \\infty} f(x) \\text{ if } a \\leq 0 $$\n$$ \\lim \\inf_{x \\rightarrow - \\infty} f(x) + \\lim \\inf_{x \\rightarrow - \\infty} \ng(x) \\leq \\lim \\inf_{x \\rightarrow - \\infty} f(x) + g(x) \\leq \\lim \\inf_{x \\rightarrow - \\infty} f(x) + \\lim \\sup_{x \\rightarrow - \\infty} g(x)$$\nEach inequality here might be strict.\nTake the lim inf $x\\rightarrow - \\infty$ in the inequality $f(x+y) \\leq y f(x) + f(f(x))$ to get :\n$$a \\leq ay + \\lim \\sup_{x \\rightarrow - \\infty} f(f(x)) \\text{ for } y \\geq 0$$\n$$a \\leq ay + \\lim \\inf_{x \\rightarrow - \\infty} f(f(x)) \\text{ for } y \\leq 0$$\nSo if you suppose $\\lim \\sup_{x \\rightarrow - \\infty} f(f(x)) < +\\infty$ (which implies $\\lim \\inf_{x \\rightarrow - \\infty} f(f(x)) < +\\infty$) you get a contradiction as soon as $a \\neq 0$.\nTake again the lim inf $x\\rightarrow - \\infty$ in the inequality $f(x+y) \\leq y f(x) + f(f(x))$, but use this time $\\lim \\inf u(x) + v(x) \\leq \\lim \\sup u(x) + \\lim \\inf v(x)$ to get :\n$$a \\leq by + \\lim \\inf_{x \\rightarrow - \\infty} f(f(x)) \\text{ for } y \\geq 0$$\n$$a \\leq by + \\lim \\sup_{x \\rightarrow - \\infty} f(f(x)) \\text{ for } y \\leq 0$$\nWith the same hypothesis $\\lim \\sup_{x \\rightarrow - \\infty} f(f(x)) < + \\infty$, you get a contradiction as soon as $b \\neq 0$.\nIt seems you need the assumption $\\lim \\sup_{x \\rightarrow - \\infty} f(f(x)) < + \\infty$ to get something with your argument. ]]\n\nFor any $\\lambda$ we can always choose a finite $y$ such that, where we get $\\lambda\\ > y\\lambda+f(\\lambda)$ which gives a contradiction. So, we deduce that  $\\lim_{x\\to -\\infty}f(x)=-\\infty$.\n\nOk, since the case $a = - \\infty$, $b \\neq - \\infty$ can be covered by the preceding argument (you should have mentionned it).\n\nThen, applying $x\\to-\\infty$ from $f(0)\\leq-xf(x)+f(f(x))$ we get $f(0)\\longrightarrow -\\infty$. But, this contradicts with $f : \\mathbb R → \\mathbb R$.\nSo, we can deduce that $f(0)=0$.\n\nFor (X), you need to suppose $f$ continuous to make a rigorous proof (do it ! ). I must say your redaction looked messy because you did'nt skip lines. There is really little effort to do to improve this.\nUPDATE :\n\nCase 2. $f(0)<0$ (with the correct argument)\nWe have, from $f(x)\\leq xf(0)+f(f(0))$ we deduce $\\lim_{x\\to +\\infty}f(x)=-\\infty.$ From $f(x + y) \\leq yf(x) + f(f(x))$ we have:\n$\\begin{cases} f(x)\\leq f(f(x)) \\\\ f(x) \\leq xf(0)+ f(f(0)) \\end{cases} \\Longrightarrow f(x)\\leq f(x)f(0)+f(f(0)) \\Longrightarrow f(x)(1-f(0))\\leq f(f(0))$.\n\nCorrect.\n\nThen applying $x=f(0)$, we get $f(f(0))\\leq 0$, which imply $f(x)\\leq 0$, which gives $f(f(x))\\leq 0$.\n\nNice.\n\nIn this case, we have $f(x)<0.$\n\nIt you be nice to add quantifiers. I think you mean : for all $x \\in \\mathbb{R}$.\n\nBecause, if $f(x)=0$, from $f(x)\\leq f(f(x))$, we get $f(0)\\geq 0$,\n\nBe more precise : \"if $f(x) = 0$ for some $x \\in \\mathbb{R}$\". Ok for the argument.\n\nwhich gives a contradiction. Applying $x\\to-\\infty$ from $f(0)\\leq-xf(x)+f(f(x))$ we get $f(0)\\longrightarrow -\\infty$. Again a contradiction.\n\nAre you supposing $-xf(x) \\rightarrow - \\infty$ ? It seems not to be necessarily the case (e.g. $f(x) = - \\exp(-x)$) (XX)\n\nSo, we can deduce that $f(0)=0$.\nQ.E.D.\n\nYou have to check (XX).\nUPDATE 2 : (knowing $f < 0$) :\n\nThen, from  $f(x + y) \\leq yf(x) + f(f(x))$  we have:\n$f(z)\\leq(z-x)f(x)+f(f(x)) \\Longrightarrow f(x) \\leq (x-y)f(y)+f(f(y))\\Longrightarrow 0\\leq(f(y)-y)f(y) \\Longrightarrow f(x)(f(x)-x)\\geq 0 \\Longrightarrow f(x) \\leq x $\n\nGreat. This enables to conclude indeed. Good job.\n",
    "tags": [
      "algebra-precalculus",
      "inequality",
      "proof-writing",
      "contest-math",
      "solution-verification"
    ],
    "score": 8,
    "answer_score": 6,
    "is_accepted": true,
    "question_id": 3546435,
    "answer_id": 3549596
  },
  {
    "theorem": "natural language proof assistant",
    "context": "I was wondering whether there has been any attempt to create a proof assistant that you write in it, in english,\nI mean you write your proof the usual way in TeX(maybe use a 'simpler english') then instead of sending it to a journal to have it verified you use the proof assistant to have it verified for you.\nThere are programming languages like inform7 in which you program in english. I think what is needed is a set of macro's to turn the tex into, lets say, something that Coq can verify. Is there any such thing out there?\nDo you think if this happens casual mathematicians will use it? or are there deeper problems that people don't use them now??\n",
    "proof": "The proof-checker CalcCheck takes input via $\\TeX{}$ in the form of formulas and\naccompanying English hints/justifications.\nGiven the input file, the system will output that the proof is valid at all steps or indicate which steps are poorly justified.\nTo the best of my knowledge, it currently recognizes most theorems of first order logic and set theory ---based on the great text ``A Logical Approach to Discrete Math.''\nIf I recall correctly, the back-end is in Haskell.\n\nMain system site is at http://calccheck.mcmaster.ca/.\nManual: http://calccheck.mcmaster.ca/CalcCheckDoc/\n\nOn a final note, this system has been used in first-year logic courses to assist students in proof-writing. It is helpful to have a system check one's proof when in-doubt.\n\nEdit The above was 2013, now as of 2017, it now supports\n\ncreation of logical theories via named modules in the style of the Agda language\nunicode input directly via latex-style bindings\n\"code completion\" for theorem names and definitions\ncoloured and somewhat helpful error messages.\n\nMoreover, the system now no-longer needs to be installed as an application but can be used directly via a browser.\nIt has been successfully used in-place of paper-and-pencil examinations at the university level with over 200 students in the 2017 fall term alone.\nUnfortunately, it is not open source.\n",
    "tags": [
      "proof-writing",
      "automated-theorem-proving",
      "theorem-provers"
    ],
    "score": 8,
    "answer_score": 5,
    "is_accepted": false,
    "question_id": 403163,
    "answer_id": 408340
  },
  {
    "theorem": "Abstract algebra subgroup proof verification",
    "context": "This is my first attempt at a formally written proof so I would appreciate any pointers as far as proof-writing technique or the validity of the actual proof itself. \nNote: I have not formally taken abstract algebra or a proof-writing course so I am sure that I am lacking in many proof-writing aspects, so I would really love a lot of constructive criticism both on the actual proof itself, and the way I wrote the proof. I also go into greater detail than might be appropriate for this type of proof because I am shaky on a lot of the math foundations so I figure that any imperfections in my knowledge will be more easily seen with a more explicit construction of this proof. Thank you all in advance.\n\nLet $G$ be a finite group, and let $S$ be a nonempty subset of $G$. Suppose $S$ is closed with respect to multiplication. Prove that $S$ is a subgroup of $G$. (HINT: It remains to prove that $S$ contains $e$ and is closed with respect to inverses. Let $S$ = {$a_1$ ... $a_n$}. If $a_i$ $∈$ $S$, consider the distinct elements $a_ia_1$, $a_ia_2$, $...$ $a_ia_n$\n\nProof:\nFirst we will define a function $A_1 : S \\rightarrow S$ that maps $s \\mapsto a_1s$. This function is injective because $$a_1y = a_1x$$ $$a^{-1}_1a_1y = a^{-1}_1a_1x$$ $$y = x$$\nThe function is then surjective because $A_1(S) \\subseteq S$ and since $A_1$ is injective, it contains $|S|$ elements. Therefore $A_1$ maps onto every element in $S$ and is therefore surjective as well.\nThis means that $a_1$ is in the image of $A_1$. Therefore $$A_1(a_1) = a_1$$ $$a_1s = a_1$$ $$s = e$$\nSince $S$ is closed under multiplication, $e \\in S$.\nNext, we will define a function $A_2 : S \\rightarrow S$ that maps $s \\mapsto a^2_1s$. This function is also injective $$a^2_1x = a^2_1y$$ $$x = y$$\nIt follows that this function is also surjective since it too is injective and contains |S| elements.\nThis means that $a_1$ is in the image of $A_2$ as well. Therefore $$A_2(z) = a_1$$ $$a^2_1z = a_1$$ $$z = a^{-1}_1$$\nSince $S$ is closed under multiplication $a^{-1}_1 \\in S$.\nTherefore $e, a^{-1}_1 \\in S$ so $S$ is a subgroup of $G$.\nPlease tear this apart! Thanks in advance.\n",
    "proof": "You don't need to use the second map $A_2$.  Once you know that $e \\in S$, you know by the first part of your proof that $A_1$ is surjective, so for some $s \\in S, a_1s=e$.  Then $s=a^{-1}$.\nAnother way to prove this result, by the way, is to just take all possible powers of $a_1$, which all are contained within $S$ because $S$ is multiplicatively closed.  They have to repeat at some point because $S$ is finite.  If $a^m=a^{m+k}=a^ma^k$, then $a^k=e \\in S$, and $aa^{k-1}=a^k=e \\Rightarrow a^{k-1}=a^{-1} \\in S$.\nAs for proof-writing, one small nit.  I don't think the line $A_1(a_1) = a_1$ conveys your intended meaning.  I think you mean to say $a_1 \\in A_1(S)$.\n",
    "tags": [
      "abstract-algebra",
      "group-theory",
      "proof-verification",
      "proof-writing",
      "finite-groups"
    ],
    "score": 8,
    "answer_score": 3,
    "is_accepted": false,
    "question_id": 3123182,
    "answer_id": 3123223
  },
  {
    "theorem": "In a 3x3 grid with 9 consecutive integers, prove there is at least one sum of 3 integers horizontally, vertically or diagonally that is divisible by 3",
    "context": "When I read a question in this forum Maximize the prime sums in a table, I noticed that when $9$ consecutive integers are to be filled into a $3\\times3$ grid, no matter how the integers are arranged, there must be at least one sum of $3$ numbers either horizontally, vertically or diagonally that can be divisible by $3$. I am interested to find a proof, but what I have achieved is to simplify the $9$ integers to become {$0$, $0$, $0$, $1$, $1$, $1$, $2$, $2$, $2$} (i.e., taking the $\\mod 3$ results of the $9$ consecutive integers), and then trying them by brute force. Surely this method does not give a nice proof. Is there any better way to prove it?\n",
    "proof": "Working over $\\mathbb{Z}_3$ is a good idea. I suspect there is an elegant pigeon-hole principle argument lurking about, but here is another one.\nNotice that the entries in any line (= row/column/diagonal) sums to zero (modulo $3$) if and only if all entries are equal or all are different. So suppose you have a matrix $M$ (over $\\mathbb{Z}_3$) such that every line has exactly two entries that are equal.\nLet's draw a bipartite graph $G$ with bipartition $(L,R)$ where the $8$ vertices of $L$ represent the $8$ different lines and $R=\\{0,1,2\\}$. Join a vertex $\\ell\\in L$ and an $x\\in R$ if $x$ is an entry in the line $\\ell$. \nBy our assumption on $M$, every vertex in $L$ has degree exactly $2$, and thus $G$ has exactly $16$ edges.\nOn the other hand, let's look at the degrees of the three vertices in $R$.\nClaim: Every vertex $x\\in R$ has degree at least $5$.\nProof: If $(i,j),(k,l),(m,n)$) are the coordinates of the $x$'s in $M$, then at least two of the row indices are distinct and at least two of the column indices are distinct. This gives at least $4$ lines (all being rows or columns). But if either the set of row indices, or the set of column vertices are distinct, then we have $5$ lines. On the other hand, if two of the row indices are equal and two of the column indices are equal, then there is a diagonal containing $x$, and so a $5$th line.\nSince every vertex in $R$ has degree at least $5$, but there are only $16$ edges in $G$, we must have two vertices in $R$ with degree $5$, and one of degree $6$. \nLet $x$ be the entry in the center of $M$. This entry is on $4$ lines, and so corresponds to $4$ edges in $G$. There are two more $x$'s in $M$. \nCase 1:  one of the other $x$'s is on a corner entry. then we get $2$ more edges in $G$ from these two lines (the corresponding row and column). So the degree of $x$ is at least $6$. Since the degree can't be higher than $6$, the final $x$ can then only be in one of two positions. Rotating/reflecting if necessary, this means that $M$ looks like: $$M=\\begin{bmatrix} x & x & \\cdot \\\\ \\cdot  & x & \\cdot\\\\ \\cdot & \\cdot & \\cdot\\end{bmatrix}.$$ Let $y$ be the entry in coordinates $(1,3)$. This forces $M$ to look like: $$M=\\begin{bmatrix} x & x & y \\\\ y  & x & \\cdot\\\\ y & \\cdot & \\cdot\\end{bmatrix}.$$\nBut then we can fill in the third possible entry in one way, producing a line with distinct entries: $$M=\\begin{bmatrix} x & x & y \\\\ \\color{red}{y}  & \\color{red}{x} & \\color{red}{z}\\\\ y & z & z\\end{bmatrix}.$$\nCase 2: None of the corner entries of $M$ equal $x$. Since not all three $x$'s are in the same line, this means that (rotating if necessary) $M$ looks like: $$M=\\begin{bmatrix} \\cdot & x & \\cdot \\\\ x & x & \\cdot\\\\ \\cdot & \\cdot & \\cdot\\end{bmatrix}.$$\nLet $y$ be the entry in coordinate $(1,1)$. Then $M$ can only look like $$M=\\begin{bmatrix} y & x & y \\\\ x & x & \\cdot\\\\ y & \\cdot & \\cdot\\end{bmatrix}.$$ But now we can fill in the $z$'s obtaining a line with distinct entries, a contradiction: $$M=\\begin{bmatrix} \\color{red}{y} & x & y \\\\ x & \\color{red}{x} & z\\\\ y & z & \\color{red}{z}\\end{bmatrix}.$$\n",
    "tags": [
      "elementary-number-theory",
      "proof-writing"
    ],
    "score": 8,
    "answer_score": 5,
    "is_accepted": true,
    "question_id": 1183997,
    "answer_id": 1185354
  },
  {
    "theorem": "Spanning trees in planar dual graph",
    "context": "The amount of spanning trees in a planar graph G is equal to the amount of spanning trees in the dual graph G*. \nI would like to prove this, i know it's true, but i would like to show that it holds for every spanning tree in G that there exist one and only one co spanning tree in G* for the original spanning tree in G.\nI made a drawing of the cubegraph that illustrates what i'm trying to prove \n\nHere i have added a vertex inside every mask in G, to create the dual graph G*.\nYou can clearly see the idea. If you have a spanning tree in G, the co spanning tree is the edges not colored in G. But i would like to prove that this always count.\nI have tried with a proof by bijection, but it is not making sense, and i would like someone to explain to me how i would go about this. \n",
    "proof": "Let $T$ be a spanning tree in a connected plane graph $G$. Let $G^*$ be the dual graph corresponding to an embedding of $G$ in the plane, and let $T^*$ be the subgraph of $G^*$ consisting of the edges of $G^*$ that correspond to the edges of $G$ not in $T$. We want to prove $T^*$ is a spanning tree of $G^*$. \nFirst note that $T$ has $|V(G)|-1$ edges, so $T^*$ has $|E(G)|-(|V(G)|-1)$ edges. By Euler's formula, there are exactly $2 - |V(G)| + |E(G)|$ faces in the embedding of $G$, so   $|V(G^*)| = 2 - |V(G)| + |E(G)|$, and we have that $|E(T^*)| = |V(G^*)|-1$. It remains to show that $T^*$ is acyclic, since an acyclic subgraph of a graph with 1 fewer edge than the number of vertices in the graph is a spanning tree.\nNow suppose $T^*$ has a cycle $C$. Note that $C$ separates the embedding of $G^*$ into two connected pieces, each of which contains at least one face of the embedding. But then the faces of $G^*$ correspond to vertices of $G$, and the edges of $C$ must correspond to an edge cut of $G$. But $T$ does not contain any of the edges in that edge cut, so $T$ cannot be a spanning tree, a contradiction.\nThus $T^*$ is acylic and $|E(T^*)| = |V(G^*)|-1$, and $T^*$ is a spanning tree in $G^*$. \nYou might want to look into matroid theory. These notions are quite a bit easier to understand in that light. A spanning tree of a graph is a basis in the cycle matroid $M$ for the graph. The complement of basis is a basis in the dual matroid. In other words, the complement of a spanning tree in a planar graph is a spanning tree in the dual graph.\n",
    "tags": [
      "graph-theory",
      "proof-writing",
      "planar-graphs"
    ],
    "score": 8,
    "answer_score": 5,
    "is_accepted": true,
    "question_id": 764566,
    "answer_id": 764695
  },
  {
    "theorem": "Inductive hypothesis vs induction hypothesis",
    "context": "I'm doing a proof by induction. Should I refer to induction hypothesis or to inductive hypothesis in the proof? \n",
    "proof": "It is acceptable, especially in technical English, to use nouns as adjectives. Indeed there are some cases where it is preferable: for example, our China correspondent refers to the correspondent based in China (who may not be Chinese). However, generally, keeping nouns and adjectives in their respective roles makes reading easier. This is particularly true in a many-word noun phrase. Since inductive is well established and understood, using it in this case costs nothing, and many people who care about such things would prefer it. (Incidentally, noun phrase is better than substantive phrase here, for ease of recognition in the present context.) \n",
    "tags": [
      "terminology",
      "proof-writing",
      "induction"
    ],
    "score": 8,
    "answer_score": 5,
    "is_accepted": true,
    "question_id": 566585,
    "answer_id": 566732
  },
  {
    "theorem": "Check my workings: Prove the limit $\\lim\\limits_{x\\to -2} (3x^2+4x-2)=2 $ using the $\\epsilon,\\delta$ definition.",
    "context": "Prove the limit $\\lim\\limits_{x\\to -2} (3x^2+4x-2)=2 $ using the $\\epsilon,\\delta$ definition.\n\nPrecalculations\nMy goal is to show that for all $\\epsilon >0$, there exist a $\\delta > 0$, such that\n  $$0<|x+2|<\\delta\\ \\ \\text{implies}\\  |3x^2+4x-2-2|<\\epsilon$$\n$|3x^2+4x-2-2|=|3(x+2)^2-8x-16|$\n$=|3(x+2)^2-4(x+2)|$\n$\\leq3|x+2|^2+4|x+2|$ by triangle inequality\n$<3\\delta^2+4\\delta$\nHence, it is sufficient to show that $3\\delta^2+4\\delta=\\epsilon$\n\nProof\nFor all $\\epsilon>0$, choose $\\delta=\\min\\left(\\sqrt{\\dfrac{\\epsilon}{6}},\\dfrac{\\epsilon}{8}\\right)$\n$$\\begin{align*}0<|x+2|<\\delta\\ \\ \\to\\ \\ &|3x^2+4x-2-2|<3\\delta^2+4\\delta\\\\&<3\\left(\\sqrt{\\frac{\\epsilon}{6}}\\right)^2+4\\delta\\\\&=\\frac{\\epsilon}{2}+4\\delta\\\\&<\\frac{\\epsilon}{2}+4\\frac{\\epsilon}{8}\\\\&=\\frac{\\epsilon}{2}+\\frac{\\epsilon}{2}\\\\&=\\epsilon\\end{align*}$$\n\n\nTherefore proven? Hehe. Not sure this will work or not. \nMy doubts lies in the steps. \n\nHence, it is sufficient to show that $3\\delta^2+4\\delta=\\epsilon$\nchoose $\\delta=\\min\\left(\\sqrt{\\dfrac{\\epsilon}{6}},\\dfrac{\\epsilon}{8}\\right)$\n\nAnd hey, I am looking out for other possible ways to do this question too.\n",
    "proof": "You made a mistake here:\n$$|3x^2+4x-2-2|=|3(x+2)^2-8x-16|=|3(x+2)^2-4(x+2)|$$\nIt should be $\\,8\\,$ instead $\\,4\\,$ in the RHS. All the rest you did is fine, fixing this little mistake. \nI show you now how'd I do it:\n$$|3x^2+4x-2-2|=|3(x+2)^2-8(x+2)|=$$\n$$|x+2|\\,|3x-2|\\stackrel{\\text{for}\\,|x+2|<0.5\\Longrightarrow |3x-2|<10}<10|x+2|$$\nThus, we're fine if\n$$10|x+2|<\\epsilon\\Longrightarrow |x+2|<\\frac{\\epsilon}{10}$$\nThus we can choose \n$$\\delta =\\min\\left(\\frac{\\epsilon}{10}\\,,\\,\\frac{1}{24}\\right)$$\n",
    "tags": [
      "limits",
      "proof-writing",
      "definition",
      "solution-verification"
    ],
    "score": 8,
    "answer_score": 4,
    "is_accepted": true,
    "question_id": 204340,
    "answer_id": 204347
  },
  {
    "theorem": "Given sequence of $L-$Lipschitz functions which converges pointwise, prove uniform convergence",
    "context": "\nLet $f_n:[a.b]\\rightarrow \\mathbb{R}$ be sequence of $L-$Lipschitz functions, that is: $$\\forall x,y\\in[a,b]: |f_n(x)-f_n(y)|\\leq L|x-y|$$\n  Suppose $f_n \\rightarrow f$ pointwise, prove $f_n \\rightrightarrows f$\n\nI have all the parts of the puzzle for the proof, and I'm trying to put them all together, I'm using this in my answer.\nI would appreciate is you could correct my proof, and if you have an alternative proof, I would be more then happy to see it.\nMy proof: \nLet $\\epsilon>0.$\n$f_n$ are uniformly continuous on $[a,b]:$\n$\\tag{1} \\exists \\delta>0\\ \\forall x,y\\in[a,b]: |f_n(x)-f_n(y)|<\\frac{\\epsilon}{3}$\n$f$ is also $L-$Lipschitz:\n$\\tag{2} \\forall x,y\\in[a,b]:|f(x)-f(y)|<L|x-y|=\\frac{\\epsilon}{3}$\nLet us set a partition of $[a,b]$ such as Stephen Montgomery-Smith suggests:\n\nPick points $x_1,\\dots,x_m \\in [a,b]$ which are distance\n  $\\frac{\\epsilon}{3}$ from each other.  \nFor each $1 \\le i \\le m$, find a number $N_i$ so that for all $n \\ge\n N_i$ we have $|f_n(x_i)-f(x_i)| \\le \\epsilon/3$.\n  Let $N = \\max_{1 \\le i \\le m} N_i$\nNow given any $x \\in [a,b]$, pick $1 \\le i \\le m$ such that $|x-x_i| <\\frac{\\epsilon}{3L}: \n|f_n(x)-f(x)|<\\frac{\\epsilon}{3} \\tag{3}$\n\n$$\\begin{align}|f_n(y)-f(y)| &=|f_n(y)-f_n(x)+f_n(x)-f(x)+f(x)-f(y)| \\\\ &\\leq |f_n(y)-f_n(x)| + |f_n(x)-f(x)|+|f(x)-f(y)| \\\\ &< \\frac{\\epsilon}{3} + \\frac{\\epsilon}{3} + \\frac{\\epsilon}{3} = \\epsilon \\\\\\end{align}$$\n",
    "proof": "Let $\\varepsilon>0$ be given, and set $\\delta=\\min\\left[\\frac{\\varepsilon}{3}, \\frac{\\varepsilon}{3L}\\right]$. Since the collection of open balls $\\mathcal{B}: = \\{B(\\, x, \\delta) : x \\in [a,b] \\}$ is a cover for $[a,b]$ we may find a finite subcover, say $\\{B(\\,x_1, \\delta),  \\, \\ldots, \\, B(\\,x_M, \\delta)\\}$ (Heine-Borel Theorem).  Since $f_n$ converges pointwise on $[a,b]$, for each point $x_j \\: \\left(\\,j=1,\\ldots, M \\right)$ we may find a positive integer $N_j$ so that \n\\begin{equation} \\left|\\, f_n(x_j) -\n f_m(x_j) \\right| <  \\frac{\\varepsilon}{3}  \\text{ whenever } n, m \\geq N_j \\,.\n\\end{equation}\nSetting $N = \\max [N_1,  \\ldots, N_M]$ shows that\n\\begin{aligned}\n\\left|\\,f_n(x)- f_m(x) \\right| & \\leq  \\left| \\,f_n (x)- f_n(x_j) \\right| + \\left|\\, f_n (x_j)- f_m(x_j) \\right| + \\left|\\, f_m(x_j)- f_m(x) \\right| \\\\\n& < \\frac{\\varepsilon}{3} + \\frac{\\varepsilon}{3} + \\frac{\\varepsilon}{3} = \\varepsilon\n\\; \\: \\text{ whenever } \\, n,m \\geq N \\text{ and } x \\in [a,b] . \n\\end{aligned}\nSince $\\mathbb{R}$ is complete, it follows that the sequence of functions $\\{\\,f_n\\}_{n=1}^\\infty$ converges uniformly on $[a,b]$ (Cauchy Criterion).\nI prefer uniform convergence first, ask $\\,f$ questions later -_-.\n",
    "tags": [
      "real-analysis",
      "proof-writing",
      "uniform-convergence",
      "lipschitz-functions",
      "sequence-of-function"
    ],
    "score": 8,
    "answer_score": 4,
    "is_accepted": true,
    "question_id": 2356399,
    "answer_id": 2356838
  },
  {
    "theorem": "Prove or disprove my conjecture about triangles.",
    "context": "Prove or disprove: For a set of at least 3 points not all collinear, we can always construct a triangle that contains the points, with the added condition that each of the triangle's edges has a point at its center.\nExample: See the figure below. 7 points are scattered about at random, and the black triangle contains all of them. (Note that I consider the red point in the bottom left contained despite being intersected by the edge of the triangle.) Furthermore, the triangle's sides each have a point from the set at their center. I have colored these center points orange for convenient viewing, but there is nothing special about them: I might have selected another three center points when building a triangle to contain this set.\n\n",
    "proof": "Your conjecture is true, and for a surprisingly (to me) elementary reason. I strongly suggest you draw my solution for yourself - it isn't deep.\nA finite set of points $S$ will determine finitely many triangles; take the largest of these in area, $T$. I claim that all points will fall inside the triangle $M(T)$ which has $T$ as its medial triangle (which will clearly satisfy our requirements on each side of the triangle having a midpoint belonging to $S$). Now suppose that some point $s\\in S$ fell outside $M(T)$; let $L$ be the side of $M(T)$ that $s$ lies above. (By this I mean that if we extend the sides of $M(T)$ to infinity, there will be three sections of the plane, determined by these lines, which touch the sides of the triangle; if $s$ is in the section that touches $L$, we can visualize it as \"lying above $L$.\")   \nNow, take the side $K$ in the original triangle $T$ which is parallel to $L$ in $M(T)$. Because $s$ lies above $L$, it lies above the vertex $v$ in $T$ that falls along $L$, which means - since $K$ and $L$ are parallel - that the altitude of $s$ from $K$ is higher than that of $v$ from $K$. But then the triangle having $s$ as a vertex instead of $v$ would have strictly larger area (take $K$ as the triangle's base to see this). This would contradict our hypothesis that $T$ was the largest triangle we could make from points in $S$!\n",
    "tags": [
      "geometry",
      "proof-writing"
    ],
    "score": 8,
    "answer_score": 4,
    "is_accepted": true,
    "question_id": 2111403,
    "answer_id": 2111528
  },
  {
    "theorem": "Prove that $\\mathcal{P}(A)\\cup \\mathcal{P}(B) \\subseteq \\mathcal{P}(A\\cup B)$.",
    "context": "I have a presentation this Monday. I thought it was pretty straight forward but my professor wrote \"You need to show why $X$ is in $\\mathcal{P}(A\\cup B)$, not just state that it is.\" I thought that I had.\nHere's what I have:\nProof\nSuppose $X\\in\\mathcal{P}(A)\\cup\\mathcal{P}(B)$.\nBy definition of union, this means $X\\in\\mathcal{P}(A)$ or $X\\in\\mathcal{P}(B)$.\nBy definition of power sets $X \\subseteq A$ or $X \\subseteq B$.\nCase 1: Suppose $X \\subseteq A$. Then $X \\subseteq A\\cup B$, and this means $X\\in\\mathcal{P}(A\\cup B)$.\nCase 2: Suppose $X \\subseteq B$. Then $X \\subseteq A\\cup B$, and this means $X\\in\\mathcal{P}(A\\cup B)$. \nBy case 1 and 2, $X\\in\\mathcal{P}(A\\cup B)$.\nThus $X\\in\\mathcal{P}(A)\\cup \\mathcal{P}(B)$ implies $X\\in\\mathcal{P}(A\\cup B)$, and\ntherefore $\\mathcal{P}(A)\\cup \\mathcal{P}(B)\\subseteq \\mathcal{P}(A\\cup B). \\blacksquare$\nI am so bad at this. I feel so stupid.\n",
    "proof": "Here's an easier approach (anyway, a different one). We'll use these two general facts:\n$$\\text{If } X \\subseteq Y \\text{ then } \\mathscr{P}(X) \\subseteq \\mathscr{P}(Y) \\tag{1}.$$\n$$\\text{If } X \\subseteq Z \\text{ and } Y \\subseteq Z \\text{ then } X \\cup Y \\subseteq Z \\tag{2}.$$\nThese are (very) easy to prove if you haven't already proved them in your course. (\"Proving\" them amounts to unpacking the definitions of $\\subseteq, \\cup$ and $\\mathscr{P}(.)$, which reveals that they're trivial. In both cases, the converse is true too.) Using these truisms, the result follows simply:\nNecessarily $A \\subseteq A \\cup B$, so by (1):\n$$\\mathscr{P}(A) \\subseteq \\mathscr{P}(A \\cup B) \\text{;} \\tag{a}\n$$\nsimilarly, $B \\subseteq A \\cup B$, so again by (1):\n$$\\mathscr{P}(B) \\subseteq \\mathscr{P}(A \\cup B) \\text{.} \\tag{b}\n$$\nFrom (a) and (b), using (2), we conclude:\n$$\n\\mathscr{P}(A) \\cup \\mathscr{P}(B) \\subseteq \\mathscr{P}(A \\cup B) \\text{.}\n$$\n",
    "tags": [
      "elementary-set-theory",
      "proof-writing"
    ],
    "score": 8,
    "answer_score": 4,
    "is_accepted": true,
    "question_id": 1482120,
    "answer_id": 1482140
  },
  {
    "theorem": "Proving a complete and totally bounded metric space is compact.",
    "context": "I'm having trouble writing down the details of this proof formally.\n\nStatement: Suppose $(X, d)$ is a metric space that is complete, and totally bounded (i.e., for every $\\epsilon > 0$, $\\exists$ finitely many points $x_{i} \\in X$, $i = 1, \\dots, n$ such that $X = B(x_{1}, \\epsilon) \\cup \\dots \\cup B(x_{n},\\epsilon) )$.  Prove that $X$ is sequentially compact.\n\nHere is the idea of the proof:\nLet $\\{ y_{n} \\}_{n=1}^{\\infty}$ be a sequence in $X$.  We want to show we can find a convergent subsequence.  But the space is totally bounded, so for $\\epsilon = \\frac{1}{2}$, we can find finitely many $x_{i} \\in X$ ($i = 1, \\dots, n $) such that $X = B(x_{1}, \\frac{1}{2}) \\cup \\dots \\cup B(x_{n},\\frac{1}{2}) $.  Then infinitely many terms of $\\{y_{n} \\}$ appear in some $B(x_{i}, \\frac{1}{2})$.\nIf we look at the subsequence of infinitely many terms from $\\{y_{n} \\}$ that appear in $B(x_{i}, 1)$, it's clear the distance between each two terms in this subsequence is less than $1$.  I'll call this subsequence $\\{y_{n}^{(1)} \\}$.\nWe can repeat this process with $\\epsilon = \\frac{1}{2^{2}}$, that is, find finitely many open balls the union of which equals $X$, and one of these balls contains infinitely many terms of $\\{ y_{n}^{(1)} \\}$.  I'll call this subsequence $\\{ y_{n}^{(2)} \\}$, and clearly the distance between every two terms in this sequence is less than $\\frac{1}{2}$.\nDoing this iteratively, I can then pick an element from each of these subsequences I am finding, and this will construct my Cauchy subsequence of the original sequence, which will converge by the completeness of the space.\nIt's clear that from this subsequence, the distance between two terms, say, $y_{n}^{(m)}$ and $y_{n}^{(p)}$ is less than $\\frac{1}{2^{p}}$ if $p < m$.\nSo for each $\\epsilon > 0$, all I need to do is find $N$ such that $\\frac{1}{2^{N-1}} < \\epsilon$.  Then $t, s \\geq N$ implies $d(y_{n}^{(t)}, y_{n}^{(s)})< d(y_{n}^{(t)}, y_{n}^{(N)})+ d(y_{n}^{(N)}, y_{n}^{(s)}) < \\frac{1}{2^{N}} + \\frac{1}{2^{N}} < \\frac{1}{2^{N - 1}} < \\epsilon $.\nDid I manage to keep all of the details straight?  Thanks for any help.\n",
    "proof": "You're proof seems mostly all right, though I find it hard to follow towards the end.  If I were to write a proof along the same lines, I would write it as follows:\n\nLet $\\{ y_{n} \\}_{n=1}^{\\infty}$ be a sequence in $X$.  $X$ is totally bounded, so we can find finitely many $x_{i} \\in X$ ($i = 1, \\dots, n $) such that $X = B(x_{1}, \\frac{1}{2}) \\cup \\dots \\cup B(x_{n},\\frac{1}{2})$.  \nWe note that infinitely many terms of $\\{y_{n} \\}$ must appear in some ball $B(x_{i}, \\frac{1}{2})$.  Define $\\{y_{n}^{(1)}\\}$ to be the subsequence of terms that appear in this ball.\nIf we look at the subsequence of infinitely many terms from $\\{y_{n} \\}$ that appear in $B(x_{i}, 1)$, it's clear the distance between each two terms in this subsequence is less than $1$.  I'll call this subsequence $\\{y_{n}^{(1)} \\}$.\nIn fact, we can repeat this process in the following way: for any integer $k > 1$, we may select a ball $B$ of radius $1/2^k$ containing infinitely many elements of $\\{y^{(k-1)}\\}$.  Define $\\{y_{n}^{(k)}\\}$ to be the subsequence whose elements are the elements of $\\{y_{n}^{(k-1)}\\}$ that fall in $B$.\nNow, consider the subsequence of $y_n$ given by $x_n = y_n^{(n)}$.  We note that for any $\\epsilon > 0$, we may select an $N$ such that $1/2^{N} < \\epsilon$.  For $n>m>N$, we note that $x_n,x_m$ are both elements of the sequence $\\{y_n^{(N)}\\}$, which means that both are elements of a ball of radius $1/2^N$.  Thus, we may state that\n$$\nd(x_n,x_m) < 1/2^N < \\epsilon\n$$\nThus, $\\{x_n\\}$ is a Cauchy subsequence of $\\{y_n\\}$.  Since $X$ is compact, $\\{x\\}_n$ converges.\nThus, every sequence in $X$ has a convergent subsequence, which is to say that $X$ is sequentially compact.\n",
    "tags": [
      "metric-spaces",
      "proof-verification",
      "proof-writing"
    ],
    "score": 8,
    "answer_score": 3,
    "is_accepted": true,
    "question_id": 990865,
    "answer_id": 990912
  },
  {
    "theorem": "A proof by strong induction that $a_n\\le3^n$ where $a_n=a_{n-1}+a_{n-2}+a_{n-3}$",
    "context": "I am not sure whether this is right. Can anyone verify, whether this proof is valid? Thanks!\n\n\n\nDefine a sequence $\\{a_n\\}_{n\\ge0}$ as follows:\n  $$a_0=1,\\qquad,a_1=3,\\qquad,a_2=9,\\qquad,a_n=a_{n-1}+a_{n-2}+a_{n-3}\\text{ for }n\\ge3.$$\n  Prove that for any positive integer $n$, $a_n\\le3^n$.\n\n\n\nProof. Let $P(n)$: $a_n\\le 3^n$, $n$ is a non-negative integer.\n(i) Base case:\n\nConsider when $n=0$. LHS$=a_0=1$, RHS=$3^0=1$ $\\therefore$ LHS$\\le$RHS, then $P(0)$ holds.\nConsider when $n=1$. LHS$=a_1=3$, RHS=$3^1=3$ $\\therefore$ LHS$\\le$RHS, then $P(1)$ holds.\nConsider when $n=1$. LHS$=a_2=9$, RHS=$3^2=9$ $\\therefore$ LHS$\\le$RHS, then $P(2)$ holds.\n\n(ii) Inductive case:\n  Assume $P(i)$ is true for $0 \\le i \\le k$, $k\\ge2$.\n(iii) Inductive conclusion: Consider $n=k+1$.\n  $$\n\\begin{align*}\n\\mathrm{LHS}=a_{k+1}&=a_k+a_{k-1}+a_{k-2} \\qquad\\text{(by definition, since $k\\ge 2$)}\\\\\n                    &\\le 3^k+3^{k-1}+3^{k-2} \\qquad\\text{(by Induction Hypothesis)}\\\\\n                    &=3^k(3^{-1}+3^{-2}+3^{-3})\\\\\n                    &=3^k\\left(\\frac13+\\frac19+\\frac1{27}\\right)\\\\\n                    &=\\frac{13}{27} 3^k\\\\\n                    &\\le 3^{k+1} = \\mathrm{RHS}\n\\end{align*}\n$$\n  Therefore, by the Principle of Strong Induction, $P(n)$ is true for all non-negative positive integers $n$.\n\n",
    "proof": "Nice write-up; looks great! Perhaps a slightly slicker way of doing the induction step (and probably what the problem intended you to do) is to observe that:\n\\begin{align*}\n3^k + 3^{k-1} + 3^{k-2}\n&< 3^k + (3)3^{k-1} + (3^2)3^{k-2} \\\\\n&= 3^k + 3^k + 3^k \\\\\n&= 3(3^k) \\\\\n&= 3^{k+1}\n\\end{align*}\n",
    "tags": [
      "inequality",
      "proof-verification",
      "proof-writing",
      "induction",
      "recurrence-relations"
    ],
    "score": 8,
    "answer_score": 4,
    "is_accepted": true,
    "question_id": 947850,
    "answer_id": 947853
  },
  {
    "theorem": "Proof of Hermite-Minkowski&#39;s Theorem regarding finite number fields for a given discriminant.",
    "context": "I want to prove the following theorem by Hermite and Minkowski:\n\nFor any given discriminant there are at most finitely many number fields with this discriminant.\n\nA very helpful step is that if $\\Delta$ is the discriminant of a number field of degree $n$ (over $\\mathbb{Q}$), then $|\\Delta| \\ge f(n)$ where $f$ is some term in $n$ which goes to infinity as $n$ becomes large.\nHence, the above theorem is reduced to showing that for any given $n$ and $\\Delta$, there are only finitely many number fields of degree $n$ with discriminant $\\Delta$. This is where I am stuck (I thought about this for a while and have absolutely no idea how to tackle this part of the proof).\nCan someone help me out with this one, please? I have the ambition to find the solution on my own, so I'd prefer little hints rather than a full solution.\n",
    "proof": "Note: I tried to formulate a series of hints for you, but with the approach I know and made use of I figured that would simply be more confusing than anything. So here's my proof picking up from where you were unsure how to continue, if you're still interested in the proof. \n\nAs you have determined, for a proof it suffices to show that there are only finitely many solutions to $|\\text{disc}(K)| < N$, where we let $K$ denote number fields and $N \\in R$. \nLet's consider $K(\\sqrt{-1})$. We have two cases, either $K = K(\\sqrt{-1})$ (i.e. our discriminant remains unchanged), or $[K(\\sqrt{-1}) : K] = 2 $ (i.e. our discriminant changes by bounded amount). We can consequently restrict our attention to $K$ of degree $n$ with $\\sqrt{-1} \\in K$. Then note that as there are finitely many $d \\in \\mathbb{Z}, 0 \\leq d \\leq N$, we can also fix $d$. \nSo for our fixed $d$ and $n$, it is enough to prove that there are finitely many $K \\ni \\sqrt{-1}$ with $|\\text{disc}(K)| = d$. Notice that in this case, the Minkowski space is $\\mathbb{C}^{\\frac{n}{2}}$, and the lattice is $\\phi({\\mathscr{O}}_{K})$. Now we need to find a centrally symmetric, convex subset such that for any solution $K$, there exists a non-zero point in $B \\cap L$, where $B$ is some convex body. Then based on this convex body, we will show that there are only finitely many ways of finding the correct $K$, and then show that in $L \\cap B$ we have that it is the image of a unique $K$. \nLet $B = \\{(z_{1}, ..., z_{n/2} : |\\mathscr{T}_{m}(z_{1})| < c\\sqrt{d},|\\mathscr{R}_{e}(z_{1})| < 1 |, |z_{i}| < 1, i = 2, ..., \\frac{n}{2}\\}$. Now pick $L$ sufficiently large so that $\\text{vol}(B) \\geq 2^{n/2} \\cdot \\det(L) \\geq 2^{n/2}\\sqrt{|d|}$. Note that $c$ depends on $n$ and $d$ but not on $K$. \nNow given a solution, we know good bounds on the roots of its minimal polynomial, which gives us good bounds on the coefficients of an integer polynomial. There are only finitely many such minimal polynomials. Therefore, there are only finitely many images of $K$ mapped to $B\\cap L$. It remains to prove that one of the $(z_{1}, ..., z_{n/2}) \\in B\\cap L$ corresponds to a unique $K$. We know that $\\mathbb{Q}(z_{1}) \\subseteq K$. If $\\mathbb{Q}(z_{1}) \\neq K$, then there exists $i \\neq 1$ such that $\\phi_{i}(z_{1}) = z_{i}$. So then $z_{1} = z_{i}$ for some $i$, and we know that $|z_{i}| < 1$. This means that \n$$\n\\mathbb{N}(z_{1}) = \\bigg|\\prod_{i=1}^{n/2} z_{i}\\bigg| < 1,\n$$\nwhich is clearly a contradiction. Therefore $\\mathbb{Q}(z_{1}) = K$. Thus there are a finite number of $K = K(\\sqrt{-1})$ with $|\\text{disc}(K)| = d$ of degree $n$. Hence, we have that there are only finitely many $K$ with $|\\text{disc}(K)| < N$, as desired. \n$$ \\square $$\n",
    "tags": [
      "proof-writing",
      "algebraic-number-theory"
    ],
    "score": 8,
    "answer_score": 4,
    "is_accepted": false,
    "question_id": 920447,
    "answer_id": 1362196
  },
  {
    "theorem": "Intuition behind proof and verification partially ordered sets",
    "context": "Hi everyone in the book that I read I have trouble to understand the argument of the proof at the below proposition. There is a lot of point which are  left as exercises, which is great. One of these is the following claim I put all the proposition for the sake of the completeness but not the entire proof. \nMy questions are: Is the proof of the claim correct (I use the hints which the book give us)? What is the intuition behind the claim? I mean, the entire construction of the \"good sets\" is rather artificial to me, what would be a possible motivation for them? I don't quite understand all the approach. Any suggestion, advice, whatever would be great thanks in advance. If someone consider opportune I'd put the entire proof or change the title (if there is one better). \nProposition: Let $X$ be a partially ordered set with ordering relation $\\le_X $, and let $x_0\\in X$. Then there is a well ordered subset $Y$ of $X$ which has $x_0$ as its minimum element, and which has no strict upper bound.\nProof: Suppose for the sake of contradiction that every well-ordered subset $Y$ of $X$ which has $x_0$ as its minimum element contain at least one strict upper bound. Using the axiom of choice we can thus assign a strict upper bound $s(Y)$ to each well-ordered subset $Y$ of $X$ which has $x_0$ as its minimum element.  \nLet us define a special class of subsets $Y$ of $X$. We say that a subset $Y$ of $X$ is a good set iff is well-ordered, contain $x_0$ as its minimum element and obey the property \n$$x=s(\\{y\\in Y: y<x\\})\\; \\text{for all }\\;  x\\in Y\\backslash \\{x_0\\}$$\nThe collection $\\Omega:=\\{ Y: Y\\; \\text{is a good set}\\;\\}$ is non-empty since contains the set $\\{x_0\\}$. \nClaim 1: Let $Y, Y' \\in \\Omega$, i.e., both are good sets. Then each element in $Y'\\backslash Y$ is a strict upper bound for $Y$. Similarly each element in $Y\\backslash Y'$ is a strict upper bound for $Y'$.\nProof of the Claim 1: Note that $Y\\cap Y' \\not= \\varnothing$ because both contains at least $x_0$. First we'd like to show that for all $x\\in Y\\cap Y'$ we have the following equality:\n$$\\{y\\in Y: y\\le x \\}=\\{y\\in Y': y\\le x \\}=\\{y\\in Y\\cap Y': y\\le x \\}$$\nWe may use strong induction for this purpose (this is possible because everyone of the above sets is well-ordered). Suppose that the assertion hold for each $y\\in Y\\cap Y'$ such that $y<x$, in other words we have the equality $\\{y\\in Y: y< x \\}=\\{y\\in Y': y< x \\}=\\{y\\in Y\\cap Y': y< x \\}$. We shall show that also hold when $y=x$. \nWe have $x=s(\\{y\\in Y: y< x \\})=s(\\{y\\in Y': y< x \\})$ because both are good sets. Then the equality of the first two sets hold. So, if $y\\in Y,\\; y=x$ and $y\\in Y,\\;y= x$ are equal for $x\\in Y\\cap Y'$. Thus the claim follows for the third set.\n(2) Now we wish to show that $Y\\cap Y'$ is itself a good set. Note that automatically is a well-ordered set, since every subset of a well-ordered set is well-ordered. And also contain $x_0$ as its minimum element. So, only we have to show that obey the third property. In other words our task is to show \n$$x=s(\\{y\\in Y \\cap Y': y<x\\})\\; \\text{for all }\\;  x\\in (Y\\cap Y')\\backslash \\{x_0\\}$$\nWe may assume $(Y\\cap Y')\\backslash \\{x_0\\} \\not= \\varnothing$ since otherwise is a good set and there is nothing to prove. Let $x\\in (Y\\cap Y')\\backslash \\{x_0\\} $, and we set $\\{y\\in Y \\cap Y': y<x\\}$. For the first part of the proof we already known $\\{y\\in Y \\cap Y': y<x\\}=\\{y\\in Y: y< x \\}$ for all $x\\in (Y\\cap Y')\\backslash \\{x_0\\} \\subset Y\\cap Y'$. Thus $x=s(\\{y\\in Y: y< x \\})=s(\\{y\\in Y \\cap Y': y<x\\})$, which shows that the set is a good set. \n(3) For the above part of the claim we know that $s(Y\\cap Y')$ exists. We shall show that if $Y'\\backslash Y\\not= \\varnothing$, then  $s(Y\\cap Y')= \\text{min}(Y'\\backslash Y)$. Similarly with the roles of $Y$ and $Y'$ interchanged.\nSuppose the set $Y'\\backslash Y$ is non-empty, then contains a minimum element because is a well-ordered set. Let a call it for brevity $x_0$. Thus for all $y<x_0$ we have $\\{y\\in Y': y<x_0 \\} \\subset Y$ using the minimality of $x_0$. Also is easy to check that $y\\in Y \\cap Y'$ iff $y<x_0$. \nSo $\\{y\\in Y': y<x_0 \\}=\\{y\\in Y\\cap Y': y<x_0 \\}$. Then $x_0=s(\\{y\\in Y\\cap Y': y<x_0 \\})$ because as we have shown is a good set. In other words $x_0= \\text{min} (Y'\\backslash Y) =s(\\{y\\in Y\\cap Y': y<x_0 \\})$. \nIt's trivial to show $s(\\{y\\in Y\\cap Y': y<x_0 \\}) =s(Y\\cap Y') $ this follows immediately for the way in which $x_0$ was specified since $\\{y\\in Y\\cap Y': y<x_0 \\}=Y\\cap Y'$, one inclusion is obvious and the other follows because if $y\\in Y\\cap Y' \\subset Y$ thus it can't be greater than $x_0$. \nNote that exactly the same argument applies only with the roles interchanged of $Y, Y'$ when we assumme $Y\\backslash Y'\\not= \\varnothing$.\n(4) If $Y,Y' \\in \\Omega$ are good sets. Then either $Y\\backslash Y'$ is non-empty or $Y'\\backslash Y$ is non-empty but not both at the same time. \nSuppose for the sake of contradiction that both cases occurs at the same time, i.e., $Y\\backslash Y'\\not= \\varnothing$ and $Y' \\backslash Y\\not= \\varnothing$. Let $x= \\text{min} (Y\\backslash Y')$ and $x'= \\text{min} (Y'\\backslash Y)$. We know by (3) that $x= s(Y\\cap Y')$ and also  $x'=s(Y\\cap Y')$, which would imply that $x=x'$ but this means that $x,x'\\in Y\\cap Y'$ which is a contradiction. Hence, it is not possible that both holds at the same time. \nAlso we can show that either $Y\\subset Y'$ or $Y' \\subset Y$, if were not the case, i.e., $Y\\not\\subset Y'$ and $Y' \\not\\subset Y$. Then $Y'\\backslash Y$ and $Y\\backslash Y'$ are both non-empty which as we have shown above leads a contradiction.\n(5) To conclude the proof of the claim we shall show that all the elements in $Y'\\backslash Y$ are strict upper bounds for $Y$. Note it is not necessary shows that the elements in $Y\\backslash Y'$ a strict upper bounds for $Y'$ because exactly the same arguments apply.\nLet $Y,Y' \\in \\Omega$, so both are good sets.Then either $Y'\\backslash Y$ is empty or not. If the set is empty is vacuously true that each of its element is a strict upper bound for $Y$. If were not the case, this meant $Y\\subset Y'$ and $Y\\cap Y'=Y$ for what shown in (4). Then $s(Y\\cap Y')= s(Y)= \\text{min}(Y'\\backslash Y)$. So, the minimum element of $Y'\\backslash Y$ is an upper bound for $Y$ and so, each element in $Y'\\backslash Y$ is strictly greater than each element in $Y$ by transitivity, i.e., are strict upper bounds for $Y$. \nHence the claim follows  as desired.\nI think there is an intimate relation of this proposition and the Zorn's lemma. Am I right? \n",
    "proof": "Jose Antonio, I haven't checked the entire proof above in detail, but it appears clear to me that it is possible to prove the proposition by that method. So I will address only the question of intuition regarding \"good sets.\"\nIn the considerations below, I will use some facts (about transfinite induction) that are known only after the proofs of the basic properties of well-ordered sets have been carried out. Evidently, at this stage of the book, the author doesn't yet have these facts available, so he must use an indirect approach to prove the proposition. I will use a direct approach to say what these good sets really are, intuitively.\nA proper initial segment of a well-ordered set $A$ is a subset of the form $I_x = \\{ y \\in A \\mid y < x \\}$. For example, if $A = \\mathbf{N}$, then the proper initial segments of $A$ are $I_0 = \\varnothing$, $I_1 = \\{0\\}$, $I_2 = \\{0, 1\\}$, $I_3 = \\{0, 1, 2\\}, \\ldots$\nNow assume temporarily that $A = \\mathbf{N}$, and let $B$ be some other set. Say that you would like to define a function $f \\colon A \\to B$ by induction. What information do you need? To define $f(n)$, you need a rule that tells you how to select a value for $f(n)$ depending on all the values $f(m)$ for $m < n$. In other words, you need a function $F \\colon \\cup_{I \\in S_A} B^I \\to B$, where $S_A$ is the set of proper initial segments of $A$, and $B^I$ is the set of functions from $I$ to $B$. Thus whenever $\\sigma \\colon I \\to B$ is a function defined on some proper initial segment $I$ of $A$, we have a well-defined element $F(\\sigma)$ of $B$. To define your function $f$, you then declare that for all $n \\in A$, we should have \n$$f(n) = F\\left(\\left.f\\right|_{I_n}\\right).$$\nIt turns out that such definitions by induction are possible even for arbitrary well-ordered sets $A$, not just $\\mathbf{N}$. This is definition by transfinite induction. There is also a method of proof by transfinite induction.\nTurning now to the example, imagine that there exists some very large well-ordered set $A$, much larger than your set $X$. Define a function $f \\colon A \\to X \\cup \\{*\\}$, where $\\{*\\}$ is some point not in $X$, in the following way. Let $f(0) = x_0$, where $0$ is the smallest element of $A$. If $\\alpha > 0$, let $f(\\alpha) = s(f(I_{\\alpha}))$  if $f(I_\\alpha)$ is a well-ordered subset of $X$ that has $x_0$ as its least element, and $f(\\alpha) = *$ otherwise.\nThen it can be checked by (transfinite) induction that $f(I_\\alpha)$ is always a well-ordered set with minimum element $x_0$. Thus $f(\\alpha) \\ne *$ for all $\\alpha$. In fact, $f$ realizes an isomorphism of $A$ with some well-ordered subset of $X$. This is a contradiction because we assumed $A$ to be larger than $X$.\nIf we hadn't made the absurd assumption that every well-ordered subset of $X$ (with least element $x_0$) had a strict upper bound, then this construction would have stopped at some point $\\alpha_0$, and then we would have had $f(\\alpha) = *$ for all $\\alpha \\geq \\alpha_0$. Then $f$ would have realized an isomorphism of the initial segment $I_{\\alpha_0}$ with a well-ordered subset of $X$ without a strict upper bound.\nThe \"good sets\" that appear in the proof are precisely the sets $f(I_\\alpha) \\subseteq X$ in the above construction. Intuitively, start with $x_0$, add an element $x_1 > x_0$, then $x_2 > x_1$, etc. Once you have added all the $x_n$ for $n \\in \\mathbf{N}$, these elements form a well-ordered set, so add an $x_{\\omega}$ that is greater than all the $x_n$. Then keep going like this. The good sets are those sets which, at some point in time, consist of all the elements that have already been added up to that point.\nThe facts that I've used here will become obvious to you (even on a rigorous level) once you've learned about transfinite induction. Unfortunately, some authors prefer to explain only Zorn's lemma instead of transfinite induction, and then there is much less intuition about what you are doing, even in some practical cases where Zorn's lemma may be sufficient. (Also, occasionally, there are cases when transfinite induction cannot be avoided.) If that is the case in the book you're reading, this material can be found in the first chapter of Introductory Real Analysis By Kolmogorov and Fomin, or for a fuller treatment, in Introduction to Set Theory by Jech and Hrbáček. \nEdit: I've corrected an error in notation concerning the domain of the function $F$.\n",
    "tags": [
      "elementary-set-theory",
      "proof-writing",
      "self-learning",
      "proof-verification",
      "order-theory"
    ],
    "score": 8,
    "answer_score": 4,
    "is_accepted": false,
    "question_id": 635035,
    "answer_id": 643542
  },
  {
    "theorem": "Does this paradox prove that the halting problem is undecidable?",
    "context": "A real number is said to be computable if a finite, terminating algorithm can compute it to arbitrary precision. Since algorithms are countable (for example, one may list all possible c programs in lexicographical order), so are computable numbers.\nThen, one may build the list of all computable numbers, in the order they appear in the list of all algorithms. One may now extract a new number from this list, much like in Cantor's diagonal argument, by having it's $n$th digit to be different from the $n$th digit of the $n$th computable number (e.g., $x\\mapsto (x+1)\\;mod\\;10$). This number will not be computable as it is different from every number in the list. Yet, it looks like we have an algorithm to find it.\nWhat seems to be the trick here, is that no algorithm for building the list of computable numbers exists, as it should have to decide whether a given algorithm does not halt in order to skip it. To avoid a contraddiction, the halting problem must be undecidable.\nCan this paradox be turned into an actual formal proof, or some fine details of formal logic get in the way, making the argument flawed?\n",
    "proof": "It is correct that you can effectively list all \"algorithms\" (or better, all partial computable functions $\\mathbb{N}\\to\\mathbb{N}$), but this does not imply that you can list all the computable (real) numbers. For the sake of this argument (and to ignore problems like multiple representations of reals and such), let us forget about numbers and let's talk about infinite strings of natural numbers. Your argument is essentially showing that you cannot computably list all the computable infinite strings, or, if prefer, you cannot computably list all and only the total computable functions $\\mathbb{N}\\to\\mathbb{N}$.\nAs mentioned in the comments, the problem is that you can't (computably) tell if a machine is never going to halt or if it is just needs more time. Imagine for example that your $n$-th program does not seem to converge on the $n$-th digit. Your algorithm for computing the diagonal string should, in finite time, commit to choosing the $n$-th digit for the diagonal string. After that stage, the $n$-th algorithm can choose to produce exactly the digit that your algorithm picked.\nTo make it even more precise, let $(\\varphi_e)_{e\\in\\mathbb{N}}$ be a computable list of all partial computable functions. Let $f:\\subset \\mathbb{N}\\to\\mathbb{N}$ be the partial computable function defined as $f(e):=\\varphi_e(e)+1$. Clearly $f$ is partial (e.g. take $e$ be the index of the algorithm that never converges on any input). If there were a computable enumeration $(\\varphi_{e_i})_{i\\in\\mathbb{N}}$ of all total computable functions (i.e. if the map $g(i):=e_i$ were computable), then $f$ restricted to $\\{e_i : i\\in\\mathbb{N}\\}$ would be total. In particular, $f\\circ g$ would be a total computable function not listed in $(\\varphi_{e_i})_{i\\in\\mathbb{N}}$.\nObserve that knowing whether a program halts on a given input is not enough to tell whether a function is total or not (i.e. if it halts on every input). However, you argument can certainly be adapted to show that the halting problem is not computable: if it were computable you could effectively find an enumeration $(\\psi_j)_{j\\in\\mathbb{N}}$ of all problems that halt on their index. It would be enough to start from $(\\varphi_e)_{e\\in\\mathbb{N}}$ and then, for each $e$, check whether $\\varphi_e(e)$ halts. Then modify the definition of $f$ saying that $f(e):=0$ if $\\varphi_e(e)$ does not halt. Such $f$ would be total and computable, hence it would halt on its index, but is not enumerated in $(\\psi_j)_{j\\in\\mathbb{N}}$, contradiction.\n",
    "tags": [
      "logic",
      "solution-verification",
      "proof-writing",
      "computability",
      "paradoxes"
    ],
    "score": 8,
    "answer_score": 2,
    "is_accepted": true,
    "question_id": 4758653,
    "answer_id": 4758881
  },
  {
    "theorem": "Proving that a list of perfect square numbers is complete",
    "context": "Well, I have a number  $n$ that is given by:\n$$n=1+12x^2\\left(1+x\\right)\\tag1$$\nI want to find $x\\in\\mathbb{Z}$ such that $n$ is a perfect square.\nI found the following solutions:\n$$\\left(x,n\\right)=\\left\\{\\left(-1,1^2\\right),\\left(0,1^2\\right),\\left(1,5^2\\right),\\left(4,31^2\\right),\\left(6,55^2\\right)\\right\\}\\tag2$$\n\nIs there a way to prove that this a complete set of solutions? So I mean that the solutions given in formula $(2)$ are the only ones?\n\n\nMy work:\n\nWe know that:\n$$\n1 + 12x^2 \\left(1+x \\right) \\ge 0\n  \\space \\Longleftrightarrow \\space\n  x \\ge -\\frac{1+2^{-2/3}+2^{2/3}}{3}\n  \\approx -1.07245\n\\tag3\n$$\nSo we know that for $x<-1$ there are definitely no solutions.\n\n",
    "proof": "$y^2=1+12x^2(1+x) \\implies  (12 y)^2 = (12 x)^3 + 12 (12 x)^2 + 144$\nMagma code for positive $y$ only:\nS:= IntegralPoints(EllipticCurve([0,12,0,0,144]));\nfor s in S do\n  x:= s[1]/12;\n  if x eq Floor(x) then\n    print \"(\",x,\", \",Abs(s[2]/12),\")\";\n  end if;\nend for;\n\nOutput:\n( -1 ,  1 )\n( 0 ,  1 )\n( 1 ,  5 )\n( 4 ,  31 )\n( 6 ,  55 )\n\n",
    "tags": [
      "number-theory",
      "proof-writing",
      "elliptic-curves",
      "square-numbers"
    ],
    "score": 8,
    "answer_score": 3,
    "is_accepted": true,
    "question_id": 3463339,
    "answer_id": 3465333
  },
  {
    "theorem": "Proof of $\\angle$ sum of polygon.",
    "context": "First, I know this question might have been asked by several times, see here, for an example.\nBefore someone may want to mark it as dulplicate, I would like to calrify what I want to ask.\nMainly, I am asking for a rigorous proof, or why it is rigorous enough?\nNotes: I will be considering simple polygon.\nConsider the answer of this post by Misha Lavrov,\n\nWhat makes things worse is that people often work with polygons on a\n  somewhat intuitive level\n\nI agree with this.\nI definitely know that\n$$Interior\\space\\angle\\space sum\\space of\\space a\\space N-sided\\space polygon=(N-2)180^\\circ$$ as every high school text shall states.\nMost of the proofs which I have seen about the problem, has a similar idea as the accepted answer of this post.\nMain idea of the proof:\nFor a polygon, we will just select points to join segments, and then we can devide the polygon into several pieces, and by $\\angle$ sum of $\\triangle$, we can find the $\\angle$ sum of the polygon.\nThis proof is very intuitive, but  I don't think it is rigorous enough, as I wonder, can we still connect every vertex to the point, even for a extremely ugly concave polygon, to seperate the polygon, into several $\\triangle$s, such that each of the interior $\\angle$ of each of the $\\triangle$s is in an interior $\\angle$ of the polygon and won't be counted twice. For an explaination and example for what I said right above, see below.\ne.g.)\nFor a 'ugly' 23-sided polygon, which I drew 'randomly':\nAs in the image above, I discovered a way to divide the polygon into 21 pieces of $\\triangle$, while it sounds to be eligible. However, I don't think this will certainly happen, if the polygon is even more ugly. How to explicitly consider the case, when different $\\triangle$ share a same interior $\\angle$ in the proof?\nAlso, as I asked above, can we always find a way to devide it properly? (I think it is important to prove it)\nI also considered the way in this post.\nFor the induction part, asumming the $\\angle$ sum formula for polygon is true for $N$-sided polygon. Then, consider any $N+1$-sided polygon, I used to think that we can select $2$ vertex which is saperate by one vertex in middle (can I present it more precisely?), join them together, and we form a $\\triangle$ (Do we need to consider whether 'convex' or 'concave'?). Then we form a $N$-sided polygon, and by the induction hypothesis and $\\angle$ sum of $\\triangle$, we can prove the formula holds for $N+1$.\nBut, moreover, do we need to consider this case and/or this case, when the remaining polygon might not be $N$-sided?\nTO SUM UP, How can we consider all possible cases and make a rigorous proof? Most importantly, I want a justification of the constructability(I mean whether the graph is constructable/valid, not for the Compass-and-straightedge construction)and generality of a graph, if there is a graph in the proof.\nRemember, as I said above, I am looking for a rigorous proof, not an usual one.\nThank you so much for your answer :)\nNOTES:Does the ways in this help?\n",
    "proof": "The key fact is that every simple polygon, not necessarily convex, can be decomposed into $n-2$ triangles by drawing $n-3$ diagonals. Then the sum of the interior angles of the polygon is equal to the sum of interior angles of all triangles, which is clearly $(n-2)\\pi$.\nThe existence of triangulations for simple polygons follows by induction once we prove the existence of a diagonal.\nFor a proof, see Chapter 1 of Discrete and Computational Geometry by Devadoss and O'Rourke. This chapter is freely available.\nThe picture below from that chapter that captures the gist of the proof:\n\nSee also Diagonals: Feature Column from the AMS by Malkevitch.\n",
    "tags": [
      "geometry",
      "proof-writing",
      "euclidean-geometry",
      "polygons",
      "angle"
    ],
    "score": 8,
    "answer_score": 3,
    "is_accepted": false,
    "question_id": 2656934,
    "answer_id": 2656953
  },
  {
    "theorem": "Proving with AM-GM Inequality",
    "context": "$$\\frac{4}{abcd}\\geq\\frac{a}{b}+\\frac{b}{c}+\\frac{c}{d}+\\frac{d}{a}$$\nGiven: $a+b+c+d=4$ and $a$, $b$, $c$ abd $d$ are positives.\nHow to prove the above inequality using Arithmetic Geometric Mean Inequality?\nI tried the following but, I am getting stuck after last step.\n\n",
    "proof": "Let $\\{a,b,c,d\\}=\\{x,y,z,t\\}$, where $x\\geq y\\geq z\\geq t$.\nHence, by Rearrengement and AM-GM we obtain:\n$$\\frac{a}{b}+\\frac{b}{c}+\\frac{c}{d}+\\frac{d}{a}=\\frac{1}{abcd}(a^2cd+b^2da+c^2ab+d^2bc)=$$\n$$=\\frac{1}{abcd}(a\\cdot acd+b\\cdot bda+c\\cdot cab+d\\cdot dbc)\\leq$$\n$$\\leq\\frac{1}{abcd}(x\\cdot xyz+y\\cdot xyt+z\\cdot xzt+t\\cdot yzt)=$$\n$$=\\frac{1}{abcd}(x^2yz+y^2xt+z^2xt+t^2yz)=\\frac{1}{abcd}(xy+zt)(xz+yt)\\leq$$\n$$\\leq\\frac{1}{abcd}\\left(\\frac{xy+zt+xz+yt}{2}\\right)^2=$$\n$$=\\frac{1}{4abcd}\\left((x+t)(y+z)\\right)^2\\leq\\frac{1}{4abcd}\\left(\\frac{x+y+z+t}{2}\\right)^4=\\frac{4}{abcd}.$$\nDone!\n",
    "tags": [
      "inequality",
      "proof-writing",
      "fractions",
      "a.m.-g.m.-inequality",
      "rearrangement-inequality"
    ],
    "score": 8,
    "answer_score": 3,
    "is_accepted": true,
    "question_id": 2233552,
    "answer_id": 2233635
  },
  {
    "theorem": "How to write well in analysis (calculus)?",
    "context": "This is kind of a subjective question, I know; often I find myself failing exams and homeworks because of the way i write down proofs. Either I don't know how to start, or somehow the main point of the proof is lost. I've noticed that in many books there's an \"style\" but doesn't matter how much i try to mimic it, i can't seem to do it right. I'm more of an algebra person, I love it, but Analysis, well... isn't my strong suit.\nIf you have any tips I'll appreciate it.\n\nAfter a while of waiting, I couldn't get the writting that I wanted to post as an example, so here it is a \"fresh\" example of an excerisize I didn't finish:\n \n\n\nIf $S$ is the set of all the sequences of real numbers, for $\\bar x=(x_i), \\bar y=(y_i) \\in S$ we define: $$d(\\bar x, \\bar y)= \\sum_{i=0}^\\infty \\frac {|x_i -y_i|}{2^i(1+|x_i-y_i|)}$$\n  (a) Prove that $d(\\bar x, \\bar y)$ is a metric in $S$.\n   (b) Let $\\bar x^k=(x_i^k),\\bar x=(x_i)\\in S$. Prove that: \n  $$\\lim_{k\\to \\infty} d(\\bar x^k,\\bar x)=0 \\Leftrightarrow \\lim_{k\\to \\infty}x^k_i=x_i \\quad \\forall \\; i\\in \\mathbb N$$\n\n\n So I proved (a), and I don't feel there's much to say there. However with (b) I got in trouble very easyly. \n($\\Leftarrow$) We know that $x_i^k \\to x_i$ if $k\\to \\infty$, that means that $\\forall \\; \\varepsilon>0 \\; \\exists \\; m\\in \\mathbb N$ such that $\\forall k>m \\;\\; |x_i^k -x_i|<\\varepsilon $. On the other hand, what we want to prove is $\\forall \\; \\varepsilon>0 \\; \\exists \\; N\\in \\mathbb N$ such that $\\forall k>N$ $$|\\sum_{i=0}^\\infty \\frac {|x_i^k -x_i|}{2^i(1+|x_i^k-x_i|)}|<\\varepsilon$$ So what I tough is that, since we already have that $|x_i^k -x_i|<\\varepsilon$ for any $\\varepsilon >0$ so I did some \"reverse engineering\", I took the absolut value that I want to prove and started to operate: \n$$\\mathbf {(1)}\\;\\;|\\sum_{i=0}^\\infty \\frac {|x_i^k -x_i|}{2^i(1+|x_i^k-x_i|)}| = \\sum_{i=0}^\\infty \\frac {|x_i^k -x_i|}{2^i(1+|x_i^k-x_i|)}<\\varepsilon $$\n$$\\Rightarrow \\sum_{i=0}^ n \\frac {|x_i^k -x_i|}{2^i(1+|x_i^k-x_i|)}<\\varepsilon ,\\;\\; if\\;\\; n\\to \\infty $$\n$$\\Rightarrow \\frac {|x_i^k -x_i|}{2^i(1+|x_i^k-x_i|)}<\\varepsilon $$\n$$\\Rightarrow \\frac {|x_i^k -x_i|}{1+|x_i^k-x_i|}<2^i \\varepsilon, \\;\\;where\\;\\; 2^i\\;\\;is\\;\\;constant\\;\\;with\\;\\;respect\\;\\;to\\;\\; k$$\nFrom here, I'm pretty much frozen. What I wanted was to get to what we already had: $|x_i^k -x_i|<\\varepsilon$, but I couldn't, somehow it seems futile. In class we already saw how to do it, and it is completly different from what I tried.\n",
    "proof": "I can't shake the feeling that you should try your hand with simpler proofs first. I am trying to address here some of the difficulties that I spotted by making comparisons with a simpler proof, and then discussing the extra difficulties brought about by this proof. This will be long, and not necessarily help you much towards the end. From the point of view of your teacher the most pressing concern is that your proof should show that you understood the need to do things differently.\nLet's try the following. In $\\mathbb{R}^2$ we use the metric\n$$\nd((x_1,x_2),(y_1,y_2))=\\sqrt{(x_1-y_1)^2+(x_2-y_2)^2}.\n$$\nThe analogue of your task is to prove that if (the $k$ is a superscript, not an exponent)\n$$\\lim_{k\\to\\infty} x_i^k=x_i$$\nfor $i=1$ and $i=2$, then \n$$\n\\lim_{k\\to\\infty}(x_1^k,x_2^k)=(x_1,x_2).\n$$\nSo you are asked to show that no matter how small $\\varepsilon>0$ you are given, you can produce a lower bound $m$ such that the inequality\n$$\nd((x_1^k,x_2^k),(x_1,x_2))<\\varepsilon\n$$\nwill hold, if $k>m$. Let's take a look at that distance. We want to see that\n$$\n\\sqrt{(x_1^k-x_1)^2+(x_2^k-x_2)^2}<\\varepsilon\n$$\nfor large enough $k$. Getting rid of that pesky square root is easy, so instead we want to show that\n$$\n(x_1^k-x_1)^2+(x_2^k-x_2)^2<\\varepsilon^2,\n$$\nagain for all large enough indices $k$.\nOk. Not too hard as for large $k$ both $|x_1^k-x_1|$ and $|x_2^k-x_2|$ become small. We do our usual \"split the $\\varepsilon$\" business. Our assumptions imply the existence of boundaries $m_1$ and $m_2$ such that A) $|x_1^k-x_1|<\\varepsilon/\\sqrt2$ whenever $k>m_1$ and B) $|x_2^k-x_2|<\\varepsilon/\\sqrt2$ whenever $k>m_2$. So if $k>m:=\\max\\{m_1,m_2\\}$ both of these inequalities hold simultaneously, and squaring them and adding them up proves the desired inequality for all $k>m$.\nA few remarks about this. There was a bit of \"reverse engineering\". But the reason for that part was simply to get an idea what kind of upper bounds we must place on the differences $|x_i^k-x_i|$ so that the desired inequality will follow. Note the direction of the flow of logic. The desired inequality must be a consequence of the simpler inequalities. Even though we reverse engineer here, the direction of the flow of implications must not be reversed. Another key point was that there were only finitely many terms contributing to the distance. Therefore we could follow the simple idea of splitting the elbow room provided by that $\\varepsilon^2$ equally between the two terms that we can control using our assumptions. Furthermore, the assumptions gave two lower bounds\nfor the index $k$. It was easy to make sure that both of them are satisfied by taking the maximum of the two lower bounds as the lower bound for this new process.\nLet's move from $\\mathbb{R}^2$ to $\\mathbb{R}^n$. No major changes. We can still split that $\\varepsilon^2$ evenly between the $n$ terms. Each of the $n$ terms will be below its allotted share $\\varepsilon/\\sqrt{n}$ from some lower bound $m_i$ onwards. Again we can use $m=\\max\\{m_1,\\ldots,m_n\\}$ as the lower bound for the limit process being studied, as we can find the largest among a finite set of numbers.\nLet's try something harder still and move to $\\mathbb{R}^\\omega$, the infinite dimensional space of sequences $(x_i)$. Let's try the metric\n$$\nd((x_i),(y_i))=\\sqrt{\\sum_{i=0}^\\infty(x_i-y_i)^2}.\n$$\nThe same task. We assume that $x_i^k\\to x_i$ as $k\\to\\infty$ for all $i$. Does it follow that $d((x_i^k),(x_i))\\to0$? Well, there are several problems before we actually get to this point. For the metric to make sense, we need the sequences to be square summable. Ok, so we restrict our space to square summable sequences. Then we need to worry about the square summability of the componentwise limit sequence $(x_i)$. Let's assume that is the case, as it is besides my main points. The main point is that the earlier proof method no longer works. There are infinitely many terms contributing to the distance, so we cannot split the elbow room of $\\varepsilon$ evenly among them. Another major obstacle will be that even though we can control any component of the sequence, and find a lower bound $m_i$ such that $|x_i^k-x_i|$ will be as small as we wish, whenever $k>m_i$, but we cannot make all of these to hold simultaneously. This is because there may not be a largest one among the bounds $m_i$ now that there are infinitely many of them.\nSo what? Well the first obstacle can be overcome. We don't need to split $\\varepsilon$ evenly for the reverse engineering part to succeed. We can give to the squared first component the elbow room of $\\varepsilon^2/2$, $\\varepsilon^2/4$ to the second $\\varepsilon^2/8$ to the third and continute in the style of a geometric progression. Each and every component still gets a positive amount of elbow room, which means that we can control its contribution.\nBut the second obstacle cannot be overcome. The theorem is actually false, but I skip the part of giving a counterexample. With this metric we simply do not have this result.\nWell, that's why you are hit with this different metric. A key difference is that the contribution of the $i$th components\n$$\n\\frac{|x_i-y_i|}{2^i(1+|x_i-y_i|)}<\\frac1{2^i}\n$$\nIRRESPECTIVE of the values of $x_i$ and $y_i$. This means that the larger indices will, so to speak, take care of themselves. A proof should reflect that. This gives us a way around the second obstacle above. We give half the elbow room to the infinite tail, and split the remaining elbow room among the finite early parts in some sensible way. \nSo this proof is somewhat delicate. I fear that your difficulties originate from an earlier course, where you didn't pay due attention to the need to have a way of producing that lower bound $m$ given a fixed $\\varepsilon$. If you were not aware of these obstacles along the way, there was zero change for your proof to overcome them. In math there is no monster-truck option of plowing through an obstacle. Your problem is not one of style. My advice is to revisit earlier textbooks and to use the office hour of your instructor. Fixing this will take quite a bit of work. Give yourself some time.\n",
    "tags": [
      "real-analysis",
      "soft-question",
      "proof-writing"
    ],
    "score": 8,
    "answer_score": 3,
    "is_accepted": true,
    "question_id": 476783,
    "answer_id": 480447
  },
  {
    "theorem": "Good book for learning and practising axiomatic logic",
    "context": "I want to learn axiomatic (Hilbert style ) logic.\nnot just a book that says that it exist and is an good way to proof theorems.\nWhat is a good book to learn and practice this method?\nwould like:\n- a book published after 2000\n- not limited to a particular axiomset or set of connectives\n- lots of examples.\nWhat I especially would like is a book that teaches how to transform Natural Deduction or Sequent Calculi style proofs to axiomatic Hilbert Style proofs.\n(I know it is a complex subject, it depends on the axioms and is not even always possible)\nAt the moment I am studying Bergmann's \"An introduction to many valued and fuzzy logic\" that uses this style of proof just because other proof styles are either invalid or even more complicated for this type of proof\n",
    "proof": "I've started studying L. H. Hackstaff's 1966 book Systems of Formal Logic.  It looks useful for this purpose and seems to have fundamentally sound advice.  Its systems don't have a rule of simultaneous substitution as a primitive rule of inference or involve axiom schema.  If more systems did this it might actually make it possible to teach the axiomatic method in logic at a lower level than presently done.\nIt has sections on the axiomatic method which says things like:\n\nKnow (the major ones/some of the major ones) the derivable rules of inference of the logical systems by heart.\nKnow the axioms of the logical system by heart.  Otherwise you'll have to refer to the axioms constantly when trying to construct a proof.\nKnow the rules of inference by heart.\nKnow the problem.\nKnow the evidence.\n\nUnlike many other books I have seen it also has plenty of examples axiomatic proofs (with the abbreviations for wffs adopted by the text).\nAs an example of how to prove something using just one variable at a time substitution.  Suppose our first thesis is that of suffixing or reverse hypothetical syllogism.\nOur language here has only has variables {a, b, c, ..., f} and subscripted variables if necessary.  Our meta-language will happen with {p, ..., z} so we can talk about rules of inference.\n1 C Cab C Cca Ccb\nIf we used condensed detachment would tell us that the next most general result of this system will be C CdCab C d C Cca Ccb.  Can we prove this using only single variable at a time substitution and detachment?\n1 has form Cxy, and so does Cab.  So, since we only have one thesis, it seems to try to make or find some form of CCabCCcaCcb such that we can detach something.  How might we do this?  Well the antecedent of 1 is Cab and the antecedent of Cab is a.  So, we need to substitute something for a such that a has form Cxy.  Would Cab work?\n2? C CCabb C CcCab Ccb.\nWe want to have \"b\" in one have the form CCcaCcb.  But, the rule of uniform substitution happen tells us we have to substitute for all propositional variables.  So, now if we used 2 to substitute something, we'd have to substitute the b in the first Cab.  But, then we'd have to make even more substitutions.  Do we have a simpler path possible here?\nWell, maybe we could adopt a procedural technique to vary substitutions such that when substituting for variables, more variables appear than in the original wff.  So, let's try substituting a with Cad in 1.  Then we obtain 2:\n2 C CCadb C CcCad Ccb.\nNow we want to have some form of 1 become the antecedent.  So we want something like CCbaCbd.  Oops, I didn't follow my own technique.  I mean 2 b/CCeaCed yields 3\n3 C CCadCCeaCed C CcCad CcCCeaCed *\nNow we want CCabCCcaCcb to match 3 (this exercise, surprising to me, I find kind of hard, because I know the form CCqrCCpqCpr by heart, but CCabCCcaCcb I don't know by heart).  So, 1 b/d yields\n4 CCadCCcaCcd\nNow 4 c/e yields 5\n5 C Cad C Cea Cea\nNow we can detach 6 from 3 and 5:\n6 C CcCad C c C Cea Ced  We want the following:\nG C CdCab C d C Cca Ccb\nSo, 6 c/d yields\n7? C CdCad C d C Cea Cdb\nNo, that won't work since substitutions have to happen uniformly.  b does not appear in 6.  So, if we substitute d for b first, we can still apply substitutions later.  So, 6 d/b yields\n7 C CcCab C c C Cea Ceb  Now 7 c/d yields\n8 C CdCab C d C Cea Ceb  Now 8 e/c yields\n9 C CdCab C d C Cca Ccb\nThis proof isn't unique since I might have made other substitutions first.  Let's look at 3 again:\n3 C CCadCCeaCed C CcCad CcCCeaCed\nThe Detachment Theorem gives us a tip or hint that this from a wff of the type CCadCCeaCed, another wff of the type CcCad, we may infer CcCeaCed.  Do we have a more general rule?  Let's back up to 1\n1 C Cab C Cca Ccb\nBut this time let's substitute b with CCeaCed first.  We then obtain\n10 C CaCCeaCed C Cca CcCCeaCed.\nAlong with The Detachment Theorem this gives us a clue about more general rules of inference than 3 would.  So, we have\nCpCCqpCqr => CC s p C s CCqpCqr (s does not appear on the left-side of the statement of the rule)\nCpCCqpCqr, Csp => CsCCqpCpr\nCpCCqpCqr, Csp, s => CCqpCpr\nCpCCqpCqr, Csp, s, Cqp => Cpr and\nCpCCqpCqr, Csp, s, Cqp, p => r\n",
    "tags": [
      "logic",
      "reference-request",
      "proof-writing",
      "book-recommendation",
      "nonclassical-logic"
    ],
    "score": 8,
    "answer_score": 1,
    "is_accepted": true,
    "question_id": 465640,
    "answer_id": 475525
  },
  {
    "theorem": "Interchanging two limits in a proof. Is it legitimate?",
    "context": "Landau defines\n$$\\log x = \\lim\\limits_{k \\to 0} {x^k-1 \\over k}$$\nI wanted to prove the elemental properties of the logaritm with this, namely:\n\n$\\log xy = \\log x +\\log y $\n$\\log x^a = a\\log x  $\n$1-\\dfrac 1 x\\leq\\log x \\leq x-1  $\n$\\lim\\limits_{x\\to 0}\\dfrac{\\log(1+x)}{x}=1 $\n$\\dfrac{d}{dx}\\log x = \\dfrac 1 x$\n\nI proved them all, however, in the last case I did this\n$$\\eqalign{\n  & \\frac{d}{dx}\\log x = \\lim \\limits_{h \\to 0} \\frac{\\log \\left( x + h \\right) - \\log x}{h}  \\cr \n  &  = \\lim\\limits_{h \\to 0} \\lim \\limits_{k \\to 0} \\frac{\\left( x + h \\right)^k - x^k}{kh}  \\cr \n  &  = \\lim \\limits_{k \\to 0} \\frac{1}{k}\\lim \\limits_{h \\to 0} \\frac{\\left( x + h \\right)^k - x^k}{h}  \\cr \n  &  = \\lim \\limits_{k \\to 0} \\frac{1}{k}\\lim \\limits_{h \\to 0} kx^{k - 1} = \\lim \\limits_{k \\to 0} \\lim \\limits_{h \\to 0} x^{k - 1} = x^{ - 1} } $$\nSince I'm not familiar with multivariable calculus, I don't know how to justify this. What could work here?\n",
    "proof": "On Peter's request I'm posting my comment as an answer:\nNote that 1. with $y=1+ \\frac{x}{h}$ gives\n$$\n\\log{(x+h)}-\\log{x} = \\log{(1+h/x)},\n$$\nso,\n$$\\frac{d}{dx}\\log{x} = \n\\lim_{h\\to0}\\frac{\\log{(x+h)}-\\log{x}}{h} = \\frac{1}{x} \\cdot \\lim_{h\\to0} \\frac{\\log{(1+h/x)}}{h/x} = \\frac{1}{x} \\cdot \\lim_{k\\to0} \\frac{\\log{(1+k)}}{k} \\stackrel{4.}{=} \\frac{1}{x},\n$$\nas desired.\n",
    "tags": [
      "limits",
      "multivariable-calculus",
      "proof-writing"
    ],
    "score": 8,
    "answer_score": 3,
    "is_accepted": true,
    "question_id": 136511,
    "answer_id": 136545
  },
  {
    "theorem": "I feel the need to prove every result for myself",
    "context": "I am, at best, a novice mathematician. I started teaching myself the subject while writing my thesis in computer science. I find that I have a strong urge to prove every relationship or formula that I come across while studying. I routinely find it hard to accept or understand a relationship that I cannot prove and often go to great lengths to do so. Is this practice normal or recommended or otherwise? \n",
    "proof": "It is very highly recommended. Although when you begin doing things it is sometimes more fruitful to simply gaze at some ideas and accept them to let blossom more amazing ideas, at some point you just gotta start thinking and that is where foundation becomes important. It will never hurt you to start making some rigor go into your mind, as long as it does not stop it from finding new ideas. I believe that it takes much, much work to have both rigor and intuition, but that is the price to pay, because it's the best thing to do. Simply focusing on rigor won't get you anywhere though ; proving things is one thing, understanding why they're true is another.\nHope that helps,\n",
    "tags": [
      "soft-question",
      "proof-writing"
    ],
    "score": 8,
    "answer_score": 3,
    "is_accepted": true,
    "question_id": 78450,
    "answer_id": 78455
  },
  {
    "theorem": "Prove / Disprove / Complete the proof That $f$ is Infinitely Differentiable",
    "context": "\nTHE NEW (MUCH SHORTER) POST\n\nMy original post perhaps was too long for most people, and understandably not a lot of people tried to go over it. In this new post my question is much shorter please prove or disprove that following function is infinitely differentiable on $\\mathbb{R}$, or alternatively answer my original post (it is below here), which basically means helping me completing my proposed proof. Even if you don't want to answer my original post because it 's too long, it might be worth at least looking at some of it, because it could be useful.\nLet $E$ be some closed set of real numbers. Then, we define $f$ by 3 cases:\n$\\quad$(I) If $x \\in E$, $f(x) = 0$.\n$\\quad$(II) If $x \\in (a,b)$, where $a,b \\in E$ and $(a,b) \\subseteq \\mathbb{R} \\setminus E$, define $$F_{a,b}(x) = \\frac{\\pi}{b-a}\\left(x + \\frac{3b-5a}{2}\\right),$$\n$\\quad$and then $$f(x) = X_{a,b}\\cos^{F_{a,b}(x)}\\left(F_{a,b}(x)\\right),$$\n$\\quad$where $X_{a,b}$ is some constant real based on $a, b$, which you need to define appropriately (probably different for every $(a,b)$), to prove that $f$ is infinitely differentiable, or if you want to prove that $f$ is not infinitely differentiable, you need to show that this is true for every choice for $X_{a,b}$.\n$\\quad$(III) If $E$ is bounded above, $M=\\sup E$, then $f(x)=e^{-\\frac{1}{(x-M)^2}}$ for $x > M$. Similarly, $f(x)=e^{-\\frac{1}{(x-N)^2}}$ if $E$ is bounded below and $\\quad N=\\inf E$.\n\nMY ORIGINAL POST: Baby Rudin Ex. 5.21 - Assessment of a Proposed Partial Solution and Help Completing It\n\nThis exercise is driving me crazy.\nI'm honestly ashamed to tell you how long I actually stack with this problem not continuing to study mathematics until I solve it. In many points a long the way I was very close to giving up and in many points I thought that I finally managed to solve the problem but then realized there is some mistake or a missing piece in the proof. The proposed solution here is the closest I managed to get at this point. At this point I decided it's time to ask for help on the last part missing from this proof and get some people to critic this partial proof in general.\nThe Problem: Ex. 21, Chap. 5\nLet $E$ be a closed subset of $\\mathbb{R}$. We saw in Exercise 22, Chap. 4, that there is a real continuous function $f$ on $\\mathbb{R}$ whose zero set is $E$. Is it possible, for each closed set $E$, to find such an $f$ which is differentiable on $\\mathbb{R}$, or one which is $n$ times differentiable, or even one which has derivatives of all orders on $\\mathbb{R}$?\nMy Idea (Informally)\nI think such an infinitely differentable function does exist. I want to give you a visual explanation of how I thought of constructing this function before going to the formal solution. We obviously set $f$ to be 0 for all points of $E$. For every open interval of points of $\\mathbb{R} \\setminus E$, where the end points are points of $E$, $f$ is a \"wave\" that smoothly approches 0 in the end points so that the derivative of any order on those end points is always 0. The shorter this open interval is, the smaller the maximum point of this \"wave\". If $E$ is bounded above, $f$ is just some increasing function after $\\sup E$, so that it smoothly approches 0 at $\\sup E$ (the same goes if $E$ is bounded below).\nI drew an illustration of this idea:\n\nThe red parts are points of $f(E)$, the blue parts are points of $f(\\mathbb{R} \\setminus E)$. $P$ is a limit point of $E$.\nMy Partial Solution\nMy proof uses two previous results from the book and two additional lemmas. I shall present them here (and prove the lemmas) before going to the main proof.\nEx. 5.9. Let $f$ be a continuous real function on $\\mathbb{R}$, of which it is known that $f'(x)$ exists for all $x \\neq 0$ and that $f'(x) \\to 3$ as $x \\to 0$. Does it follow that $f'(0)$ exists?\nIn the solution to this exercise we saw that $f'(0)$ does in fact exists and is 3. In general to any real continuous function $f$ on $\\mathbb{R}$ for which the derivative is known to exist except for a specific point $a$ and $f'(x) \\to D$ as $x \\to a$, we can use the same proof we use in this exercise to show that $f'(a)$ exists and $f'(a)=D$.\nTheorem 4.15. If $f$ is a continuous mapping of a compact metric space $X$ into $\\mathbb{R}^k$, then $f(X)$ is closed and bounded. Thus, $f$ is bounded.\nLemma 1. Let $f, g: \\mathbb{R} \\to \\mathbb{R}$ differentiable functions. Then,\n$$\\left(f(x)^{g(x)}\\right)' = f(x)^{g(x)}\\left[g'(x)\\ln(f(x)) + g(x)\\frac{f'(x)}{f(x)}\\right]$$\nProof. Let $h(x)=f(x)^{g(x)}$, then $$\\ln(h(x)) = \\ln\\left(f(x)^{g(x)}\\right) = g(x)\\ln(f(x)).$$ Differentiating both sides we get: $$\\frac{h'(x)}{h(x)} = g'(x)\\ln(f(x)) + g(x)\\frac{f'(x)}{f(x)}.$$ Thus, $$h'(x) = f(x)^{g(x)}\\left[g'(x)\\ln(f(x)) + g(x)\\frac{f'(x)}{f(x)}\\right].$$\nLemma 2. Let $f,g : \\mathbb{R} \\to \\mathbb{R}$ infinitely differentiable functions. Then, for every $n \\in \\mathbb{N}$:\n$\\quad$(a) $f+g$ is infinitely differentiable and $$(f+g)^{(n)}(x)=f^{(n)}(x)+g^{(n)}(x)$$.\n$\\quad$(b) $f\\cdot g$ is infinitely differentiable and $$(f\\cdot g)^{(n)}(x)=\\sum_{i=1}^{2^n} f^{\\left(\\alpha_{n_i}\\right)}(x)\\cdot g^{\\left(\\beta_{n_i}\\right)}(x),$$ $\\quad$where $\\{\\alpha_{n_i}\\}$, $\\{\\beta_{n_i}\\}$ are sequences of non-negative integers.\n$\\quad$(c) $\\frac{f}{g}$ is infinitely differentiable whenever $g(x) \\neq 0$ and $$\\left(\\frac{f}{g}\\right)^{(n)}(x)=\\frac{F_n(x)}{g^{2^n}(x)},$$ $\\quad$where $F_n$ is some infinitely differentiable function.\n$\\quad$(d) $f\\circ g$ is infinitely differentiable and $$(f\\circ g)^{(n)}(x)=\\sum_{j=1}^{z_n}\\prod_{i=1}^{m_{n_j}}t_{n_{j_i}},$$ $\\quad$for some $z_n \\in \\mathbb{N}$, $m_{n_1} and \\dots, m_{n_{z_n}} \\in \\mathbb{N}$ and where each term $t_{n_{j_i}}$ is either of the form $f^{(m)}(g(x))$ or $g^{(m)}(x)$, $m$ a non-negative $\\quad$integer.\nProof. We use induction for all 4 of the sections of this lemma.\n$\\quad$(a) For $n=1$, $(f+g)'(x) = f'(x) + g'(x)$. Suppose that $(f+g)^{(k)}(x)=f^{(k)}(x)+g^{(k)}(x)$ for some $k \\in \\mathbb{N}$. Then, $$(f+g)^{(k+1)}(x)=\\left(f^{(k)}(x)\\right)' + \\left(g^{(k)}(x)\\right)' = f^{(k+1)}(x) + g^{(k+1)}(x).$$\n$\\quad$(b) For $n=1$, $(f \\cdot g)'(x) = f'(x)g(x) + f(x)g'(x)$. Suppose that for some $k \\in \\mathbb{N}$, there are sequences $\\{\\alpha_{k_i}\\}$, $\\{\\beta_{k_i}\\}$ of non-negative integers such that $(f\\cdot g)^{(k)}(x)=\\sum_{i=1}^{2^k} f^{\\left(\\alpha_{k_i}\\right)}(x)\\cdot g^{\\left(\\beta_{k_i}\\right)}(x)$. Then, $$(f\\cdot g)^{(k+1)}(x)=\\sum_{i=1}^{2^k} f^{\\left(\\alpha_{k_i}+1\\right)}(x)\\cdot g^{\\left(\\beta_{k_i}\\right)}(x) + f^{\\left(\\alpha_{k_i}\\right)}(x)\\cdot g^{\\left(\\beta_{k_i}+1\\right)}(x).$$ Now, let $$\\alpha_{\\left(k+1\\right)_i} = \\begin{cases}\n\\alpha_{k_i+1}, \\quad 1 \\leq i \\leq 2^k\\\\\n\\alpha_{k_{\\left(i-2^k\\right)}}, \\quad 2^k + 1 \\leq i \\leq 2^{k+1},\n\\end{cases}$$\n$$\\beta_{\\left(k+1\\right)_i} = \\begin{cases}\n\\beta_{k_i}, \\quad 1 \\leq i \\leq 2^k\\\\\n\\beta_{k_{\\left(i-2^k\\right)}+1}, \\quad 2^k + 1 \\leq i \\leq 2^{k+1}.\n\\end{cases}$$\nThus, $$(f\\cdot g)^{(k+1)}(x) = \\sum_{i=1}^{2^{k+1}}f^{\\left(\\alpha_{(k+1)_i}\\right)}(x)\\cdot g^{\\left(\\beta_{(k+1)_i}\\right)}(x).$$\n$\\quad$(c) For $n=1$, $\\left(\\frac{f}{g}\\right)'(x) = \\frac{f'(x)g(x)+f(x)g'(x)}{g^2(x)}$. Suppose that for some $k \\in \\mathbb{N}$, $\\left(\\frac{f}{g}\\right)^{(k)}(x) = \\frac{F_k(x)}{g^{2^k}(x)}$, where $F_k$ is some infinitely differentiable function. Then, if we put $F_{k+1}(x) = F_k(x)g^{2^k}(x)-2^kg^{2^k-1}(x)g'(x)F_k(x)$, we get the desired result that $$\\left(\\frac{f}{g}\\right)^{(k+1)}(x) = \\frac{F_{k+1}(x)}{g^{2^{k+1}}(x)}.$$\n$\\quad$(d) For $n=1$, $(f \\circ g)'(x) = f'(g(x))g'(x)$. Suppose that for some $k \\in \\mathbb{N}$, $(f\\circ g)^{(k)}(x)=\\sum_{j=1}^{z_k}\\prod_{i=1}^{m_{k_j}}t_{k_{j_i}}$ for some $z_k \\in \\mathbb{N}$, $m_{k_1} and \\dots, m_{k_{z_k}} \\in \\mathbb{N}$ and where each term $t_{k_{j_i}}$ is either of the form $f^{(m)}(g(x))$ or $g^{(m)}(x)$, $m$ a non-negative integer. Then, $(f\\circ g)^{(k+1)}(x)=\\sum_{j=1}^{z_k}\\left(\\prod_{s=1}^{m_{k_j}}t_{k_{j_s}}\\right)'$. We then prove by induction\n$$\\bullet \\quad \\left(\\prod_{i=1}^{m_{k_j}}t_{k_{j_i}}\\right)' = \\sum_{i=1}^{m}\\left(\\frac{t_{k_{j_i}}'}{t_{k_{j_i}}}\\prod_{s=1}^{m_{k_j}}t_{k_{j_s}}\\right) + \\prod_{i=1}^{m}t_{k_{j_i}}\\left(\\prod_{i=m+1}^{m_{k_j}}t_{k_{j_i}}\\right)',$$ for every $m,j \\in \\mathbb{N}$ such that $j \\leq z_k$ and $m < m_{k_j}$.\nFor $m=1$, $\\left(\\prod_{i=1}^{m_{k_j}}t_{k_{j_i}}\\right)' = t_{k_{j_1}}'\\prod_{i=2}^{m_{k_j}}t_{k_{j_i}} + t_{k_{j_1}}\\left(\\prod_{i=2}^{m_{k_j}}t_{k_{j_i}}\\right)'$. Suppose that $\\bullet$ is true for some natural $m < m_{k_j}-1$. Then,\n$$\\begin{align*}\n\\left(\\prod_{i=1}^{m_{k_j}}t_{k_{j_i}}\\right)' \n& = \\sum_{i=1}^{m}\\left(\\frac{t_{k_{j_i}}'}{t_{k_{j_i}}}\\prod_{s=1}^{m_{k_j}}t_{k_{j_s}}\\right) + \\prod_{i=1}^{m}t_{k_{j_i}}\\left[t_{k_{j_{m+1}}}'\\left(\\prod_{i=m+2}^{m_{k_j}}t_{k_{j_i}}\\right) + t_{k_{j_{m+1}}}\\left(\\prod_{i=m+2}^{m_{k_j}}t_{k_{j_i}}\\right)'\\right]\\\\\n& = \\sum_{i=1}^{m+1}\\left(\\frac{t_{k_{j_i}}'}{t_{k_{j_i}}}\\prod_{s=1}^{m_{k_j}}t_{k_{j_s}}\\right) + \\prod_{i=1}^{m+1}t_{k_{j_i}}\\left(\\prod_{i=m+2}^{m_{k_j}}t_{k_{j_i}}\\right)'.\n\\end{align*}$$\nThen, if we put $m=m_{k_j}-1$ we get that\n$$\\left(\\prod_{i=1}^{m_{k_j}}t_{k_{j_i}}\\right)' = \\sum_{i=1}^{m_{k_j}}\\left(\\frac{t_{k_{j_i}}'}{t_{k_{j_i}}}\\prod_{s=1}^{m_{k_j}}t_{k_{j_s}}\\right).$$\nSince $t_{k_{j_i}}'$ is either of the form $f^{(u)}(g(x))g'(x)$ or $g^{(u)}(x)$, $u \\in \\mathbb{N}$,\n$$\n(f \\circ g)^{(k+1)}(x) = \\sum_{j=1}^{z_k}\\sum_{i=1}^{m_{k_j}}\\left(\\frac{t_{k_{j_i}}'}{t_{k_{j_i}}}\\prod_{s=1}^{m_{k_j}}t_{k_{j_s}}\\right)\n$$\ncan clearly be arranged in our desired form.\nThe main proof.\nYes, such a function which is infinitely differentiable exists. We shall construct an example. Let $E \\subseteq \\mathbb{R}$ be closed.\nDefining $f$\nFor the trivial case that $E=\\emptyset$ take $f(x)=1$.\nOtherwise, we define $f$ by 3 cases:\n$\\quad$(I) If $x \\in E$, $f(x) = 0$.\n$\\quad$(II) If $x \\in (a,b)$, where $a,b \\in E$ and $(a,b) \\subseteq \\mathbb{R} \\setminus E$, define $$F_{a,b}(x) = \\frac{\\pi}{b-a}\\left(x + \\frac{3b-5a}{2}\\right),$$\n$\\quad$and then $$f(x) = X_{a,b}\\cos^{F_{a,b}(x)}\\left(F_{a,b}(x)\\right),$$\n$\\quad$where $X_{a,b}$ is some constant real based on $a, b$, which for now is irrelevant. We shall define $X_{a,b}$ later in the proof when its value will be $\\quad$relevant.\n$\\quad$(III) If $E$ is bounded above, $M=\\sup E$, then $f(x)=e^{-\\frac{1}{(x-M)^2}}$ for $x > M$. Similarly, $f(x)=e^{-\\frac{1}{(x-N)^2}}$ if $E$ is bounded below and $\\quad N=\\inf E$.\nClearly $f$ is defined for every $x \\in E$. $f$ is also defined for every $x \\in \\mathbb{R} \\setminus E$ because such $x$ have some neighborhood in $\\mathbb{R} \\setminus E$, otherwise $x$ would be a limit point of $E$ and therefore contained in $E$.\n$E$ is the zero-set of $f$\nTo show that $E=Z(f)$, we need to show that $x \\not\\in E$ implies $f(x) \\neq 0$. For $x$ in case (II)\n$$* \\quad \\frac{3}{2}\\pi < F_{a,b}(x) < \\frac{5}{2}\\pi, $$\ntherefore $f(x)>0$. For $x$ in case (III) clearly $f(x)>0$ as well.\n$f$ is infinitely differentiable\nwe treat points of $E$ and points of $\\mathbb{R} \\setminus E$ separately.\nPoints of $\\mathbb{R} \\setminus E$: $\\quad$ For $x$ in case (II), $f(x)=X_{a,b}\\cos^{F_{a,b}(x)}\\left(F_{a,b}(x)\\right)$. To derive $f'(x)$, we first note that\n$$F_{a,b}'(x)=\\frac{\\pi}{b-a}$$ and using lemma 1\n$$\\left(\\cos^x(x)\\right)' = \\cos^x(x)\\left(\\ln\\left(\\cos x\\right)-x\\tan x\\right).$$\nWe now prove by induction that for every $n \\in \\mathbb{N}$, $f^{(n)}(x)$ exists and $f^{(n)}(x) = f(x)\\cdot f_n(x)$, where $f_n$ is some infinitely differentiable function. For $n=1$, $$f'(x) = f(x)\\left[\\ln\\left(\\cos F_{a,b}(x)\\right) - F_{a,b}(x)\\tan F_{a,b}(x)\\right]\\frac{\\pi}{b-a},$$\nand we denote $f_1(x) = \\frac{\\pi}{b-a}\\left[\\ln\\left(\\cos F_{a,b}(x)\\right) - F_{a,b}(x)\\tan F_{a,b}(x)\\right]$. From $*$ it follows that $\\cos F_{a,b}(x) > 0$, hence $\\ln\\cos F_{a,b}(x)$ is well defined and is infinitely differentiable by lemma 2(d). Then, additionally using lemmas 2(a),(b),(d) we see that $f_1$ is infinitely differentiable. Suppose that for some $k \\in \\mathbb{N}$ $f^{(k)}(x)=f(x)f_k(x)$, where $f_k$ is some infinitely differentiable function. Then,\n$$\\begin{align*}\nf^{(k+1)}(x)\n&= f'(x)f_k(x) + f(x)f_k'(x)\\\\\n&= f(x)(f_1(x)f_k(x) + f_k'(x)).\n\\end{align*}$$\nDenoting $f_{k+1}(x) = f_1(x)f_k(x)+f_k'(x)$, it's clear from lemmas 2(b),(c) that $f_{k+1}$ is infinitely differentiable.\n$\\quad$ For $x$ in case (III), suppose without loss of generality that $E$ is bounded above and $x > M$. Then, since $e^x$ and $-\\frac{1}{(x-M)^2}$ are infinitely differentiable (for $x \\neq M$) we can use lemma 2(d) to get that $f(x)$ is infinitely differentiable.\nPoints of $E$: We shall describe the area \"to the left\" and \"to the right\" of $x$. Given $d > 0$, we denote $l_d=(x-d,x)$ and $r_d=(x,x+d)$. Then, we claim that there is some $\\varepsilon_0>0$, such that at least one of the following must be true:\n$\\quad$(1)$\\quad$ $l_{\\varepsilon_0} \\subseteq E$.\n$\\quad$(2)$\\quad$ $l_{\\varepsilon_0} \\subseteq \\mathbb{R} \\setminus E$.\n$\\quad$(3)$\\quad$ For all $0<\\varepsilon\\leq \\varepsilon_0$, there's some $t \\in l_\\varepsilon$ such that $t \\in E$.\nAnd the same goes for \"the right side\" of $x$. To show that one of these options must be true, suppose by contradiction that they're all false. Then, since (3) is false, (2) is true and that's a contradiction.\n$\\quad$ We shall prove that for every non-negative integer $n$, $f^{(n)}(t) \\to 0$ as $t \\to x$. Then we use it to prove by induction that $f^{(n)}(x)=0$ and therefore $f^{(n)}(x)$ exists. Here is this induction proof:\nFor $n=0$ we already know that $f(x)=0$. Suppose that for some non-negative integer $k$, $f^{(k)}(x)=0$. Then, since $f^{(k)}(t) \\to 0$ as $t \\to x$ it follows that $f^{(k)}$ is continuous at $x$ and since we know that $f^{(k+1)}$ exists for points of $\\mathbb{R} \\setminus E$, $f^{(k)}$ is continuous. Then, since $f^{(k+1)}(t) \\to 0$ as $t \\to x$, using Ex.9 it follows that $f^{(k+1)}(x)=0$.\n$\\quad$If (1) is true, then $f$ is $0$ constant in $l_{\\varepsilon_0}$, therefore for every $n \\in \\mathbb{N}$ $f^{(n)}$ is 0 constant as well. Thus, for every non-negative integer $n$, $f^{(n)}(t) \\to 0$ as $t \\to x$ for $x \\in l_{\\varepsilon_0}$. This argument of course can be applied to $r_{\\varepsilon_0}$ as well.\n$\\quad$If (2) is true, then either $x=N=\\inf E$ or $x=b$ for some $(a,b)$ as in case (II).\nIf $x=b$, denote $w(z)=X_{a,b}\\cos^{F_{a,b}(z)}\\left(F_{a,b}(z)\\right)$, for $z \\in [a,b]$ (note that $w=f$ on $[a,b]$). Then, since $w$ is continuous on $[a,b]$ (composition of continuous functions is continuous, $\\lim_{t \\to x}f(t) = 0$, for $t \\in l_{\\varepsilon_0}$. Since $w^{(n)}(t) = w(t)\\cdot f_n(t)$ and $w^{(n+1)}(t)$ exists for all non-negative integer $n$ and $t \\in [a,b]$, it follows that $w^{(n)}$ is continuous on $[a,b]$, therefore $f^{(n)}(t) \\to w^{(n)}(x) = 0$ as $t \\to x$ for $t \\in l_{\\varepsilon_0}$.\nIf $x=N$, then $f(t)=e^{-\\frac{1}{(t-N)^2}}$, for $t \\in l_{\\varepsilon_0}$. Then, we show that $f^{(n)}(t) = f(t)\\cdot p_n(t)$, where $p_n(t)$ is some polynomial of $\\frac{1}{t-N}$. We use induction: For $n=1$, $f'(t) = f(t)\\cdot 2\\left(\\frac{1}{t-N}\\right)^3$. If for some $k \\in \\mathbb{N}$, $f^{(n)} = f(t)p_k(t)$, where $p_k(t) = \\sum_{j=0}^{i}c_j\\left(\\frac{1}{t-N}\\right)^j$, $i \\in \\mathbb{N}$ and $c_0, \\dots, c_i \\in \\mathbb{R}$. Then,\n$$\\begin{align*}\nf^{(k+1)}(t) \n&= f(t)2\\left(\\frac{1}{t-N}\\right)^3\\sum_{j=0}^{i}c_j\\left(\\frac{1}{t-N}\\right)^j + f(t)\\sum_{j=0}^{i}jc_j\\left(\\frac{1}{t-N}\\right)^{j-1}\\\\\n&= f(t)\\underbrace{\\left[\\sum_{j=0}^{i}2c_j\\left(\\frac{1}{t-N}\\right)^{j+3} + \\sum_{j=0}^{i}jc_j\\left(\\frac{1}{t-N}\\right)^{j-1}\\right]}_{\\text{clearly a polynomial of } \\frac{1}{t-N}}\n\\end{align*}$$\nNow, if we prove that $\\lim_{t \\to N}\\frac{f(t)}{|t-N|^m} = 0$, for every $m \\in \\mathbb{N}$, then clearly $f^{(n)}(t) \\to 0$ as $t \\to x$ for $t \\in l_{\\varepsilon_0}$.\n$$\n\\lim_{t \\to N}\\frac{f(t)}{|t-N|^m} = \\lim_{t \\to N}\\frac{e^{-\\frac{1}{|t-N|^2}}}{|t-N|^m} = \\lim_{h \\to 0}\\frac{e^{-\\frac{1}{h^2}}}{h^m} = \\lim_{n \\to \\infty}\\frac{n^{\\frac{m}{2}}}{e^n}.\n$$\nIf $m=2z$, for some $z \\in \\mathbb{N}$, we apply l'Hôpital's rule $z$ times and get that\n$$\\lim_{t \\to N}\\frac{f(t)}{|t-N|^m} = \\lim_{n \\to \\infty}\\frac{z!}{e^n} = 0.$$\nIf $m=2z-1$, for some $z \\in \\mathbb{N}$, we apply l'Hôpital's rule $z$ times and get that\n$$\\lim_{t \\to N}\\frac{f(t)}{|t-N|^m} = \n\\lim_{n \\to \\infty}\\frac{\\left(\\frac{m}{2}\\right)\\left(\\frac{m}{2}-1\\right)\\cdots\\frac{1}{2}}{\\sqrt{n}e^n} = 0.$$\nIn a very similar way we can prove this for $r_{\\varepsilon_0}$ when either $x=a$ or $x=M=\\sup E$.\nThe missing piece from the proof\nThe last thing left to prove which I didn't mange to prove is that $f^{(n)}(t) \\to 0$ as $t \\to x$ for points $x \\in E$ for which (3) is true. I should show how I tried to do it and what I did manage to show, that perhaps could be useful:\n$\\quad$Suppose (3) is true (but (1) is false). We should now define $X_{a,b}$. Let $n_{a,b} \\in \\mathbb{N}$ be the smallest integer such that $\\frac{1}{n_{a,b}} < b-a$. Since $f_n$ is differentiable on $[a,b]$, $f_n$ is continuous on $[a,b]$, therefore by Theorem 4.15, there's some $M_n \\in \\mathbb{R}$ such that $\\left|f_n(t)\\right| \\leq M_n$ for all $t \\in [a,b]$ (notice that $f_n$ is independent of the value of $X_{a,b}$). Similarly, there is some $M_* \\in \\mathbb{R}$ such that $\\left|\\cos^{F_{a,b}(t)}\\left(F_{a,b}(t)\\right)\\right| \\leq M_*$ for all $t \\in [a,b]$. Put $M_{a,b} = \\max \\{M_n|n \\leq n_{a,b}\\}$. Finally, we define\n$$X_{a,b} = \\frac{1}{n_{a,b}M_*M_{a,b}}.$$\nWe shall now use this definition of $X_{a,b}$ to demonstrate that for every $d > 0$, there is some $\\delta > 0$ such that if $t \\in l_\\delta$ and $t \\not\\in E$, then $|f^{(n)}(t)|< d$. Given $d > 0$, there is some $x_d \\in l_d$, such that $x_d \\in E$. Put $\\delta = x - x_d$. If $t \\in l_\\delta$ and $t \\not\\in E$, then $t \\in (a,b)$ as in (II), where $b-a \\leq \\delta$. Thus\n$$\n|f^{(n)}(t)|=\nX_{a,b}|f_n(t)|\\left|\\cos^{F_{a,b}(t)}\\left(F_{a,b}(t)\\right)\\right|\n\\leq \\frac{1}{n_{a,b}M_*M_{a,b}}M_*M_{a,b}\n= \\frac{1}{n_{a,b}}\n< b-a \\leq \\delta\n< d\n.$$\nFor points $t$ for which (1) is true we know that $f^{(n)}(t) = 0$, so there's no problem there. But the problem is points for which only (3) is true. We don't know that they are 0 (we only think they suppose to be 0), but how can we prove this? It seems a bit circular.\nWhat I'm Asking For\nI have a few questions / things I want to get in an answer to this question:\n\nGeneral assessment / critique of the incomplete proof (ignore the missing piece): I would like to get your opinion about every aspect of this proof - soundness, rigor, style, clarity and any other possible aspect you can think of.\nAnd most importantly is there some big unfixable mistake in this proof?\nDo you have a proposal to solve the missing piece? If needed feel free to change the definition of the coefficient $X_{a,b}$ as you see fit. If you think there's no way to solve this missing piece and this construction simply doesn't work, please don't suggest me a completely different solution to this exercise, I can look for solutions online, that's not a problem. Instead, please prove why this construction can't work for any choice of $X_{a,b}$.\nHow long does such a question should take to solve for a first time analysis student? Does it make sense that a single exercise would take so much time and effort, even when we talk about Rudin?\n\n",
    "proof": "Unfortunately, I think your proof will not work with your choice of function $f.$\nTo see this, consider the case $E=\\mathbb{R}\\setminus(0,1)$ so $a=0,b=1$ and on $(0,1)$ your function now becomes\n$$f(x)=\\sin(\\pi x)^{\\pi x+ 3\\pi/2}$$ (I set the constant $X_{a,b}=1,$ $f$ is infinitely differentiable if and only if $X_{a,b}^{-1}f$ is infinitely differentiable). The function vanishes on $E$ so all the left derivatives at $0$ are $0.$ Now assume towards a contradiction that $f$ is infinitely differentiable, then, by continuity, we must have $f^{(n)}(0)=0$ for all $n\\in\\mathbb{N}.$ By using Taylor expansion for the first five derivatives at $0$ we get $f(x)=o(|x|^5).$ Now observe that for some small $\\delta>0$ and $x\\in(0,\\delta)$ we get the inequalities\n$$f(x)=\\sin(\\pi x)^{\\pi x+ 3\\pi/2}\\geq (\\pi x/2)^{\\pi x+3\\pi/2}\\geq(\\pi x/2)^{4.8}$$\nwhich contradicts $f(x)=o(|x|^5).$\nThe same argument will work for arbitrary $a$ and $b,$ this is because you have constructed $F_{a,b}$ such that $F_{a,b}(a)=3 \\pi/2$ so your $f$ will only vanish to this order at $a.$\nI have not looked at where exactly the error is in your proof that $f$ is infinitely differentiable but I think your idea for $f$ can work. If you take\n$$f(x)=e^{-1/(x-a)^2}e^{-1/(x-b)^2}$$\non each interval $(a,b)$ of the complement $\\mathbb{R}\\setminus E$ you should get a smooth $f$ and the calculations should get a bit easier.\n",
    "tags": [
      "real-analysis",
      "derivatives",
      "solution-verification",
      "proof-writing",
      "smooth-functions"
    ],
    "score": 8,
    "answer_score": 2,
    "is_accepted": true,
    "question_id": 4609760,
    "answer_id": 4617938
  },
  {
    "theorem": "Purely geometric proof of inverse trigonometric functions derivatives",
    "context": "Can you compute the derivatives of $\\sin^{-1}(x),\\cos^{-1}(x),$ and $\\tan^{-1}(x)$ using only geometry?\nI know how to use geometry to find the derivatives of $\\sin x$ and $\\cos x$ like this:\n\nWe can use the fact that we know the tangent of the circle to show that $\\frac{d}{dx}\\cos(x)=\\sin(x)$.\nWondering if you can do the same with the inverse functions.\n",
    "proof": "\nGiven that $\\theta = \\arcsin y,$ by looking at the picture, we have $$\\theta + \\delta \\theta = \\arcsin (y + \\underbrace{\\delta \\theta \\cos \\theta}_{=\\delta y})$$\nHence, $\\delta y = \\delta \\theta \\cos \\theta$. Recalling that $\\cos \\theta = \\sqrt{1 - \\sin^2 \\theta } = \\sqrt{1 - y^2},$ we have\n$$\\boxed{\\delta \\arcsin y = \\frac{1}{\\sqrt{1 - y^2}}\\delta y}.$$\nSimilarly, we can see that $\\delta x = - \\delta \\theta \\sin \\theta$, so we get the corresponding result:\n$$\\boxed{\\delta \\arccos x = - \\frac{1}{\\sqrt{1 - x^2}}\\delta x}.$$\n",
    "tags": [
      "calculus",
      "geometry",
      "trigonometry",
      "proof-writing"
    ],
    "score": 8,
    "answer_score": 1,
    "is_accepted": true,
    "question_id": 3759552,
    "answer_id": 3760191
  },
  {
    "theorem": "Conditions that $\\sqrt{a+\\sqrt{b}} + \\sqrt{a-\\sqrt{b}}$ is rational",
    "context": "Motivation\nI am working on one of the questions from Hardy's Course of Pure Mathematics and was wondering if I could get some assistance on where to go next in my proof. I have attempted rearranging the expression in numerous ways from the step I am at, but seem to get no-where.\nQuestion\nIf $a^2-b>0$, then the necessary and sufficient conditions that $\\sqrt{a+\\sqrt{b}} + \\sqrt{a-\\sqrt{b}}$ is rational are $a^2-b$ and $\\dfrac{1}{2} (a+ \\sqrt{a^2-b})$ be squares of rational numbers.\nAttempt\nSuppose that $\\sqrt{a+\\sqrt{b}} + \\sqrt{a-\\sqrt{b}}$ is rational. Then it can be written as the ratio of two integers, p and q, that have no common factor. Write this as:\n$\\dfrac{p}{q}=\\sqrt{a+\\sqrt{b}} + \\sqrt{a-\\sqrt{b}}$\nThen by squaring both sides we have:\n$\\dfrac{p^2}{q^2} = (\\sqrt{a+\\sqrt{b}} + \\sqrt{a-\\sqrt{b}})^2=2a+ 2\\sqrt{a^2-b}$\n-Note sure where to go from here.\n",
    "proof": "The assertion that \"If $a^2−b>0$, then the necessary and sufficient conditions that $\\sqrt{a+\\sqrt{b}}+\\sqrt{a-\\sqrt{b}}$ is rational are $a^2−b$ and $\\frac{1}{2}(a+\\sqrt{a^2-b})$ be squares of rational numbers.\" is false. Take $a=2-\\sqrt[4]{2}, b=4-4\\sqrt[4]{2}, a^2-b=\\sqrt{2}, \\sqrt{a+\\sqrt{b}}+\\sqrt{a-\\sqrt{b}}=2$.\nI believe the assertion should read: \"If $a^2−b>0$, then the necessary and sufficient condition that $\\sqrt{a+\\sqrt{b}}+\\sqrt{a-\\sqrt{b}}$ is rational is that $\\frac{1}{2}(a+\\sqrt{a^2-b})$ is a square of a rational number.\"\n",
    "tags": [
      "proof-writing",
      "rational-numbers"
    ],
    "score": 8,
    "answer_score": 3,
    "is_accepted": true,
    "question_id": 319201,
    "answer_id": 319336
  },
  {
    "theorem": "Are these proofs correct? (Number Theory)",
    "context": "I'm finishing Chapter 1 of Apostol's book Introduction to Analytic Number Theory. I have made almost half of the 30 problems posed. I have some doubts on the proofs I produce, since sometimes I seem to assume extra information, or seem  assume things that are obvious, when that is precisely what is to be proven. I am unsure about this few, however $(3)$ and $(4)$ seem right.\n$(1)$\nTHEOREM If $(a,b)=1$ and $ab=c^n$, then $a=x^n$ and $b=y^n$ for some $x,y$. \nPROOF If $(a,b)=1$ then we have \n$$a = \\prod p_i^{a_i}$$\n$$b = \\prod p_j^{b_j}$$\nwhere $p_j \\neq p_i$ for any $i,j$. Let $c=\\prod p_m ^{c_m}$, and the $p_j$ and $p_i$ are uniquely determined. Then\n$$ab=\\prod p_i^{a_i}p_j^{b_j}=\\prod p_m ^{nc_m}$$\nBut this means, since $p_i \\neq p_j$, that\n$$a_i =nc_{m_i}$$\n$$b_j =nc_{m_j}$$\nThus\n$$a = \\prod p_i^{nc_{m_i}}=x^n$$\n$$b = \\prod p_j^{nc_{m_j}}=y^n$$\nWhat I seem to be saying is \"if $a$ and $b$ have no common prime factors and $ab=c^n$, then $a$ and $b$'s prime factors must be of multiplicity $n$. Else, $c$ wouldn't be a perfect $n$th power.\"\nApostol suggests considering $d=(a,c)$.\n$(2)$ \nTHEOREM For every $n \\geq 1$ there exist uniquely determined $a<0$, $b>0$ such that $n=a^2b$, where $b$ is squarefree.\nPROOF From the fundamental theorem of arithmetic, one has \n$$n=\\prod p_i^{a_i}$$ where the $p_i$ are unique. Group the product into two factors, according to the parity if the $a_i$s. If $a_i=2m_i$, write\n$$n=\\left(\\prod p_i^{m_i} \\right)^2 \\prod p_l^{a_l}$$\nThe remaining $a_l$ are all odd, viz $a_i=2n_i+1$. Then write\n$$n=\\left(\\prod p_i^{m_i} \\prod p_l^{n_l}\\right)^2 \\prod p_l$$\n$$n=a^2 b$$\nSince the $p_i$ were unique, so are $a^2$ and $b$, and $b$ is clearly squarefree.\n$(3)$ \nTHEOREM If $2^n-1=p$, where $p$ is prime, then $n$ is prime.\nPROOF Reductio ad absurdum.\nSuppose $2^n-1$ is prime, and write $n=qp$. Then\n$$2^n-1=2^{qp}-1=(2^q-1)(1+2^q+2^{2q}+\\cdots+2^{q(p-1)}$$\nthus $2^{q}-1\\mid 2^n-1$, $\\Rightarrow \\Leftarrow$\n$(4)$ \nTHEOREM If $2^n+1$ is prime, then $n$ is a power of two.\nPROOF Reductio ad absurdum.\nSuppose that $2^n+1=p$, $p$ a prime, and $n$ is composite\n$$n=ed$$ where $e$ is odd. Then it is clear $n \\neq 2^m$ and $$2^n+1=2^{ed}+1=(2^d+1)(1-2^d+-\\cdots+2^{d(e-1)})$$\nThus $2^d+1 \\mid 2^n+1$. $\\Rightarrow \\Leftarrow$\nThen $n$ can't have any odd factors, that is $n=2^m$ for some $m$.\nNOTE: I mostly care about the proofs being correct or not. If they aren't let me know what the flaw is, and please hint a correction. I'm not looking for alternative proofs unless the proof is absolutely hokum.\n",
    "proof": "Your proof is correct of 1 is correct; it is indeed saying that if a product of two relatively prime (positive) integers is a perfect $n$th power, then each is a pefect $n$th power. This is a generalization of a result of Euclid's which states that if $ab$ is a perfect square, and $a$ and $b$ are relatively prime, then each of $a$ and $b$ is a perfect square; this result is used to characterize Pythagorean triples in the Elements.\nI'll assume that $n\\gt 1$. To follow Apostol's hint, let $d=\\gcd(a,c)$; then $d^n|c^n = ab$, and since $\\gcd(d,b)|\\gcd(a,b) = 1$, then $d^n|a$. Since $\\gcd(a/d, c/d) = 1$, if we write $a=d^ny$ and $c=dx$, then $\\gcd(d^{n-1}y,x) = 1$. But since $y|x^n$, it follows that $y=1$, so $a=d^n$. \nNow use a symmetric argument to show to show $b$ is an $n$th power. Note that (I think) we did not use unique factorization into irreducibles, so this argument should work in any gcd-domain, not just in UFDs.\nYour proof for 2 is almost okay, but you should really specify that $n=pq$ with $1\\lt p,q\\lt n$ if you want to argue by contradiction. Then your contradiction arises from the fact that you cannot have $2^q-1 = 1$ nor $1+2^q + \\cdots + 2^{q(p-1)}=1$ (you never took this into account!). Alternatively, you can do it directly, by noting that if $n=pq$, then the factorization you give forces either $2^{q}-1=1$, or $1+2^q + \\cdots 2^{q(p-1)}=1$, hence $p-1=0$. \nYour argument for $3$ is not quite right, because we should not assume that $n$ is composite. If you want to argue by contradiction, you should only assume that $n$ is divisible by an odd number greater than $1$. And you also need to account for the possibility that the second factor in your factorization equals $1$; that is, that\n$$1-2^d + 2^{2d} -2^{3d}+ \\cdots +2^{(2k)d} = 1.$$\nThe argument is that since $2^{2r} - 2^{2r-1}$ is positive, this can only happen if $2k=0$, hence the odd factor $e = 2k+1$ must be equal to $1$. \n",
    "tags": [
      "elementary-number-theory",
      "proof-writing"
    ],
    "score": 8,
    "answer_score": 2,
    "is_accepted": true,
    "question_id": 157073,
    "answer_id": 157101
  },
  {
    "theorem": "Prove the converse of &quot;The sum of two odd consecutive numbers is a multiple of 4&quot;",
    "context": "The sum of two odd consecutive numbers is a multiple of 4. I've tried rewriting this as:\nIf $a$ and $b$ are two consecutive odd numbers, then $a+b=4p$, where $p\\in\\mathbb{N}$.\nI'm trying to prove the converse of the statement, which I think is:\nIf $z$ is a number of the form $4p,\\ p\\in\\mathbb{N}$, then $z$ can be written as the sum of two odd consecutive numbers $a$ and $b$.\nProof.\nWe write $4p$ as $2p-1+2p+1$, and we get $z=(2p-1)+(2p+1)$. If we denote $2p-1$ by $a$ and $2p+1$ by $b$, then we can notice that $a<b$ and $b-a=2$, so $a$ and $b$ are two odd consecutive numbers.\nIs the above proof complete? I've recently started studying a proof writing book, and I want to make sure that I don't assume what need to be proved or other statements that may or may not be true.\n",
    "proof": "Another nice proof is as follow,\nlet $k = p-1$,\n$$z = 4p = 4(k+1) \\\\ \\implies z = 4k + 4 = (2k+1) + (2k+3) $$\n",
    "tags": [
      "algebra-precalculus",
      "proof-writing"
    ],
    "score": 8,
    "answer_score": 1,
    "is_accepted": false,
    "question_id": 4339097,
    "answer_id": 4788590
  },
  {
    "theorem": "Neighbor fractions problem",
    "context": "I have posted an exercise below about basic proof writing with fractions and I would appreciate if you could check my work/comment on the logical structure of my proofs.\nProblem\nFractions $\\frac{a}{b}$,$\\frac{c}{d}$ are called neighbor fractions if their difference $\\frac{ad-bc}{bd}$ has numerator $±1$, that is, $ad-bc=±1$.\nProve that:\n(a) in this case neither fractions can be simplified.\n(b) if $\\frac{a}{b}$,$\\frac{c}{d}$ are neighboring fractions, then $\\frac{a+c}{b+d}$ is between them and is a neighboring fraction for both $\\frac{a}{b}$ and $\\frac{c}{d}$.\n(c) no fraction $\\frac{e}{f}$ with positive integer $e$ and $f$ such that $f<b+d$ is between $\\frac{a}{b}$ and $\\frac{c}{d}$.\nProof of (a):\nAssume that $\\frac{a}{b}$ can be simplified. Then there exist $k,p,q\\not= 0\\inℤ$ s.t. $a=kp$ and $b=kq$. Substituting in for $a$ and $b$ we obtain, $kpd-kqc=±1$. Factoring out $k$ we get $k(pd-qc)=±1$. Now,  $k\\not=0$, and the product with $k\\not=0$, so $(pd-qc)\\not=0$. Therefore, k=$±1/(pd-qc)$ with $(pd-qc)\\in ℤ$. So $k\\notinℤ$, a contradiction. An analogous argument shows for $\\frac{c}{d}$.      $\\blacksquare$\nProof of (b):\nWithout loss of generality, assume $\\frac{a}{b}<\\frac{c}{d}$. It follows that $ad<bc$. Adding $dc$ to both sides we get $ad+dc<bc+dc$, and then factoring out $d$ from LHS and $c$ from RHS we get $d(a+c)<c(b+d)$ and so $\\frac{a+c}{b+d}<\\frac{c}{d}$. Likewise, adding $ab$ to both sides we get $ad+ab<bc+ab$, and then factoring out $a$ from LHS and $b$ from RHS we get $a(b+d)<b(c+a)$ and so $\\frac{a}{b}<\\frac{a+c}{b+d}$.\nBy the definition of neighboring fractions, we need to show $a(b+d)-b(a+c)=±1$ and $d(a+c)-c(b+d)=±1$. Because $\\frac{a}{b}$,$\\frac{c}{d}$ are neighboring fractions, we get $ad-bc=±1$. It follows that $-bc=±1-ad$ and $ad=±1+bc$.\n$a(b+d)-b(a+c)=ab+ad-ba-bc=ad-bc=±1+bc-bc=±1$\n$d(a+c)-c(b+d)=da+dc-cb-cd=da-cb=da±1-ad=±1$ $\\blacksquare$\nProof of (c):\nHow would you show part (c)?\n",
    "proof": "Your proof of (a): I'll ignore the typos for now, and focus on the maths. You need to assume $k\\neq \\pm 1$ as well (otherwise, you're not simplifying at all). Then, (WLOG that $ad-bc = 1$ and $k=1$) the contradiction comes from that $k$ is forced to be an integer between $0$ and $1$.\nYour proof of (b): Both parts look fine.\nFor part c: Are you assuming that $\\frac{a}{b}$ and $\\frac{c}{d}$ are neighbor fractions? Otherwise, there are counterexamples such as $\\frac12 < \\frac35 < \\frac34$, for example.\nGood work on writing this up. You've asked a good question.\n",
    "tags": [
      "proof-writing",
      "solution-verification",
      "fractions"
    ],
    "score": 8,
    "answer_score": 1,
    "is_accepted": true,
    "question_id": 4033628,
    "answer_id": 4036227
  },
  {
    "theorem": "Need help proving an interval $\\frac {1} {ek} \\le \\frac {1}{k} (1 - \\frac {1}{k} )^{k-1} \\le \\frac {1}{2k}$",
    "context": "I am trying to proof\n$$\\frac {1} {ek}  \\le  \\frac {1}{k}   (1 - \\frac {1}{k} )^{k-1} \\le \\frac {1}{2k} $$ \nfor k>=2\nto prove this I first multiply by k getting\n$$\\frac {1} {e}  \\le  \\left(1 - \\frac {1}{k} \\right)^{k-1} \\le \\frac {1}{2} $$ \nthen use case $k=2$ as a base case\n$$\\frac {1} {e}  <=  \\frac {1}{2}  <= \\frac {1}{2} $$ \nwhich is good, then assumed \n$$\\frac {1} {e}  <=  (1 - \\frac {1}{k} )^{k-1}  <= \\frac {1}{2} $$  \nto be true for any k>2 and try to prove for k+1\nso I sustitute k+1 on k getting\n$$\\frac {1} {e}  <=  (1 - \\frac {1}{k+1} )^{k}  <= \\frac {1}{2} $$ \nwhich equals\n$$\\frac {1} {e}  <=  (\\frac {k}{k+1} )^{k}  <= \\frac {1}{2} $$ \nso I am trying to get $$ (\\frac {k}{k+1} )^{k}  $$ to any of the original formulas to finish the prove but I have been unsuccesful. I have devoted a lot of time to it and dont see the solution, if anyone does thanks in aadvance.\nif you see any other choice that is easy to prove this pls let me know because I dont have to do it by induction\n",
    "proof": "Your last step is already very close to the solution:\n$$\\left( \\frac{k}{1+k} \\right)^k=\\left( \\frac{1}{1+\\frac{1}{k}} \\right)^k= \\frac{1}{\\left(1+\\frac{1}{k}\\right)^k} $$\nUse the very well known limit and bounds for the number $e$ (for $k>1$):\n$$2<\\left(1+\\frac{1}{k}\\right)^k<e<\\left(1+\\frac{1}{k}\\right)^{k+1}<4$$\nThe values on the left and right are for $k=1$, you can prove by induction that the sequence on the left is increasing while the sequence on the right side is decreasing.\n$$\\frac{1}{2}>\\frac{1}{\\left(1+\\frac{1}{k}\\right)^k}>\\frac{1}{e}$$\nI used $>$ and $<$ everywhere, since the equality on the right side would only appear for $k \\to \\infty$\n",
    "tags": [
      "calculus",
      "algebra-precalculus",
      "proof-verification",
      "proof-writing"
    ],
    "score": 8,
    "answer_score": 1,
    "is_accepted": true,
    "question_id": 1168623,
    "answer_id": 1706208
  },
  {
    "theorem": "About Theorem 3.4 Hartshorne: detailed proof.",
    "context": "I propose a detailed version of part of the proof of Theorem 3.14 from Hartshorne's book Algebraic Geometry. The questions are inserted from time to time within the proof. Thanks for your patience.\nNotation Let $Y\\subseteq\\mathbb{P}^n$ be a projective variety. Letting $S=k[x_0,\\dots, x_n]$ (equipped with the usual grading $S=\\oplus_{r\\ge 0} S_r$), there is a naturale grading on $S(Y)=S/I(Y)$ which is constructed as follows: for each integer $r\\ge 0$ define $$S(Y)_r:=S_r/I(Y)_r:=\\{f+I(Y)_r\\;:\\; f\\in S_r\\}$$ where $I(Y)_r=I(Y)\\cap S_r.$\n\nTheorem. Let $Y\\subseteq\\mathbb{P}^n$ be a projective variety with homogeneous coordinate ring $S(Y)$. Then we have:\n$(a)\\quad$ for any point $P\\in Y$, let $\\mathcal{m}_P\\subseteq S(Y)$ denote the ideal generated by the set of all homogeneous elements $f\\in S(Y)$ such that $f(P)=0$. Then $\\mathcal{O}_P=S(Y)_{(\\mathcal{m}_P)}$;\n$(b)\\quad$ $K(Y)\\cong S(Y)_{((0))}$\n\nProof. Let $U_i\\subseteq\\mathbb{P}^n$ be the open set defined by $x_i\\ne 0$ and set $Y_i:= Y\\cap U_i$. We may consider $Y_i$ as an affine variety. We can construct a natural isomorphism $$\\varphi_i^*\\colon A(Y_i)\\to S(Y)_{(x_i)}$$ as $$\\varphi_i^*\\bigg(f(y_1,\\dots, y_{i-i},1, y_i, \\dots, y_n)+I(Y_i)\\bigg)=\\frac{f(x_0,\\dots, x_n)+I(Y)_r}{x_i^r+I(Y)_r},$$ where $f\\in S_r$.\n$(b)\\quad$ Let $P\\in Y$ and chose any $i$ so that the $i-$th coordinate of $P$ is non-zero. Then $P\\in Y\\cap U_i$. We may write $P=[a_0:\\dots: a_{i-1}:1:a_i:\\dots: a_n]$. Set $P_0:=(a_0,\\dots, a_{i-1}, a_{i+1},\\dots, a_n)=\\varphi_i(P)$, the corresponding point in the affine space.\nDenote with $\\mathcal{m'}_{P_0}$ the maximal ideal of $A(Y_i)$ corresponding to $P_0$. More precisely, $$\\mathcal{m'}_{P_0}=\\bigg\\{g+I(Y_i)\\in A(Y_i)\\;:\\; g(P_0)=0\\bigg\\}.$$ It's easy to prove that: $$\\boxed{\\varphi_i^*(\\mathcal{m'}_{P_0})=\\mathcal{m}_P\\cdot S(Y)_{(x_i)}}$$ Recall that the product in the right of the above equation is interpreted in the localization as follows (with the natural induced grading on the quotient ring $S(Y)=S/I(Y))$\n$$\\mathcal{m}_p\\cdot S(Y)_{(x_i)}=\\bigg\\{\\frac{f(x_0\\dots, x_n)+I(Y)_r}{x_i^r+I(Y)_r}\\;:\\; f+I(Y)_r\\in\\mathcal{m}_P\\cap S(Y)_r\\;\\text{for same}\\; r\\ge 0\\bigg\\}.$$\nUsing this isomorphism, we then have the isomorphism of the localizations $$A(Y_i)_{\\mathcal{m'}_{P_0}}\\cong \\big(S(Y)_{(x_i)}\\big)_{\\mathcal{m}_P\\cdot S(Y)_{(x_i)}}$$\n\nQuestion 1. I believe that the last isomorphism can be obtained from exercise number 4 on page 44 of Introduction to Commutative Algebra by Atiyay - MacDonald, is that so?\n\nWe need to argue that $$\\big(S(Y)_{(x_i)}\\big)_{\\mathcal{m}_P\\cdot S(Y)_{(x_i)}}\\cong S(Y)_{(\\mathcal{m}_P)}\\tag1$$\n\nQuestion2. Why $(1)$ is not usual transitivity of localizations?\n\nA typical element in $\\big(S(Y)_{(x_i)}\\big)_{\\mathcal{m}_P\\cdot S(Y)_{(x_i)}}$ is of the form\n$$\\frac{\\frac{f+I(Y)_{s+r}}{x_i^{r+s}+I(Y)_{s+r}}}{\\frac{g+I(Y)_r}{x_i^r+I(Y)_r}}\\equiv \\frac{f+I(Y)_{s+r}}{x_i^sg+I(Y)_{s+r}}$$ where $g+I(Y)_r\\notin\\mathcal{m}_p\\cap S(Y)_r$, and the identification takes place in the quotient field of $S(Y)$. Now since $x_i^s+I(Y)_s\\notin \\mathcal{m}_p\\cap S(Y)_s$ it follows that the product $x_i^sg +I(Y)_{s+r}\\notin\\mathcal{m}_p\\cap S(Y)_{s+r}$. So the element on the right is a degree $0$ element of the localization\n$$S(Y)_{\\mathcal{m}_P}=\\bigoplus_{n\\in\\mathbb{Z}}\\big(S(Y)_{\\mathcal{m}_P}\\big)_n$$ where\n$$\\big(S(Y)_{\\mathcal{m}_P}\\big)_n=\\bigg\\{\\frac{p+I(Y)_{n+r}}{q+I(Y)_r}\\;:\\; q+I(Y)_r\\notin \\mathcal{m}_P\\cap S(Y)_r\\;\\text{for same}\\; r\\ge 0\\bigg\\}.$$ This proves that $$A(Y_i)_{\\mathcal{m'}_{P_0}}\\cong (S(Y)_{\\mathcal{m}_p})_0=S(Y)_{(\\mathcal{m}_P)}$$\n$(b)\\quad$ Observe that $K(Y)\\cong K(Y_i)$. The final part is to show that $$K(Y_i)\\cong S(Y)_{((0))}.$$ We have already obtained the isomorphism $\\varphi_i^*\\colon A(Y_i)\\to S(Y)_{(x_i)}$. Extending this isomorpgism to $\\tilde{\\varphi_i^{*}}$ on the quotient fields fo both the sides we get $$\\tilde{\\varphi_i^*}\\bigg(\\frac{f(y_1,\\dots, y_{i-1},1, y_i, \\dots, y_n)+I(Y_i)}{g(y_1,\\dots, y_{i-1},1, y_i, \\dots, y_n)+I(Y_i)}\\bigg)=\\frac{\\frac{f(x_0,\\dots, x_n)+I(Y)_r}{x_i^r+I(Y)_r}}{\\frac{g(x_0,\\dots, x_n)+I(Y)_s}{x_i^s+I(Y)_s}}=\\frac{x_i^s f(x_0,\\dots, x_n)+I(Y)_{r+s}}{x_i^rg(x_0, \\dots, x_n)+I(Y)_{r+s}}$$\nIn the denominator of the last expression above we have $g(x_0,\\dots, x_n)+I(Y)_s\\ne 0$ So to make sense of this, we need $x_i^r+I(Y)_r\\ne 0$ as well. Since $I(Y)$ is a homogeneous prime ideal, this is violated only if $x_i\\in I(Y)$. I have proved that $x_i\\in I(Y)$ is equivalent to $Y_i=\\emptyset$, which is absurd according to our choise of $U_i$.\nFinally, notice that the element in the image of $\\tilde{\\varphi_i^*}$ in above are elements of $S(Y)_{((0))}$.\n\nQuestion 3 It remains to show that every element of $S(Y)_{((0))}$ is also an image of an element of $K(Y_i)$. How can I show this?\n\nMy Solution for question 3.\nA typical element in $S(Y)_{((0))}$ is of the form $$\\frac{f(x_0,\\dots, x_n)+I(Y)_r}{g(x_0,\\dots, x_n)+I(Y)_r},$$ where $f+I(Y)_r\\in S(Y)_r$ and $g+I(Y)_r\\notin S(Y)_r\\cap (0)$. Let us consider the dehomogenized of $f$ and $g$: $$f(y_1,\\dots, y_{i-1}, 1, y_i,\\dots, y_n)\\quad\\text{and}\\quad g(y_1,\\dots, y_{i-1}, 1, y_i,\\dots, y_n),$$ where $\\deg f(y_1,\\dots, y_{i-1}, 1, y_i,\\dots, y_n)= \\deg g(y_1,\\dots, y_{i-1}, 1, y_i,\\dots, y_n) = r$, results\n$$\\begin{split}\\tilde{\\varphi_i^*}\\bigg(\\frac{f(y_1,\\dots, y_{i-1},1, y_i, \\dots, y_n)+I(Y_i)}{g(y_1,\\dots, y_{i-1},1, y_i, \\dots, y_n)+I(Y_i)}\\bigg)=&\\frac{\\frac{f(x_0,\\dots, x_n)+I(Y)_r}{x_i^r+I(Y)_r}}{\\frac{g(x_0,\\dots, x_n)+I(Y)_r}{x_i^r+I(Y)_r}}\\\\\n=\\frac{f(x_0,\\dots, x_n)+I(Y)_{r}}{g(x_0, \\dots, x_n)+I(Y)_{r}}\\end{split}$$\nThe above happens just in case $x_i\\nmid f$ and $x_i \\nmid g$.\nSuppose now that $x_i\\mid f(x_0,\\dots, x_n)$ and that $x_i\\nmid g(x_0,\\dots, x_n)$\nWe denote by $h(x_0,\\dots, x_n)$ the homogenized polynomial of $f(y_1,\\dots, y_{i-1},1,\\,y_i,\\dots, y_n)$, then $h\\in S_{\\deg f}$, then\n$$\n\\begin{split}\n\\tilde{\\varphi_i^*}\\bigg(\\frac{f(y_1,\\dots, y_{i-1},1, y_i, \\dots, y_n)+I(Y_i)}{g(y_1,\\dots, y_{i-1},1, y_i, \\dots, y_n)+I(Y_i)}\\bigg)=\n\\frac{\\frac{h(x_0,\\dots, x_n)+I(Y)_{\\deg f}}{x_i^{\\deg f}+I(Y)_{\\deg f}}}{\\frac{g(x_0,\\dots, x_n)+I(Y)_r}{x_i^r+I(Y)_r}}=&\\frac{x_i^{r-\\deg f} h(x_0,\\dots, x_n)+I(Y)_{r}}{g(x_0, \\dots, x_n)+I(Y)_{r}}\\\\\n=&\\frac{f(x_0,\\dots, x_n)+I(Y)_r}{g(x_0,\\dots, x_n)+I(Y)_r}\n\\end{split}\n$$\n",
    "proof": "Question 1: You can directly decude this isomorphism using Corollary 3.2 in Atyiah, MacDonald:\nIn general, let $A$ be a ring and let $S$ be a multiplicatively closed subset of $A$. Then the localization $S^{-1}A$ and the homomorhpism $f:A\\to S^{-1}A: x\\mapsto x/1$ have the following properties:\n\n$s\\in S\\Rightarrow f(s)$ is a unit in $S^{-1}A$.\n\n$f(a)=0 \\Rightarrow as=0$ for some $s\\in S$.\n\nEvery element of $S^{-1}A$ is of the form $f(a)f(s)^{-1}$ for some $a\\in A$ and some $s\\in S$.\n\n\nFurther, Corollary 3.2 states that if $g:A\\to B$ is a ring homomorhism with this three properties, then $B$ is isomorphic to $S^{-1}A$.\nNow let $h:S(Y)_{(x_i)}\\to \\big( S(Y)_{(x_i)} \\big)_{\\mathfrak{m}_P\\cdot S(Y)_{(x_i)}}: h(x)=x/1$ with the properties from above. Further, let $S=A-\\mathfrak{m'_p}$. Then\n$$ h\\circ\\varphi^*_i: A(Y_i) \\to \\big( S(Y)_{(x_i)} \\big)_{\\mathfrak{m}_P\\cdot S(Y)_{(x_i)}}$$\nfulfills the properties from above. A the proof is simple when using that $\\varphi^*_i$ is an isomorphism and that $h$ fulfills these properties. Consequently, by the corollary,\n$$ \\big( S(Y)_{(x_i)} \\big)_{\\mathfrak{m}_P\\cdot S(Y)_{(x_i)}} \\cong A(Y_i)_{\\mathfrak{m}'_P}.$$\nQuestion 2:\nThis should precisely be the transitivity of the localization, see also this question.\nQuestion 3:\nNow our construction so far does not make use of the fact that $\\mathfrak{m}_P$ is a maximal ideal. Indeed, it suffices that it is prime. Therefore, for question 3 we can just consider the prime ideal $(0)$ as $A(Y_i)$ is a domain. $\\varphi^*_i$ maps this to $(0)\\subseteq S(Y)$, thus localizing and taking the degree $0$ elements yields this isomorphism, since $K(Y_i)=A(Y_i)_{(0)}$.\n",
    "tags": [
      "algebraic-geometry",
      "proof-writing",
      "solution-verification",
      "proof-explanation"
    ],
    "score": 8,
    "answer_score": 0,
    "is_accepted": false,
    "question_id": 4209122,
    "answer_id": 4978036
  },
  {
    "theorem": "Can a product of a number and its reverse consist of only $1$&#39;s?",
    "context": "Problem:\n\nLet $n \\gt 1$. If you write the digits of $n$ in reverse, then multiply by original $n$, is it possible for the product to consist only of $1$'s?\n\nThis came from a competition I recently did, and I found this question quite interesting. Below is the proof I submitted. It's a little tedious in the middle, so feel free to correct any errors.\nProof: Let $\\bullet n$ denote reversed $n$.\nAssume $n$ has $k$-digits. Then \n$$n = 10^{k-1}a_1+10^{k-2}a_2 + \\dots + 10a_{k-1}+a_k$$\nwhere $0 \\le a \\le 9$, $a \\in \\mathbb Z$, and $a$ is the digit. $\\bullet n$ multiplied by $n$:\n\\begin{align}\n& \\bullet n \\cdot n \\\\\n& = (10^{k-1}a_k + 10^{k-2}a_{k-1} + \\dots)(10^{k-1}a_1+10^{k-2}a_2 + \\dots) \\\\\n& = 10^{2k-2}a_ka_1 + 10^{2k-3}a_ka_2 + 10^{2k-3}a_{k-1}a_1+\\dots+10^{2k-k}a_1^2+10^{2k-k}a_2^2+\\dots \\\\\n& = 10^{2k-2}(a_ka_1) + 10^{2k-3}(a_ka_2 + a_{k-1}a_1)+\\dots + 10^{k}(a_1^2 + a_2^2 + \\dots ) + \\dots \\\\\n\\end{align}\nWe now have the digits of the product of $\\bullet n$ and $n$. Equate all digits to $1$:\n\\begin{align}\n1 & = a_ka_1 \\tag{1} \\\\\n1 & = a_ka_2 + a_{k-1}a_1 \\tag{2} \\\\\n1 & = a_1^2 + a_2^2 + \\dots + a_{k-1}^2 + a_k^2 \\tag{3} \\\\\n\\end{align}\nObserve $(1)$. $a_ka_1 = 1 \\implies a_k = a_1 = 1$. Now observe $(3)$. We have:\n\\begin{align}\na_1^2 + a_2^2 + \\dots + a_{k-1}^2 + a_k^2 & = 1 \\tag{3} \\\\\n1^2 + a_2^2 + \\dots + a_{k-1}^2 + 1^2 & = 1 \\\\\na_2^2 + a_3^2 + \\dots + a_{k-2}^2 + a_{k-1}^2 & = -1 \\\\\n\\end{align}\nThe sum of the squares of real, positive integers cannot be a negative number. Hence, we have a contradiction. By reductio ad absurdum, we have proved that the product of $n$ and $\\bullet n$ cannot consist only of $1$'s. $\\Box$\n",
    "proof": "Let us assume that $n$ has $m+1$ digits: $n=\\sum_{i=0}^{m}a_i10^{i}$, where $m\\geq 1$. The product $P$ is given as follows:\n\\begin{align}\nP&= \\sum_{i=0}^{m}a_010^{i}\\sum_{j=0}^{m}a_{m-j}10^{j} = \\sum_{i,j=0}^m a_{i}a_{m-j}10^{i+j} \\\\\n&= a_0a_m + 10(a_0a_{m-1}+a_1a_m)+\\ldots+ 10^{2m-1}(a_0a_{m-1}+a_1a_m)+10^{2m}a_0a_m \\hspace{3cm} (1)\n\\end{align}\nWe first note that the last digit of $P$ is same as the last digit of $a_0a_m$. So we have $(a_0,a_m)$ as either $(1,1),(3,7),(7,3)$, or $(9,9)$.\nNext, we look at the range of $P$. Since $n$ has $m+1$ digits, we get the following:\n\\begin{equation}\n10^{m}+1\\leq n\\leq 10^{m+1}-1\\implies (10^{m}+1)^2\\leq P\\leq (10^{m+1}-1)^2 \\implies 10^{2m}<P<10^{2m+2}.\n\\end{equation}\nThus, the number of digits in $P$ is either $2m+1$ or $2m+2$. So, we get\n\\begin{equation}\nP = \\sum_{k=0}^{2m}10^k=\\frac{10^{2m+1}-1}{9}, \\text{ or } P = \\sum_{k=0}^{2m+1}10^k=\\frac{10^{2m+2}-1}{9}.\n\\end{equation}\nAlso, (1) gives that\n\\begin{equation}\n(10^{2m}+1)a_0a_m \\leq P,\n\\end{equation}\nwhere $(a_0,a_m)$ can be either $(1,1),(3,7),(7,3),$ or $(9,9)$\nHence, we have two cases corresponding to the two values of $P$:\n\\begin{align*}\n\\begin{cases}\n(10^{2m}+1)a_0a_m \\leq \\frac{10^{2m+1}-1}{9} \\implies 9a_0a_m+1\\leq 10^{2m}(10-9a_0a_m)\\implies (a_0,a_m)=(1,1)\\\\\n(10^{2m}+1)a_0a_m \\leq \\frac{10^{2m+2}-1}{9} \\implies 9a_0a_m+1\\leq 10^{2m}(100-9a_0a_m)\\implies (a_0,a_m)=(1,1).\n\\end{cases}\n\\end{align*}\nFurther, when $a_0,a_m=(1,1)$, we get that\n\\begin{equation}\nP\\leq (1\\underset{m-1 \\text{ times}}{\\underbrace{99\\ldots99}}1)^2 \\leq (2.10^m)^2=4.10^{2m} < \\frac{10^{2m+2}-1}{9}.\n\\end{equation}\nThus, we have only possible value for $P=\\frac{10^{2m+1}-1}{9}$.\nSo, the problem reduces to finding integers $a_1,a_2,\\ldots,a_{m-1}$ such that\n\\begin{align}\n1 + 10(a_{m-1}+a_1)+\\ldots+ 10^{2m-1}(a_{m-1}+a_1)+10^{2m} &= \\frac{10^{2m+1}-1}{9}\\\\\n\\implies  (a_{m-1}+a_1)+\\ldots+ 10^{2m-2}(a_{m-1}+a_1) &= \\frac{10^{2m}-1}{9}\n\\end{align}\nNext, we look at the last digit of $\\frac{10^{2m}-1}{9}$, which is same as the digit of $a_{m-1}+a_1$. Since $0\\leq a_{m-1}+a_{1}\\leq 18$, we get that $a_{m-1}+a_{1}=1,11$. \nMoreover, we have\n\\begin{equation*}\n\\frac{10^{2m}-1}{9}\\geq  (a_{m-1}+a_1)+ 10^{2m-2}(a_{m-1}+a_1)\\implies (a_{m-1}+a_1) \\geq \\frac{10^{2m}-1}{9(1+10^{2m-2})}.\n\\end{equation*}\nHowever, $\\frac{10^{2m}-1}{9(1+10^{2m-2})}> 11$, for $m>2$. Hence, $m=1,2$.\nWhen $m=1$, $(a_0,a_m)=(1,1)$ implies that $n=11$, which is not a solution.\nSimilarly, when $m=2$, $(a_0,a_m)=(1,1)$ and $a_1+a_{m-1}=1,11$ implies that $n=111$, which is also not a solution.\nHence, we conclude that there is no solution to the problem\n",
    "tags": [
      "algebra-precalculus",
      "proof-verification",
      "proof-writing"
    ],
    "score": 8,
    "answer_score": 0,
    "is_accepted": false,
    "question_id": 2570067,
    "answer_id": 3216819
  },
  {
    "theorem": "Proving that $\\{x\\in\\Bbb{R}\\mid 1+x+x^2 = 0\\} = \\varnothing$ without the quadratic formula and without calculus",
    "context": "I'm asked to prove that $\\{x\\in\\Bbb{R}\\mid 1+x+x^2 = 0\\} = \\varnothing$  in an algebra textbook.\nThe formula for the real roots of a second degree polynomial is not introduced yet. And the book is written without assuming any prior calculus knowledge so I can't  prove this by finding the minimum and the limits as x approaches $\\infty \\text{ and} -\\infty  $. \nSo there has to be a simple algebraic proof involving neither the quadratic formula nor calculus but I'm stuck. \nHere are some things I thought:\nMethod 1:\n\n$1+x+x^2 = 0 \\iff 1+x+x^2+x = x$\n$\\iff x^2+2x+1 = x$\n$\\iff (x+1)^2 = x $ \nAnd here maybe prove that there is no x such that $(x+1)^2 = x$ ???\n\nMethod 2:\n\n$1+x+x^2 = 0$\n$\\iff x^2+1 = -x$\nBy the trichotomy law only one of these propositions hold: $x=0$ or\n  $x>0$ or $x<0$.\nAssuming $x=0$:\n$x^2+1= 0^2+1 = 0 +1 = 1$\n$-x = - 0 = 0$\nAnd $1\\neq 0$\nAssuming $x>0$:\n$x>0 \\implies -x < 0$\nAnd $x^2+1 \\ge 1 \\text{  } \\forall x$\nWith this method I have trouble proving the case $x<0$:\nI thought maybe something like this could help but I'm not sure:\n$x<0 \\implies -x=|x|$ \n$x^2 = |x|^2$\nAnd then prove that there is no x such that $|x|^2 + 1 = |x|$??\n\nCan anyone please help me? Remember: No calculus or quadratic formula allowed.\n",
    "proof": "Clearly, if $x\\ge 0$ then $x^2+x+1\\ge 1>0$. And if $x<0$, then $x^2+x+1>x^2+2x+1=(x+1)^2\\ge 0$\n",
    "tags": [
      "algebra-precalculus",
      "proof-writing",
      "arithmetic",
      "quadratics"
    ],
    "score": 7,
    "answer_score": 14,
    "is_accepted": true,
    "question_id": 2410300,
    "answer_id": 2410305
  },
  {
    "theorem": "Any reasons why the basis case can&#39;t be at the end of a mathematical induction proof?",
    "context": "When doing a proof by mathematical induction, I was wondering if there is any logical reason why the assumption (n=k) and induction (n=k+1) steps couldn't be done first, then do the basis case (n=1) afterwards, rather than the traditional way of doing this basis case first? I am teaching this to my students and I think it would make more sense to them to first show that P(k+1) is true if P(k) is true, and then show that P(1) is true, then P(2) must be true, P(3) is true, etc. and P(n) is true for all n. Anyway, I can't see any flaws in the logic? Please advise.\n",
    "proof": "Formally, there is no requirement to prove P(1) first. Practically, though, verifying P(1) first can save a lot of aggravation in cases where the inductive step works, but the base step turns out to not hold true. Try to prove that $2n+1$ is even for $\\forall n \\in \\mathbb{N}$ by induction, for example: the inductive step certainly works, but the proposition is false since the base case $2 \\cdot 1 + 1$ turns out to be odd - and you'd only realize that at the very end if you were to check P(1) last.\n(And then, there are those funny cases like All horses are the same color where the fallacy falls squarely in between the base step and the proper induction step.)\n",
    "tags": [
      "proof-writing",
      "induction"
    ],
    "score": 7,
    "answer_score": 29,
    "is_accepted": false,
    "question_id": 2421129,
    "answer_id": 2421149
  },
  {
    "theorem": "Is an argument valid if assuming its premises and conclusion leads to no contradiction?",
    "context": "To show that an argument is valid, why can we not assume that both its premises and conclusion are true then show that there's no contradiction?\n\nExample:\nIf $x^2=4$ and $x\\neq-2,$ then $x=2.$\nProof\nSuppose that $x=2$ (assuming the conclusion).\nThen $x^2=4$ and $x\\neq-2.$\nThus, if $x^2=4$ and $x\\neq-2$, then $x=2.$\n\n\nWith proof by contradiction, it is legitimate to assume that the conclusion is false; why?\n",
    "proof": "\nIf $x^2=4$ and $x\\neq-2,$ then $x=2.$\nProof\n\nSuppose that $x=2$ (assuming the conclusion).\n\nThen $x^2=4$ and $x\\neq-2.$\n\nThus, if $x^2=4$ and $x\\neq-2$, then $x=2.$\n\n\n\nIn your first two steps, you proved the given statement's converse $$C\\implies A \\land B.$$ Then I think you implicitly inferred that $$\\color\\red {C\\implies} C \\land A\\land B,$$ though you didn't make it explicit. Then you implicitly inferred that $$\\color\\red {C\\implies} \\Big(A \\land B\\implies C\\Big).\\tag0$$ So far, so good. Unfortunately, you carelessly wrote statement $(0)$ just as $$A \\land B\\implies C,\\tag3$$ making Step 3 an invalid inference! You'd neglected to discharge the supposition $\\color\\red C$ that was made in Step 1.\nIn other words, you have not validly deduced the required statement $(3);$ you have validly deduced statement $(0),$ which says that if $\\color\\red C$ (i.e., the supposition $\\color\\red {x=2}$ that you made in Step 1) is actually true then statement $(3)$ is true. Unfortunately, statement $(0)$ is not useful information, because it is a tautology: no knowledge of mathematics or elementary algebra is required to validly deduce statement $(0),$ which can be proven simply using a truth table.\nBegging the question is the fallacy of arguing by assuming the conclusion. In your Step 1, assuming the consequent x = 2 of the given statement amounts to assuming that the statement is true (notice that if B is true then A→B is automatically true), that is, assuming the conclusion of the proof/argument; tautology $(0)$ exhibits the structure of this fallacy.\n\nTo show that an argument is valid, why can we not assume that both its\npremises and conclusion are true then show that there's no\ncontradiction?\n\nIf the argument's premise $P$ and conclusion $C$ are both true, then by definition they don't contradict each other (i.e., $P\\not\\equiv\\lnot C$).\nWhen you say \"show that there's no contradiction\" (if you in fact mean \"...leads to no contradiction\" then how would you actually do that??), do you perhaps instead mean the weaker condition $\\text“P_1,\\ldots,P_n,C$ are consistent with one another”, which is an immediate consequence of $(P_1\\land\\ldots \\land P_n \\land C)$ being true? In other words, are you perhaps just meaning to ask this simpler question:\n\ndoes its premises and conclusion being consistent with one another imply that the argument is valid?\ndoes showing that $(P_1\\land\\ldots \\land P_n \\land C)$ is true (this implies that $P_1,\\ldots,P_n,C$ are consistent with one another) prove that $(P_1\\land\\ldots \\land P_n \\to C)$ is a tautology?\n\nThe answer is No: in real analysis,\n\n$7>4;$ therefore, every squared number is nonnegative\n\nhas premise and conclusion both true yet is an invalid argument, because the corresponding conditional is not a tautology, because in complex analysis this conditional is actually false.\n\nWith proof by contradiction, it is legitimate to assume that the\nconclusion is false; why?\n\nBecause this assumption is merely provisional and is discharged by the end of the proof. On the other hand, your proof attempt does not eventually discharge the $\\color\\red{\\text{assumption}}$ that $\\color\\red C$ is true. (The point is that when we make a supposition/assumption, we need to keep track of it and remember to also consider the case in which it is actually false.)\nP.S. To say that the argument $(P_1\\land\\ldots \\land P_n,\\,\\therefore C)$ is valid is to say that its premise forces the conclusion to be true even in an alien world/context; remember, we are investigating the logical truth of $(P_1\\land\\ldots \\land P_n \\to C)$ regardless of the truth value of \"every squared number is nonnegative\" in any particular world.\n",
    "tags": [
      "logic",
      "proof-writing"
    ],
    "score": 7,
    "answer_score": 3,
    "is_accepted": true,
    "question_id": 4697979,
    "answer_id": 4700239
  },
  {
    "theorem": "How rigorous must my set theory proof be?",
    "context": "I'm working on a problem from the book \"Introduction to Topology\" by Bert Mendelson:\n\nIf $A_1\\subset A_2, A_2\\subset A_3, \\ldots , A_{n-1}\\subset A_n$, and $A_n \\subset A_1$, prove that $A_1=A_2=\\cdots=A_n$.\n\nI know how to prove this, but my question is how rigorous my proof should be. For example, to make my proof easier, I proved the following \"lemma\":\n\nIf $H$ and $J$ are sets, $H \\subset J$, and $J\\subset H$, then $H=J$.\n\nMy proof went like this:\n\nFrom the givens, we can determine that\n  $$\\alpha \\in J, \\forall \\alpha \\in H$$\n  $$\\beta \\in H, \\forall \\beta \\in J$$\n  Which means that\n  $$\\alpha \\in J, \\forall \\alpha \\in H$$\n  $$\\neg \\beta \\notin H, \\forall \\beta \\in J$$\n  and so $H=J$.\n\nI then went on to prove that\n$$A_k\\subset A_{k+1}, A_{k+1}\\subset A_k, \\forall k \\le n$$\nIs my \"lemma\" proof enough of a proof? This is such a basic lemma that it seems like it should be obvious... but then again, when something seems obvious, it sometimes isn't. Is this rigorous enough? Is it too rigorous?\n",
    "proof": "In set-theory the notation $A\\subseteq B$ is actually an abbreviation for:$$\\forall x[x\\in A\\implies x\\in B]$$\nThis makes $\\subseteq$ a preorder on the sets (reflexive and transitive).\nThen the axiom of extensionality is the statement that this relation is also anti-symmetric: $$A\\subseteq B\\wedge B\\subseteq A\\implies A=B\\tag1$$\nThis makes the relation $\\subseteq$ a partial order.\nIn my view $(1)$ is not a statement that can be proved, but is a statement based on an abbreviation and an axiom.\n",
    "tags": [
      "elementary-set-theory",
      "proof-writing"
    ],
    "score": 7,
    "answer_score": 6,
    "is_accepted": true,
    "question_id": 2349715,
    "answer_id": 2349734
  },
  {
    "theorem": "Showing $a^2 &lt; b^2$, if $0 &lt; a &lt; b$",
    "context": "Lately, I've been stumbling with proofs of inequalities.\nFor example:\n\nGiven $0 < a < b$\n  Show $a^2 < b^2$\n\nThe only thing I've been able to come up with so far:\n\n$a^2 < b^2$\n  $\\sqrt{a^2} < \\sqrt{b^2}$\n  $a < b$\n\nOR\n\n\n$a < b$\n  $a^2 < b^2$\n\nHowever, neither of these solutions seem to be really \"showing\" that $a^2 < b^2$, assuming $0 < a < b$. I've tried some other things, but to no avail. Am I merely overthinking the problem when, in fact, these are actually acceptable solutions, or am I truly missing something here?\n",
    "proof": "You have \n\\begin{align*}\n(a-b) &< 0  \\\\ \\Longrightarrow (a-b) \\cdot (a+b) &<0  \\qquad\\qquad \\Bigl[\\small\\text{Multiplying both sides by}\\ (a+b) \\ \\text{doesn't change the sign.} \\Bigr]\\\\ \\Longrightarrow a^{2}-b^{2} &< 0\n\\end{align*}\nOr consider the function $f:\\mathbb{R}_{>0} \\to \\mathbb{R}_{>0}$ given by $f(x) =x^{2}$. Clearly this monotonic because $f'(x) = 2x > 0$ for all $x >0$. Hence your claim follows.\n",
    "tags": [
      "algebra-precalculus",
      "inequality",
      "proof-writing"
    ],
    "score": 7,
    "answer_score": 14,
    "is_accepted": true,
    "question_id": 52877,
    "answer_id": 52879
  },
  {
    "theorem": "Why, logically, is proof by contradiction valid?",
    "context": "How does proof by contradiction work logically? \nNormally in a proof we might have a true premise leading to a true conclusion, i.e. it is true that $T \\rightarrow T$.\nBut then how does proof by contradiction work? We assume the premise is false and then the goal is to what, show $F \\rightarrow F$? Or $F \\rightarrow T$? (both of which are true?)\nLike what exactly is the logical mechanism underneath all this that lets proofs work as well as proof by contradiction?\n",
    "proof": "Yes, well, a proof by contradiction involves two rules of inference.\n$$\\begin{split}\\text{Negation introduction}\\quad&\\quad (r\\implies q) \\text{ and } (r\\implies \\neg q), \\text{ infers } \\neg r\\\\\\text{Double Negation elimination:}\\quad &\\quad \\neg\\neg p\\text{ infers } p\\end{split}$$\n(1) the \"Negation introduction\" rule of inference argues that if something implies a contradiction then it must be false, since we usually assert that contradictions are not true and so cannot be infered by true things.\nThis is acceptable in both intuitionistic and classical logic systems.   Although there are other systems (such as minimal logic) which do not accept this.\n  ($\\def\\false{\\mathsf F}\\def\\true{\\mathsf T}$Semantically, this is because $\\false \\to \\false$ is true while $\\true\\to\\false$ is false.   This leads some systems to define negation as $\\neg \\phi ~\\equiv~ \\phi\\to\\mathsf F$ .)\n(2) the \"Double negation elimination\" rule is that if the negation of a premise is false, then the premise must be true.   This is not accepted in intuitionistic logic, but it is in classical logic.\n(3) Combining these rules give the schema for a proof by contradiction: assume a negation of a predicate, demonstrate that infers a contradiction, thereby deducing that the predicate is true.\n$$\\begin{split}\\text{Proof by Contradiction}\\quad&\\quad (\\neg p \\implies q) \\text{ and }(\\neg p\\implies \\neg q) \\text{, infers }p\\end{split}$$\n",
    "tags": [
      "logic",
      "proof-writing",
      "definition",
      "boolean-algebra"
    ],
    "score": 7,
    "answer_score": 15,
    "is_accepted": true,
    "question_id": 2704096,
    "answer_id": 2704154
  },
  {
    "theorem": "Prove that there are no $x,y ∈ \\mathbb N$ for which $x^2-y^2 = 10$",
    "context": "I began by factoring and got $(x+y)(x-y) = 10$\nThen I tried cases and was able to prove the ones where $x$ and $y$ are equal-> because the equation will result to zero.\nand also where $x < y$, because the answer will be negative. \nHow can I prove when $x > y$... or is there an easier way to do it? I think this is a proof by contrapositive?\n",
    "proof": "Note that $x+y$ and $x-y$ must be of the same parity. If both are even, then $4 \\mid (x^2-y^2)$. If both are odd, then $(x^2-y^2)$ is also odd. $10$ is neither divisible by $4$ nor odd.\n",
    "tags": [
      "proof-writing"
    ],
    "score": 7,
    "answer_score": 13,
    "is_accepted": true,
    "question_id": 527757,
    "answer_id": 527760
  },
  {
    "theorem": "Is this direct proof of an inequality wrong?",
    "context": "My professor graded my proof as a zero, and I'm having a hard time seeing why it would be graded as such.  Either he made a mistake while grading or I'm lacking in my understanding.  Hopefully someone can help sort it out.  The proof is as follows:\nGoal: If $n$ is a positive integer, then $\\frac{n}{n+1} > \\frac{n}{n+2}$.\nProof: Assume $n$ is a positive integer.\nObserve, $\\frac{n}{n+1} > \\frac{n}{n+2}$\n$\\frac{n(n+1)}{n+1} > \\frac{n(n+1)}{n+2}$\n$n > \\frac{n^2+n}{n+2}$\n$n(n+2) > \\frac{(n^2 + n)(n+2)}{n+2}$\n$n^2 + 2n > n^2 + n$\n$n^2 - n^2 + 2n > n^2 - n^2 + n$\n$2n > n$\nSince $2n > n$ for all positive integers, then $\\frac{n}{n+1} > \\frac{n}{n+2}$ for all positive integers.\nTherefore, if $n$ is a positive integer, then $\\frac{n}{n+1} > \\frac{n}{n+2}$. Q.E.D.\nHere are the notes on the problem by the professor:\n\"You assumed Q!  You cannot assume your conclusion!\"\nShows that $2n > n$ reduces down to $n > 0$ and points an arrow to 'Assume n is a positive integer'  \"Circular logic.\"\n\"By the way... reducing to falsehood is a valid truth technique(proof by contradiction) but reduction to truth tells you nothing.\"\n",
    "proof": "The problem is that you did not state that those are equivalences (usually denoted by $\\iff$) between your lines. And you do not even need the equivalences, you only need the implicatiosn from the bottom to the top, so you should perhaps write your proof \"upside down\".\nThe way your proof is presented right now makes it look like the top implies the bottom. (Which it does) but that doesn't mean that the bottom implies the top. So I'd write down:\n\nFor any positiv integer $n$ the following inequality obviously holds:\n$$2n > n$$\nThis implies\n$$n^2-n^2+2n > n^2-n^2+n$$\netc.\n\nNotice that this is also the way you'd want to read your proof so that people understand it. And it is important to realize that this is not necessarly the way you found the proof.\nWhenever you find a proof somewhere you can be quite sure that the way it is presented to you has nothing to do with how the one who proved it found the proof. It is really just written down nicely in order for the reader to be able to nicely follow the chain of arguments.\nBut don't worry, writing \"nice\" proofs does take a while at the beginning of your math career=)\nRegarding the comment: You could prove the inequality by contradicion, e.g. suppose that the inequality does not hold, and then find a contradiction, but this is not necessary here.\n\nEDIT: It seems to me that you are unfamiliar with the concept of logical implications. If two mathematical statements $A,B$ (e.g. equations, inequalities etc.) mean the same thing, they are called equivalent, denoted by the bidirectional double arrow. $$A \\iff B$$\nExample: Let $x$ be a real number. Then following equivalence holds: $$x -5 = 0 \\iff x = 5$$\nBut if $A$ only hold if $B$ holds, then we say $A$ implies $B$ or alternatively, \"if $A$ holds then $B$ must hold too.\" This is denoted by a simple doublearrow: $$A \\implies B$$\nExample: Let $x$ be a real number. Then following implication holds:\n$$x = 5 \\implies x^2 = 25$$ But \"the other way around\" is not necessarily true, as the right statement is also true for $x=-5$ but not the left one.\n",
    "tags": [
      "inequality",
      "proof-verification",
      "proof-writing",
      "real-numbers"
    ],
    "score": 7,
    "answer_score": 20,
    "is_accepted": true,
    "question_id": 1723937,
    "answer_id": 1723938
  },
  {
    "theorem": "There exist no integers for which $x^2-4y=2$",
    "context": "I am working on a new exercise in my textbook:\n$$\\text{Prove that: (P): }\\;\\nexists \\;x,y \\in \\mathbb{Z}, x^2-4\\cdot y = 2 $$\nI am stuck and I would really like to see a correct proof so I can move on while understanding the \"trick\".\nThank you.\n",
    "proof": "Suppose $x^2=4y+2$.\nThe RHS is divisible by $2$ but not by $4$. But if the LHS is divisible by $2$, it must be divisible by $4$.\n",
    "tags": [
      "logic",
      "proof-writing",
      "square-numbers"
    ],
    "score": 7,
    "answer_score": 14,
    "is_accepted": false,
    "question_id": 953708,
    "answer_id": 953713
  },
  {
    "theorem": "Proof that every number has at least one prime factor",
    "context": "Prove that for $ n \\geq 2$, n has at least one prime factor.\nI'm trying to use induction. For n = 2, 2 = 1 x 2. For n > 2, n = n x 1, where 1 is a prime factor. Is this sufficient to prove the result? I feel like I may be mistaken here.\n",
    "proof": "For a formal proof, we use strong induction.  Suppose that for all integers $k$, with $2\\le k\\lt n$, the number $k$ has at least one prime factor. We show that $n$ has at least one prime factor.\nIf $n$ is prime, there is nothing to prove. If $n$ is not prime, by definition there exist integers $a$ and $b$, with $2\\le a\\lt n$ and $2\\le b\\lt n$, such that $ab=n$.\nBy the induction assumption, $a$ has a prime factor $p$. But then $p$ is a prime factor of $n$. \n",
    "tags": [
      "prime-numbers",
      "proof-writing"
    ],
    "score": 7,
    "answer_score": 27,
    "is_accepted": false,
    "question_id": 934660,
    "answer_id": 934677
  },
  {
    "theorem": "How to prove that $x^2 +1 \\geq 2x$?",
    "context": "I am trying to prove that $x^2 +1 \\geq 2x$ without using circular logic (meaning first assuming that this inequality is true and then moving to the $2x$ to the left side and factoring it). Thanks.\n",
    "proof": "$$x^2 + 1 = x^2 - 2x + 1 + 2x = \\underbrace{(x-1)^2}_{\\geq 0} + 2x \\geq 2x$$\n",
    "tags": [
      "algebra-precalculus",
      "inequality",
      "proof-writing",
      "quadratics"
    ],
    "score": 7,
    "answer_score": 15,
    "is_accepted": false,
    "question_id": 2424538,
    "answer_id": 2424547
  },
  {
    "theorem": "Is this a correct way to prove T is not a linear transformation?",
    "context": "I have the following transformation $T:\\mathbb{R}^2 \\longrightarrow \\mathbb{R}^3$ defined by $T\\left( x, y \\right) = \\left( y, x, x^2 + y^2 \\right).$ I know the transformation is not linear but would like to prove it, so I deviced the following \"proof.\"\nWe know every linear transformation $T$ has a unique matrix representation for the standard basis of $\\mathbb{R}^2,$ which is given by $$A = \\left[ \\begin{array}{ccc}\nT(\\mathbf{e}_1) & T(\\mathbf{e}_2) \\\\\\end{array} \\right],$$\nand this matrix $A$ would move me back to the linear transformation by $T\\left( \\mathbf{x} \\right) = A \\mathbf{x}.$ \nSo, I assume $T$ is a linear transformation and construct it standard matrix representation, which would be $$A = \\left( \\begin{array}{ccc}\n0 & 1 \\\\\n1 & 0 \\\\\n1 & 1 \\end{array} \\right).$$\nNow, to get my original transfomation back I would have to do $$T\\left( \\mathbf{x} \\right) = A \\mathbf{x} = \\left( \\begin{array}{ccc}\n0 & 1 \\\\\n1 & 0 \\\\\n1 & 1 \\end{array} \\right) \\cdot \\left( \\begin{array}{ccc}\nx \\\\\ny \\end{array} \\right) = \\left( \\begin{array}{ccc}\ny \\\\\nx \\\\\nx+y \\end{array} \\right).$$\nSince this transformation I got is not the original one, I conclude $T$ is not a linear transformation.\nMy question is, the above reasoning is correct?\nAnd in general, can I apply this method to prove or disprove any transformation is a linear transformation?\n\nEDIT:\nPlease do not sugegst alternative methods of proof; I know them well. All I need is to know if the method described works.\n",
    "proof": "Suppose instead of $\\mathbb R$ we had used the field \n$\\mathbb Z_2 = \\mathbb Z/(2\\mathbb Z),$\nin other words, the set $\\{0,1\\}$ with the usual operations\n$+$ and $\\cdot$ modulo $2.$\nNow we would be asking about $T: \\mathbb Z_2^2 \\to \\mathbb Z_2^3,$\nwith $T\\left( x, y \\right) = \\left( y, x, x^2 + y^2 \\right).$\nEvery part of your proof would then work just as well as it did for\n$T: \\mathbb R^2 \\to \\mathbb R^3,$\nwith the exception of the conclusion.\nThe conclusion would be false, because\n$x^2 + y^2 = x + y$ when $x, y \\in \\mathbb Z_2.$\nI do not see any point in the proof where you invoke any property of\n$\\mathbb R$ that $\\mathbb Z_2$ does not have.\nTherefore I would say the proof is not valid.\nIn order to make a valid proof, you could invoke (for example)\nthe fact that $\\mathbb R$ contains an element named $2$ that is distinct from $0$ and $1,$ and you could have used the properties of that element to find a counterexample to the statement $x^2 + y^2 = x + y$ for $x,y \\in \\mathbb R.$\nI think it is noteworthy that you used many more facts than you needed in this proof, which I think also is a bad thing to do in a proof,\nbut of course that alone does not invalidate a proof.\n",
    "tags": [
      "linear-algebra",
      "proof-writing",
      "linear-transformations"
    ],
    "score": 7,
    "answer_score": 19,
    "is_accepted": false,
    "question_id": 2460541,
    "answer_id": 2460572
  },
  {
    "theorem": "Validity of the law of excluded middle",
    "context": "I recently just started the MIT openware Mathematics for computer science course. The method of proof by contradiction was introduced in one of the lectures, I did not really accept it easily as the other methods such as the proof by contraposition, proof by cases or by a direct proof. This is because what I thought about is:\nIf I assume A and then I find a contradiction, if all my steps were valid and true then A must not be true, but why must it be false, maybe it’s something else that we just do not know about, this is related to the law of excluded middle.\nSo why is the law of excluded middle true?\nWhat exactly is classical logic and intuitionist logic?\n",
    "proof": "This is an interesting question and a common mistake that people make when discussing constructive logic (which is also known as intuitionist logic, although in some contexts the terms \"constructive\" and \"intuitionist\" have slightly different meanings).\nConstructive logic is a form of logic where the claim $P \\lor \\neg P$ is not assumed to be true. Neither are equivalent logical rules such as $\\neg \\neg P \\implies P$ (double negation elimination) and $(\\neg P \\implies \\neg Q) \\implies (Q \\implies P)$ (proof by contrapositive). Constructive logic does not claim these three logical rules are wrong. It simply takes no position on whether these rules are valid or not.\nClassical logic is constructive logic plus accepting the validity of $P \\lor \\neg P$. Note that this means anything you can prove with constructive logic can also be proved with classical logic.\nThe proof that you're outlining is basically using the statement\n$$(P \\implies \\bot) \\implies \\neg P$$\nThis rule is known as the \"proof of negation\" rule.\nIn case you haven't seen the symbol, $\\bot$ is the logical symbol for \"False\".\nThis principle is completely valid in constructive logic. In fact, in constructive logic, $\\neg P$ is typically defined to be the statement $P \\implies \\bot$. In other words, $\\neg P$ is just shorthand for $P \\implies \\bot$.\nSo if you start by assuming $P$ and proving $\\bot$ (aka starting with $P$ and deriving a contradiction), then you have proved $P \\implies \\bot$, which is exactly the meaning of $\\neg P$.\nThe principle that most people have trouble with is the following one: $\\bot \\implies P$. This principle is known as \"ex falso quodlibet\" or \"the principle of explosion\". I personally call it \"false implies everything\".\nThis principle is a valid principle in constructive logic. It is always the case that $\\bot \\implies P$, no matter what $P$ is.\nNow if we start with $P$ and derive a contradiction, we have actually started with $P$ and proved $\\bot$. This means we have proved $P \\implies \\bot$. We also get $\\bot \\implies P$ for free from the \"ex falso\" rule. So we have actually shown $P \\iff \\bot$.\nIn other words, if we start with $P$ and derive a contradiction, we have shown that $P$ and \"false\" are logically equivalent. That is, we have shown that $P$ is false.\nWhat is not valid in constructive logic is \"proof by contradiction\". This is the rule stating that\n$$(\\neg P \\implies \\bot) \\implies P$$\nOr, in other words, $\\neg \\neg P \\implies P$. This rule is also known as \"double negation elimination\".\nThe \"law of excluded middle\", which is the principle $P \\lor \\neg P$, is actually misnamed. In fact, even without the \"law of excluded middle\", we can still prove that $P$ cannot be neither true nor false. In other words, we can still prove that $P$ doesn't take a \"middle value\" which is both not true and not false.\nFormally speaking, the statement \"It cannot be the case that both $P$ is not true, and $P$ is not false\" can be stated succinctly as $\\neg (\\neg P \\land \\neg \\neg P)$. This is because \"$P$ is not true\" translates to $\\neg P$, and \"$P$ is not false\" translates to $\\neg \\neg P$.\nThe statement $\\neg (\\neg P \\land \\neg \\neg P)$ is just a special case of the Law of Noncontradiction, which states $\\neg (Q \\land \\neg Q)$. The Law of Noncontradiction is always true in constructive logic. In fact, if we expand all the $\\neg$s in $\\neg (Q \\land \\neg Q)$ to $\\implies \\bot$, we get the statement $(Q \\land (Q \\implies \\bot)) \\implies \\bot$, which is just a special case of modus ponens and is quite straightforward to prove.\nI hope this helps! Please comment if you have any questions.\n",
    "tags": [
      "logic",
      "proof-writing"
    ],
    "score": 7,
    "answer_score": 20,
    "is_accepted": true,
    "question_id": 4236804,
    "answer_id": 4236821
  },
  {
    "theorem": "Prove that there is no number that divides both n and n+1",
    "context": "Statement\nThere is no number $x > 1$ that divides both $n$ and $n+1$.\nProof (my attempt)\nIndirect proof:\n\\begin{align}\nx\\mathbin{\\vert} n     & \\implies n   = xt_1  \\\\\nx\\mathbin{\\vert}(n+1) & \\implies n+1 = xt_2\n\\end{align}\nHaving $n$ as a multiple of $x$ that is $x t_1$, the next larger multiple of $x$ is $x(t_1+1)$ which is always greater than $n+1$ as $x>1$.\nTherefore, $x$ does not divide $n+1$ and we have a contradiction.\nThus the original statement is true.\nQuestion\nIs this how you can prove the statement? Is there anything wrong or something that can be improved formally?\n",
    "proof": "There is nothing wrong with your proof. However, the critical sentence, the one starting \"Having...\", is true, but is (at least to me) not convincing, or at least not as simple as it could be. It would be easier to subtract the two equations, giving $1 = x(t_2-t_1)$, deriving a contradiction.\n",
    "tags": [
      "elementary-number-theory",
      "proof-writing",
      "divisibility",
      "solution-verification"
    ],
    "score": 7,
    "answer_score": 9,
    "is_accepted": true,
    "question_id": 1043667,
    "answer_id": 1043674
  },
  {
    "theorem": "Disproving surjectivity of $f : \\Bbb Z \\times \\Bbb Z \\rightarrow \\Bbb Z$, $f(u,v) = 3u + 6v$",
    "context": "A function $f : \\Bbb Z \\times \\Bbb Z \\rightarrow \\Bbb Z$ is defined as $f(u,v) = 3u + 6v.$\nIs the function surjective? Prove it.\nI had the following proof.\nProof\nPick $x = 2$, then $3u + 6v = 2 \\Rightarrow 3(u + 2v) = 2$\nLet $y = u + 2v$ $\\exists y \\in \\Bbb Z \\times \\Bbb Z$.\nThus $3y = 2 \\Rightarrow y = \\frac{2}{3}$.\nThis is a contradiction because $\\frac{2}{3} \\not\\in \\Bbb Z \\times \\Bbb Z.$\nThe function is therefore not surjective.\nI am a novice at this whole LaTex thing and relatively new to proofs and these surjective proofs are killing me I can't seem to get anything right. Any help is greatly appreciated.\n",
    "proof": "You've got the right idea, but the second half is a bit jumbled, mostly because you introduced a lot of unnecessary variables. I'd phrase it this way:\nSuppose that there are $u,v\\in\\mathbb{Z}$ such that $3u+6v=2$. Then $3(u+2v)=2$, so $u+2v=2/3$. This is a contradiction, since $2/3$ is not an integer and the sum of two integers is also an integer.\nI'd say that captures the same idea of your proof, but using half the number of variables.\n",
    "tags": [
      "functions",
      "solution-verification",
      "proof-writing"
    ],
    "score": 7,
    "answer_score": 14,
    "is_accepted": false,
    "question_id": 4908608,
    "answer_id": 4908610
  },
  {
    "theorem": "Preimage of Intersection of Two Sets = Intersection of Preimage of Each Set : $f^{-1}(A \\cap B) = f^{-1}(A) \\cap f^{-1}(B)$",
    "context": "Prove If $f$ is a function , $f^{-1}(A \\cap B) = f^{-1}(A) \\cap f^{-1}(B)$. \nProof attempt\nI am guessing here $A$ and $B$ are sets in the range of $f$. Let's assume $x$ belongs to both $A$ and $B$ and $f^{-1}$ exists for both $A$ and $B$. \nThen there must exist a $y$ such that $y = f^{-1}(x)$.\nNow by our assumptions $x$ is in intersection of $A$ and $B$ and since $f^{-1}(A)$ and $f^{-1}(B)$ exist then $f^{-1}(A) \\cap f^{-1}(B)$ must also exist. Also since $A$ and $B$ exist and are not equal to null set so $A \\cap B$ exists and $f^{-1}(A\\cap B)$ also must exist and contain our $y$?? Not sure about the ending in this attempt. Any help would be much appreciated. \nSources : ♦ 2nd Ed $\\;$  P219 9.60(e) $\\;$  Mathematical Proofs by Gary Chartrand,\n♦ P214 $\\;$ Theorem 12.4.#3 $\\;$  Book of Proof by Richard Hammack,\n♦ P257-258 $\\;$  Theorem 5.4.2.#2(a) $\\;$   How to Prove It by D Velleman. \n",
    "proof": "Note that by definition of inverse image, we have $$x\\in f^{-1}(A \\cap B)$$\n$$\\Leftrightarrow f(x)\\in A\\cap B$$\n$$\\Leftrightarrow f(x)\\in A\\mbox{ and }f(x)\\in B$$\n$$\\Leftrightarrow x\\in f^{-1}(A)\\mbox{ and }x\\in f^{-1}(B)$$\n$$\\Leftrightarrow x\\in f^{-1}(A)\\cap f^{-1}(B).$$\nTherefore, we have $f^{-1}(A \\cap B) = f^{-1}(A) \\cap f^{-1}(B)$.\n",
    "tags": [
      "elementary-set-theory",
      "functions",
      "proof-writing"
    ],
    "score": 7,
    "answer_score": 23,
    "is_accepted": true,
    "question_id": 105956,
    "answer_id": 105961
  },
  {
    "theorem": "Proofs in Real Analysis are too &#39;convenient&#39;",
    "context": "I'm doing a first course in real analysis and I have studied nearly 10-15 theorems and proofs by now. One thing I've noticed in all of them is that they all seem too 'convenient' and full of assumptions. This, I find very peculiar to real analysis. To understand my point, consider this one for instance:\n\nTheorem: Let $\\{x_n\\}$ be a sequence of $\\mathbb R^+$ such that $\\lim_{n\\to\\infty} |\\frac{x_{n+1}}{x_n}| = l$. If $0\\le l \\lt 1$, $\\lim_{n\\to\\infty } x_n = 0$.\nProof: Consider $\\epsilon \\gt 0$ such that $l+\\epsilon \\lt 1$. There exists $N \\in \\mathbb N$ such that $||\\frac{x_{n+1}}{x_n}| - l |\\lt \\epsilon$ for all $n \\ge N$.\n$l-\\epsilon \\lt |\\frac{x_{n+1}}{x_n}| \\lt l+\\epsilon$ (for all $n \\ge N$.)\nLet $m = l + \\epsilon$. Given that $0 \\le l \\lt 1$, we could say that $0 \\lt m \\lt 1$. This gives $|\\frac{x_{n+1}}{x_n}| \\lt m$ for all $n \\ge N$\n$x_{N+1} \\lt mx_N$$x_{N+2} \\lt mx_{N+1}\\lt m^2x_N$ So for all $n \\ge N+1$, $x_n \\lt mx_{n-1} \\lt m^{n-N}x_N$.\n We are therefore left with $0 \\lt x_n \\lt Am^n$ where $A = \\frac{x_N}{m^n}$. As $\\lim_{n\\to \\infty} Am^n = 0$ as $m\\lt 1$,using the Squeeze Theorem, we are able to prove the theorem.\n\nYou see, the whole thing is dependent on one assumption that $l+\\epsilon \\lt 1$. But this should ideally hold true for any $\\epsilon$. I wouldn't call this proof 'complete'!\nHere's another such proof of the quotient law for limits:\n\nLet $\\epsilon, k \\gt 0.$ Then $\\frac{\\epsilon}{k}$ is also an arbitrary positive number. If $\\{x_n\\}$ and $\\{y_n\\}$ are two sequences, we need to prove that the limit of the quotient of the terms equals the quotient of the limits of the terms( say $l$ and $m$).\nFor a certain $N$, $|\\frac{x_n}{y_n} - \\frac{l}{m}| = |\\frac{m(x_n-l) + l(m-y_n)}{my_n}| \\le |\\frac{|m||x_n-l| + |l||m-y_n|}{|m||y_n|}| \\lt \\frac{\\epsilon}{ky_n} + \\frac{\\epsilon}{ky_n}\\frac{|l|}{|m|} = \\frac{\\epsilon}{k}\\frac{|m|+|l|}{|m||y_n|} $$\nlim_{n \\to \\infty} y_n = m$ so $lim_{n \\to \\infty} |y_n| = |m| $.\n\nLet $ 0 <H<|m|$. Then $ |y_n| > H $ for all $n \\ge N_0, N_0 \\in \\mathbb N$\n\nChoose $N' = max\\{N_0, N\\}$ so that for all $n \\ge N',|\\frac{x_n}{y_n} - \\frac{l}{m}| < \\frac{\\epsilon}{k}\\frac{|l|+|m|}{|m|H}$\nNow choose k such that $ \\frac{|l|+|m|}{|m|H} < 1$ so that $|\\frac{x_n}{y_n} - \\frac{l}{m}| < \\epsilon$. Q.E.D.\n\n\nThe last part again contains too convenient choices of constants. I think this might mean that unless you are choosing them in such a manner, the theorem won't hold. It's as though we are creating the proof such that the theorem comes true, which I find strange.\nHopefully I've made myself clear. I wonder if there exist 'more convincing' and more elegant proofs which do not take into account so many arbitrary constants. Thank you!\nEdit As suggested in one of the comments, I am inserting a theorem whose proof seems elegant to me-the Squeeze Theorem.\n\nTheorem:Given that $\\{x_n\\}$, $\\{y_n\\}$ and $\\{z_n\\}$ are three sequences where $x_n \\le y_n \\le z_n $ for all $n \\ge N,$ where $N \\in \\mathbb N$, and\n$\\lim_{n\\to\\infty } x_n = \\lim_{n\\to\\infty } z_n = l,$ then $lim_{n\\to\\infty } y_n = l$\nProof: For a given $\\epsilon \\gt 0$, we have natural numbers $N_1$ and $N_2$ such that $|x_n-l| < \\epsilon$ for all $n \\ge N_1$ and $|z_n-l| < \\epsilon$ for all $n \\ge N_2$.\nLet $N_3 = max\\{N_1, N_2\\}$, then for all $n \\ge N_3$, $|x_n-l| < \\epsilon$ and $|z_n-l| < \\epsilon$. \nThis means $l-\\epsilon < x_n<l+\\epsilon$ and $l-\\epsilon < z_n<l+\\epsilon$ for all $n \\ge N_3$. Let $N_4 = max\\{N, N_3\\}$. Then it holds that $l-\\epsilon < x_n < y_n < z_n <l+\\epsilon$ and therefore $l-\\epsilon < y_n<l+\\epsilon$ or  $|y_n-l| < \\epsilon$. Q.E.D\n\nWe certainly have considered multiple constants here, but we are not arbitrarily assigning them values/choosing them to satisfy certain equations, like so: '$l+\\epsilon<1$' or 'choose k such that $ \\frac{|l|+|m|}{|m|H} < 1$ '.\n",
    "proof": "These kinds of proofs rely on careful analysis of what you want to prove. Usually, you want to prove something of the form:\nFor every number $A$ there exists a number $B$ with certain properties related to $A$.\nTo prove such a statement, you take the number $A$ as a given because the theorem requires the statement to hold for all $A$. So no convenient choices allowed here. But then your task is to find one specific possible choice for $B$, because the statement is only that at least one such number exists.\nFor this reason it is often perfectly fine to make convenient choices in order to construct one specific convenient choice for $B$.\n",
    "tags": [
      "real-analysis",
      "proof-writing"
    ],
    "score": 7,
    "answer_score": 8,
    "is_accepted": false,
    "question_id": 4340354,
    "answer_id": 4340394
  },
  {
    "theorem": "Why only two tangents can be drawn to a parabola from a point outside it?",
    "context": "It's intuitively obvious to me that only two tangents can be drawn to a parabola from a point $(a,b)$ outside it but I want a mathematical pre-calculus proof of this.  \n",
    "proof": "Let the parabola be given by $y=ax^2+bx+c$, and let the exterior point be $(x_0,y_0)$. A line through that point is $y=m(x-x_0)+y_0$. To look for intersections, we solve the parabola equation and the line equation simultaneously:\n$$ax^2+bx+c = m(x-x_0)+y_0$$\nor:\n$$ax^2 + (b-m)x+(c+mx_0-y_0)=0$$\nIf the line is tangent to the parabola, then this equation has exactly one solution, which means its discriminant must equal $0$. The discriminant is:\n$$B^2-4AC = (b-m)^2-4a(c+mx_0-y_0)$$\nSetting this equal to $0$, and taking $m$ as our variable, we get:\n$$m^2 - (2b + 4ax_0)m + (b^2 - 4ac + 4ay_0)=0$$\nBeing quadratic, this equation can have, at most, two solutions for $m$.\nDoes that work for you?\n\nAs a side note, that final quadratic could also have one solution, or no solutions. These situations correspond to the cases where $(x_0,y_0)$ is actually on the parabola, or respectively, inside the parabola.\n",
    "tags": [
      "algebra-precalculus",
      "proof-writing",
      "analytic-geometry"
    ],
    "score": 7,
    "answer_score": 20,
    "is_accepted": true,
    "question_id": 2467883,
    "answer_id": 2467894
  },
  {
    "theorem": "$|\\text{det}(A)| = 1$ implies $A$ is orthogonal",
    "context": "I know that $A$ orthogonal $\\Rightarrow$ |det($A$)| = 1. Now I need to prove or disprove the reversed statement:\n$$\n|\\det(A)| = 1 \\Rightarrow A \\,\\text{ is orthogonal}\n$$\nThis is what I'm currently trying:\n$$\n|\\det(A)| = 1 \\Rightarrow \\det(A)^2 = 1 \\Rightarrow \\det(AA^t) = 1\n$$\nBut I'm unsure whether this implies, that $AA^t = E_n$. Any help is welcome at this point. Maybe the statement isn't even true.\n",
    "proof": "It's not true:\n$$\\begin{bmatrix} 1 & 1 \\\\\n 0 & 1 \\end{bmatrix}$$\nhas determinant $1$ but it's not orthogonal since the columns are not orthonormal.\nFurthermore, \n$$\\begin{bmatrix} 1 & 1 \\\\\n 0 & 1 \\end{bmatrix}\\cdot\\begin{bmatrix} 1 & 1 \\\\\n 0 & 1 \\end{bmatrix}^T = \\begin{bmatrix} 1 & 1 \\\\\n 0 & 1 \\end{bmatrix}\\cdot\\begin{bmatrix} 1 & 0 \\\\\n 1 & 1 \\end{bmatrix} = \\begin{bmatrix} 2 & 1 \\\\\n 1 & 1 \\end{bmatrix}$$\n",
    "tags": [
      "linear-algebra",
      "proof-writing",
      "determinant",
      "orthogonal-matrices",
      "transpose"
    ],
    "score": 7,
    "answer_score": 16,
    "is_accepted": false,
    "question_id": 2449077,
    "answer_id": 2449082
  },
  {
    "theorem": "Is there a method that can allow us to know how to prove theorems (number theory)?",
    "context": "I am stuck on proving a lot of the theorems that are discussed in number theory. Most of the theorems that I've seen in number theory so far are the ones I've already been shown how to prove, but I don't get the general approach for finding out how to prove them.\nLet's say, for instance, we want to prove that there are infinitely many prime numbers. I know the basic methods of proving statements like direct proofs, proof by contradiction etc., so we could suppose that there are finitely many primes $p_1, p_2, \\cdots,p_k$ with $p_1 < p_2 <\\cdots < p_k$ by contradiction. Then in the next step of the proof, a new integer n is defined as $n = p_1\\cdot p_2\\cdot\\space\\cdots\\space\\cdot p_k + 1$, and because it is greater than $p_k$, it is composite since we assumed that there are finitely many primes. But the thing I don't get though is how $n = p_1\\cdot p_2\\cdot\\space\\cdots\\space\\cdot p_k + 1$ just came up so randomly. \nI understand that continuing the proof eventually leads to the contradiction that 1 is composite, but the trouble is I don't know where to start just in general. \nFor example, if I wanted to prove that there are infinitely many primes of the form $6\\cdot k + 5$ for some integer k, where would I start? I've tried doing something like defining $n = (6k_1 + 5)(6k_2 + 5)\\cdots +(6k_r+5) + 5$ for $r$ primes of the form since  I'm assuming that n could be a new prime of the form maybe (not too sure), but from there, nothing seems to be working out and I cannot get a contradiction in any way.  And even if I did want to get a contradiction, how would I know what will end up being the contradiction in the end?\nIs anybody able to help me with this? Thank you in advance. \n",
    "proof": "Proving almost anything in math can often involve dead ends. If you get to a certain level on this site, you will be allowed to see deleted answers.\nIn those deleted answers, you will see professionals and amateurs alike get close to the right answer and then realize that one little detail invalidates their whole argument.\nOf course that wasn't an option for Andrew Wiles for his first attempt at a proof of the Fermat conjecture, and I certainly don't recommend you try to prove that one.\nThe theorem about infinitely many primes of the form $6k + 5$ (or $6k - 1$, if you prefer) seems a bit more manageable than the Fermat conjecture, now the Wiles theorem.\nI would start by multiplying some arbitrary numbers of the form $6k + 5$ and then add 6. But the first difficulty I run into is that if I choose an even amount (what you call $r$) of numbers of the form $6k + 5$, their product is actually of the form $6k + 1$, e.g., $5 \\times 11 = 55 = 6 \\times 9 + 1$.\nNext I would try either stipulating that $r$ has to be odd, or I would try multiplying that product by 6 and subtracting 1 from that, e.g., $6 \\times 55 - 1 = 329 = 7 \\times 47$. Hmm... this could actually work.\nEven if it doesn't work, it helps me make the point that it is often very helpful to plug specific numbers into your equations, it can help you see things more quickly.\nMaybe it would be better for you to try to prove even simpler things. Here's one for starters: given a prime $p$, prove that $p + 2$ or $p + 4$ may also be prime... but not both, with only one exception. The answer is on this site, but only look at that once you come up with the proof on your own.\n",
    "tags": [
      "elementary-number-theory",
      "proof-writing",
      "prime-numbers"
    ],
    "score": 7,
    "answer_score": 7,
    "is_accepted": false,
    "question_id": 3139609,
    "answer_id": 3139789
  },
  {
    "theorem": "Show that a vector that is orthogonal to every other vector is the zero vector",
    "context": "I have the following question, and I'd like to get some tips on how to write the proof. I know why it is, but I'm still not so great at writing it mathematically.\n\nIf $u$ is a vector in $\\mathbb{R}^n$ that is orthogonal to every vector in $\\mathbb{R}^n$, then $u$ must be the zero vector. Why?\n\nI'm starting off like this, but I don't know if it's the right way to do it, or if it is and I just don't know how to continue.\n\\begin{align}\n(\\exists u\\in\\mathbb{R}^n)(\\forall v\\in\\mathbb{R}^n)[\\text{u is orthogonal to v}]&\\iff u\\cdot v=0\\\\\n&\\iff ?\n\\end{align}\nFrom here, instinctively I want to divide both sides by $v$, but I don't know if there is such a thing as dividing a dot product.\n",
    "proof": "The dot product $\\cdot$ is an unusual type of multiplication.  It takes two vectors in and produces a scalar out.  Imagine a mommy elephant and a daddy elephant giving birth to a giraffe.\nThere's no such thing as division in this context.  You need to do the proof by looking at components of the vectors.  $u=(u_1,u_2,\\ldots, u_n), v=(v_1, v_2,\\ldots, v_n)$, and $$u\\cdot v=u_1v_1+u_2v_2+\\cdots+u_nv_n$$\nThen, following Jared's hint, you can try specific values for $v$ and learn things about the components of $u$.\n",
    "tags": [
      "linear-algebra",
      "proof-writing"
    ],
    "score": 7,
    "answer_score": 12,
    "is_accepted": true,
    "question_id": 400331,
    "answer_id": 400333
  },
  {
    "theorem": "Proving set identities: empty set case.",
    "context": "I am currently refreshing my knowledge in naive set theory, and would like to prove that for all sets $A,B,C$ we have $$A\\cap(B \\cup C) = (A \\cap B) \\cup (A \\cap C).$$\nI understand that this can be done by proving both $$A\\cap(B \\cup C) \\subset (A \\cap B) \\cup (A \\cap C) \\ \\text{and} \\ (A \\cap B) \\cup (A \\cap C) \\subset A\\cap(B \\cup C)  $$ hold true. \nWe can do this by letting $x$ be an arbitrary element of $A\\cap(B \\cup C)$ and showing that it is an element of $(A \\cap B) \\cup (A \\cap C)$, and vice versa. \nBut what about when $A \\cap (B \\cup C)$ is the empty set? Then I would think we can't let $x$ be an arbitrary element of $A \\cap (B \\cup C)$ since there are none. However, I am aware that $A \\cap (B \\cup C) \\subset (A \\cap B) \\cup (A \\cap C)$ is trivially true in this case.\nIn the discrete mathematics course I took at my university, I did not see such cases be brought to attention. Should they be mentioned in proofs of such identities? Why / why not? \nIf so, some suggestions as to how to incorporate them into proofs would be helpful :-).\n",
    "proof": "In order to show that for two sets $X$ and $Y$ it holds that $X\\subseteq Y$, you have to prove that\n\nfor every $x$, if $x\\in X$, then $x\\in Y$.\n\nNote that a statement of the form “if $\\mathscr{A}$ then $\\mathscr{B}$” is true when\n\neither $\\mathscr{A}$ is false or both $\\mathscr{A}$ and $\\mathscr{B}$ are true\n\nIf $X$ is the empty set, then “$x\\in X$” is false for every $x$; hence “if $x\\in X$ then $x\\in Y$” is true.\nThe phrase “take an arbitrary element $x\\in X$” is possibly misleading, but its intended meaning is “suppose $x\\in X$”.\n",
    "tags": [
      "elementary-set-theory",
      "proof-writing"
    ],
    "score": 7,
    "answer_score": 11,
    "is_accepted": true,
    "question_id": 3049718,
    "answer_id": 3049732
  },
  {
    "theorem": "What character can replace word &quot;let&quot; in proofs?",
    "context": "For example, suppose I have a line of a proof introducing new “variable” $x$:\n$$\\textrm{Let}\\:\\:x\\in f(y)$$\nI am looking for ways to express the word “let” in this context and I would like to avoid using natural language because Math is itself a universal language for expressing complex ideas.\nI used to use a character like a right square bracket or similar to it. I once encountered it somewhere, but I don't know if it is a common practice. For example, I would write that line like this:\n$$\\sqsupset x\\in f(y)$$\nNow I failed to find this character anywhere, neither on the Web nor in Unicode symbol set. Instead, I discovered some similar symbols like $\\buildrel \\text{def}\\over=$, or $:=$, or $\\buildrel\\triangle\\over=$, or $:\\Leftrightarrow$ in Wikipedia, but those are very limited and not so much useful in my case.\n",
    "proof": "\nI would like to avoid using natural language because Math is itself a universal language for expressing complex ideas.\n\nHonestly, I think this is a bad (by which I mean non-mathematical) reason to do anything. You will have great difficulty introducing any symbol into any widespread use, and consequently you will not be able to use such a symbol in any piece of work you wish to be taken seriously (because nobody will be able to read it!). I also don't recommend you teach people bad habits.\nOn the other hand, if these are just for personal notes, by all means invent your own symbol. I use lots of imprecise squiggly arrows, equals signs with quotation marks round them, equals signs decorated with question marks, and the like. I often use := to conjure a symbol into existence at the same time as defining it (because, unlike when programming, I don't need to declare my variables and I don't often redefine them in the same 'subroutine'), or simply =. I've seen people write an equals sign with \"def\" or \"$\\triangle$\" above it too.\nThis is not unlike how real mathematicians work with each other. When two people collaborate, it's very convenient for there to be an implicit assumption along the lines of \"every time I say X, until we solve this problem, I mean this particular object\", or vague terminology like \"nice\" to describe classes of objects that you can't quite pin down. But of course, once it comes to a seminar or a paper, you start from scratch, (mostly) give everything real, sensible words, and don't force your audience to learn a page of jargon and squiggles when there's perfectly good English available for it.\n",
    "tags": [
      "notation",
      "proof-writing"
    ],
    "score": 7,
    "answer_score": 9,
    "is_accepted": true,
    "question_id": 453096,
    "answer_id": 453221
  },
  {
    "theorem": "Epsilon delta proof of a limit problems",
    "context": "I am attempting to decipher the word of Stewart but I can't really understand any of the epsilon delta stuff. I watched several videos online and I have a better understanding but I still don't quite get how to do the math as no one really describes that part. As far as I understand I need to find the distance between x and delta that is less than delta and greater than zero that will coorespond to an epsilon (y).\nSo I have the problem\n$$\r\n\\lim_{x \\to 1}\\frac{2+4x}{3}=2\r\n$$\nso I do some algebra magic and I get $x=1$ but from there I am not sure what to do.\n",
    "proof": "The construction of an $\\epsilon$-$\\delta$ proof is usually exactly opposite its presentation.  This should be a good example.  Given $\\epsilon$, you need to be able to produce some $\\delta$ such that for all $x$ with $|x-1|<\\delta$, $\\frac{2 + 4x}{3} - 2|<\\epsilon$.\nHow do you do this?  You (usually) need some formula to produce $\\delta$ in terms of $\\epsilon$.  So start with the expression you need, \n$$\\bigg|\\frac{2 + 4x}{3} - 2\\bigg|<\\epsilon,$$\nand start solving for $x$ in terms of $\\epsilon$.\nI'll let you work out the details here; it's a simple algebraic exercise.  At the end, you'll get something like \n$$\\bigg|x - 1\\bigg| < \\frac{3}{4}\\epsilon.$$\nAh-hah! Now you have produced a constraint on $|x-1|$ in terms of $\\epsilon$.  If I give you $\\epsilon$, you can pick $\\delta < \\frac{3}{4}\\epsilon$ and, as you can quickly verify, this $\\delta$ will satisfy the $\\epsilon$ bound. In fact, it has to --- that's how you made it.\nThe take-home lesson here, again, is that the way you build the proof is backwards. Start with what you want and backsolve for what you need.  Then when you write the proof out, you know how to choose $\\delta$, so you can quickly verify that $\\lim_{x\\to 1}\\frac{2 + 4x}{3} = 2.$\n",
    "tags": [
      "calculus",
      "limits",
      "proof-writing"
    ],
    "score": 7,
    "answer_score": 16,
    "is_accepted": true,
    "question_id": 101441,
    "answer_id": 101450
  },
  {
    "theorem": "Prove that $\\dim(U+W) + \\dim(U\\cap W) = \\dim U + \\dim W$",
    "context": "\nLet $V$ be a vector space over a field $k$ and let $U$, $W$ be finite-dimensional subspaces of $V$.\nProve that $\\dim(U+W) + \\dim(U\\cap W) = \\dim U + \\dim W$\n\nI'm given that to begin this problem I can find the bases:\n$\\{v_1,\\dots,v_p\\}$ for $U\\cap W$\n$\\{v_1,\\dots,v_p, u_1,\\dots,u_q\\}$ for $U$\nand $\\{v_1,\\dots,v_p, w_1,\\dots,w_r\\}$ for $W$\nand then I just need to show that $\\{v_1,\\dots,v_p, u_1,\\dots,u_q, w_1,\\dots,w_r\\}$ is a basis for $U+W$.\nMy question is: how does one go about showing that it is a basis for $U+W$ and then use that to prove the above question?\n\nSide note: This question has already been asked here: Given two subspaces $U,W$ of vector space $V$, how to show that $\\dim(U)+\\dim(W)=\\dim(U+W)+\\dim(U\\cap W)$\nHowever, the first answer given does not apply to solving it the way I want to with finding the bases. The second answer simply gives me what I already knew to start with. Thus, I am asking this question again since I'm asking how to solve it a particular way instead of just any general hints towards solving it.\n",
    "proof": "Just do the computations; the fact that the set spans $U+W$ should be clear, so we prove linear independence.\nSuppose\n$$\n\\alpha_1v_1+\\dots+\\alpha_pv_p+\n\\beta_1u_1+\\dots+\\beta_qu_q+\n\\gamma_1w_1+\\dots+\\gamma_rw_r=0\n$$\nThen\n$$\nx=\\underbrace{\\alpha_1v_1+\\dots+\\alpha_pv_p+\n\\beta_1u_1+\\dots+\\beta_qu_q}_{\\in U}=\n-(\\underbrace{\\gamma_1w_1+\\dots+\\gamma_rw_r}_{\\in W})\n$$\nbelongs to $U\\cap W$. Thus\n$$\nx=\\delta_1v_1+\\dots+\\delta_pv_p\n$$\nand therefore\n$$\n\\delta_1v_1+\\dots+\\delta_pv_p=-(\\gamma_1w_1+\\dots+\\gamma_rw_r)\n$$\nso that\n$$\n\\delta_1v_1+\\dots+\\delta_pv_p+\\gamma_1w_1+\\dots+\\gamma_rw_r=0\n$$\nSince the set $\\{v_1,\\dots,v_p,w_1,\\dots,w_r\\}$ is linearly independent, we conclude\n$$\n\\delta_1=0,\\quad\\dots,\\quad\\delta_p=0,\\quad\n\\gamma_1=0,\\quad\\dots,\\quad\\gamma_r=0\n$$\nand also that\n$$\n\\alpha_1v_1+\\dots+\\alpha_pv_p+\\beta_1u_1+\\dots+\\beta_qu_q=0\n$$\nso, from linear independence of $\\{v_1,\\dots,v_p,u_1,\\dots,u_q\\}$ we get\n$$\n\\alpha_1=0,\\quad\\dots,\\quad\\alpha_p=0,\\quad\n\\beta_1=0,\\quad\\dots,\\quad\\beta_q=0\n$$\n",
    "tags": [
      "linear-algebra",
      "vector-spaces",
      "proof-writing"
    ],
    "score": 7,
    "answer_score": 14,
    "is_accepted": true,
    "question_id": 1637740,
    "answer_id": 1637761
  },
  {
    "theorem": "How to interpret &quot;let&quot; in mathematics?",
    "context": "Before explaining my issue, I wanted to first explain what things do make sense to me.\nSo, statements (used at the beginning of proofs) like \"Suppose $x$ is an integer\" or \"Assume $x$ is an integer\" make sense to me. The way I read them is like: \"Let's just pretend that the symbol $x$ happens to represent an integer\".\nStatement (also used in proofs) like \"Let $x$ be 3\" or \"Let $x$ equal 3\" also make sense to me. The way I read them is like: \"Let's just temporarily name 3 with the symbol $x$.\nMy issue comes with statements like \"Let $x$ be an integer\" or \"Let $x$ $\\in$ $\\mathbb{Z}$\". I really do not know how I should intuitively interpret such a statement. I can't interpret it in the same way as like \"Let $x$ be 3\" because there is not a specific object being assigned like 3. Should I interpret it like how I interpret \"Suppose $x$ is an integer\"?\n\nEDIT:\nSo, from what I gathered from the responses, I think I understand now how I should think of \"Let $x$ be an integer\".\nI could think of it as \"Assume a newly created symbol x happens to represent an integer\". However, this can cause issues as then \"Let $x$ be an element of the empty set\" is also completely valid.\nInstead, I should not think of \"Let $x$ be an integer\" as an assumption, but an assignment/declaration, just like \"Let $x$ be 3\". One tangible way to think about this is to imagine myself assigning the newly created symbol $x$ to an integer chosen in secret by a friend. With this mindset, $x$ is not assumed to be an integer - it is an integer. It is just unknown to me what integer it is.\nI hope this made sense to anybody with similar questions. If anyone thinks I have made an incorrect finding, please feel free to correct me.\n",
    "proof": "\"Let\" is usually used to introduce a new symbol along with an assumption about it (\"Let $G$ be a group\") or when defining notation (\"Let $\\mathcal P(S)$ denote the power set of $S$\"). \"Suppose\" and \"assume\" are more conventional when introducing new assumptions using only existing symbols and notation (\"Suppose $x^2<5$\"). This isn't a rule, though - you can certainly introduce new symbols with an \"assume\" or \"suppose\".\nI'd also say \"suppose\" is slightly more common when introducing a proposition that is \"in doubt\" and will shortly be refuted or discarded (e.g. with a proof by contradiction), whereas \"assume\" is more neutral in tone and is more common when introducing long-lasting assumptions or \"background information\".\nThe three words have the same technical meaning, in that we'd translate them into formal logic in the same way.\n",
    "tags": [
      "discrete-mathematics",
      "proof-writing",
      "soft-question",
      "article-writing"
    ],
    "score": 7,
    "answer_score": 5,
    "is_accepted": true,
    "question_id": 4053156,
    "answer_id": 4053185
  },
  {
    "theorem": "How do you get a paper to be peer reviewed",
    "context": "I have a proof that I want to undergo peer review. I unfortunately am not affiliated with any university. How should I go about getting it reviewed and either rejected or published?\nThanks!\n",
    "proof": "I don't publish in pure mathematics journals (I'm a computer science researcher) but at least in my field, there is no affiliation requirement in order to submit to peer-reviewed journals.\nFirst a quick comment: you haven't told us the subject of your proof. I don't mean to sound negative or discouraging, but if it relates to a famous long-standing conjecture (Riemann hypothesis, twin prime conjecture, etc), there is very little chance your paper will be taken seriously, as these problems have been so well studied that the odds of an amateur resolving them in 6 pages is very very low. If you do find yourself in this situation, then surely along the way to proving the famous conjecture you have also developed new theory and partial results that are interesting in their own right -- I would focus on getting some of these published first.\nThe first step would be for you to identify a relevant journal, and read their submission instructions. All journals have very specific instructions about how your submission should be formatted and how it should be submitted/uploaded to them. Many have LaTeX templates you'll be able to use.\nNext, the editor will determine whether to send the article out for peer review, or reject it immediately. To minimize the chances of outright rejection, be sure you structure and format your paper professionally. Reading accepted papers from the journal is surely the best way to learn the standards of form, but a few quick tips:\n\nFormat your document using LaTeX. If you don't know LaTeX, learning it and converting your paper to it is well worth the few weeks' investment. I do not recommend submitting a paper written in MS Equation Editor, even if the journal allows it, as it will make an unprofessional first impression.\nBe sure your paper has an appropriate abstract, outlining in ~100 words what you prove and how it fits into the existing body of work.\nThroughout the paper, and in the introduction in particular, be sure you've appropriately acknowledged (and cited) all relevant existing work. Ignorance of existing published partial or related results is one of the brightest red flags of crankery, so you want to leave the editor with no doubts about whether you have done your homework.\n\nIf your submission looks and smells like a professional paper, it will be submitted for peer review. The usefulness of those reviews is always uncertain, but with any luck you will get good feedback on how to improve your paper, even in the unfortunate event that the reviewers recommend rejection.\n",
    "tags": [
      "proof-writing",
      "publishing"
    ],
    "score": 7,
    "answer_score": 13,
    "is_accepted": true,
    "question_id": 294863,
    "answer_id": 294892
  },
  {
    "theorem": "Writing a proof",
    "context": "I am stuck at showing how if:\n$P \\rightarrow Q$ then this implies ($Q \\rightarrow R) \\rightarrow (P \\rightarrow R)$\nI know that if we assume $P$ is true then $Q$ also must be true. \nTherefore if $Q$ is true then $R$ must be true. And because $Q$ is true because $P$ is true so therefore $R$ must also be true.  \nHowever it would be appreciated if someone could help me understand this why is the case and what proof technique it is. \n",
    "proof": "This property is called transitivity of implication.\nYou pretty much gave a proof in your question, but here's a proof written out in slightly more precise terms.\nSuppose $P \\Rightarrow Q$ is true. To prove $(Q \\Rightarrow R) \\Rightarrow (P \\Rightarrow R)$, you need to assume $Q \\Rightarrow R$ and derive $P \\Rightarrow R$. So assume that $Q \\Rightarrow R$ is true. To prove $P \\Rightarrow R$ is true, you need to assume $P$ is true and derive $R$. So assume $P$ is true. All we have to do now is prove that $R$ is true.\nAt this point, we're assuming that $P \\Rightarrow Q$, $Q \\Rightarrow R$ and $P$ are all true. So:\n\nSince $P$ and $P \\Rightarrow Q$ are true, we have that $Q$ is true; and\nSince $Q$ and $Q \\Rightarrow R$ are true, we have that $R$ is true.\n\nSo we're done.\n",
    "tags": [
      "proof-writing",
      "proof-explanation"
    ],
    "score": 7,
    "answer_score": 13,
    "is_accepted": false,
    "question_id": 3084690,
    "answer_id": 3084703
  },
  {
    "theorem": "If $\\{q_\\alpha: X_\\alpha \\to Y_\\alpha\\}$ is a family of quotient maps, then $q:\\coprod_\\alpha X_\\alpha \\to \\coprod_\\alpha Y_\\alpha$ is a quotient map.",
    "context": "\nIf $\\{q_\\alpha: X_\\alpha \\to Y_\\alpha\\}$ is an indexed family of quotient maps, then the map $q:\\coprod_\\alpha X_\\alpha \\to \\coprod_\\alpha Y_\\alpha$ whose restriction to each $X_\\alpha$ is equal to $q_\\alpha$ is a quotient map.\n\nA subset of the disjoint union $\\coprod_\\alpha X_\\alpha$ is open if and only if its restriction to $X_\\alpha$ (considered as a subset of the disjoint union) is open in $X_\\alpha$.\n\nProof:\nWe want to show that $V$ is open in $\\coprod_\\alpha Y_\\alpha$ if and only if $q^{-1}(V)$ is open in $\\coprod_\\alpha X_\\alpha$.\nFirst $q$ is continuos since the restriction to $X_\\alpha$ is a quotient map for each $\\alpha$. So, if $V$ is open in $\\coprod_\\alpha Y_\\alpha$ then $q^{-1}(V)$ is open in $\\coprod_\\alpha X_\\alpha$.\nNow, suppose $q^{-1}(V)$ is open in $\\coprod_\\alpha X_\\alpha$. Then $q^{-1}(V) \\cap X_\\alpha$ is open in $X_\\alpha$ for each $\\alpha$.\nBut $q^{-1}(V) \\cap X_\\alpha=q_\\alpha^{-1}(V)$. So, $V$ is open in $Y_\\alpha$ for each $\\alpha$.\nSo, $V$ is open in $\\coprod_\\alpha Y_\\alpha$.\n\nMy concern is when I write $q^{-1}(V) \\cap X_\\alpha=q_\\alpha^{-1}(V)$ because $q^{-1}(V) \\cap X_\\alpha \\subset \\coprod_\\alpha X_\\alpha$ whereas $q_\\alpha^{-1}(V)\\subset X_\\alpha$ (as a standalone space, not as a subset of disjoint union).\nHow can I justify this?\n",
    "proof": "This will discuss some general theory on final topologies, which\nare a common way that topologies are defined or characterised. This is a dual version\nof my posting on initial topologies here,\nyou could say, and a lot of it is analogous.\nDefinition:\n\nLet $(X,\\mathcal{T})$ be a topological space, and let $Y_i, (i \\in I)$\nbe topological spaces and let $f_i : Y_i \\to X$ be a family of functions.\nThen $\\mathcal{T}$ is called the final topology with respect to the maps $f_i$\niff:\n\n\n\n$\\mathcal{T}$ makes all $f_i$ continuous.\n\n\n\n\nif $\\mathcal{T}'$ is any other topology on $X$ that makes all $f_i$ continuous, then\n$\\mathcal{T}' \\subseteq \\mathcal{T}$.\n\n\n\nNote that asking for the weakest such topology will always\nresult (regardless of the maps or the topology on $Y_i$) in the indiscrete topology\non $X$, as this topology makes all arriving maps continuous. So this would be a trivial notion.\nWhen we have a situation where we already have the spaces $Y_i, (i \\in I)$ with a topology\nand we also have set $X$ with somehow natural maps to $X$, then the final topology w.r.t.\nthese maps is a quite natural candidate topology to put on $X$.\nAnd this can always be done, by the following:\n\nExistence theorem for final topologies.\n\n\nLet $X$ be a set and $f_i : (Y_i,\\mathcal{T}_i) \\to X$ be\na collection of topological spaces and functions.\nThen there is a topology $\\mathcal{T}_f$ on $X$ that is final\nw.r.t. the maps $f_i$. Moreover, this topology is unique and is given by:\n$$\\mathcal{T}_f = \\{O \\subseteq X: \\forall i \\in I: (f_i)^{-1}[O] \\in \\mathcal{T}_i \\}$$\n\nProof:\nAs $$(f_i)^{-1}[\\bigcup_j O_j] = \\bigcup_j (f_i)^{-1}[O_j]$$ and\nsimilarly for intersections\n(for all indexed collections $O_j$ of subsets of $Y$ and all $i$), we see that\n$\\mathcal{T}_f$  is closed under unions and finite intersections, as the topologies on\n$Y_i$ are.\nIt's also clear that $\\emptyset$ and $X$ are in $\\mathcal{T}_f$, so $\\mathcal{T}_f$\nis indeed a topology on $X$.\nNow, if $O$ is from $\\mathcal{T}_f$, by definition all inverse images of $O$ under any\n$f_i$ are open in $Y_i$, so that indeed $\\mathcal{T}_f$ makes all $f_i$ continuous.\nIf $\\mathcal{T}$ is another such topology, then let $O \\in \\mathcal{T}$.\nThen for all $i$, we have, as $f_i$ is continuous, that $(f_i)^{-1}[O]$ is open in $Y_i$.\nBut this means exactly that $O \\in \\mathcal{T}_f$, so we see that\n$\\mathcal{T} \\subseteq \\mathcal{T}_f$.\nUnicity is clear: if $\\mathcal{T}$ and $\\mathcal{T}'$ are both final,\nthen both make all $f_i$ continuous and applying property 2. to $\\mathcal{T}$ we get\n$\\mathcal{T}' \\subseteq \\mathcal{T}$, and applying it to $\\mathcal{T}'$\ngives us $\\mathcal{T} \\subseteq \\mathcal{T}'$; hence $\\mathcal{T}=\\mathcal{T}'$.\n\nNote: comparing to the case of initial topologies: there we could (in general,\nan exception occurs when we have one map) only give a subbase for the initial topology,\nnot a complete description as we have here. Of course, here we needed\nto check that $\\mathcal{T}_f$ is in fact a topology, while any collection of subsets is a subbase\nfor some topology, without further effort.\nAlso, as $(f_i)^{-1}[X\\setminus O] = Y_i \\setminus (f_i)^{-1}[O]$, we see that\nthe collection of closed sets of the final topology is given by\n$\\{F \\subset X: (f_i)^{-1}[F] \\text{ is closed for all } i \\in I \\}$\n\nExample: the quotient topology.\nThis is just the final topology w.r.t. a single map.\nIf $f:X \\to Y$ is some continuous map, then if $f$ is such that $Y$ has the final\ntopology w.r.t. $f$, we say that $Y$ has the quotient topology, and that $f$ is a\nquotient map.\nThis is applied to the situation where we have an equivalence relation $R$ on a\ntopological space $X$, and $Y$ is the set of all classes of $X$ w.r.t. the relation $R$.\nThe map $f$ (often denoted $q$) here is the one that sends $x$ to its class $[x]$.\n$Y$ is then also denoted by $X/R$, with the final topology w.r.t. $f$ (or $q$).\nIn this case $f$ is onto (surjective). If we give $Y$ (under $f:X \\to Y$) the quotient topology\nw.r.t. $f$, we have that $Y\\setminus f[X]$ is a discrete subspace, because for all $A$ that are\ndisjoint from $f[X]$ we have that $f^{-1}[A] = \\emptyset$, which is open in $X$, so that\nall such $A$ are open under the final topology w.r.t. $f$. So this topology would trivialise\nthe topology outside $f[X]$ anyway, which is one of the reasons that one often assumes,\nin the one map setting, that this one map is onto. Note that in the equivalence relation\nsituation the standard map $q$ is automatically onto anyway.\n\nExample: the sum topology.\nSuppose we have a family $X_i, i \\in I$ of topological spaces, and let\n$X$ be the union of all sets $X_i \\times \\{i\\}$.\n( The \"$\\times \\{i\\}$\" is needed to ensure that this is a disjoint union of the $X_i$)\nThen we have natural maps $k_i : X_i \\to X$, by sending $x \\in X_i$ to $(x,i) \\in X$.\nThese maps are all 1-1, and map onto $X_i \\times \\{i\\}$.\nThe set $X$ with the final topology w.r.t. these maps $k_i (i \\in I)$ (the standard embeddings)\nis called the topological sum of the spaces $X_i$, sometimes denoted\nby $\\coprod_{i \\in I} X_i$ as well.\nIt is easily checked that this topology\nis given by the subsets $O$ of $X$ such that for all $i$,\nthe set $\\{x \\in X_i: (x,i)\\in O\\}$ is open in $X_i$. Hence all open sets are given\nby all unions $\\bigcup_i k_i[O_i]$, where $O_i$ is open in $X_i$.\nNote that this makes all $k_i$ also open (and closed) maps and so\n$k_i: X_i \\to X_i \\times  \\{i\\}$\nis a homeomorphism when the latter space gets the subspace topology from $X$.\nIt also follows that all $X_i \\times \\{i\\}$ are themselves open (and closed) in $X$,\nand no non-trivial sum-space is connected.\nA sort of converse also holds: if $X$ is not connected, then $X$ can be written as $A \\cup B$\nwhere $A$ and $B$ are both non-empty, open and disjoint.\nThen the topology of $X$ is the final topology w.r.t. the 2 embeddings $i_A: A \\to X$ and\n$i_B: B \\to X$, so $X$ is homeomorphic to the sumspace of $A$ and $B$ (where $A$ and $B$ have the subspace\ntopology induced by $X$).\n\nExample:\n(A generalisation of the previous remark on disconnected spaces)\nLet $X$ be a space and let $A_i (i=1\\ldots n)$ be finitely many closed\n(not disjoint, they're arbitary) subspaces of $X$\nin the subspace topology, of course. Assume that $\\bigcup_{i=1}^n A_i = X$.\nThen the topology on $X$ is equal to the final topology w.r.t. the inclusion\nmaps $k_i: A_i \\to X$.\nProof:\n$F$ is closed in the final topology w.r.t. the $k_i$, iff for all $i \\in \\{1,\\ldots,n\\}$,\n$(k_i)^{-1}[F]$ is closed in $A_i$. And this means, as $(k_i)^{-1}[F] = A_i \\cap F$,\nthat $F$ is closed in the final topology iff for all $i \\in \\{1,\\ldots,n\\}$\n$F \\cap A_i$ is closed. Now, if $F$ is closed in the original topology, then $A_i \\cap F$\nis closed in $A_i$, by the definition of the subspace topology, for all $i$.\nHence $F$ is closed in the final topology as well.\nOn the other hand, if $F$ is closed in the final topology, we know that $A_i \\cap F$ is closed\nin $A_i$. Then $A_i \\cap F$ is also closed in $X$\n($A_i \\cap F$ is closed in $A_i$ iff there is a closed set $C$ in $X$ such that $A_i \\cap F = A_i \\cap C$,\nand as $A_i$ is closed in $X$, and $C$ too, the intersection $A_i \\cap F$ is also closed in $X$)\nand $$F = F \\cap (\\bigcup_i A_i) = \\bigcup_i (A_i \\cap F)$$ is a finite union of closed sets of $X$, and\nthus $F$ is also closed in $X$. So these topologies indeed coincide.\nExample: (the same for open subspaces)\nLet $X$ be a space and let $O_i$ ($i \\in I$) be any collection of open subspaces of X\n(in the subspace topology), and suppose that $\\bigcup_i O_i = X$ (an open cover of $X$)\nThen the topology of $X$ is the final topology w.r.t. the inclusion maps\n$k_i : O_i \\to X$.\nProof: as in the previous example. Now work with open sets instead of closed ones, use\nthat an open subset of $O_i$ is open in $X$ too, and use that arbitrary unions of open\nsets are open.\nDefinition: a $T_1$ space $X$ is called a $k$-space, iff $X$ has the final topology w.r.t.\nall inclusion maps $i_C: C \\to X$, where $C$ is a closed and compact subspace of $X$.\n(the $T_1$ is mostly added to ensure we always have closed and compact subspaces,\nnamely the finite ones).\nOne easily shows that all first countable $T_1$ spaces and all locally compact $T_2$ spaces\nare examples of $k$-spaces. These spaces are important in some parts of algebraic topology\nand in the theory of quotient maps.\nA similar definition can be made for so-called sequential spaces (an equivalent definition\nto the usual one). Let $X$ be a $T_2$ space, and let $\\mathcal{S}$ be all sets of the form\n$\\{x_n : n \\in \\mathbb{N}\\} \\cup \\{x\\}$ (all points of $X$) such that $x_n \\to x$ in $X$.\nThen $X$ is sequential iff $X$ has the final topology w.r.t. all inclusions $i_A: A \\to X$ where $A$ is from $\\mathcal{S}$.\n\nAgain, as with the initial topology for maps into that space,\nhaving a final topology w.r.t. a family of maps makes it easy to see which maps\nstarting from that space are continuous. We have the following useful:\n###Universal theorem of continuity for final topologies.###\nLet $X$ be a space and $f_i: Y_i \\to X$ be a family of spaces and functions, such that\n$X$ has the final topology w.r.t. the maps $f_i$. Let $g: X \\to Z$ be a function from $X$\nto a space $Z$. Then\n\n($\\ast$) $g$ is continuous iff $\\forall i : g \\circ f_i : Y_i \\to Z$ is continuous.\n\nProof: if $g$ is continuous, then (as property 1 of the definition of final topology\ngives continuity of the $f_i$) all $g \\circ f_i$ are continuous as compositions of continuous\nmaps.\nSuppose now that all $g \\circ f_i$ are continuous, and let $O$ be any open subset of $Z$.\nWe want to show that $g^{-1}[O]$ is open in $X$, but by the existence theorem:\n$g^{-1}[O]$ is open in $X$ iff for all $i \\in I$ : $(f_i)^{-1}[ g^{-1}[O] ]$ is open\nin $Y_i$.\nBut the latter sets are just equal to $(g \\circ f_i)^{-1}[O]$, and so these sets are\nindeed all open, as we assumed that all $(g \\circ f_i)$ to be continuous. So $g^{-1}[O]$\nis open in $X$ and $g$ is continuous.\n\nThere is a converse to this as well:\n###Characterisation of the final topology by the continuity theorem.###\nLet $(X,\\mathcal{T})$ be a space, and $f_i: Y_i \\to X$ be a family of spaces and functions.\nSuppose that $X$ satisfies the universal continuity theorem in the following sense:\n\nIf for all spaces $Z$, and for all functions $g: X \\to Z$ property ($\\ast$) holds,\nthen $X$ has the final topology w.r.t. the maps $f_i$.\n\nProof:  the identity $1_X$ on $X$ is continuous, so applying ($\\ast$)\nfrom right to left with $g = 1_X$ gives us that all $f_i$ are continuous.\nIf $\\mathcal{T}'$ is another topology on $X$ that makes all\n$f_i$ continuous, then consider the map $g: (X, \\mathcal{T}) \\to (X, \\mathcal{T}')$,\ndefined by $g(x) = x$.\nThen all maps $g \\circ f_i$ are just the maps $f_i$ as seen between $Y_i$ and $(X, \\mathcal{T}'))$\nwhich are by assumption continuous. So by the other direction of ($\\ast$)\nwe see that $g$ is continuous,\nand thus (as $g(x) = x$, and thus $g^{-1}[O] = O$ for all $O$) we have that\n$\\mathcal{T}' \\subseteq \\mathcal{T}$,\nas required for the second property of the final topology.\n\nApplication: if $q:X \\to Y$, and $Y$ is a quotient space w.r.t. $q$, then a map $f: Y\\to Z$\nis continuous iff $f \\circ q$ is continuous between $X$ and $Z$.\nApplication: (sum maps).\nIf $f_i : X_i \\to Y$ is continuous, and $X$ is the (topological) sum of the spaces $X_i$,\nthen define the sum map $f: X \\to Y$ by $f((x,i))$ = $f_i(x)$. Then $f$ is continuous.\nThis follows as $k_i \\circ f = f_i$ by definition of $f$,\nand the universal continuity theorem.\nApplication: (more sum maps)\nLet the maps $f_i: X_i \\to Y_i$ be functions between spaces $X_i$ and $Y_i$.\nLet $X$ be the sum of the $X_i$ (with embeddings $k_i$) and $Y$ be the sum of the spaces $Y_i$\n(with embeddings $k'_i$). Define $f: X \\to Y$ by (whenever $x \\in X_i$) $f((x,i)) = (f_i(x), i)$.\nThen $f$ is continuous iff for all $i$ in $I$: $f_i$ is continuous.\nProof: we have the relation\n$$\\forall i \\in I:  f \\circ k_i = k'_i \\circ f_i$$\nSo if all $f_i$ are continuous, we see that all compositions $f \\circ k_i$\nare continuous, as composition of $k'_i$ and $f_i$, are continuous.\nAnd as $X$ has the final topology w.r.t. the $k_i$, we see by the universal continuity\ntheorem that $f$ is continuous.\nIf, on the other hand, $f$ is continuous, and $i$ is fixed, we see that\nf restricted to $k_i[X_i]$ maps to $k'_i[Y_i]$ by construction.\nAs $k_i$ is a homeomorphism between $X_i$ and its image, and likewise for $Y_i$ and $k'_i$\n(see above where we defined the sum topology) we have that\n$f_i = (k'_i)^{-1} \\circ f \\circ k_i$, which is thus continuous, again as a composition of\ncontinuous maps.\n\nApplication (glueing lemma for continuous maps)\nLet $X$ be a space $X = \\bigcup_{i=1}^n A_i$, where all $A_i$ are closed in $X$.\nLet $f$ be a function from $X$ to a space $Y$. Then\n$f$ is continuous iff for all $i \\in \\{1,\\ldots,n\\}$: $f|_{A_i}$ is continuous.\nProof: this follows from the similar example above, where we showed that\n$X$ has the final topology w.r.t. the inclusions $k_i: A_i \t\\to X$.\nIf $f$ is continuous, then all $f|_{A_i}$ are continuous, as compositions of $f$ and $k_i$,\nand if all $f \\circ k_i = f|_{A_i}$ are continuous, the universal continuity property\ngives that $f$ is continuous.\nSecond glueing lemma for continuous maps.\nLet $X$ be a space such that $X = \\bigcup_i O_i$, where all $O_i$ are open\nsubspaces of $X$.\nLet $f$ be a function from $X$ to a space $Y$. Then\n$f$ is continuous iff for all $i \\in I$ we have that $f|{A_i}$ is continuous.\nProof: analogous to the previous one, based on the similar example.\nApplication to $k$-spaces and sequential spaces:\nA map $f$ from a $k$-space $X$ to a space $Y$ is continuous iff for all\ncompact closed subspaces $C$ of $X$, $f|_C$ is continuous.\nA map $f$ from a sequential space $X$ to a space $Y$ is continuous iff\nfor all $A in \\mathcal{S}$, $f|_A$ is continuous (where $\\mathcal{S}$\nis defined as in the definition of a sequential space above), iff $f(x_n) \\to f(x)$ in $Y$\nwhenever $x_n \\to x$ in $X$.\n\nAs in the case of initial topologies we can state a transitive law here as well:\n###Transitive law of final topologies.###\n\nSuppose that we have a family of spaces $Y_i (i \\in I)$, a space $X$, and for each\n$i \\in I$ a collection of spaces $Z_j ( j \\in J_i)$ and functions $g_{i,j}: Z_j \\to Y_i$\nwhere $j \\in J_i, i \\in I$, and $f_i: Y_i \\to X$.\nAlso suppose that each space $Y_i$ has the final topology\nw.r.t. the maps $g_{i,j}$ ($j \\in J_i$). Then the following are equivalent:\n\n\nA. $X$ has the final topology w.r.t. the maps $\\{f_i \\circ g_{i,j}:  i \\in I, j \\in J_i\\}$\n\n\nB. $X$ has the final topology w.r.t. the maps $f_i$ ($i \\in I$).\n\nProof:\nSuppose A holds.\nI will apply the characterisation of the final topology\nby the universal continuity theorem, so let $h: X \\to T$ be an arbitrary function\nto some space $T$ then\n\n$h$ is continuous iff $h \\circ (f_i \\circ g_{i,j})$ is continuous\nfor all $i \\in I$ and all $j \\in J_i$. This is what A says, in essence.\n$h$ is continuous iff $(h \\circ f_i) \\circ g_{i,j}$ is continuous\nfor all $i \\in I$ and all $j \\in J_i$. (assocociativity of composition).\nApplying the fact that the $Y_i$ have the final topology w.r.t. the $g_{i,j}$ we conclude\n$h$ is continuous iff $h \\circ f_i$ is continuous for all $i \\in I$.\nthe last statement is equivalent to B.\n\nThis concludes the proof.\n\nAs applications we have e.g. that compositions of quotient maps are quotient maps,\nthat the sum of quotient maps is again a quotient map, and that the quotient\n$T_1$ image of a $k$-space is a $k$-space etc., etc.\nWe also see that direct limit spaces (for those who know them) are, as quotient\nspaces of sum spaces, final topologies themselves.\nNote however that initial and final topologies do not mix very well:\n\nthe product of quotient maps need not be a quotient map.\n\nA subspace of a quotient space of $X$, with restricted map, need\nnot be the quotient of a subspace of $X$.\n\n\nThere are some special cases where this does hold, but I do not know of very\ngeneral commutativity results, so to say, between general initial and final topologies.\nBut I do hope to have shown that these general considerations, though not very hard,\ndo give interesting generalisations of several well-known, seemingly isolated, facts.\n",
    "tags": [
      "general-topology",
      "proof-verification",
      "proof-writing",
      "quotient-spaces"
    ],
    "score": 7,
    "answer_score": 8,
    "is_accepted": false,
    "question_id": 2808047,
    "answer_id": 2813473
  },
  {
    "theorem": "Proof by induction; $a^n$ divides $b^n$ implies $a$ divides $b$",
    "context": "I want to prove by induction that $a^n \\mid b^n$ implies that $a \\mid b$ holds for all integers $n\\geq 1$.\nClearly for $n=1$ this is true, since if $a \\mid b$, then $a \\mid b$.\nSuppose this is true for some $n = k$. Then $a^k \\mid b^k$, so $a|b$.\n$a^k \\mid b^k$ means there exists some integer $m$ such that $b^k = ma^k$, and $a \\mid b$ means there exists an integer $r$ such that $b=ra$.\nThen we can multiply both sides of the above equality by any constant, namely $b$. \nThen $b \\cdot b^k = bma^k$\nso $b^{k+1} = (ra)ma^k = rma^{k+1}$\nwhich means that $a^{k+1} \\mid b^{k+1}$ completing the induction step, and thus proving that for all integers $n \\geq1$ the original statement holds.\nAre there any logical holes in the proof?\n",
    "proof": "Let $a,b$ be integers and let $P(k)$ be the statement \"$a^k \\mid b^k$ implies $a \\mid b$.\"  You are trying to prove by induction that $P(k)$ is true for all $k\\in \\mathbb{N}$.\nYou start out by proving the base case $P(1)$, which is fine.  Then you say \"suppose this is true for some $k$,\" by which I mean you are supposing that $P(k)$ is true.  But then you say, \"then $a^k \\mid b^k$.\"  This does not follow from $P(k)$, since $P(k)$ says nothing about whether $a^k$ divides $b^k$ or not, just about what happens if it does.\nWhat you need to do is show that if $P(k)$ is true, then $P(k+1)$ is also true.  Others have already talked about how to approach this, I just wanted to clarify a bit more what goes wrong with your attempt.\n",
    "tags": [
      "number-theory",
      "elementary-number-theory",
      "proof-writing",
      "induction"
    ],
    "score": 7,
    "answer_score": 6,
    "is_accepted": false,
    "question_id": 346034,
    "answer_id": 346079
  },
  {
    "theorem": "Prove the integral of $f$ is positive if $f ≥ 0$, $f$ continuous at $x_0$ and $f(x_0)&gt;0$",
    "context": "Prove that $\\int_a^b f(x)\\,dx  \\gt 0$ if $f \\geq 0$ for all $x \\in [a,b]$ and $f$ is continuous at $x_0 \\in [a,b]$ and $f(x_0) \\gt 0$\nEDIT. Please ignore below. It is very confusing actually -.-\n\nNote: After typing this all out, I think I realized that my proof is complete, but I'll just post it to make sure :)\nAttempt: \nFind a partition $P = \\{t_0,\\ldots,t_n\\}$ of $[a,b]$ s.t. $f(x)\\gt f(x_0)/2$ for any $x \\in [t_{i-1},t_i]$\nThus, the lower sum, $L(f,P)\\geq x_0 (t_{i}-t_{i-1})/2 > 0$\n[Basic Idea: since f is continuous at $x_0$, in the worst case scenario (i.e. $f=0$ at all points except in a nbhd of $x_0$) there must be some \"bump\" at $x_0$, which prevents the integral from actually equaling $0$. If we can find just one lower sum of a partition to be $> 0$, we will be done (I think...)]\n",
    "proof": "I take it for granted that you know that this integral is never negative. So you need to give a single interval over which it's positive.\n$f(x_0) > 0, f(x)$ continuous there means that there is a $\\delta$ such that for $|x-x_0|< \\delta$, $|f(x) - f(x_0)| < \\frac{f(x_0)}{2}$.\nSo consider the interval $[x-\\delta, x+\\delta]$.\nI think this is the easiest way to do it, perhaps.\n",
    "tags": [
      "integration",
      "proof-writing"
    ],
    "score": 7,
    "answer_score": 13,
    "is_accepted": true,
    "question_id": 82839,
    "answer_id": 82847
  },
  {
    "theorem": "IMO 2017: Determine all functions $f: \\mathbb{R} \\to\\mathbb{R}$ such that, for all real numbers $x$ and $y$, $f(f(x)f(y)) + f(x +y) = f(xy)$.",
    "context": "\nEDİT: I think I've repair the error in the solution. I want to know if I'm fixing it properly. I'm just a student, not a mathematician. Please focus on the \"backbone\" of my writing. \n\nI want to prove that, the following substitution is correct:\n\n$$ x\\mapsto f^{-1}(x)$$\n\nStatement: For a function to have an inverse, each element $y ∈ Y $ must correspond to no more than one $x ∈ X; $a function $f$ with this property is called one-to-one or an injection.\nThe function $f(x)$  is an injective and $f$ is an invertible function. In other words, if $f(0)≠0$, for function $f(x)$, the inverse function $f^{-1}(x)$ is exist.\n$f(x)=f(0)-\\frac {x}{f(0)} \\Rightarrow f^{-1}(x)=f(0)(f(0)-x)$ and  $f(f^{-1}(x))=x$ for all $x\\in\\mathbb {R}$. For this reason, we can applying the substitution $ x\\mapsto f^{-1}(x), x\\in\\mathbb{R}.$\nFor example:\nIf $f(f(x))=f(x)-5 , x\\in\\mathbb{R}$, then  $f(x)=x-5$ must be. Because,  the function $f(x)$  is an injective and $f$ is an invertible function. Applying  $x\\mapsto f^{-1}(x)$ we have $f(x-5)=x-10 \\Rightarrow f(x)=x-5.$\nI hope you can understand what I mean. Can you tell me that this approach is wrong or true? \n\nProblem:  Let $\\mathbb{R}$ be the set of real numbers. Determine all functions $f:  \\mathbb{R} \\to\\mathbb{R}$ such that, for all real numbers $x$ and $y$, $$f \\big(f(x)f(y)\\big) + f(x +y) = f(xy).$$ \n\nI ask You to confirm that the solution is sufficient / insufficient / missing / incorrect or correct. Here is my attempts: \n\nAre there any problems in my substitutions?\n\n\n\n",
    "proof": "Empy2 pointed out the error in your proof.  You cannot deduce that $f\\big(f(x)f(0)\\big)+f(x)=f(0)$ for all $x$ implies $f\\big(xf(0)\\big)+x=f(0)$ for all $x$.  This only works only for $x$ in the range of $f$.  I am presenting a different solution.\nIf $f(0)=0$, as you showed, we have\n$$f(0)+f(x)=f\\big(f(x)f(0)\\big)+f(x+0)=f(x\\cdot 0)=f(0)$$\nso $f(x)=0$ for all $x$.  We assume that $f(0)\\ne 0$ from now on.  We claim that $f(c)=0$ implies that $c=1$.\nSuppose that $f(c)=0$ for some $c\\ne 1$.  Then, we have\n$$f\\Biggl(f(c)f\\left(\\frac{c}{c-1}\\right)\\Biggr)+f\\left(c+\\frac{c}{c-1}\\right)=f\\Biggl(c\\left(\\frac{c}{c-1}\\right)\\Biggr).$$\nThat is,\n$$f(0)+f\\left(\\frac{c^2}{c-1}\\right)=f\\left(\\frac{c^2}{c-1}\\right),$$\nor $f(0)=0$, contradicting the assumption that $f(0)\\ne 0$.  Therefore, the hypothesis that $c\\ne1$ is false.\nNow, we note that for every $x\\ne1$, \n$$f\\Biggl(f(x)f\\left(\\frac{x}{x-1}\\right)\\Biggr)+f\\left(x+\\frac{x}{x-1}\\right)=f\\Biggl(x\\left(\\frac{x}{x-1}\\right)\\Biggr),$$\nso\n$$f\\Biggl(f(x)f\\left(\\frac{x}{x-1}\\right)\\Biggr)+f\\left(\\frac{x^2}{x-1}\\right)=f\\left(\\frac{x^2}{x-1}\\right).$$\nSo,\n$$f\\Biggl(f(x)f\\left(\\frac{x}{x-1}\\right)\\Biggr)=0$$\nand we then conclude from our claim above that\n$$f\\left(\\frac{x}{x-1}\\right)=\\frac{1}{f(x)}\\tag{1}$$\nfor every $x\\ne 1$.  In particular, this shows that $f(1)=0$, since there does exist $c$ such that $f(c)=0$.\nFrom (1), we get $f(0)=\\pm 1$.  Observe that $f$ is a solution if and only if $-f$ is also a solution.  We can without loss of generality assume that $f(0)=-1$.  Plugging $y\\mapsto 1$ in the original functional equation, we have\n$$-1+f(x+1)=f(0)+f(x+1)=f\\big(f(x)f(1)\\big)+f(x+1)=f(x\\cdot1)=f(x),$$\nor\n$$f(x+1)=f(x)+1.\\tag{2}$$\nBy induction, $f(x+n)=f(x)+n$ for all integers $n$.  \nHere, we claim that $f$ is injective.  Suppose that $f(u)=f(v)$ for some $u,v\\in\\mathbb{R}$.  Pick a positive integer $n$ so large that $4(u+n)< (v+n+1)^2$.  Hence, there are two distinct $a,b\\in\\mathbb{R}$ such that $u+n=ab$ and $v+n+1=a+b$ (since $a$ and $b$ are the roots of the polynomial $t^2-(v+n+1)t+(u+n)$, which have two distinct real roots).  Therefore,\n$$f\\big(f(a)f(b)\\big)+f(a+b)=f(ab)=f(u+n)=f(u)+n.$$\nBut $f(a+b)=f(v+n+1)=f(v)+n+1=f(u)+n+1$.  That is, by (2), we have $$f\\big(f(a)f(b)+1\\big)=f\\big(f(a)f(b)\\big)+1=0.$$  This means $f(a)f(b)+1=1$, or $f(a)f(b)=0$.  Consequently, $f(a)=0$ or $f(b)=0$, which means $a=1$ or $b=1$.  Without loss of generality, $b=1$, so $u+n=ab=a$ and $v+n+1=a+b=a+1$.  That is, $u=a-n=v$.\nNow, substitute $y\\mapsto 1-x$ in the original functional equation.  We then have\n$$f\\big(f(x)f(1-x)\\big)=f\\big(f(x)f(1-x)\\big)+f(1)=f\\big(x(1-x)\\big).$$\nThus, by injectivity,\n$$f(x)f(1-x)=x(1-x).$$\nThen, we take $y\\mapsto -x$ in the original functional equation to get\n$$f\\big(f(x)f(-x)\\big)-1=f\\big(f(x)f(-x)\\big)+f(0)=f\\big(x(-x)\\big).$$\nThus,\n$$f\\big(f(x)f(-x)\\big)=f(-x^2)+1=f(-x^2+1)$$\nby (2).  By injectivity,\n$$f(x)f(-x)=-x^2+1.$$\nFrom (2), we also have\n\\begin{align}f(x)&=f(x)\\Big(\\big(f(-x)+1\\big)-f(-x)\\Big)\\\\&=f(x)\\big(f(1-x)-f(-x)\\big)=f(x)f(1-x)-f(x)f(-x),\\end{align}\nso\n$$f(x)=x(1-x)-(-x^2+1)=x-1.$$\nIt is easy to see that $f(x)=x-1$ is indeed a solution.  Therefore, all solutions are $f(x)=0$, $f(x)=x-1$, and $f(x)=1-x$.\n",
    "tags": [
      "functions",
      "proof-verification",
      "proof-writing",
      "contest-math",
      "functional-equations"
    ],
    "score": 7,
    "answer_score": 12,
    "is_accepted": false,
    "question_id": 2968359,
    "answer_id": 2969134
  },
  {
    "theorem": "Some trouble with the induction",
    "context": "\nProve, that for any positive integer $n \\geqslant 2$ we have the\n  inequality $$ \\frac{ 4^n }{ n+1 } < \\frac{ (2n)! }{ (n!)^2 }.$$\n\n\nFor $n=2$ the inequality is true. Directly just take and prove inequality for $k+1$ problematically. So, I think we need to find the recurrence relation to one of the members of inequality. Need some hint!\n",
    "proof": "Let $a_n = \\frac{4^n}{n+1}$ and $b_n=\\frac{(2n)!}{n!^2}=\\binom{2n}{n}$. Then $a_1=b_1$ and:\n$$ \\frac{a_{n+1}}{a_n} = 4 \\frac{n+1}{n+2},\\qquad \\frac{b_{n+1}}{b_n} = 2\\,\\frac{2n+1}{n+1}\\tag{1} $$\nhence we just need to check that:\n$$ \\forall n\\geq 1,\\qquad \\frac{2n+2}{n+2}< \\frac{2n+1}{n+1} \\tag{2} $$\nholds to prove our claim by induction.\n\nAlso notice that:\n$$\\binom{2n}{n}=\\sum_{j=0}^{n}\\binom{n}{j}^2 > \\frac{\\left(\\sum_{j=0}^{n}\\binom{n}{j}\\right)^2}{\\sum_{j=0}^{n} 1}=\\frac{4^n}{n+1}\\tag{3}$$\nfollows from the Vandermonde's identity and the Cauchy-Schwarz inequality.\n",
    "tags": [
      "inequality",
      "proof-writing",
      "induction",
      "binomial-coefficients",
      "factorial"
    ],
    "score": 7,
    "answer_score": 10,
    "is_accepted": true,
    "question_id": 1343243,
    "answer_id": 1343247
  },
  {
    "theorem": "Complex numbers - Exponential numbers - Proof",
    "context": "Let $z$ be a complex number, and let $n$ be a positive integer such that $z^n = (z + 1)^n = 1$. Prove that $n$ is divisible by 6.\nFor this problem I am stumped...how should I begin?\nAlso there's a hint for it:\nFrom $z^n = 1$, prove that $|z| = 1$. What does the equation $(z + 1)^n = 1$ tell you? What do the resulting equations tell you about $z$?\nCould someone give me a hint on where to begin? thanks in advance\n",
    "proof": "A problem way too cool and cute to pass up, so check this out:\n$(1.) \\; z^n = 1 \\Rightarrow \\vert z \\vert^n = 1, \\tag{1}$\n$(2.) \\; \\vert z \\vert^n = 1 \\Rightarrow \\vert z \\vert = 1 \\Rightarrow \\exists \\theta \\in \\Bbb R  \\;\\text{such that} \\; z = e^{i\\theta}, \\tag{2}$\n$(3.) \\; \\vert z \\vert = 1 \\Rightarrow z \\bar z = 1, \\tag{3}$\n$(4.) \\; (z + 1)^n = 1 \\Rightarrow \\vert z + 1 \\vert^n = 1 \\Rightarrow \\vert z + 1\\vert = 1, \\tag{4}$\n$(5.) \\; \\vert z + 1 \\vert = 1 \\Rightarrow (1 + z)(1 + \\bar z) = 1, \\tag{5}$\n$(6.) \\;  (1 + z)(1 + \\bar z) = 1 \\Rightarrow z \\bar z + z + \\bar z + 1 = 1 \\Rightarrow z \\bar z + z + \\bar z = 0, \\tag{6}$\n$(7.) \\; \\text{by (3) and (6),} \\; z + \\bar z = -1, \\tag{7}$\n$(8.) \\; \\text{by (2) and (7),} \\; 2 \\Re{z} = 2 \\cos \\theta = -1 \\Rightarrow \\cos \\theta = -\\dfrac{1}{2}, \\tag{8}$\n$(9.) \\; \\cos \\theta = -\\dfrac{1}{2} \\Rightarrow \\Re{(1 + z)} = 1 + \\cos \\theta = \\dfrac{1}{2}, \\tag{9}$\n$(10.) \\; \\Re{(1 + z)} = \\dfrac{1}{2} \\; \\text{and} \\; \\vert 1 + z \\vert = 1 \\Rightarrow 1 + z = e^{(\\pm 2\\pi i/ 6) + 2k \\pi}, k \\in \\Bbb Z, \\tag{10}$\n$(11.) \\;  1 + z = e^{(\\pm 2\\pi i/ 6) + 2k \\pi} \\Rightarrow 1 = (1 + z)^n = e^{\\pm 2n \\pi i / 6}, \\tag{11}$\n$(12.) \\; e^{\\pm 2n \\pi i / 6} = 1 \\Rightarrow 6 \\mid n. \\tag{12}$\nQED\n",
    "tags": [
      "trigonometry",
      "complex-numbers",
      "proof-writing"
    ],
    "score": 7,
    "answer_score": 4,
    "is_accepted": true,
    "question_id": 643024,
    "answer_id": 643110
  },
  {
    "theorem": "Proof that $\\sqrt{2}$ is irrational",
    "context": "I am having hard time flowing a proof from Introduction to the theory of computation (3rd ed.) by Michael Sipser.   Please points out what I am missing.  My questions is \"Now, at least one of $m$ and $n$ must be add odd number.\" Why can we make such a statement already? \nPROOF First, we assume for the purpose of later obtaining a contradiction\nthat $\\sqrt{2}$ is rational.\nThus $\\sqrt{2} = \\dfrac{m}{n}$,\nwhere $m$ and $n$ are integers. If both m and n are divisible by the same integer greater than $1$, divide both by the largest such integer. Doing so doesn’t change the value of the fraction. Now, at least one of m and n must be an odd number. We multiply both sides of the equation by n and obtain\n$n\\cdot \\sqrt{2} = m$.\nWe square both sides and obtain \n$2\\cdot n^2 = m^2$.\nBecause m2 is 2 times the integer n2, we know that m2 is even. Therefore, m, too, is even, as the square of an odd number always is odd. So we can write m = 2k for some integer k. Then, substituting 2k for m, we get\n$2n^2 = (2k)^2 = 4k^2$. \nDividing both sides by $2$, we obtain \n$n^2 = 2k^2$.\nBut this result shows that $n^2$ is even and hence that $n$ is even. Thus we have established that both $m$ and $n$ are even. But we had earlier reduced $m$ and $n$ so that they were not both even—a contradiction.\n",
    "proof": "If $m$ and $n$ are both even you can still divide both by 2. Absurd we already divided them by all integers possible greater than 1. Thus, at least 1 of them is odd.\n",
    "tags": [
      "proof-verification",
      "proof-writing",
      "proof-explanation"
    ],
    "score": 7,
    "answer_score": 10,
    "is_accepted": true,
    "question_id": 2705454,
    "answer_id": 2705456
  },
  {
    "theorem": "Show that if $f : \\mathbb{R}^{2} \\to \\mathbb{R}$ continuously differetiable then $f$ is not inyective",
    "context": "Well my question this time is:\nHow to show that $f : \\mathbb{R}^{2} \\to \\mathbb{R}$ continuously differetiable then $f$ is not inyective \nI was trying to consider the function $g(x,y)=(f(x,y),y)$, but I don't know how to proceed with $g$, I know I have to use the implicit function theorem but when I was thinking of it, I remember that nearly $(x_0,y_0)$, $f$ must be zero, well $f(x_0,y_0)=0$, then the only tool I thought it could help I can't apply it.\nCan someone help me please with this problem?, and once you have it for this case, Can we generalize to $R^{m}$ and $R^{n}$?(of course $m<n$) \nThanks a lot in advance\nMy implicit function theorem:\nLet $A \\subset R^{m} \\times R^{n}$ open, $(x_0,y_0) \\in A$, $f:A \\to R^{m}$ continously differentiable nearly $(x_0,y_0)$ and  $f(x_0,y_0)=0$. Let M be the matrix of $m \\times m$ given by:\n$$D_{n+j}f^{i}(x_0,y_0)$$\nand suppose that $\\det(M) \\not= 0$, then there exists an open set $U \\subset R^{n}$ $x_0 \\in U$, and an open $V \\subset R^{m}$, $y_0 \\in V $, such that, for each $x$ there exists a unique $g(x) \\in V$ such that $f(x,g(x))=0$, $g$ differentiable\nI don't know Topology, or more than introduction to analysis and a little bit of multivariable analysis, thanks a lot for your help \n",
    "proof": "Actually the differentiable hypothesis is unnecessary, we only need continuity of f. If f were injective then $f^{-1}(a)$ is a single point for all a in the image of f. However $\\mathbb{R}$ - {a} is disconnected while $\\mathbb{R}^2$ minus a point is connected. A fact from general topology says that the image of a connected set under a continuous mapping must be connected and so we have a contradiction.\nNote: I am not sure if this is an answer you will find acceptable, but this question has been tagged as general topology so hopefully this is relevant to you.\nP.S. A similar argument works for all dimensions.\n",
    "tags": [
      "multivariable-calculus",
      "proof-writing"
    ],
    "score": 7,
    "answer_score": 5,
    "is_accepted": false,
    "question_id": 1172814,
    "answer_id": 1172825
  },
  {
    "theorem": "Maps - question about $f(A \\cup B)=f(A) \\cup f(B)$ and $ f(A \\cap B)=f(A) \\cap f(B)$",
    "context": "I am struggling to prove this map statement on sets. \nThe statement is: \nLet $f:X \\rightarrow Y$ be a map. \ni) $\\forall_{A,B \\subset X}: f(A \\cup B)=f(A) \\cup f(B)$\nii) $\\forall_{A,B \\subset X}: f(A \\cap B) \\subset f(A) \\cap f(B)$\niii) $f$ is injective $ \\Longleftrightarrow$ $\\forall_{A,B \\subset X}: f(A \\cap B)=f(A) \\cap f(B)$\nMy problem is: I know how to operate on sets, I know how to operate on sets, but I don't know how and where to start the proof,  the biggest problem in mathematics, I think. \nThanks for help!\n",
    "proof": "For the first $y \\in f(A \\cup B) \\iff y=f(x)$ for some $x \\in A \\cup B \\iff y = f(x)$ for some $x \\in A$ or $x \\in B \\iff y \\in f(A)$ or $y \\in f(B) \\iff y \\in f(A)\\cup f(B).$\nThe second should be $f(A\\cap B)\\subseteq f(A)\\cap f(B)$ and the proof is similar. \nFor the third if $f(A \\cap B) = f(A) \\cap f(B) \\Rightarrow$ for every $x,y \\in X$ with $x \\neq y, \\ \\emptyset=f(\\emptyset)=f(\\{x\\} \\cap \\{y\\}) = f(\\{x\\}) \\cap f(\\{y\\})$. Thus $f(x)\\neq f(y)$ and $f$ is injective. Can you prove the other direction?\nThis is the difficult one. In case you give up here is the solution:\n\nIts enough to show that if $f(A \\cap B) \\subset f(A) \\cap f(B)$ for some $A,B \\subseteq X$ then $f$ is not injective. So we suppose that there is a $y \\in f(A) \\cap f(B)$ s.t. $y \\not \\in f(A \\cap B)$. So $y=f(x_1)=f(x_2)$ for some $x_1 \\in A-(A\\cap B)$ and $x_2 \\in B-(A\\cap B)$. The proof now is finished.\n\n",
    "tags": [
      "functions",
      "elementary-set-theory",
      "proof-writing",
      "problem-solving"
    ],
    "score": 7,
    "answer_score": 9,
    "is_accepted": true,
    "question_id": 239783,
    "answer_id": 239791
  },
  {
    "theorem": "Prove that $\\sqrt{11}-1$ is irrational by contradiction",
    "context": "I am working on an assignment in discrete structures and I am blocked trying to prove that $\\sqrt{11}-1$ is an irrational number using proof by contradiction and prime factorization.\nI am perfectly fine doing it with only $\\sqrt{11}$, but I am completely thrown off by the $-1$ when it comes to the prime factorization part.\nMy current solution looks like this :\n$$ \\sqrt {11} -1 = \\frac {a}{b}$$\n$$ \\sqrt {11} = \\frac {a}{b} + 1$$\n$$ \\sqrt {11} = \\frac {a+b}{b}$$\n$$ 11 = \\left(\\frac {a+b}{b}\\right)^2$$\n$$ 11 = \\frac {(a+b)^2}{b^2}$$\n$$ 11 = \\frac {a^2 + 2ab + b^2}{b^2}$$\n$$ 11 b^2 = a^2 + 2ab +b ^2$$\n$$ 10b^2 = a^2 + 2ab $$\nAt that point, is it acceptable to conclude that a² is a multiple of 11 even though we have a trailing $2ab$?\nThe required method is then to conclude using prime factorization that $a = 11k$ and replace all that in the formula above to also prove $b$, however, I am again stuck with the ending $2ab$.\nWould it instead be correct to prove that $\\sqrt{11}$ is rational using the usual method and that, by extension, $\\sqrt{11} - 1$ is also rational?\nThank you\n",
    "proof": "No, you cannot conclude that $a^2$ is a multiple of $11$. You can instead rewrite\n$$\na^2=10b^2-2ab=2(5b^2-ab)\n$$\nso $2\\mid a$. Write $a=2c$, with $c$ integer. Then\n$$\n4c^2=2(5b^2-2bc)\n$$\nor $5b^2=2(c^2+bc)$. Since $2\\nmid 5$, we conclude $2\\mid b$.\nThis is a contradiction to $a$ and $b$ being coprime.\n",
    "tags": [
      "elementary-number-theory",
      "proof-verification",
      "proof-writing",
      "irrational-numbers"
    ],
    "score": 7,
    "answer_score": 4,
    "is_accepted": true,
    "question_id": 2789537,
    "answer_id": 2790098
  },
  {
    "theorem": "Proving or disproving product of two stochastic matrices is stochastic",
    "context": "\nLet $P$ and $Q$ be two stochastic matrices. Does the product $PQ$ have to be stochastic? Prove or disprove.\n\nWhat Im thinking is that since matrix multiplication is only defined for two matrices $A$ and $B$ where $A$ has the same amount of columns as $B$ has rows and vice versa. Therefore the product of any two stochastic matrices $P$ and $Q$ can not be stochastic. This is not much of a proof though.\n",
    "proof": "Note that 'stochastic' can have three meanings: the matrix $A$ might be row stochastic, so $\\sum_{i=1}^nA_{ji}=1$ for each row $j\\in\\{1,\\ldots,n\\}$, column stochastic, so $\\sum_{i=1}^nA_{ij}=1$ for each column $j\\in\\{1,\\ldots,n\\}$, or both, and in that case the matrix is called doubly stochastic. I show a way to prove that the product of two row stochastic matrices is again row stochastic, and I leave the proof for column stochastic matrices to you. Obviously if a matrix is doubly stochastic, it follows from the first two cases that the product is again doubly stochastic.\nNow suppose that we have two row stochastic matrices $A$ and $B$, so for each row $j\\in\\{1,\\ldots,n\\}$ we have $\\sum_{i=1}^nA_{ji}=1=\\sum_{i=1}^nB_{ji}$. Now consider the sum of the elements on row $j\\in\\{1,\\ldots,n\\}$ of the product of the two matrices, $AB$. We find \n$$\\sum_{i=1}^n(AB)_{ji}=\\sum_{i=1}^n\\left(\\sum_{k=1}^nA_{jk}B_{ki}\\right)=\\sum_{k=1}^n\\left(A_{jk}\\left(\\sum_{i=1}^nB_{ki}\\right)\\right)=\\sum_{k=1}^nA_{jk}\\cdot1=1.$$\nYou can swap the order of the summation because the sum is finite and because each $A_{jk}$ does not depend on $i$. You can also see this by rewriting the summation as $\\sum_{i=1}^n\\langle A_{j},B_i\\rangle=\\langle A_j,(1,\\ldots,1)^T\\rangle=1$, with $A_j^T$ the $j$'th row of $A$, and $B_i$ the $i$'th column of $B$, and using the linearity of the inner product. Thus the product is indeed row stochastic.\nNote that troughout I assumed that a stochastic matrix is square, which is true by definition.\nNote that it simplifies a lot if you recognize that a matrix being row stochastic is equivalent to $Ae=e$, with $e:=(1,\\ldots,1)^T\\in\\mathbb{R}^{n\\times1}$.\n",
    "tags": [
      "linear-algebra",
      "matrices",
      "proof-writing",
      "stochastic-matrices"
    ],
    "score": 7,
    "answer_score": 9,
    "is_accepted": true,
    "question_id": 2773442,
    "answer_id": 2773481
  },
  {
    "theorem": "Essential vs. non-essential uses of proof-by-contradiction",
    "context": "I often come across proofs that resort to proof-by-contradiction unnecessarily.  For example, I recently came across the following.\n\nLet $\\mathbf{v}_1, \\ldots, \\mathbf{v}_n$ be linearly independent vectors.  Then, all linear combinations $$a_1 \\mathbf{v}_1 + \\cdots + a_n \\mathbf{v}_n$$ are unique.\nProof:  We will assume the contrary, and deduce a contradiction.  Therefore, suppose that $$a_1 \\mathbf{v}_1 + \\cdots + a_n \\mathbf{v}_n = b_1 \\mathbf{v}_1 + \\cdots + b_n \\mathbf{v}_n \\;,$$\nwhere $a_i \\ne b_i$ for at least one index $1 \\le i \\le n$.  This means, for such an index $i$, we have $a_i - b_i \\ne 0$.  But then this, together with the equality $$(a_1 - b_1) \\mathbf{v}_1 + \\cdots + (a_i - b_i) \\mathbf{v}_i + \\cdots + (a_n - b_n) \\mathbf{v}_n = 0 \\;,$$ contradict the linear independence of the $\\mathbf{v}_1, \\ldots, \\mathbf{v}_n$.  QED.\n\nIn this case at least, proof-by-contradiction can be easily avoided, as shown by this very minor rewording:\nProof:  Suppose that $$a_1 \\mathbf{v}_1 + \\cdots + a_n \\mathbf{v}_n = b_1 \\mathbf{v}_1 + \\cdots + b_n \\mathbf{v}_n \\;.$$\nTherefore $$(a_1 - b_1) \\mathbf{v}_1 + \\cdots + (a_i - b_i) \\mathbf{v}_i + \\cdots + (a_n - b_n) \\mathbf{v}_n = 0 \\;,$$ together with the linear independence of the $\\mathbf{v}_1, \\ldots, \\mathbf{v}_n$ imply that $a_i - b_i = 0$ for all indices $1 \\le i \\le n$.  This means that two linear combinations of the $\\mathbf{v}_1, \\ldots, \\mathbf{v}_n$ are equal only when all their coefficients agree.  Therefore, these linear combinations are unique.  QED.\n\nSince proof-by-contradiction is still taught in schools as an important technique I have to assume that not all uses of proof-by-contradiction are avoidable.\nI have two questions:\n\nis there a technical term for such avoidable uses of proof-by-contradiction?\nis there a way to characterize those uses of proof-by-contradiction that are not avoidable?; is it possible to characterize the class of statements that can be proved only by contradiction?\n\nI realize that, the price of avoiding a proof-by-contradiction may be an unbearably tedious proof.  I also realize that, in my second question above, I may be \"rushing where angels fear to tread\", namely areas of undecidability, etc.  Please let me know.\n\nEDIT: removed mentions of reductio ad absurdum, which (as I just learned from @DerekElkins's comment) I was using incorrectly.\n",
    "proof": "The most common class of logics that don't support proof by contradiction are intuitionistic or constructive logics. (I'll use \"constructive\" and \"intuitionistic\" more or less interchangeably here.) Specifically, this means that they don't support any of the following equivalent statements (or any other equivalent statement): the Law of Excluded Middle ($P\\lor\\neg P$), Double Negation Elimination ($\\neg\\neg P\\implies P$), Peirce's Law ($((P\\implies Q)\\implies P)\\implies P$), Contraposition ($(\\neg Q\\implies \\neg P)\\implies (P\\implies Q)$).\nProof by contradiction corresponds to a slight variant of Double Negation Elimination. Namely, even intuitionistically $\\neg P \\equiv (P\\implies \\bot)$. (In fact, this is often true by definition in intuitionistic systems.) Proof by contradiction is then $(\\neg P \\implies \\bot)\\implies P$. This is not to be conflated with reductio ad absurdum which is $(P\\implies \\bot)\\implies\\neg P$ which is valid intuitionistically. (Again, often by definition.) Notice how we can't use reductio ad absurdum to get proof by contradiction by simply instantiating it with $\\neg P$; that just gives us $(\\neg P \\implies \\bot)\\implies \\neg\\neg P$. We'd still need Double Negation Elimination to get $P$.\nIt's extremely common to conflate the above rules so it is often unclear which people mean when they say \"proof by contradiction\" or \"reductio ad absurdum\". I make the choice I do because reductio ad absurdum (as I've defined it) doesn't prove something, it refutes it, while proof by contradiction does prove something. When I want to be clear though, I usually call use Double Negation Elimination for proof by contradiction and $\\neg$ introduction for reductio ad absurdum as $\\neg$ introduction is unambiguous (but this terminology is usually not familiar to those who haven't studied a bit of [structural] proof theory).\nTerminology out of the way and to answer your actual questions, theorems that can be proven constructively (i.e. in constructive or intuitionistic logic) don't \"essentially\" require proof by contradiction, even if there is a proof that does use it. Theorems that aren't true in intuitionistic logic do \"essentially\" require proof by contradiction (assuming they are true classically). If we interpret \"true\" as \"not refutable\", then we can embed classical logic into intuitionistic logic. This process is mechanical and not only does it allow us to mechanically translate classically provable theorems into intuitionistically provable theorems, but it also mechanically translates classical proofs into intuitionistic proofs. Of course, the theorems are \"weaker\" on the intuitionistic side, e.g. Double Negation Elimination would be translated to $\\neg\\neg P \\implies \\neg\\neg P$ by these translations.\nTheoretically, constructive proofs can be no easier than classical proofs since a constructive proof is a classical proof. In practice, many classical proofs are constructive and restricting oneself to a constructive proof limits the search space a bit (and makes it a bit better structured). It's pretty easy to argue that classical principles like Double Negation Elimination are non-intuitive, and I will touch on that in a bit. However, a constructive proof, in general, contains a lot more information than a classical proof. The term \"constructive\" comes from the idea that a proof should provide a means to \"construct\" a \"witness\". A constructive proof corresponds to an algorithm for actually constructing a witness to a statement. This is most dramatic for existential statements. To constructively prove an existential statement means you have to actually build the thing that is claimed to exist. This can be a lot more work (and even impossible) than merely claiming something exists without providing any means (and there may be no means) of creating the thing that supposedly exists as many classical proofs do. On the other hand, explicit constructions are often highly informative, and a significant part of the content of a theorem may actually be the explicit construction used in the proof. (For example, I'm reading \"Computational Complexity: A Modern Approach\". The famous Cook-Levin theorem that proves that SAT is NP-complete is proven by encoding the execution of a Turing machine as a (polynomial-sized) Boolean formula. This construction can then be used to prove that a problem is in P if and only if there is a polynomial-sized logspace-uniform family of circuits that solve the problem. It's not at all obvious why SAT being NP-complete would imply this, but from the proof of Cook-Levin it's not that hard to see.)\nSo constructive proofs can require (potentially much) more work than classical proofs not \"just\" because they are working in a weaker system (which is itself very highly valuable as many more interesting things are models of intuitionistic logic than classical logic), but also such proofs are more informative. Another way to see the difference in work is to look at the computational interpretation of these systems. The Curry-Howard correspondence famously shows that propositions, proofs, and proof transformations correspond to types, programs, and program transformations. The most specific interpretation of \"Curry-Howard correspondence\", which at this point is often taken more like a principle, is the specific result that the Natural Deduction presentation of Intuitionistic Propositional Logic corresponds to the Simply Typed Lambda Calculus. More recently, a similar correspondence has been found for classical logics. The notable thing here is that classical principles correspond to control operators like call/cc. Indeed, the type of call/cc is $((A\\to B)\\to A)\\to A)$ which looks exactly like Peirce's Law! These are (in)famously expressive but difficult to understand (just like proof by contradiction). This means that using call/cc can result in much shorter programs, but call/cc itself is difficult to understand and it complicates the understanding of every part of the program, even parts that don't themselves use call/cc. The embedding of classical logic into intuitionistic logic I mentioned earlier corresponds to Continuation-Passing Style (CPS) which is a method often used to understand and implement control operators. Here's an interactive description in story form that illustrates what actually happens with the Law of Excluded Middle computationally (modulo artistic license, but see the referenced paper by Phil Wadler for the original story [which I prefer] and the less colorfully presented details).\nAs a final note, there's a difference between a specific theorem being proven constructively and working in a constructive logic. We can have a constructive proof that manipulates non-constructively provable statements. This corresponds to writing a program that has access to subroutines that are not themselves implementable (often this is called an oracle). Working within constructive logic generally requires a decent amount of reworking of various definitions. For example, constructively various classically equivalent formulations of \"finite\" become constructively inequivalent, and so it matters how you specify that a set is \"finite\". Similarly, the usual formulation of the Intermediate Value Theorem is not true constructively, but a suitably similar statement can be proven (within the context of a constructive formulation of real numbers). At this point large swathes of mathematics have been redone in constructive frameworks. However, even within a classical framework, there is a lot of value to constructive proofs, and there is a lot of value to knowing which results are constructively provable.\n",
    "tags": [
      "logic",
      "proof-writing",
      "terminology"
    ],
    "score": 7,
    "answer_score": 7,
    "is_accepted": true,
    "question_id": 2590305,
    "answer_id": 2590593
  },
  {
    "theorem": "Prove that $r+x$ is irrational",
    "context": "If $r$ is rational ($r$$\\ne$$0$) and $x$ is irrational, prove that $r+x$ is irrational.\nAssume that $r+x$ is rational. Then $r+x=(\\frac{p}{q})$, where $p$ , $q$ are $\\in$ $\\mathbb{Z}$, and $p$ and $q$ are in lowest terms. Then we have $x=(\\frac{p}{q})-r$= $(\\frac{p-rq}{q})$. Since $p-rq$ is $\\in$ $\\mathbb{Z}$, $x$ is rational. This is a contradiction since $x$ was assumed to be irrational. Therefore, $r+x$ is irrational. QED\nI wanted to try proving by contradiction. Just wanted to know if everything looked okay.\n",
    "proof": "hint\nObserve that if $x+r $ is rational, then\n$$(x+r)+(-r)=x $$\nas a sum of two rationals will be rational. $(\\Bbb Q $ is a field $) $.\n",
    "tags": [
      "proof-writing"
    ],
    "score": 7,
    "answer_score": 6,
    "is_accepted": false,
    "question_id": 2415800,
    "answer_id": 2415869
  },
  {
    "theorem": "Probability of symmetric difference",
    "context": "How do we prove the following inequality?\n$$\\mathbb{P}[A\\triangle B ]\\geq \\max\\left \\{ \\mathbb{P}[A-B],\\mathbb{P}[B-A] \\right \\}$$\nI already proved that\n$$\\mathbb{P}[A\\triangle B ]=\\mathbb{P}[A]+\\mathbb{P}[B]-2\\mathbb{P}[A\\cap B]$$\n$$\\left | \\mathbb{P}[A]-\\mathbb{P}[B] \\right |\\leq \\mathbb{P}[A\\triangle B ]$$\n",
    "proof": "We have\n$$P(A-B)=P(A)-P(A\\cap B)$$\n$$P(B-A)=P(B)-P(B\\cap A)$$\nUsing these we can rewrite the first relation as\n$$P(A\\triangle B)=P(A-B)+P(B-A)$$\nSince probabilities are non-negative:\n$$P(A\\triangle B)\\ge\\max(P(A-B),P(B-A))$$\n",
    "tags": [
      "probability",
      "proof-writing"
    ],
    "score": 7,
    "answer_score": 6,
    "is_accepted": true,
    "question_id": 2683936,
    "answer_id": 2683949
  },
  {
    "theorem": "prove that $2\\sqrt5 +\\sqrt{11}$ is irrational",
    "context": "how would you prove that $2\\sqrt5 +\\sqrt{11}$ is irrational?\nI started with a proof by contradiction that assumes that $2\\sqrt5 +\\sqrt{11}$ is rational and therefore there exist integers $a$ and $b$ such that $\\frac{a}{b}=2\\sqrt5 +\\sqrt{11}$ and squaring both sides yields\n$\\frac{a^2}{b^2}=31 +4\\sqrt5\\sqrt{11}$ and from this point im stuck as i dont know how to continue to arrive at a contradiction.\n",
    "proof": "Here's a more advanced approach, with some common details to the above. \nA monic polynomial with integer coefficients:\n$$ (x^2 - (2\\sqrt 5 + \\sqrt{11})^2)(x^2 - (2\\sqrt 5 - \\sqrt{11})^2) = x^4 - 62x^2 +81 $$\nNow $x = 2\\sqrt 5 + \\sqrt{11}$ is a root of this polynomial, but by the Rational Roots Thm., any rational root would be an integer divisor of $81$.  \nThus one only needs to verify that $2\\sqrt 5 + \\sqrt{11}$ is not an integer.  A simple hand computation (or calculator computation) shows this positive number lies strictly between $7$ and $8$.\n",
    "tags": [
      "algebra-precalculus",
      "proof-writing",
      "irrational-numbers",
      "rational-numbers"
    ],
    "score": 7,
    "answer_score": 5,
    "is_accepted": false,
    "question_id": 2533270,
    "answer_id": 2534726
  },
  {
    "theorem": "What is the most used method for proving continuity for simple functions such as $f(x) = x^{1/3}$",
    "context": "In analysis we talked about a very general definition of continuity:\n$f:A \\to B$ is continuous if $U \\subset B$ is open, $f^{-1}(U) = V \\subset A $ is open\nQuite elegant\nAnother definition is if $(x_n) \\to x \\in A$, then $f(x_n) \\to f(x) \\in B$\nOk but you have to construct a sequence and then assume $x$ exists\nThen there is the $(\\epsilon-\\delta) $definition, which is difficult to remember and honestly I have no clue how to use it properly\n\nI just want to show that $f(x) = x^{1/3}$ or $f(x) = x^2$ is continuous. \nWhat is the most common way to prove continuity of these simple simple functions?\n",
    "proof": "I'll proceed in a standard way to show that $x^2$ is continuous.\nLet $f:\\mathbb{R} \\to \\mathbb{R}$ be defined by $f(x)=x^2$.\nLet $\\epsilon>0$. Then we want to show that $|f(x)-f(c)|<\\epsilon$ if $|x-c|<\\delta$. So we want (working \"backwards\"): $|x^2-c^2|=|x+c|\\cdot|x-c|<\\epsilon$. The problem here, is that $\\delta$ cannot depend on $x$. So to remedy this problem, WLOG assume that $\\delta<1$, and also that $c>0$. Then\n $$|x-c|<1 \\implies -1<x-c<1 \\implies c-1<x<c+1 \\implies 2c-1<x+c<2c+1$$.\nThen $|x+c|<2c+1$. So then, we want: $$|x+c|\\cdot |x-c|<(2c+1)\\cdot \\delta<\\epsilon$$. So we want to set $\\delta=\\frac{\\epsilon}{2c+1}$.\nNow we are ready for the \"official proof\":\nLet $\\epsilon>0$. Let $\\delta=\\frac{\\epsilon}{2c+1}$. Suppose that $x \\in \\mathbb{R}$ and that $|x-c|<\\delta$. Then $|f(x)-f(c)|=|x+c|\\cdot|x-c|<\\delta\\cdot(2c+1)=\\frac{\\epsilon}{2c+1}\\cdot(2c+1)=\\epsilon$, as desired.\nTo aid intuition, this is almost the same thing as the \"open set\" definition you give:\nHere: let $V$ be a neighborhood of $f(c)$. Then there exists some open ball of $f(c)$ such that $c \\in B_{\\epsilon}(f(c),\\epsilon) \\subseteq V$. Then we want there to exist some open ball $B_{\\delta}(c,\\delta)$ such that $f(B_{\\delta}(c,\\delta)) \\subseteq B_{\\epsilon}(f(c),\\epsilon)$. But an open ball is literally defined in $\\mathbb{R}$ with the standard topology to be all $y \\in \\mathbb{R}$ such that $|f(c)-y|<\\epsilon$. All we want to show is that for each $\\epsilon-ball$ there is some corresponding $\\delta-ball$ such that $|x-c| \\implies |f(x)-f(c)|<\\epsilon$ ( or more simply that $x \\in B_{\\delta}(c,\\delta) \\implies f(x) \\in B_{\\epsilon}(f(c),\\epsilon) \\subseteq V$) , since this implies that the image of the $\\delta-ball$ is in the neighborhood $V$, implying local continuity for an arbitrary point $c$.\n",
    "tags": [
      "calculus",
      "algebra-precalculus",
      "functions",
      "proof-writing"
    ],
    "score": 7,
    "answer_score": 1,
    "is_accepted": true,
    "question_id": 1617768,
    "answer_id": 1617781
  },
  {
    "theorem": "Show $\\lim_{n \\to \\infty} \\min\\{a_{n},b_{n}\\} = \\min\\{a,b\\}$",
    "context": "If $\\lim_{n \\to \\infty} a_{n} = a$ and $\\lim_{n \\to \\infty} b_{n} = b$, how can we show that $\\lim_{n \\to \\infty} \\min\\{a_{n},b_{n}\\} = \\min\\{a,b\\}$?\nI say $\\min\\{a_{n},b_{n}\\} $ has two cases: $a_{n}$ and $b_{n}$. So (1) $\\lim_{n \\to \\infty} a_{n} = a$ and (2) $\\lim_{n \\to \\infty} b_{n} = b$. Now I don't know how to imply the $\\min\\{a,b\\}$.\n",
    "proof": "Hint:\n$$\\max\\left\\{a_n,b_n\\right\\}=\\frac{1}{2}(a_n+b_n)+\\frac{1}{2}\\left|a_n-b_n\\right|$$\nand \n$$\\min\\left\\{a_n,b_n\\right\\}=\\frac{1}{2}(a_n+b_n)-\\frac{1}{2}\\left|a_n-b_n\\right|$$\n",
    "tags": [
      "limits",
      "proof-writing",
      "problem-solving"
    ],
    "score": 7,
    "answer_score": 7,
    "is_accepted": true,
    "question_id": 260087,
    "answer_id": 260093
  },
  {
    "theorem": "How to prove the following discovery of Euler?",
    "context": "There exists a series of formulas.\n\\begin{align*}\n    \\ & \\dfrac{1}{(a-b)(a-c)}+\\dfrac{1}{(b-a)(b-c)}+\\dfrac{1}{(c-a)(c-b)} = 0, \\\\\n    \\ & \\dfrac{a}{(a-b)(a-c)}+\\dfrac{b}{(b-a)(b-c)}+\\dfrac{c}{(c-a)(c-b)} = 0, \\\\\n    \\ & \\dfrac{a^2}{(a-b)(a-c)}+\\dfrac{b^2}{(b-a)(b-c)}+\\dfrac{c^2}{(c-a)(c-b)} = 1, \\\\\n    \\ & \\dfrac{a^3}{(a-b)(a-c)}+\\dfrac{b^3}{(b-a)(b-c)}+\\dfrac{c^3}{(c-a)(c-b)} = a+b+c.\n\\end{align*}\nThey are related to a fantastic discovery made by Leonhard Euler: if $x_1,x_2,\\ldots,x_n$ are distinct numbers, then the following holds:\n$$\n    \\sum_{j=1}^n\\left(\\left. x_j^r\\right/ \\prod_{\\substack{1\\leqslant k\\leqslant n \\\\ k\\neq j}}(x_j-x_k)\\right) = \\begin{cases}0, & \\text{if } 0\\leqslant r < n-1; \\\\ 1, & \\text{if } r=n-1; \\\\ \\sum_{j=1}^nx_j, & \\text{if } r=n.\\end{cases}\n$$\nDoes someone know how to prove it rigorously? I've seen weird ways which are presumably wrong and unnecessarily complex, but I'm looking for a simple proof.\n",
    "proof": "Consider the Vandermonde-like determinant\n$$V_r = \\det \\left[ \\begin{array}{ccccc}\n 1 & 1 & 1 & \\cdots & 1 \\\\\n x_1 & x_2 & x_3 & \\cdots & x_n \\\\ \n x_1^2 & x_2^2 & x_3^2 & \\cdots & x_n^2 \\\\\n \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n x_1^{n-2} & x_2^{n-2} & x_3^{n-2} & \\cdots & x_n^{n-2} \\\\\n x_1^r & x_2^r & x_3^r & \\cdots & x_n^r\n \\end{array} \\right].$$\nIf $0 \\le r \\le n-2$ then the last row is equal to one of the others so the determinant is zero. If $r = n-1$ we get the usual Vandermonde determinant\n$$V_{n-1} = \\prod_{1 \\le i < j \\le n} (x_j - x_i)$$\n(up to possibly a sign that I haven't checked carefully). The usual proof of this is to observe that if $x_i = x_j$ for any $i \\neq j$ then two of the rows of the matrix are identical so the determinant vanishes, which establishes that the RHS divides the LHS. They also have the same degree so one is a scalar multiple of the other and then it remains to consider the coefficient of any monomial to compute this constant.\nFinally, if $r = n$ then the same argument as above establishes that $V_n$ is a polynomial of degree ${n \\choose 2} + 1$ which is divisible by the Vandermonde determinant $V_{n-1}$, so the ratio $\\frac{V_n}{V_{n-1}}$ is a homogeneous polynomial in the $x_i$ of degree $1$. Moreover, because $V_r$ is antisymmetric (if the $x_i$ are permuted then $V_r$ changes by the sign of the permutation), this ratio is a symmetric polynomial of the $x_i$, so it must be a scalar multiple of $e_1 = x_1 + x_2 + \\dots + x_n$. Considering the coefficient of any monomial as above gives\n$$V_n = \\left( \\sum_{i=1}^n x_i \\right) V_{n-1}$$\n(again, up to possibly a sign that I haven't checked carefully). Finally, I claim that\n$$\\begin{eqnarray*} V_r &=& \\sum_{i=1}^n x_i^r (-1)^{n+i}\\prod_{1 \\le j < k \\le n, j, k \\neq i} (x_k - x_j) \\\\\n &=& \\sum_{i=1}^n \\frac{x_i^r}{\\prod_{1 \\le j \\le n, j \\neq i} (x_i - x_j)} V_{n-1} \\end{eqnarray*}$$\nso this sequence of identities for $V_r$ is equivalent to the desired sequence of identities multiplied by the Vandermonde determinant $V_{n-1}$. This last identity follows by cofactor expansion along the last row, together with the observation that the cofactors are all Vandermonde determinants but with one of the variables $x_i$ omitted. (Again this is all possibly up to signs that I haven't checked carefully.)\nThis is all a special case of Jacobi's bialternant formula for the Schur polynomials but you don't need to know that to understand this proof.\n",
    "tags": [
      "summation",
      "proof-writing",
      "alternative-proof",
      "products"
    ],
    "score": 7,
    "answer_score": 11,
    "is_accepted": true,
    "question_id": 4619148,
    "answer_id": 4619191
  },
  {
    "theorem": "Prove that there are no positive integers $x$ and $y$ such that $x^3 + y^3 = 10^3$.",
    "context": "\nProve that there are no positive integers $x$ and $y$ such that $x^3 + y^3 = 10^3$.\n\nThis is a homework question, and I understand that its part of Fermat's Last Theorem, but when I looked that up to try to figure out the homework, I realized that it is way further than what we've learned in class so far. We have to prove this, but the only way I can think to do it is by exhaustion, which would be extremely lengthy. Any little shove in the right direction would be appreciated. Thanks!\n",
    "proof": "Here’s the stupidest possible way to do it: $10^3$ is only one thousand, and there are only nine cubes less than that. They are $1$, $8$, $27$, $64$, $125$, $216$, $343$, $512$ and $729$. Since all are less than $500$ except $512$ and $729$, you only need to check whether $488$ or $271$ is on the list, and neither is.\n",
    "tags": [
      "elementary-number-theory",
      "proof-writing",
      "diophantine-equations"
    ],
    "score": 7,
    "answer_score": 5,
    "is_accepted": false,
    "question_id": 2230179,
    "answer_id": 2234834
  },
  {
    "theorem": "Prove that $n!&gt;n^2$ for all integers $n \\geq 4$.",
    "context": "I am working on induction problems to prep for Real Analysis for the fall semester. I wanted proof verification and editing suggestions for part (a), and assistance understanding part (b). For part (b), the portion that has the additional indentation is where I am unclear.\n\nThe principle of mathematical induction can be extended as follows. A list $P_m, >P_{m+1}, \\cdots$ of propositions is true provided (i) $P_m$ is true, (ii) >$P_{n+1}$ is true whenever $P_n$ is true and $n \\geq m$.\n(a) Prove $n^2 > n + 1$ for all integers $n \\geq 2$.\nAssume for $P_n$: $n^2 > n + 1$, for all integers $n \\geq 2$. Observe for $P _2$:\n$P_2: 2^2=4 > 2+1 = 3$,\nthus the basis step holds. Now, let $n=k$ such that $k^2 > k + 1$, and assume this also holds. We now consider the case $P_{k+1} : (k+1)^2 > (k+1) + 1$. \nObserve:\n$(k+1)^2 = k^2 + 2k + 1$\n= $k(k+2) + 1$\nClearly, $k(k+2)+1$ must be greater than $(k+1)+1$. Thus, by the principle of mathematical induction, the case holds for all $n \\geq 2$.\n*I am aware that part (a) does not require induction to prove, but the book problem suggests doing so...\n\n\n\n(b) Prove $n! > n^2$ for all integers $n \\geq 4$.\nAssume for $P_n$: $n!>n^2$ for all integers $n \\geq 4$. Observe for $P_4$:\n$P_4: 24 = 4! > 16 = 4^2$,\nthus the basis step holds. Let $n=k$ such that $k! > k^2 $, and assume this also holds. We now consider the case $P_{k+1} : (k+1)! > (k+1)^2$. Observe:\n\n$(k+1)! = (k+1)k!$\n$> (k+1)k^2$\n$= k^3 + k^2$\n$> k^2 + 2k + 1$\n$= (k + 1)^2$\n\n\nI was able to write some of this on my own, and I used my book + internet to help me figure out how to write this out. First, I am not clear why we are adding $(k+1)$ to the right hand side (the $k^2$) side of the equation (or what that's allowed, really). Also, I'm not clear on the jump from $k^3+k^2>k^2 + 2k + 1$.\nAdditionally, while looking around MSE, I have noticed many people talk about induction with LHS and RHS notation. I have not seen this in any books--would someone be able to explain using that method as a form of bookkeeping, or be able to suggest a site or stack that could do that?\n",
    "proof": "Note: Your question should really be two questions since they're completely distinct and separate. You are simply trying to cover too much for one single question on this site, but I'll help with what I can. \nBefore addressing both parts of your question, I would encourage you to read the following three posts because I think they would go a long way in helping you with induction proofs in general.\n\nHow to write a clear induction proof.\nThe difference between weak and strong induction.\nUse of LHS/RHS notation or terminology in an induction answer.\n\nNow on to your question(s).\nPart (a): Your write-up here is really struggling in a number of ways. I'm going to be straight with you--your statement of strong induction is sloppy (you don't actually need strong induction even if you do use an inductive argument which is why I included (2) above) and your inductive \"proof\" never even uses the inductive hypothesis. See (1) above for how to actually write a clear induction proof; this will more or less force you to understand how the proof actually works, where you use the inductive hypothesis, etc. Also, as Hirshy notes, you actually assume what you are trying to prove (you do this again in part (b)). Regardless, the following is one way you could structure the main part of your induction proof for part (a):\n\\begin{align}\n(k+1)^2&= \\color{blue}{k^2}+2k+1\\tag{expand}\\\\[0.5em]\n&\\color{blue}{> (k+1)}+2k+1\\tag{by inductive hypothesis}\\\\[0.5em]\n&= 3k+2\\tag{simplify}\\\\[0.5em]\n&> k+2\\tag{since $k\\geq 2$}\\\\[0.5em]\n&= (k+1)+1.\\tag{simplify}\n\\end{align}\nDid you see how that worked? Can you see how the inductive hypothesis is used in the part highlighted with $\\color{blue}{\\mathrm{blue}}$? \nPart (b): The part you have indented is actually not too bad at all when you actually write out what is happening and why at each step; nonetheless, you still need to clean up the beginning, as you are again assuming what you are trying to prove. The goal here is to move from the left-hand side (LHS) of the statement $P_{n+1}$ to the right-hand side (RHS) of $P_{n+1}$. To that end, note the following (the indented steps have explanatory notes in the margin now):\n\\begin{align}\n(k+1)! &= (k+1)k!\\tag{by definition}\\\\[0.5em]\n       &> (k+1)k^2\\tag{by induction hypothesis}\\\\[0.5em]\n       &= k^3+k^2\\tag{expand}\\\\[0.5em]\n       &> k^2+2k+1\\tag{since $k\\geq 4$}\\\\[0.5em]\n       &= (k+1)^2.\\tag{factor}\n\\end{align}\nCan you see how all of that worked? \n",
    "tags": [
      "proof-verification",
      "proof-writing",
      "induction"
    ],
    "score": 7,
    "answer_score": 7,
    "is_accepted": true,
    "question_id": 1393957,
    "answer_id": 1394007
  },
  {
    "theorem": "Prove that if function f is monotonic, then it one-to-one",
    "context": "What I have so far:\nSuppose $f$ is monotonic. It is therefore either increasing or decreasing.\nProof for increasing:\nIf $f$ is increasing, then $f(x_1) <f(x_2)$ whenever $x_1 < x_2$, which means $f(x_1) = f(x_2)$ if, and only if, $x_1 = x_2$.\nTherefore $f$ is one to one.\nI think this is wrong though. Could anyone help me out?\n",
    "proof": "Here's a nice way to phrase the argument for $f$ increasing.\n$$\\begin{align}\nf(x)=f(y) & \\iff f(x)\\le f(y)\\text{ and }f(x)\\ge f(y)\\\\\n& \\iff x\\le y\\text{ and }x\\ge y\\\\\n& \\iff x=y.\n\\end{align}$$\nCome to think of it. This exact argument works for $f$ decreasing.\n",
    "tags": [
      "algebra-precalculus",
      "proof-verification",
      "proof-writing"
    ],
    "score": 7,
    "answer_score": 11,
    "is_accepted": false,
    "question_id": 1332943,
    "answer_id": 1332969
  },
  {
    "theorem": "Formal logic behind defining variables in a proof (not E.I. or U.G, etc.)",
    "context": "This is something I have been curious about and hopefully has a simple answer. Often when looking at proofs I will come upon a step that goes along the lines of \"define   y  = ...\" and then proceed to use the defined y to show some conclusion. \nFor example in Euclids infinity of primes theorem \nhe defines \" let k be the product of  all the prime  numbers less  than  n\"\nfollowed by \"now let  m = k + 1\".\nand proceeds to reason with these definitions to arrive at the desired results. My question is to what sort of constraints are there to defining new objects to a proof? I am aware of universal generalization rule which requires some restrictions in introducing an arbitrary constant as well well as the other quantifier rules of inference such as existential instantiation and so on. However this whole defining a variable does not seem to be any of these rules.\nSo at the end of the day can you simply define objects like this all willy nilly like? What sort of justification can you write for such a step? It seems problematic to me because I feel like you need to ensure the existence of such objects before you can introduce them like this, and in particular a step such as  \" let k be the product of  all the prime  numbers less  than  n\" seems to define an object that has more meaning than simply a definition given by an equation that defines the object of previously defined terms (like \"define m = k + 1\").\nSo this begs the question of whether you can arbitrarily define objects that have unconfirmed properties. This does not seem right because then you could be reasoning with statements that are false but since you \"defined\" them without restriction you mistakenly inferred something that turns out to be false.\nSo long story short: what is this method known as formally and what sort of restrictions are associated with it?\n",
    "proof": "It seems to me that you are looking for the $\\exists$-Elimiantion rule of Natural Deduction.\nSee Ian Chiswell & Wilfrid Hodges, Mathematical Logic (2007), page 179 :\n\nWe turn to ($\\exists$E). This is the most complicated of the natural deduction rules, and many first courses in logic omit it. \nHow can we deduce something from the assumption that ‘There is a snark’? If we are mathematicians, we start by writing :\n\n(*) Let $c$ be a snark\n\nand then we use the assumption that $c$ is a snark in order to derive further statements, for example,\n\n(§) $c$ is a boojum [imaginary dangerous animal].\n\nIf we could be sure that (§) rests only on the assumption that there is a snark, and not on the stronger assumption that there is a snark called ‘$c$’, then we could discharge the assumption (*) and derive (§). Unfortunately, (§) does explicitly mention $c$, so we cannot rule out that it depends on the stronger assumption. Even if it did not mention $c$, there are two other things we should take care of. First, none of the other assumptions should mention $c$; otherwise (§) would tacitly be saying that some other element already mentioned is a snark, and this was no part of the assumption that there is a snark. Second, the name $c$ should be free of any implications; for example, we cannot write $cos \\theta$ in place of $c$, because then we would be assuming that some value of $cos$ is a snark, and again this assumes more than that there is a snark. However, these are all the points that we need to take on board in a rule for eliminating $\\exists$. \n\nThus, the rule for $\\exists$-Elimination is formalized as follows :\n\nfrom a derivation $\\Gamma \\vdash \\exists \\phi$ and a derivation of $\\Delta, \\phi[t/x] \\vdash \\chi$, where t is a term [constant symbol or a variable] which does not occur in $\\chi, \\phi$ or in any undischarged assumption in $\\Gamma$ or $\\Delta$ except in $\\phi[t/x]$, we can infer $\\Gamma, \\Delta \\vdash \\chi$, i.e. we can infer $\\chi$ from the undischarged assumptions of the two previous derivations except $\\phi[t/x]$.\n\n\nIn your example, taken from Euclid, Book IX,Prop.20 :\n\nPrime numbers are more than any assigned multitude of prime numbers.\n\nProvided that Euclid defines [Book VII, Def.11] :\n\nA prime number is that which is measured by a unit alone [i.e. its only divisor not equal to itself is the unit]\n\nand thus we know that there are prime numbers [$1$ is], Euclid makes the assumption :\n\nSuppose that there are $n$ primes, $a_1, a_2,\\ldots, a_n$. (Euclid, as usual, takes an specific small number, $n = 3$, of primes to illustrate the general case.) \n\nLet $m$ be the least common multiple of all of them. Consider the number $m + 1$. If it’s prime, then there are at least $n + 1$ primes.\nSo suppose $m + 1$ is not prime. Then according to VII.31, some prime $g$ divides it. But $g$ cannot be any of the primes $a_1, a_2,\\ldots, a_n$, since they all divide $m$ and do not divide $m + 1$. Therefore, there are at least $n + 1$ primes. Q.E.D.\nThe proof runs as follows :\n\nlet $a_1, a_2,\\ldots, a_n$ primes; then $\\exists x (x = lcm(a_1, a_2,\\ldots, a_n))$ : this is $\\exists x \\phi$\nlet $k=lcm(a_1, a_2,\\ldots, a_n)$ : this is $\\phi[k/x]$ \nlet $m=k+1$; clearly $m \\ne a_1, a_2,\\ldots, a_n$; if $m$ is a prime we are done;\notherwise, there is a prime $g$ which divides $m+1$ and $g \\ne a_1, a_2,\\ldots, a_n$.\n\nIn both case we have that :\n\n$\\exists x (x \\ne a_1, a_2,\\ldots, a_n \\, \\text {and} \\, x \\, \\text {is prime} \\, )$, and this is $\\chi$.\n\nThe sentence $\\chi$ does not contain $k$, and thus the conditions for $\\exists$-E are satisfied and the conclusion holds.\n\nRegarding the new example : \"There is no largest real [or natural] number\", we can \"formalize\" it as follows.\n1) $\\forall x(x < x+1)$ -- theorem of number theory\n2) $x < x+1$ --- from 1) by $\\forall$E \n3) $\\exists y(x < y)$ --- from 2) by $\\exists$I\n\n4) $\\forall x \\exists y(x < y)$ --- from 3) --- by $\\forall$I, $x$ is not free in 1).\n\nThus, as you have noted, we do not need the existential elimination rule. \n\n\nIf you are searching for another example of \"real life\" use of $\\exists$E, consider :\n\n$a < b \\land b < c \\to a < c$ --- $a,b,c \\in \\mathbb N$\n\nProof : \n1) assume : $a < b$ and : $b < c$ i.e. $\\exists d(s(d)+a=b)$ and $(\\exists e)(s(e)+b=c)$, where $s(x)$ is the successor function\n2) assume for $\\exists$E : $s(d)+a=b$ and $s(e)+b=c$\n3) $(s(e)+s(d))+a=c$ --- by propety of equality\n4) $s((s(e)+d))+a=c$ --- by recursive axiom for $+$ : $n+s(x)=s(n+x)$ and equality\n5) $(\\exists f)(s(f)+a=c)$ --- by $\\exists$I i.e. $a < c$.\nThe last formula does not contain $d$ nor $e$; thus we can apply $\\exists$E twice, discharging the \"temporary\" assumptions in 2), and we can conclude from 1) with :\n\n$a<b, b<c \\vdash a<c$.\n\nThen, by $\\to$I twice and tautological equivalence :\n\n\n$(a<b \\land b<c) \\to a<c$.\n\n\n",
    "tags": [
      "logic",
      "proof-writing",
      "predicate-logic",
      "first-order-logic"
    ],
    "score": 7,
    "answer_score": 7,
    "is_accepted": true,
    "question_id": 1150857,
    "answer_id": 1151066
  },
  {
    "theorem": "What&#39;s the point of a Story Proof?",
    "context": "I've been working through the Stat 110 Lecture Series and have come across the idea of Story Proofs.\nI've read the following from the author's book on the subject:\n\nA story proof is a proof by interpretation. For counting problems, this often means counting the same thing in two different ways, rather than doing tedious algebra. A story proof often avoids messy calculations and goes further than an algebraic proof toward explaining why the result is true. The word “story” has several meanings, some more mathematical than others, but a story proof (in the sense in which we’re using the term) is a fully valid mathematical proof. - Introduction To Probability, page 20.\n\nBut what does the proof consist of?\nIt appears a story proof creates two real-world scenarios that supposedly count the same thing, defines both counting situations in mathematics, and equates them.\nFine, but without checking the mathematics, how can you be certain you are counting the same thing? Surely you end up checking the numerical proof in any case? Doesn't that make the story proof redundant?\n",
    "proof": "This is the first time I have encountered the term \"story proof\".\nI have certainly seen many proofs that would be considered \"story proofs\" according to this definition, but the style of proof has always been called something more specific, such as a counting argument or a combinatorial proof.\nI doubt that a real-world scenario is required.\nI think the author means just what they said about the nature of such a proof:\ncounting something two different ways is an example of such a proof,\nand such proofs generally have more explanation and less algebra (or other manipulation of symbols) than other proofs of the same facts.\nTry reading the definition of \"story proof\" without any preconceived notions of what the \"story\" part of it is. You might try substituting a random word for \"story\"; for example, \"Basingstoke\". So we have,\n\"A Basingstoke proof is a proof by interpretation,\" and so forth.\nOnce you have understood what is meant by such a proof, and perhaps seen a few examples, it might be clearer why an author might consider \"story proof\" to be a more descriptive name for such proofs than \"Basingstoke proof\".\n",
    "tags": [
      "probability",
      "proof-writing",
      "proof-explanation"
    ],
    "score": 7,
    "answer_score": 7,
    "is_accepted": true,
    "question_id": 4804268,
    "answer_id": 4804336
  },
  {
    "theorem": "Let $f$ be a real uniformly continuous function on the bounded set $E$ in $\\mathbb{R^1}$. Prove that $f$ is bounded on $E$",
    "context": "\nLet $f$ be a real uniformly continuous function on the bounded set $E$ in $\\mathbb{R^1}$. Prove that $f$ is bounded on $E$\n\nMy (Attempted) Proof\n\nSince $E$ is bounded, put $\\alpha = \\sup E$, $\\beta = \\inf E$. Now since $f$ is uniformly continuous on $E$, we only have to prove convergence of $f$ as $x \\in E \\to \\alpha$ and $x \\in E \\to \\beta$.\nSo let $\\{x_n\\}$ be a  Cauchy sequence and fix $\\epsilon > 0$. Let $\\delta$ be such that $d(f(p), f(q)) < \\epsilon$.\nNow take $N$ so large such that $m, n > N \\implies d(x_n, x_m) < \\delta$. We then have $d(f(x_n), f(x_m)) <  \\epsilon$ (By uniform continuity of $f$)\nThus $\\{f(x_n)\\}$ is a Cauchy sequence, and thus converges to some point $L \\in \\mathbb{R^1}$\n$$\\begin{aligned}\\therefore \\text{As} \\ \\ x_n \\to \\beta \\ , \\  \\ f(x_n) \\to L_1\\\\\n\\ \\ x_n \\to \\alpha \\ , \\  \\ f(x_n) \\to L_2\\\\\\end{aligned}$$\n(Comment: Since we let $\\{x_n\\}$ be an arbitrary Cauchy sequence, we can pick two different Cauchy sequences, one that converges to $\\beta$, and one that converges to $\\alpha$, and thus the above statement holds)\nNow put $\\gamma = \\max\\{L_1, L_2, f(x)\\  | \\  x \\in E\\}$ and $\\eta = \\min\\{L_1, L_2, f(x)\\  | \\  x \\in E\\}$.\nThen we have $\\gamma = \\sup f[E]$, and $\\eta = \\inf f[E]$, and thus $f[E]$ is bounded. $\\ \\square$\n\nFirst of all if my proof rigorously correct? Secondly, if you have any criticism on my proof-writing skills, please let me know, as I'm always looking to improve. \nFinally, I proved that $f$ is bounded on $E$ by proving that the sequence $\\{f(x_n)\\}$ was Cauchy, and would thus converge, but are there other cleaner/more efficient ways of proving that $f$ is bounded on $E$?\n",
    "proof": "Fix $\\epsilon$. There is a $\\delta$ such that $|x-y| < \\delta$ implies $|f(x) - f(y)| < \\epsilon$.\nSince $E$ is bounded, there exists a sequence  $x_1, ...x_n$ of elements of $E$ such that \n$$\\bigcup_{1 \\leq i \\leq {n}} (x_i -  \\delta, x_i+\\delta) $$\nis a finite open cover of $E$. \nNote $\\{f(x_1), f(x_2)... f(x_n)\\}$ is finite hence bounded. Thus $f$ is bounded above by $\\sup \\{f(x_1), f(x_2)... f(x_n)\\} + \\epsilon$ and bounded below by $\\inf \\{f(x_1), f(x_2)... f(x_n)\\} - \\epsilon$\n",
    "tags": [
      "real-analysis",
      "proof-verification",
      "metric-spaces",
      "proof-writing",
      "alternative-proof"
    ],
    "score": 7,
    "answer_score": 8,
    "is_accepted": true,
    "question_id": 1975339,
    "answer_id": 1975374
  },
  {
    "theorem": "Proving That The Product Of Two Different Odd Integers Is Odd",
    "context": "Okay, here is how I begin my proof:\nLet $q$ and $r$ be odd integers, then $q = 2k+1$ and $r = 2m+1$, where $k,m \\in Z$.\n$q \\times r = (2k+1)(2m+1) \\implies q \\times r = 4mk + 2k + 2m + 1 \\implies q \\times r = 2(2mk + k + m) + 1$\nI would then conclude that $q \\times r$ results in an odd number, because 2 times an integer with one added to it is, by definition, an odd number.\n\nHowever, how can I conclude this? Is $(2mk + k + m)$ in fact an integer? How do I know if the product of any two integers is an integer; similarly, does adding any two integers yield another integer? Now, obviously, I have an intuitive notion that these are true, but is there a way to prove them?\nSide note: I would also appreciate it if someone could critique my proof.\n",
    "proof": "Slightly more generally, note that multiplying $\\rm\\:m\\, =\\, k+an\\:$ by  any integer of the form $\\rm\\:1+bn\\:$ doesn't change the remainder that $\\rm\\,m\\,$ leaves when divided by $\\rm\\,n,\\,$ i.e. the remainder stays = $\\rm k,\\,$ by \n$$\\rm (k+an)(1+bn)\\, =\\, k+n(a+b(k+an))$$\nThis is a special case $\\rm\\,j=1\\,$ of $\\rm\\   mod\\ n\\!:\\ \\ \\begin{eqnarray} x &\\equiv&\\,\\rm k\\\\ \\rm y &\\equiv&\\,\\rm j\\end{eqnarray}\\ \\Rightarrow\\ xy\\equiv\\,  k\\, j,\\ \\ $ the Congruence Product Rule\nThat $\\rm\\: a+b(k+an)\\in \\Bbb Z\\:$ follows from the fact that $\\rm\\:a,b,c\\in \\Bbb Z\\:\\Rightarrow\\: a + b\\,c\\in \\Bbb Z,\\:$ since integers are closed under multiplication, thus $\\rm\\:bc\\in \\Bbb Z,\\:$ and also under addition, hence $\\rm\\:a + bc\\in\\Bbb Z.\\:$ Finally, that $\\rm\\,\\Bbb Z\\,$ is closed under the operations of addition and multiplication follows from the recursive definitions of addition and multiplication in Peano arithmetic.  Your proof is correct.\n",
    "tags": [
      "elementary-number-theory",
      "discrete-mathematics",
      "proof-writing"
    ],
    "score": 7,
    "answer_score": 6,
    "is_accepted": true,
    "question_id": 330471,
    "answer_id": 330510
  },
  {
    "theorem": "Prove that the sequence defined by $a_{n + 1} = 2a_n^2 - 1$ is always negative iff $a_1 = -\\frac{1}{2}$",
    "context": "I want to prove that the sequence defined by $a_{n + 1} = 2(a_n)^2 - 1$ is always negative if and only if $a_1 = -\\dfrac{1}{2}$.\nHere are some observations I've made so far. However, I haven't been able to consolidate them into a full proof:\n\nWith $a_1 = -\\dfrac{1}{2}$, all terms in the sequence are $-\\dfrac{1}{2}$. So, $-\\dfrac{1}{2}$ is a fixed point of the sequence. The only other fixed point of the sequence (found by solving the equation $a_n = a_{n + 1} \\iff a_n = 2(a_n)^2 - 1$) is $1$, but that is not negative.\nNow, say that $a_n \\leq 0$. If $(a_n)^2 > \\dfrac{1}{2}$ (iff $a_n < \\dfrac{\\sqrt{2}}{2}$), then we are guaranteed that $a_{n + 1} > 0$, and we are done. Otherwise, if $\\dfrac{\\sqrt{2}}{2} \\leq a_n \\leq 0$, we have $-\\dfrac{1}{2} \\leq a_{n + 1} \\leq 1$, and so $\\dfrac{1}{4} \\leq (a_{n + 1})^2 \\leq 1$, and so $-\\dfrac{1}{2} \\leq a_{n + 2} \\leq 1$. This is the same constraint we found on $a_{n + 1}$, so continuing this way gives us no progress.\nFinally, I've observed that when I test different values for $a_1$, the values seem to oscillate around $-\\dfrac{1}{2}$ (getting farther away every time, unless we started at $-\\dfrac{1}{2}$) until we find a value whose square is more than $\\dfrac{1}{2}$. I'm not sure what this implies, though.\n\nCan anyone provide hints or a solution to this problem?\n",
    "proof": "This is not an answer but a track to study your problem with graphic and foster study of function as precised in the comment.\nStarting at $-1$\n\nStarting at $\\sqrt{2}$\n\nStarting at $-\\sqrt{2}$\n\nStarting at $1/3$\n\nStarting at $a_1=-1/2$, note it stands at the intersection of the curve of $f$ and the $y=x$ curve\n\nStarting at $a_1=1$, note it also stands at the intersection of the curve of $f$ and the $y=x$ curve but here positively\n\nFinally\nNote that $1$ and $-1/2$, are the roots of $f-\\text{Id}$, with $f$ is such that :\n$$ f(a_n)=a_{n+1} $$\nStudying $f$ and $g=f-\\text{Id}$ especially the variation and the stability of certain intervals by $f$ (intervals defined especially by the roots of $f$ and $g$)\nFor the beginning\nStudying $f$ and $g$\nFor f\n$$f(x)=2x^2-1=(x\\pm \\sqrt{2})$$\n$$f'(x)=4x $$\nSo\n$$ f'(x)=0 \\leftrightarrow x=0 $$\n$$ f \\ \\text{is thus decreasing then increasing with minimum in $x$=0} $$\nNoting that :\n$$f(0)=-1$$\n$$f(\\pm \\sqrt{2})=0$$\nFor g\n$$g(x)=2x^2-1-x=(x-1)(x+1/2)$$\n$$g'(x)=4x-1 $$\nSo\n$$ g'(x)=0 \\leftrightarrow x=1/4 $$\n$$ g \\ \\text{is thus decreasing then increasing with minimum in $x$=1/4} $$\nNoting that :\n$$g(0)=-1$$\n$$g(1)=0=f(-1/2)$$\nNow:\n$$f(a_n)=a_{n+1}$$\n$$g(a_n)=a_{n+1}-a_n $$\n",
    "tags": [
      "sequences-and-series",
      "proof-writing",
      "contest-math"
    ],
    "score": 7,
    "answer_score": 3,
    "is_accepted": false,
    "question_id": 4931115,
    "answer_id": 4931195
  },
  {
    "theorem": "Proper use of &quot;without loss of generality&quot;",
    "context": "I'm trying to understand exactly when I can assume something \"without loss of generality in a proof.\" The technical explanation, I believe, is that it's allowed provided that the case that I'm omitted \"reduces\" to the one I prove, either through interchanging of labels or through a rather trivial extension. I can really only understand this through examples, though.\nThe example I have in mind, where I'm not totally sure I can use it, is as follows. Suppose I have countable (as in, finite or countably infinite) sets $X_1, \\ldots, $ and want to show that $\\bigcup\\limits_{i \\in \\mathbb{N}} X_i$ is countable. I want to say \"without loss of generality, suppose $X_i \\neq \\emptyset$ for all $i$,\" the justification being that if I take $I = \\{j \\in \\mathbb{N} \\mid X_j = \\emptyset\\}$, then I have\n$$ \n\\bigcup\\limits_{i \\in \\mathbb{N}} X_i = \\bigcup\\limits_{i \\in \\mathbb{N}} X_i \\setminus \\bigcup\\limits_{i \\in I} X_i,\n$$\ni.e., they contribute nothing at all to the union, so having in the union is somewhat \"harmless.\" I don't know if I'm sacrificing generality by making this assumption, though.\nI would appreciate any help on understanding this.\n",
    "proof": "\nI can really only understand this through examples, though.\n\nThere's nothing wrong with this!  \"Without loss of generality\" is an imprecise term and can be used in a wide variety of contexts, so there's no fixed rules on exactly how it can be used or what exactly it means.  Ultimately, it is intentionally introducing an easy-to-fill gap in a proof and relying on the reader to fill the gap, giving them a hint that it can be done by some sort of symmetry argument or easy reduction to a special case.  Your understanding of the meaning seems pretty much correct.\nAs for the specific example you're asking about, to justify the \"without loss of generality\" you have to actually fill in the gap in the proof that it introduces.  That is, you have to prove the statement in the general case (where some $X_i$ may be empty) using the specific case (where you assume they are all nonempty).  You should never use \"without loss of generality\" unless you yourself know how to fill in the gap in the proof and trust that your reader can do so as well.  There are various ways to do that in this case; here is one.  Let $Y_i=X_i\\cup\\{0\\}$ for each $i$.  Then each $Y_i$ is nonempty, and is still countable.  So now we are in the specific case where none of our sets are empty and we can deduce that $\\bigcup_{i\\in\\mathbb{N}}Y_i$ is countable.  But $\\bigcup_{i\\in\\mathbb{N}}Y_i=\\bigcup_{i\\in\\mathbb{N}}X_i\\cup\\{0\\}$, so $\\bigcup_{i\\in\\mathbb{N}}X_i$ is a subset of a countable set and thus countable.\nHere, the essence of the \"without loss of generality\" is that if you changed the setup in some simple way to make the assumption true (e.g. added an element to the $X_i$'s so they can't be empty), then this would still give a conclusion that would be strong enough to deduce what you originally wanted (since you would only enlarge the union, and a subset of a countable set is still countable).\n",
    "tags": [
      "elementary-set-theory",
      "proof-writing",
      "terminology"
    ],
    "score": 7,
    "answer_score": 5,
    "is_accepted": true,
    "question_id": 4154150,
    "answer_id": 4154170
  },
  {
    "theorem": "Wilson&#39;s theorem Prime Generator",
    "context": "There is a famous theorem in number theory called Wilson's Theorem. \n\nStatement:\n  $n$ satisfies, $(n-1)! + 1 = 0\\pmod n$ if and only if $n$ is prime.\n\nAnother way of looking at the statement is that,\n$\\dfrac{(n-1)! + 1}{n}$ is an integer only if $n$ is prime.\nI have noticed a pattern in these integers. They always come out to be prime.\nI have made a code, to test primes upto a large number, and all of the integers that have come from the fraction, is found to be prime.\nIf the aforementioned statement always holds true for all primes, then it would \nbehave as a prime generator. I haven't found any useful information currently \nexisting on the internet.\nHence, I am looking for a mathematical proof for the following:\n\nLet $f:\\mathbb{N}\\to\\mathbb{N}$ be defined as,\n  $$f(n) = \\frac{(n-1)! + 1}{n}$$for all $n\\in\\mathbb{N}$. \nProve that: $$f(n)\\in \\mathbb{P};\\ \\ \\forall n \\in\\mathbb{P}$$\n  Where $\\mathbb{P}$ is the set of Primes.\n\nNow, I do know that for $n = {2,3},\\  f(n) = 1$.\nThis should be ignored as sometimes, $1$ trivially shows up while working with primes.\n",
    "proof": "It is not always prime: $ f(13) = 13 \\times 2834329 $.  For the primes $ 2 \\leq p \\leq 997 $, the only time $ f(p) $ is (probably) prime is for $ p = 5, 7, 11, 29, 773 $.\n",
    "tags": [
      "number-theory",
      "proof-writing",
      "prime-numbers"
    ],
    "score": 7,
    "answer_score": 6,
    "is_accepted": true,
    "question_id": 2376665,
    "answer_id": 2376671
  },
  {
    "theorem": "$a/b &lt; (1 + \\sqrt{5})/2 \\iff a^2 - ab - b^2 &lt; 0$?",
    "context": "For positive integers $a$ and $b$, I want to show that $a/b < (1 + \\sqrt{5})/2$ if and only if $a^2 - ab - b^2 < 0$.\nI had a loose proof ready to go, but I noticed a fatal flaw. Perhaps there is a way to work around this though.\nMy tactic was to start from $a^2 - ab - b^2 < 0$ and complete the square on the LHS for $a$. I ended up with\n$$\n\\left(a-\\frac{b}{2}\\right)^2 - \\frac{5b^2}{4} < 0 \\iff \\left(a - \\frac{b}{2} \\right)^2 < \\left( \\frac{\\sqrt{5} \\, b}{2} \\right)^2.\n$$\nNow the tempting thing to do is to show this is equivalent to saying\n$$\n\\quad \\quad \\qquad \\quad \\, \\, \\iff a-\\frac{b}{2} < \\frac{\\sqrt{5} \\, b}{2}\n$$\nbut obviously it is necessary for $a > b / 2$ for this to work.\nThis is not necessarily the case because if $a = 1$ and $b = 5$, then $a^2 - ba - b^2 = -29 < 0$ and $a/b = 1/5 < (1+\\sqrt{5})/2$ so the initial claim is true but $a \\leq b/2$.\nSo is there some kind of assumption I can make to get around this, and without loss of generality? Or should I rethink the entire structure of the proof? Cheers!\n",
    "proof": "If $b=0, a^2<0$ which is impossible\nSo, $b\\ne0, b^2>0$ consequently, $$a^2-ab-b^2<0\\iff\\left(\\dfrac ab\\right)^2-\\left(\\dfrac ab\\right)-1<0$$\nNow the roots of $x^2-x-1=0$ are $$x=\\dfrac{1\\pm\\sqrt5}2$$\nWe can prove if $(x-a)(x-b)<0$ with $a<b;$\n$$a<x<b$$\n",
    "tags": [
      "sequences-and-series",
      "algebra-precalculus",
      "proof-writing",
      "fibonacci-numbers",
      "golden-ratio"
    ],
    "score": 7,
    "answer_score": 8,
    "is_accepted": true,
    "question_id": 2280587,
    "answer_id": 2280589
  },
  {
    "theorem": "An antonym for “converse”",
    "context": "Suppose you are proving $p \\leftrightarrow q$. In your first paragraph you prove $p \\rightarrow q$. Your second paragraph begins, “For the converse, assume $q$ holds.”\nIn this situation, we have a very precise way of referring to the statement $q \\rightarrow p$: “the converse.” But we have no good way to refer to $p\\rightarrow q$. Sometimes we say “the forward implication,” but I am not a big fan of this phrase and am wondering if there is a single latinate word which means the same thing. (Obverse? Inverse? Contrapositive? No, none of those mean “$p \\rightarrow q$.”)\nGranted, the word “converse” only works in context, since you are implicitly saying “the converse to $p \\rightarrow q$,” and that context can only be inferred if you are using the word “converse” right after you have proven “$p \\rightarrow q$.” But, anyway, I am simply asking if there is a better way to say “the forward implication.”\n",
    "proof": "Why do you need a special word? If you are proving $P\\iff Q$ you can just talk about the statement $P\\implies Q$ and its converse.\nIf you think about it, the converse (and inverse, and contrapositive, and negation) all implicitly refer to the original statement. It is not just \"the converse\" it is \"the converse of the statement.\" \nYou can't get around establishing what the statement is before you start saying \"converse\", so there is no sense avoiding it, and there does not really seem to be any evidence more terminology is needed for it, either.\nEven if you don't buy that argument for some reason, you should still consider the possibility you are not using the words as they were intended. \"Converse\" is not really a label for $Q\\implies P$, it is a statement about changing an existing statement, just like negation, inversion, and contraposition are. Would one similarly ask \"what is the antonym of the contrapositive?\" Probably not.\nThis is supported by the presentation on the wiki article on contraposition, where contraposition, inversion and conversion and negation are all thought of as things you can do to a given statement. \n",
    "tags": [
      "proof-writing",
      "terminology"
    ],
    "score": 7,
    "answer_score": 4,
    "is_accepted": false,
    "question_id": 1785652,
    "answer_id": 1785655
  },
  {
    "theorem": "How to show that a limit cannot be another number?",
    "context": "Let:\n$$\nG(x) = \\left\\{ \\begin{array} {cc} x \\sin \\frac{1}{x} , & x\\neq 0 \\\\\n0, & x=0 \\end{array} \\right.\n$$\nI can understand that the function is continuous at $x=0$ because:\nFor $\\epsilon>0$ and $\\delta>0$, this implies, for all $x$\n\n$$\n |x-0|<\\delta \\implies |f(x)-0| < \\epsilon \\\\\n |x|<\\delta \\implies |f(x)| < \\epsilon \\\\\n |x|<\\delta \\implies |x\\sin \\frac{1}{x}| < \\epsilon \\\\\n|x|<\\delta \\implies |x||\\sin \\frac{1}{x}| < \\epsilon \\\\\n\\therefore \\delta = \\frac{\\epsilon}{\\sin \\frac{1}{x}}\n$$\nSo, for any $\\epsilon>0$, we can find a $\\delta=\\frac{\\epsilon}{\\sin \\frac{1}{x}}$ so that $ |x-0|<\\delta \\implies |f(x)-0| < \\epsilon$ is true.\nHence, $\\lim \\limits_{x\\to 0}G(x)=G(0)=0$\n\nMy text also mentions that $G$ will only be continuos at $0$ if $G(0)=0$.\nSo now I am wondering why can't it be any number, $l$?\nSo far here is what I have come up with using the same manipulations as above:\n\n$$|x-0|<\\delta \\implies |f(x)-l| < \\epsilon \\\\\n|x|< \\frac{\\epsilon + l}{\\sin \\frac{1}{x}}\n$$\n\nI had expected this to lead to a contradiction when $l \\neq 0$ but so far I can't see it. How do I show that $\\lim \\limits_{x \\to 0}G(x)$ must be $0$ and where did I go wrong in my workings?\nThank you in advance for any help provided.\n",
    "proof": "First of all, there is a caveat about your proof, the value you choose for $\\delta$ can get very big or not even be a number, for example lets fix $x$ such that $\\sin{\\frac{1}{x}}=0$ (which actually happens an infinite number of times as x approaches to 0), then your \n$$\n\\delta=\\frac{\\varepsilon}{\\sin{\\frac{1}{x}}},\n$$\nis not well defined.\nHowever, your proof is not all wrong you are just picking the wrong delta, in this case, as your function $G$ is multiplied by the well behaved identity function you can have $\\delta=\\varepsilon$, so the proof that $G$ is continuous at $x=0$ will be as follows:\nLet $\\varepsilon>0$ given and let $\\delta=\\varepsilon$, then for any $x$ such that:\n$$\n|x-0|<\\delta\n$$\nwe have that:\n$$\n|G(x)-G(0)|=|x\\sin{\\frac{1}{x}}-0|\\\\\n           =|x\\sin{\\frac{1}{x}}|\\\\\n           =|x||\\sin{\\frac{1}{x}}|\\\\\n          \\le|x|\\\\\n           \\lt\\delta=\\varepsilon\n$$\nwhere your missing step follows from the fact that $|sin(y)|\\le1$ for all $y\\in\\mathbb{R}$, and the triangle inequality. So we conclude that $G$ is continuous at $x=0$. \nNow if you want to see why any other choice of $l$ will fail, your assumption of using a proof by contradiction is correct, nevertheless you are doing your algebra wrong.\nLets suppose there is an $l\\not=0$ such that $G(0)=l$ is continuous at $x=0$. This means that for every $\\varepsilon>0$ we can find a $\\delta_\\varepsilon>0$ (the subscript is just o make emphasis on the fact that the choice of this $\\delta$ depends on the $\\varepsilon$ given) such that:\n$$\n|x-0|<\\delta_\\varepsilon \\Rightarrow |G(x)-G(0)|<\\varepsilon.\n$$\nSo the proof goes like this:\nLet $\\varepsilon>0$ given, and $\\delta_\\varepsilon$ such that:\n$$\n|x-0|<\\delta_\\varepsilon \\Rightarrow |G(x)-G(0)|<\\varepsilon.\n$$\nTo make our life easier, lets take another $\\delta$ to be: $\\delta=\\min\\{\\delta_\\varepsilon,\\varepsilon\\}$, (Note that $|x-0|<\\delta<\\delta_\\varepsilon$ so the conclusion of our assumption still holds.) hence:\n$$\n|x-0|<\\delta,\n$$\nthen:\n$$\n\\left||G(x)|-|G(0)|\\right|\\le|G(x)-G(0)|<\\varepsilon,\n$$\nso:\n$$\n\\left||x\\sin{\\frac{1}{x}}|-|l|\\right|<\\varepsilon,\n$$\nit follows that:\n$$\n-\\varepsilon<|x\\sin{\\frac{1}{x}}|-|l|<\\varepsilon,\n$$\nin particular, using the left side:\n$$\n|l|-\\varepsilon<|x\\sin{\\frac{1}{x}}|\n$$\nbut remember that $|x\\sin{\\frac{1}{x}}|\\le|x|$ so:\n$$\n|l|-\\varepsilon<|x\\sin{\\frac{1}{x}}|\\le|x|=\\delta\\le\\varepsilon.\n$$\nso we conclude that:\n$$\n|l|<2\\varepsilon.\n$$\nWhich is clearly a contradiction because $l$ is fixed, and this should hold for any value of $\\varepsilon$.\nSo $G$ is not continuous at $x=0$ if $G(0)\\not=0$.\n",
    "tags": [
      "algebra-precalculus",
      "limits",
      "inequality",
      "proof-writing",
      "continuity"
    ],
    "score": 7,
    "answer_score": 1,
    "is_accepted": true,
    "question_id": 458375,
    "answer_id": 460717
  },
  {
    "theorem": "Prove transitivity of $\\forall X( X\\subseteq A\\setminus\\{a, b\\}\\rightarrow(X\\cup \\{a\\}\\in\\mathcal{F}\\rightarrow X\\cup\\{b\\}\\in\\mathcal{F}))$",
    "context": "This one is from Velleman's \"How to Prove It, 2nd Ed.\", exercise 4.3.23.\nSuppose $A$ is set, and $\\mathcal{F}\\subseteq\\mathcal{P}(A)$. Let $$R=\\{(a,b)\\in A\\times A : \\text{for every } X\\subseteq A\\setminus\\{a, b\\}\\text{, if } X\\cup \\{a\\}\\in\\mathcal{F}\\text{ then } X\\cup\\{b\\}\\in\\mathcal{F}))\\}$$ Show that $R$ is transitive.\n\nFirst of all, I'm not sure if I read this correctly and if my notation is correct: $$R=\\{(a,b)\\in A\\times A : \\forall X( X\\subseteq A\\setminus\\{a, b\\}\\rightarrow(X\\cup \\{a\\}\\in\\mathcal{F}\\rightarrow X\\cup\\{b\\}\\in\\mathcal{F}))\\}$$\nTo prove that $R$ is transitive, we need to prove that $$\\forall a\\in A\\;\\forall b\\in A\\;\\forall c\\in A\\;((aRb\\wedge bRc) \\rightarrow aRc),$$ so for starters we suppose and let all the usual stuff:\n\nlet $a,b,c\\in A$\nsuppose $aRb \\wedge bRc$\nexpand $aRc$ to $\\forall X( X\\subseteq A\\setminus\\{a, c\\}\\rightarrow(X\\cup \\{a\\}\\in\\mathcal{F}\\rightarrow X\\cup\\{c\\}\\in\\mathcal{F}))$\nlet $X$ be arbitrary and suppose $X\\subseteq A\\setminus\\{a,c\\}$\nsuppose $X\\cup\\{a\\}\\in\\mathcal{F}$\nshow that $X\\cup\\{c\\}\\in\\mathcal{F}$\n\nNow Velleman suggests splitting proof to cases: $b\\not\\in X$ and $b\\in X$. Showing that $R$ is transitive for $b\\not\\in X$ is rather straightforward. All we need to do is to show from $b\\not\\in X \\wedge X\\subseteq A\\setminus\\{a,c\\}$ that $X$ is also subset of both $A\\setminus\\{a,b\\}$ and $A\\setminus\\{b,c\\}$, and we just follow assumptions $aRb$ and $bRc$ to conclude $X\\cup\\{c\\}\\in\\mathcal{F}$.\nNow we must show transitivity when $b\\in X$. For this case Velleman suggests working with $X'=(X\\cup\\{a\\})\\setminus\\{b\\}$ and $X''=(X\\cup\\{c\\})\\setminus\\{b\\}$, and this is the part I totally don't get. Why would using $X'$ and $X''$ work for this proof, and how do actually connect them with all the givens/assumptions? I can see how all this connects after expanding $aRb$ and $bRc$, but I fail to see how this makes a correct proof.\n\nSo my questions are: is the 1. correct notation for given relation $R$ and how does 4. connect to givens.\nIf there is some other approach, I would be most thankful for any pointers.\n",
    "proof": "Yes, what you’ve written in (1) is equivalent to the definition in the problem, though I think that most readers will find the original version easier to absorb.\nIn the proof of transitivity, suppose that $b\\in X\\subseteq A\\setminus\\{a,c\\}$; we want to show that if $X\\cup\\{a\\}\\in\\mathscr{F}$, then $X\\cup\\{c\\}\\in\\mathscr{F}$, so assume that $X\\cup\\{a\\}\\in\\mathscr{F}$. Let $X_0=X\\setminus\\{b\\}$; then Velleman’s sets $X'$ and $X''$ are given by $X'=X_0\\cup\\{a\\}$, and $X''=X_0\\cup\\{c\\}$. \nNow $b,c\\notin X'$, and $X'\\cup\\{b\\}=X_0\\cup\\{a\\}\\cup\\{b\\}=X\\cup\\{a\\}\\in\\mathscr{F}$, so $X'\\cup\\{c\\}\\in\\mathscr{F}$, since $bRc$. Now $X'\\cup\\{c\\}=X_0\\cup\\{a\\}\\cup\\{c\\}=X''\\cup\\{a\\}$, so $X''\\cup\\{a\\}\\in\\mathscr{F}$. Moreover, $X''\\subseteq A\\setminus\\{a,b\\}$, and $aRb$, so $X''\\cup\\{b\\}\\in\\mathscr{F}$. Finally, $X''\\cup\\{b\\}=X_0\\cup\\{c\\}\\cup\\{b\\}=X\\cup\\{c\\}$, and we conclude that $X\\cup\\{c\\}\\in\\mathscr{F}$, as desired.\n",
    "tags": [
      "elementary-set-theory",
      "proof-writing",
      "relations"
    ],
    "score": 7,
    "answer_score": 9,
    "is_accepted": true,
    "question_id": 284637,
    "answer_id": 284650
  },
  {
    "theorem": "Baby Rudin Theorem 1.19 Step 5",
    "context": "I have no background in pure mathematics, and I'm trying to learn how to be more rigorous in general. To help with this, I am trying to make everything more explicit as I progress through Rudin's Principles of Mathematical Analysis. I am currently going through theorem 1.19's proof where Rudin constructs $\\mathbb{R}$ from $\\mathbb{Q}$.\nIn particular, in step 5, Rudin states that it is obvious that if $\\alpha , \\beta , \\gamma \\in \\mathbb{R}$ and $\\alpha < \\beta$, then $\\alpha + \\gamma < \\beta + \\gamma$.\nSo far, the book has defined $\\alpha , \\beta , \\gamma$ as subsets of $\\mathbb{Q}$ called cuts, and has shown that cuts respect the field axioms of addition. I am intuitively convinced that the set $\\beta + \\gamma$ contains elements that $\\alpha + \\gamma$ does not, but am struggling with how to formalize it. The following is my thought process on my (failed) attempt.\nI know that: $\\beta > \\alpha \\implies \\exists x \\in \\beta : x \\notin \\alpha$, and I think the next step is the take some value $y \\in \\gamma$ and show that $y + x \\in \\beta + \\gamma$ while $y + x \\notin \\alpha + \\gamma$. I originally thought this would work by taking $y = \\sup \\gamma$, but $\\sup \\gamma$ is a set, not a rational number. When I try to think about taking $y < \\sup \\gamma$ (edit: sorry, this should say $y \\in \\gamma$ not $y < \\sup \\gamma$), it's clear to me that $y + x \\in \\beta + \\gamma$, but it's no longer clear to me that $y + x \\notin \\alpha + \\gamma$.\nAny help would be greatly appreciated!\n",
    "proof": "If $b\\in\\beta$ and $b\\not\\in\\alpha,$ then there is some $b'>b,$ such that $b'\\in\\beta,$ by the definition of a cut.\nLet $r=b'-b>0.$ (Intuitively, we have $\\alpha+r<\\beta.$)\nLet $g_1\\in \\gamma$ and $g_2\\not\\in\\gamma$ such that $g_2-g_1<r.$\nWe see that $g_1+b'=g_1+b+r\\in \\beta+\\gamma.$\nIf $g_1+b+r\\in\\alpha+\\gamma,$ then $$g_1+b+r=a+g_3\\tag1$$ for some $a\\in\\alpha,g_3\\in\\gamma.$\nBut $a<b$ and $g_3<g_2<g_1+r,$ so:\n$$a+g_3<b+g_1+r=b+g_1,$$ which contradicts $(1).$\n\nWe are essentially constructing $g_1,g_2$ as approximations of the real number $\\gamma$ on either side of the cut so that $g_2-g_1$ is less than the intuitive real difference $\\beta-\\alpha.$\nWhat we have is, intuitively, $g_1<\\gamma<g_1+r$ and $\\alpha <b<b+r<\\beta.$\nThen for any $a<\\alpha,$ and $g<\\gamma,$ $a< b$ and $g<g_1+r,$ so $$a+g<b+g_1+r=g_1+(b+r)<\\beta+\\gamma.$$\nThis assumes you know that:\n\nGiven a cut $\\gamma,$ and rational $r>0$ there are rationals $g,h$ with $g\\in\\gamma$ and $h\\not\\in\\gamma,$ and $h-g<r.$\n\n",
    "tags": [
      "real-analysis",
      "proof-writing",
      "real-numbers",
      "rational-numbers"
    ],
    "score": 7,
    "answer_score": 5,
    "is_accepted": true,
    "question_id": 4655198,
    "answer_id": 4655217
  },
  {
    "theorem": "Show that there exists no strictly increasing function $f:\\mathbb{N}\\rightarrow\\mathbb{N}$ with $f(2)=3$...",
    "context": "Full exercise:\n\nShow that there exists no strictly increasing function $f:\\mathbb{N}\\rightarrow\\mathbb{N}$ with $f(2)=3$ which has the property that $f(mn)=f(m)f(n)$.\n\nThis is one of the first exercises in Putnam and Beyond, in the section dedicated to the proof by contradiction. \nI am quite familiar with the proof technique and feel comfortable with the mechanics of the problem but I find a nice trick here quite elusive. If anyone sees the elephant in the room a bit of subtle guidance would be much appreciated. I have the solution on hand but I would rather not look at it (where's the fun?). If you have any general suggestions that come to mind on how to handle proofs of this nature, especially those involving multiplicative homomorphisms between subsets of $\\mathbb{R}$, I am all ears. Many thanks! \nP.S. I forgot to mention that I have tried using the fact that $f$ increasing implies $f(n+1)>f(n)$ for all $n\\in\\mathbb{N}$ in several ways to produce a contradiction of the $f(2)=3$ condition. Mainly I used the factorization of $n^2-1$ to get $f(n+1)f(n-1)<f(n)f(n)$ from the inequality $f(n^2)>f(n^2-1)$ but did not find anything very helpful in this approach.\n",
    "proof": "We have $f(4)=9,$ so $f(3)=4,5,6,7,8$.  We also have $f(8)=27$, so $f(9) \\gt 27$ and $f(3) \\gt 5$.  If $f(3)=8, f(27)=512$, but $f(32)=3^5=243$.  If $f(3)=7, f(81)=2401,$ but $f(128)=2187$.  If $f(3)=6, f(243)=7776,$ but $f(256)=6561$\n",
    "tags": [
      "real-analysis",
      "proof-writing",
      "contest-math"
    ],
    "score": 7,
    "answer_score": 8,
    "is_accepted": false,
    "question_id": 2047553,
    "answer_id": 2047565
  },
  {
    "theorem": "Proof outer measure satisfies monotonicity: $A \\subseteq B \\implies m^*(A) \\leq m^*(B)$",
    "context": "Theorem: $$A \\subseteq B \\implies m^*(A) \\leq m^*(B)$$\nProof Attempt:\nBy definition, $m^*(B) = \\inf\\{\\sum\\limits_{k=1}^\\infty |J_k||\\{J_k\\} \\text{ is a cover of B }\\}$, $m^*(A) = \\inf\\{\\sum\\limits_{k=1}^\\infty |I_k||\\{I_k\\} \\text{ is a cover of A }\\}$\nThen since $A \\subset B \\implies \\bigcup_{k =1}^\\infty I_k \\subseteq \\bigcup_{k=1}^\\infty J_k$, then $\\sum\\limits_{k=1}^\\infty |I_k| \\leq \\sum\\limits_{k=1}^\\infty |J_k|$, so we have $m^*(A) \\leq m^*(B)$\nCan someone check if everything checks out?\n",
    "proof": "I am just a little bit unsighted by your proof, hence I want to give you a proof which is roughly the same, but a little more sharpened so that you can see how get rid of small discrepancies in proofs.\nSo $m^*(B) = \\inf \\{ \\sum_1^n|J_k|, \\{ J_k\\} \\text{ is an open interval cover of B}\\}$.\nSimilarly, $m^*(A) = \\inf \\{ \\sum_1^n|J_k|, \\{ J_k\\} \\text{ is an open interval cover of A}\\}$.\nNote that $A \\subset B$,whence if $\\{J_k\\}$ covers $B$, it also covers $A$. Hence, the following is true:\n$$\n\\{ \\sum_1^n|J_k|, \\{ J_k\\} \\text{ an open interval cover of B}\\} \\subset \\{ \\sum_1^n|J_k|, \\{ J_k\\} \\text{ an open interval cover of A}\\}\n$$\nNow, the infimum of the subset of a family is larger than the infimum of the family, because the family could contain an element that is  not in the subset but smaller than every element in the subset.\nHence, it follows that the infimum of the left side, which is $m^*(B)$, is greater than the right side, which is $m^*(A)$.\nEDIT: As other have pointed  out, there was something wrong in your proof. However, you were thinking in the right direction, and hopefully this proof will show you how to  put those efforts in a more rigorous direction.\n",
    "tags": [
      "measure-theory",
      "proof-verification",
      "proof-writing",
      "lebesgue-measure"
    ],
    "score": 7,
    "answer_score": 6,
    "is_accepted": true,
    "question_id": 1758780,
    "answer_id": 1758787
  },
  {
    "theorem": "Prove if $n^2$ is even, then $n^2$ is divisible by 4",
    "context": "I am working on this question\n\nProve for every integer n if $n^2$ is even, then $n^2$ is divisible by 4.\n\nprove  by contradiction\nProof:\nSince there exists an integer $n$ such that $n^2$ is even, and $n^2$ is not divisible by 4, \nwhen $n$ is odd integer, we have $n = 2k + 1$ where $k \\in \\mathbb{Z}$, \nthen $n^2 = 4k^2 + 4k + 1$, because $n^2$ is odd which is a contradiction;\nwhen $n$ is even integer, we have $n = 2j$ where $j \\in\\mathbb{Z}$,\nthen $n^2 = 4j^2 \\Rightarrow n^2 | 4$, because $n^2$ is divisible by $4$, this is a contradiction; therefore, for every integer $n$, if $n^2$ is even, then $n^2$ is divisible by $4$.\nIs my proof valid or can anyone give me hint or suggestion to write a better proof?\nThanks!\n",
    "proof": "$$n^2\\text{ even }\\implies \\text{n is even, hence:}$$$$n=2m,m\\in\\Bbb Z, n^2=4m^2\\implies 4|n^2$$\n",
    "tags": [
      "elementary-number-theory",
      "discrete-mathematics",
      "proof-writing"
    ],
    "score": 7,
    "answer_score": 4,
    "is_accepted": false,
    "question_id": 1171410,
    "answer_id": 1171414
  },
  {
    "theorem": "Is (the proof of) Fermat&#39;s last theorem completely, utterly, totally accepted like $3+4=7$?",
    "context": "If a mathematician would/does make use of Fermat's last theorem in a proof in a publication, would s/he still make use of some kind of caveat, like: \"assuming Fermat's last theorem is true\" or \"assuming the proof is correct\", or would it be considered totally unnecessary to even explicitly mention that the theorem is used?\nI assume that the theorem is beyond (reasonable?) doubt, but is it so much beyond doubt that it can be used for anything else as well? (E.g., has the proof been checked by a computer? If such a thing is possible.)\n(Is there even such a thing as a degree of belief in a proof? Perhaps based on length and complexity? Or is it really a binary thing?)\n",
    "proof": "Yes, the proof is accepted, and it would be bizarre to write \"assuming Fermat's Last Theorem is true\" in a paper. \n",
    "tags": [
      "number-theory",
      "proof-writing"
    ],
    "score": 7,
    "answer_score": 6,
    "is_accepted": true,
    "question_id": 750932,
    "answer_id": 751676
  },
  {
    "theorem": "Help understanding the proof of Lam&#233;&#39;s Theorem.",
    "context": "I think Lamé's Theorem is beautiful and really want to understand the proof.\nI am new to proofs, but after reading over the proof of Lamé's Theorem (and failing to understand it), I feel that I am capable of understanding it because it contained terms such as Fibonacci numbers, logarithms (base 10 and e), and the golden ratio - all of which I am familiar with. Although I understand those terms, I just need a little help with the reasoning. \nA step-by-step analysis of the proof would help me tremendously! \nHere is the proof I read.\n",
    "proof": "Ok, well. One of the best way to understand a proof is to reproduce it via free recall after glancing over the basic components of the proof.\nSo the theorem is:\nLet a and b be integers; let $a>b$. Applying Euclidean Algorithm to find the GCD will take n steps. The theorem says that if b has $d$ digits, then $n\\leq 5d$.\nThe components are Fibonacci's number and Euclidean Algorithm, and some other things.\nFirst, let us see why the theorem might be true.\nWell, first, let us look at Euclidean Algorithm. What does it do? We look in wikipedia and sees that\nGiven $a, b$, we first find $b_1 = a\\mod b$, and then set $a_1=b$. \nAt the $i$'th step, we find $b_{i} = a_{i-1} \\mod b_{i-1}$ and then set $a_{i} = b_{i-1}$. The algorithm ends at step n when we find that $b_n=0$.\nWell, does the theorem make any sense? Should there be a max limit to the number of steps the algorithm will take. \nWell, yes, of course.\nFirst of all, we note that $b_{n-1} \\geq 1$. \nSecond, let us look at an example. Let $b_{i-2} = 30 = {a_{i-1}}$, what is the highest possible $b_i$? Well, let us say that $b_{i-1}$ can be anything. What will give the greatest $b_i$?. Well, the whole point is, $30 \\pmod {b_{i-1}}$ cannot be greater than $14$. So, we can see that $b_i < \\frac{1}{2}b_{i-2}$.\nThis already gives us a bound. You see, $2^4>10$, so $b_i$ has at least one less digit than $b_{i-8}$.\nNow, we want a stronger bond. The other important component is the Fibonacci numbers.\nLet us look at them.\n1,2,3,5,8,13,21,34,55,89, 144 ...\nWe can see immediately that for every number, if you look 5 numbers over, there is at least one more digit.\nSo, if we can relate our thing to the Fibonacci's sequence, that'd be great.\nWell, we can.\nWe already said that $b_n = 0$. The smallest value $b_{n-1}$ could be is $b_{n-1} = 1$. (In case you need to justify this, consider the other possibilities. It must be an integer. If it is 0, then the algorithm would have ended at the $n-1$ step. But we already designated n to be the step at which it ends at.) Well, how about $b_{n-2}$?\nWell first, we see that $b_i<b_{i-1}$. This should be obvious since $b_i$ is the remainder of something divided by $b_{i-1}$.\nSo, the smallest $b_{n-2}$ could be is $2$. \nNow, let us proceed.\nWhat is the smallest $b_{n-3}$ could be. Well, to get $b_{n-1}$, we divided $a_{n-2} = b_{n-3}$ by $b_{n-2}$ and found the remainder. Therefore, $a_{n-2}=b_{n-3} = c\\times b_{n-2}+b_{n-1}$. This is obviously greater or equal to just $b_{n-2}+b_{n-1}$.\nNow, we can generalize this, can't we. Let us say we know that $b_i$, and $b_{i-1}$. Well, we know that $b_i = b_{i-2} \\mod b_{i-1}$, so $b_{i-2} = cb_{i-1}+b_i \\geq b_{i-1} + {b_i}$. $$$$$$$$\nWhat does this look like? Why, the Fibonacci's sequence!\nCOOL!\nOk, so it seems like the $b_{n-j}$ cannot be less than the jth number in the sequence.\nThere is subtle point here. We have not proved it. We have only proved that $b_{n-1} \\geq 1$, $b_{n-2}\\geq 2$, and that $b_{i-2}\\geq b_{i-1}+b_{i} $.\nWe want to prove that the least possible value of $b_{n-j}$ is the jth number in the sequence. So...\nWe still need to prove that the least possible value of $b_{i-2}$ is greater than the sum of the least possible value of $b_{i-1}$ and $b_{i}$. This is pretty easy to prove.\nFor any $b_{i-2}$, we have that $b_{i-2} \\geq b_{i-1} + b_{i} \\geq \\min(b_{i-1})+ \\min(b_{i})$. As such, $\\min (b_{i-2})\\geq \\min(b_{i-1})+ \\min(b_{i})$.\nSo now, we prove that $b_{n-j}$ is greater or equal to $F_j$ the $j'$th term of the Fibonacci sequence. \nIt is true for $j=1$ and $j=2$. $b_{n-1}>\\geq 1$, and $b_{n-2}\\geq 2$.\nIf it is true for $j=k$ and $j=k+1$, then let $i=(n-k)$. So, $b_{n-(k+2)} = b_{i-2}$. $b_{n-(k+1)} = b_{i-1}$. $b_{n-k} = b_i$. Then we see that $\\min(b_{n-(k+2)}) = \\min(b_{i-2}) \\geq \\min(b_{i-1})+ \\min(b_{i}) = F_{k+1}+F_{k} = F_k$.\nSo, $b_{n-(k+2)} \\geq F_{k+2}$, and the statement is thus true for $j=k+2$. \nBy mathematical induction then, we have that the statement is true for all integer $j$.\nSo, what have we proven so far?\nWell, we have proven that $b_{n-n} = b \\geq F_n$. In other words, the Knuth form of the theorem.\nNow, how about the original?\nWell, we just need to give the proof that each decimal place increase for the Fibonacci sequence takes at most 5 steps.\nSo.\nLet us do that.\n\\begin{align}\nF_{n+5} & = f_{n+4}+f_{n+3}\\\\\n& = f_{n+3} + f_{n+2} + f_{n+3}\\\\\n& = 2f_{n+3}+f_{n+2}\\\\\n& = 2f_{n+2}+2f_{n+1}+f_{n+2}\\\\\n& = 3f_{n+2}+2f_{n+1}\\\\\n& = 5f_{n+1}+3f_n\\\\\n& = 8f_n+5f_{n-1}\\\\\n& = 13f_{n-1}+8f_{n-2}\\\\\n& = 10f_{n-1}+3f_{n-1}+8f_{n-2}\\\\\n& = 10f_{n-1}+11f_{n-2}+3f{n-1}\\\\\n& >10f_n\n\\end{align}\nSo we have what we wanted. $F_n > 10 F_{n-5}$. So, we start with b. After 5 steps of the algorithm, $b$ has one less decimal places. If b has d decimal places, then after $5(d-1)$ steps, b only has 1 decimal place. \nHence, we are done.\nI would like to note that the definition of the first step here might be slightly different, so the answer might be off by 1.\nBut in general, this is how you would do it.\nAgain, the best way to understand a proof is to replicate it via free recall.\n",
    "tags": [
      "elementary-number-theory",
      "proof-writing"
    ],
    "score": 7,
    "answer_score": 8,
    "is_accepted": true,
    "question_id": 452139,
    "answer_id": 452223
  },
  {
    "theorem": "Necessity and Sufficiency",
    "context": "I'm learning to write mathematical proofs. When the statement to be proven is in the form \"$p$ if and only if $q$\", the proof is often broken into two parts: necessity and sufficiency. I wonder whether I should organize my proof like:\nNecessity: $p \\Rightarrow q$\nSufficiency: $ q \\Rightarrow p$\n... or vice versa?\nSince $p \\Leftrightarrow q$ is is equivalent to $q \\Leftrightarrow p$, does it really matter? Is there any accepted practise to put $p \\Rightarrow q$ in necessity or sufficiency, depending on the order in which the statements are presented?\n",
    "proof": "Strictly speaking, there is no difference, but it is common to put the \"subject\" first. An example will make more sense.\n\nA subset of $\\mathbb{R}^n$ (with the usual topology) is compact if and only if it is closed and bounded.\nvs.\nA subset of $\\mathbb{R}^n$ (with the usual topology) is closed and bounded if and only if it is compact.\n\nThey are both the same statement, but the purpose of the theorem is to characterize compactness, not to characterize (closed and bounded)-ness. For this reason, it is more pleasing (I think) to mention compactness first and also to use phrases like \"necessary for compactness\" and \"sufficient for compactness\".\n",
    "tags": [
      "logic",
      "proof-writing"
    ],
    "score": 7,
    "answer_score": 8,
    "is_accepted": true,
    "question_id": 216210,
    "answer_id": 216230
  },
  {
    "theorem": "Prove that $f(n)=n^2$ where $f$ is a strictly increasing multiplicative function with $f(2)=4$.",
    "context": "\nLet $f:\\mathbb N\\to\\mathbb N$ be a strictly increasing function with $f(2)=4$ which is completely multiplicative i.e $f(ab)=f(a)f(b)$ for all $a,b\\in\\mathbb N$. Prove that $f(n)=n^2$ for all $n\\in\\mathbb N$.\n\nThis is an exercise on induction. So I am looking for an inductive solution. Here is my progress:\nIt is easy to see that $f(1)=1$. For the base case, we need to find $f(3)$ which I had some difficulties to find. Now $$ f(3^2) > f(2^3) = 64 \\implies f(3)>8\\\\ f(3^8)<f(2^{13})=67108864\\implies f(3)<10\n$$\nSo $8<f(3)<10$ or $f(3)=9$. Now I can show that $f(2^k)=4^k$ for all $k\\in\\mathbb N$. Then I tried to prove $f(n+1)=(n+1)^2$ assuming $f(i)=i^2$ for all $i\\leq n$. But this doesn't work.\nSo how do I solve the problem?\n",
    "proof": "You have already shown the cases $n=1,2$. Now we want to do the induction step. To simplify notation I write $m=n+1$. If $m$ is not prime, we are done (as we can just use the induction hypothesis on the factors). Thus, in the following we will assume that $m$ is prime. First we show a lower bound\n$$ f(m)^2=f(m^2) > f(m^2-1)=f(m+1) f(m-1) = (m+1)^2 (m-1)^2 = (m^2-1)^2.$$\nWe used that $m$ is prime and thus $m\\pm 1$ are divisible by $2$, which implies that we can use the induction hypothesis to show $f(m\\pm 1)=(m\\pm 1)^2$. Taking square root yields $f(m)>m^2-1$.\nNext we show the upper bound. Let $N\\in \\mathbb{N}_{\\geq 1}$, then there exists a unique $\\ell(N)\\in \\mathbb{N}$ such that\n$$ 2^{\\ell(N)} \\leq  m^N < 2^{\\ell(N)+1}.$$\nThen we have\n$$ m^N < 2^{\\ell(N)+1} \\leq 2 m^N. $$\nThus, we obtain\n$$ f(m)^N < f(2^{\\ell(N)+1}) = (2^{\\ell(N)+1})^2 < 4 m^{2N}. $$\nAfter taking the $N$th root, we get\n$$ f(m) \\leq 4^{1/N} m^2. $$\nHence, for $N\\rightarrow \\infty$ we get\n$$ f(m) \\leq m^2. $$\nCombining the upper and the lower bound we get $f(m)=m^2$.\n",
    "tags": [
      "functions",
      "proof-writing",
      "induction",
      "functional-equations",
      "multiplicative-function"
    ],
    "score": 7,
    "answer_score": 5,
    "is_accepted": true,
    "question_id": 4253798,
    "answer_id": 4253906
  },
  {
    "theorem": "Direct proof that if $n^2$ is odd, $n$ is odd",
    "context": "I am rereading a book on methods of proof and thought I would try proving that if $n^2$ is odd, then $n$ is odd. The proofs for this that I have seen online mostly involve a proof by contrapositive. I was wondering if this could be done by direct proof instead. I found a direct proof in Mark Bennet's answer to this question. It goes:\n\nSuppose $n^2$ is odd, then $n^2=2m−1$ and $(n+1)^2=2(m+n)$\nNow $2$ is prime and $2∣(n+1)^2$ so $2∣n+1$ therefore $n+1=2r$ (for some integer r) whence $n=2r−1$ and $n$ is odd.\n\nI came up with a separate proof and I was wondering if it is logically sound:\n\nSuppose $n^2$ is odd, then $n^2=2m+1$ for some $m \\in \\mathbb{Z}$.\n$n^2=2m+1 \\implies n^2-1 = 2m \\implies (n+1)(n-1) = 2m$\nThis shows $(n+1)(n-1)$ is even; for this to be true, at least one of $n+1$ and $n-1$ must be even, which means that $n$ is odd.\n\nIs this proof written well? Does it have gaps? If there are problems with it, I would like to ensure that I do not make those same mistakes in the future.\n",
    "proof": "Your proof is nice. The problem it has is that when you say \"for this to be true\", you are hiding the fact that you are assuming one of two things:\n\nthe Fundamental Theorem of Arithmetic (to say that $2$ has to be a factor of either $n-1$ or $n+1$), or\n\nthat the product of odd numbers is odd.\n\n\nThe problem with the first one is that the proof of \"$n$ even if and only if $n^2$ even\" usually appears at the very beginning of Number Theory, before the FTA.\nThe problem with the second one, is that in the end you are using the kind implication you are trying to avoid by not using the contrapositive.\n",
    "tags": [
      "elementary-number-theory",
      "proof-writing",
      "solution-verification"
    ],
    "score": 7,
    "answer_score": 4,
    "is_accepted": true,
    "question_id": 4172153,
    "answer_id": 4172162
  },
  {
    "theorem": "A visually guided proof of the fundamental theorem of algebra?",
    "context": "A complex root of a polynomial $P(z)$ is a pair of real numbers $u,v$ that simultaneously make the real part and the imaginary part of $P(z)$ zero.\nThe zeros of the real part and the imaginary part are given by two curves in the complex plane\n$$\\operatorname{Re}(P(u +iv)) = 0$$\n$$\\operatorname{Im}(P(u+iv)) = 0$$\nThe zeros of the polynomial are the intersection points of these two curves.\nFor the sake of specifity here are the two curves for $P(z) = z^5 + z^3 + z^2 + z + 1$:\n$u^5-6u^3v^2-4v^2u^3+5uv^4+ u^3-v^2u-2uv^2+u^2-v^2+u+1 = 0 $\n$v^5+5vu^4-10u^2v^3+3u^2v-v^3+2uv+v=0$\nThe fundamental theorem of algebra says that such curves always do intersect. A proof of the fundamental theorem might go like this: The curves $\\operatorname{Re}(P(z)) = 0$ (red) and $\\operatorname{Im}(P(z)) = 0$ (blue) – which are tightly related – are such-and-such, so they must intersect at least once and at most $n$ times (for $n$ the degree of the polynomial).\nWhat can be seen is, that the curves always come in $n$ branches which extend to infinity and for some reason must intersect.\n $x^2 + x + 1$\n $x^3 + x^2 + x + 1$\n $x^4 + x^2 + x + 1$\n $x^5 + x^3 + x^2 + x + 1$\nHow could such a proof be spelled out?\n",
    "proof": "That idea is basically the approach to the Fundamental Theorem of Algebra taken by Gauss, in his PhD thesis Demonstratio nova theorematis omnem functionem algebraicam rationalem integram unius variabilis in factores reales primi vel secundi gradus resolvi posse. It's a nice geometrical approach to the theorem, but hard to complete rigorously. I suggest that you read C. F. Gauss’s  proofs of the Fundamental Theorem of Algebra, by Harel Cain.\n",
    "tags": [
      "abstract-algebra",
      "polynomials",
      "complex-numbers",
      "proof-writing",
      "roots"
    ],
    "score": 7,
    "answer_score": 7,
    "is_accepted": true,
    "question_id": 2961884,
    "answer_id": 2961893
  },
  {
    "theorem": "If the sum of eigenvectors is an eigenvector, then they all correspond to the same eigenvalue",
    "context": "I believe I am very close to finishing this proof, but I cannot figure out the last part. If anybody could check my work and maybe give me a little hint, it would be greatly appreciated!\nLet $V$ be a finite-dimensional vector space and $T \\in \\mathcal{L}(V)$, and let $\\mathbf{u,v} \\in V$ be eigenvectors of $T$.\nClaim:  If $\\mathbf{u} + \\mathbf{v}$ is an eigenvector of $T$, then $\\mathbf{u}, \\mathbf{v}$, and $\\mathbf{u+v}$ all correspond to the same eigenvalue.\nProof (So far!): Suppose $T(\\mathbf{u}) = \\lambda_1 \\mathbf{u}$ and $T(\\mathbf{v}) = \\lambda_2 \\mathbf{v}$ with $\\mathbf{u}, \\mathbf{v} \\neq \\mathbf{0}$. \nNow suppose $T(\\mathbf{u+v}) = \\lambda_3(\\mathbf{u+v})$ with $\\mathbf{u+v}\\neq \\mathbf{0}$.\nThen, $$\nT(\\mathbf{u}) + T(\\mathbf{v}) = \\lambda_3 \\mathbf{u} + \\lambda_3 \\mathbf{v}\\\\\n\\lambda_1\\mathbf{u} + \\lambda_2\\mathbf{v} =  \\lambda_3 \\mathbf{u} + \\lambda_3\\mathbf{v}\\\\\n\\lambda_1\\mathbf{u} + \\lambda_2\\mathbf{v} - \\lambda_3 \\mathbf{u} -\\lambda_3\\mathbf{v} = \\mathbf{0}\\\\\n(\\lambda_1 - \\lambda_3) \\mathbf{u} + (\\lambda_2 - \\lambda_3)\\mathbf{v} = \\mathbf{0}\n$$\nNow I know in order to show that $\\lambda_1 = \\lambda_2 = \\lambda_3$, I must show that the only solution to the last line is the trivial one. This would imply that $\\mathbf{u}$ and $\\mathbf{v}$ are linearly independent which I am unconvinced of!\nThe only information I have to my advantage I haven't used yet is the fact that $\\mathbf{u}, \\mathbf{v}, \\mathbf{u+v} \\neq \\mathbf{0}$. I really cannot see how this information can help me though.\nPerhaps I am going about this wrong, but that's why I want to ask! Thanks for your help.\n",
    "proof": "$$(\\lambda_1-\\lambda_3)\\vec u+(\\lambda_2-\\lambda_3)\\vec v=\\vec 0$$\nCase 1:\n$\\vec u$ and $\\vec v$ are linearly independent. Then as you say, $\\lambda_1-\\lambda_3=\\lambda_2-\\lambda_3=0$, so done.\nCase 2:\n$\\vec u$ and $\\vec v$ are linearly dependent. Then $\\vec v=k\\vec u$ for a scalar $k$. Then $T(\\vec v)=\\lambda_2 \\vec v$, but also $T(\\vec v)=T(k\\vec u)=kT(\\vec u)=k\\lambda_1\\vec u=\\lambda_1\\vec v$. Since $\\vec v$ is an eigenvector, it is non zero, and so $\\lambda_1=\\lambda_2$. Then you can go back to the previous equation $$\\lambda_1 (\\vec u+\\vec v)=\\lambda_3(\\vec u+\\vec v)$$and again using the fact that $\\vec u+\\vec v$ is an eigenvector so non-zero, it can be concluded that all the $\\lambda_i$'s are equal.\n",
    "tags": [
      "linear-algebra",
      "proof-verification",
      "proof-writing",
      "eigenvalues-eigenvectors"
    ],
    "score": 7,
    "answer_score": 7,
    "is_accepted": true,
    "question_id": 2256942,
    "answer_id": 2256957
  },
  {
    "theorem": "question on translation of operator proof",
    "context": "Has anyone studied the book 'Nonlinear Partial differential equations with applications' by Tomas Roubicek? I am interested in discussing a point of interest in this book. \nSpecifically, on page 52, in the proof of Theorem 2.36 (Leray-Lions) it states that $A_{0}$ inherits coercivity from $A$. How does one prove this?\nThanks for any assistance. \n",
    "proof": "The setup\nFor completeness I want to state the parts from the book that are needed:\nWe are given a (in general nonlinear) operator\n$$A: W^{1,p}(\\Omega) \\rightarrow W^{1,p}(\\Omega)^*,$$\nwhich is coercive (see Lemma 2.35 in the given literature). At that, coercivity means the existence of a mapping $\\xi: \\mathbb{R}^+ \\rightarrow \\mathbb{R}^+$ such that $\\xi$ is unbounded (i.$\\,$e. $\\lim_{r\\to+\\infty} \\xi(r) = +\\infty$) and\n$$\n \\forall u \\in W^{1,p}(\\Omega): A(u) \\geq \\xi(\\Vert u\\Vert) \\Vert u \\Vert.\n$$\nFurthermore, we have the shifted operator\n$$\n A_0: \\phantom{u} W^{1,p}(\\Omega)  \\rightarrow W^{1,p}(\\Omega)^* \\\\\n \\phantom{A_0: W^{1,p}(\\Omega)} u \\mapsto A(u+w)\n$$\nwhere $w \\in W^{1,p}(\\Omega)$ is a constant, which is given in the book. (Actually $A_0$ is defined on a linear subspace $V\\subseteq W^{1,p}(\\Omega)$ but since $A$ is defined on the whole space we can extend $A_0$ to the whole space as well.)\n\nProof of coercivity\nWe will show coercivity of $A_0$ on $W^{1,p}(\\Omega)$, which then directly implies coercivity of $A_0$ on the subspace $V \\subseteq W^{1,p}(\\Omega)$ by simple restriction of the operator.\nFirst of all, we state there exist $R \\in \\mathbb{R}^*$ and $c \\in \\mathbb{R}^+$ such that for all $u\\in W^{1,p}(\\Omega)$ with $\\Vert u\\Vert \\geq R$ holds\n$$\n  \\Vert u \\Vert \\leq c\\Vert u+w\\Vert. \\ \\ \\ (1)\n$$\n(Is this plausible enough without a proof? It's rather general and isn't a direct part of the main proof.)\nNow define  $\\eta: \\mathbb{R}^+ \\rightarrow \\mathbb{R}^+$ by\n$$\n\\begin{align}\n&\\eta(r) := \\begin{cases}  & c\\inf\\limits_{v \\in W^{1,p}(\\Omega), \\|v\\| = r}\\xi(\\| v+w\\|)  ~~~~~~~~~~~~~~~~~~~~\\text{ for } r \\geq R,\\\\ &0 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\\text{for } r < R  \n\\end{cases}\n\\end{align}\n$$\n(Thanks to John Doe for improving the formatting!) Note that $\\eta$ is well-defined because for all $r\\in \\mathbb{R}^+$ we have $\\inf_{v \\in W^{1,p}(\\Omega), \\Vert v\\Vert = r} \\xi(\\Vert v + w\\Vert) \\geq 0$ as of the definition of $\\xi$. Furthermore, we have $\\eta(r) \\to +\\infty$ for $r \\to \\infty$. Else there would exist a sequence $(u_k)_{k\\in \\mathbb{N}}$ with $\\Vert u_k\\Vert = k$ for all $k\\in \\mathbb{N}$ and $C \\geq \\limsup_{k\\to\\infty} \\eta(\\Vert u_k\\Vert) = \\limsup_{k\\to\\infty} \\xi(\\Vert u_k+ w\\Vert)$. In particular we would have $\\Vert u_k + w\\Vert \\to \\infty$ for $k\\to\\infty$ but $\\xi(\\Vert u_k+ w\\Vert) \\leq C$ for $k\\to \\infty$ in contradiction to $\\xi(r) \\to \\infty$ for $r\\to\\infty$. Consequently this contradiction yields $\\eta(r) \\to \\infty$ for $r\\to\\infty$ as well.\nJust by the definition of $A_0$, then the definition of coercivity applied to the operator $A$, then usage of (1) and finally using the definition of $\\eta$ we have for all $u \\in W^{1,p}(\\Omega)$ with $\\Vert u \\Vert \\geq R$\n$$\n A_0(u) = A(u+w) \\geq \\xi(\\Vert u+w\\Vert) \\Vert u+w\\Vert \\geq \\frac{1}{c}\\xi(\\Vert u+w\\Vert)\\Vert u\\Vert \\geq \\eta(\\Vert u\\Vert) \\Vert u\\Vert.\n$$\nand for all $u \\in W^{1,p}(\\Omega)$ with $\\Vert u \\Vert < R$ we have (again by definition of $A_0$, coercivity itself and $\\eta$)\n$$\n A_0(u) = A(u+w) \\geq \\xi(\\Vert u+w\\Vert) \\Vert u+w\\Vert \\geq 0 = \\eta(\\Vert u\\Vert) \\Vert u\\Vert.\n$$\nThus $A_0$ is coercive as $\\eta$ is a eligible coercivity-function as shown above.\n\nRemarks on the proof\nBecause of the comments I have a few remarks to add.\n\nThe well-definedness part above for $\\eta$ is not about showing that $\\inf_{v \\in W^{1,p}(\\Omega), \\Vert v\\Vert = r} \\xi(\\Vert v + w\\Vert)$ exists (for all $r \\geq R$). The existence of the infimum is given by the boundedness from below. But we furthermore ned $\\inf_{v \\in W^{1,p}(\\Omega), \\Vert v\\Vert = r} \\xi(\\Vert v + w\\Vert) \\geq 0$ so that $\\eta\\: \\mathbb{R}^+ \\rightarrow \\mathbb{R}^+$ is well-defined (because if the infimum was less than zero we wouldn't have a mapping into $\\mathbb{R}^+$ anymore). This property is given because $\\xi$ is bounded from below by zero (as its range is defined on $\\mathbb{R}^+$ as well; this boundary also gives us the mentioned existence of the infimum).\nTo the part with \"$\\limsup_{k\\to\\infty} \\eta(\\Vert u_k\\Vert) = \\limsup_{k\\to\\infty} \\xi(\\Vert u_k+ w\\Vert)$\" (note that the indizes $n$ were corrected to $k$). I'll repeat this part in a more explanatory way. So, before we make any assumptions, we have just by the properties of the infimum (and the definition of $\\eta$) that for all $k\\in \\mathbb{N}$ with $k\\geq R$ exists an $u_k \\in W^{1,p}(\\Omega)$ such that $\\Vert u_k\\Vert = k$ and\n$$\n \\xi(\\Vert u_k + w\\Vert ) - 1 \\leq \\inf_{u\\in W^{1,p}(\\Omega), \\Vert u\\Vert = k} \\xi(\\Vert u+w\\Vert) = \\eta(k)= \\eta(\\Vert u_k\\Vert).\n$$\n(Because we can always choose a sequence that converges to the infimum and thus choose an element that makes the distance to that infimum arbitrarily small. In particular, we find an element that realizes at most the distance 1 to the infimum.)\nSo, if $\\eta$ was bounded by some constant $c_1$ we would have for all $k\\in\\mathbb{N}$ with $k \\geq R$\n$$\n \\xi(\\Vert u_k + w\\Vert ) \\leq \\eta(\\Vert u_k\\Vert) + 1 \\leq c_1+1 =: C.\n$$\nAs $\\Vert u_k + w\\Vert$ goes to infinity for $k\\to \\infty$ we get a contradiction because of the definition of $\\xi$.\nYou could now take the $\\limsup$ over all the expressions in that inequality and the inequalities would still hold. However, you see that this is actually not necessary at all as we already got our contradiction. I maybe was thinking a little too complicated there (and also was not writing very clearly, I'm sorry).\nFor \"$\\frac{1}{c}\\xi(\\Vert u+w\\Vert)\\Vert u\\Vert \\geq \\eta(\\Vert u\\Vert) \\Vert u\\Vert$\" note that, in the definition of $\\eta$, we multiply the whole expression by $c$ from (1). That's why it cancels out. (Also I wrote $C$ instead of $c$ in the beginning of the proof; this has been corrected now.)\n\n",
    "tags": [
      "functional-analysis",
      "reference-request",
      "partial-differential-equations",
      "proof-writing"
    ],
    "score": 7,
    "answer_score": 7,
    "is_accepted": true,
    "question_id": 1049871,
    "answer_id": 1065144
  },
  {
    "theorem": "Induction Proof: Formula for Sum of n Fibonacci Numbers",
    "context": "\nThe Fibonacci sequence $F_0, F_1, F_2, \\ldots$ is defined recursively by $F_{0}:=0, F_{1}:=1 $ and $F_{n}:=F_{n-1}+F_{n-2}$.\nProve that\n$$\\sum_{i=0}^{n} F_{i}=F_{n+2}-1 \\qquad \\text{for all } n \\geq 0 .$$\n\nI am stuck though on the way to prove this statement of fibonacci numbers by induction :\nmy steps:\ndefinition:\nThe Hypothesis is: $\\sum_{i=0}^{n} F_{i}=F_{n+2}-1$ for all $n > 1$\nBase case: $n=2$\n$\\sum_{i=0}^{2} F_{i}=F_{0}+F_{1}+F_{2}=0+1+F_{1}+F_{0}=0+1+1+0=2$ which is equal to $F_{2+2}-1=F_{4}-1=F_{3}+F_{2}-1=F_{2}+F_{1}+F_{2}-1=1+1+1-1=2$ OK!\ninductive step:\nto prove: $\\sum_{i=0}^{n+1} F_{i}=F_{n+3}-1$ for all $n > 1$\n$\\sum_{i=0}^{n+1} F_{i}=\\sum_{i=0}^{n} F_{i}+F_{n+1}=F_{n+2}-1+F_{n+1}=...help...=F_{n+3}-1$\ni need help to $..help..$ please! thanks a lot\n",
    "proof": "Use  $F_{n+1}+F_{n+2}=F_{n+3}$, to get:\n$$\\sum_{i=0}^{n+1} F_{i}=\\sum_{i=0}^{n} F_{i}+F_{n+1}=F_{n+2}-1+F_{n+1}=F_{n+1}+F_{n+2}-1=F_{n+3}-1$$\n",
    "tags": [
      "induction",
      "proof-writing",
      "problem-solving",
      "fibonacci-numbers"
    ],
    "score": 7,
    "answer_score": 7,
    "is_accepted": true,
    "question_id": 243606,
    "answer_id": 243608
  },
  {
    "theorem": "absolute value of supremum is smaller than or equal to supremum of absolute values",
    "context": "Is the following statement correct? \n\"The absolute value of supremum of a set is smaller than or equal to the supremum of the absolute values of the first set\"\nIt would be helpful to proof that the absolute value of the integral of a function is smaller than or equal the integral of the absolute value.\n",
    "proof": "Yes. Because $a\\leq |a|$, $\\sup A\\leq \\sup|A|$, so if $\\sup(A)\\geq 0$ you would be done.  If $\\sup(A)<0$, then $|A|=-A$ and $|\\sup A|=-\\sup(A)=\\inf(-A)=\\inf |A|\\leq \\sup|A|$.\nHere $|A|=\\{|a|:a\\in A\\}$, and $-A=\\{-a:a\\in A\\}$.\n",
    "tags": [
      "real-analysis",
      "integration",
      "proof-writing",
      "proof-explanation",
      "supremum-and-infimum"
    ],
    "score": 7,
    "answer_score": 6,
    "is_accepted": false,
    "question_id": 2200201,
    "answer_id": 2200237
  },
  {
    "theorem": "Show that the closed unit-ball in $\\mathbb{R}^2$ is homeomorphic to the unit-2 sphere in $\\mathbb{R}^3$",
    "context": "This problem is taken from Topology: A First Course by Munkres\n\nLet $X$ be the closed unit ball $$\\{ \\langle x,  y\\rangle \\mid x^2 + y^2 \\le 1\\}$$ in $\\mathbb{R}^2$, and let $X^{\\ast}$ be the partition of $X$ consisting of all the one-point sets $\\{ \\langle x,  y\\rangle\\}$ for which $x^2 + y^2 < 1$, along with the set $S^1= \\{ \\langle x,  y\\rangle \\mid x^2 + y^2 = 1 \\}$.  One can show that $X^{\\ast}$ is homeomorphic with the subspace of $\\mathbb{R}^3$ called the unit 2-sphere, defined by $$S^2 = \\{\\langle x,  y, z\\rangle \\mid x^2 + y^2 + z^2 =1 \\}.$$\n\nMy Attempted Proof\n$(X^*, \\mathcal{T})$ is the quotient space of $X$ where $\\mathcal{T}$ is the topology induces by the quotient map $p : X \\to X^*$. $$\\mathcal{T} = \\left\\{U \\subset X^* \\ \\middle| \\ p^{-1}(U) = X \\cap W \\text{ for $W$ open in $\\mathbb{R}^2$}\\right\\}$$\nSince $S^2 \\subset \\mathbb{R}^3$, the topology $\\mathcal{M}$ on $S^2$ is given by $$\\mathcal{M} = \\left\\{S^2 \\cap V \\ | \\ V \\text{ open in $\\mathbb{R}^3$}\\right\\}$$\nNow define $h : X^* \\to S^2$ by $$h\\left(\\langle x, y \\rangle\\right) = \\begin{cases} \\langle x, y, \\sqrt{1-x^2 + y^2} \\rangle \\ \\ \\  \\text{ if $x^2 + y^2 < 1$} \\\\\n\\langle x, y, 0 \\rangle \\  \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\  \\ \\ \\ \\text{ if $x^2 + y^2 = 1$} \\\\\n\\end{cases}$$\nWe now show that $h$ is a homeomorphism. Pick $U \\in \\mathcal{T}$, we first show that $h(U) \\in \\mathcal{M}$. $U$ is of the form $U = \\{ \\langle x, y \\rangle \\ | \\ x^2 + y^2 = 1 \\text{ or } x^2 + y^2 < 1 \\}$, and by definition of $h$ we have $h(U) = \\{\\langle x, y , z\\rangle \\ | \\ x^2 + y^2 +z^2 = 1 \\} \\subset \\mathbb{R}^3$. Fix $\\epsilon > 0$ and choose $V = (-1 - \\epsilon, 1 + \\epsilon) \\times (-1 - \\epsilon, 1 + \\epsilon) \\times (-1 - \\epsilon, 1 + \\epsilon)$. Then $V$ is open in $\\mathbb{R}^3$ and we have $h(U) \\cap V = h(U) \\in \\mathcal{M}$. Thus $U \\in \\mathcal{T} \\implies h(U) \\in \\mathcal{M}$\nWe now have to prove that $h(U) \\in \\mathcal{M} \\implies U \\in \\mathcal{T}$. \n\nBut that is where I got stuck. I'm not show how to show $U \\in \\mathcal{T}$ if we pick $h(U) \\in \\mathcal{M}$.\nAm I on the correct track, are there any errors so far in my proof, is it non-rigorous. If so, please let me know. If not then how can I go about proving the reverse implication to show that $h$ is a homeomorphism?\n",
    "proof": "OK, so the question is clearer if you write it like this:\nThere's a relation on a $n$-dimensional closed unit ball $D^n$:\n$$x\\sim y\\mbox{ if and only if }x=y\\mbox{ or }\\lVert x\\rVert =\\lVert y\\rVert =1$$\nProve that $D^n\\ /\\sim$ is homeomorphic to $S^n$.\nFirst of all note that your function $h$ is not a homeomorphism because it is not \"onto\" (your last coordinate is always nonnegative).\n\nProof. Start by looking the other way around. Define\n$$\\theta:\\mathbb{R}^{n}\\to S^n$$\n$$\\theta(x_1, \\ldots, x_n)=\\bigg(\\frac{S-1}{S+1}, \\frac{2x_1}{S+1},\\cdots,\\frac{2x_n}{S+1}\\bigg)$$\nwhere $S=\\sum x_i^2$. This map is also known as the stereographic projection (or more accurately its inverse). Note that the image consists of all points on the sphere except for $(1,0,\\ldots, 0)$. Also $\\theta$ is one-to-one.\nNow define a map from open disk to $\\mathbb{R}^n$:\n$$g:B^n\\to\\mathbb{R}^n$$\n$$g(x)=\\begin{cases}\n\\tan\\bigg(\\frac{\\pi}{2}\\lVert x\\rVert\\bigg)\\frac{x}{\\lVert x\\rVert}\\mbox{ if }x\\neq 0 \\\\\n0\\mbox{ otherwise }\n\\end{cases}$$\nThis is a homeomorphism, an inverse given by scalar multiplication by $\\arctan$.\nFinally we are ready to define our function:\n$$f:D^{n}\\to S^n$$\n$$f(x)=\\begin{cases}\n\\theta\\big(g(x)\\big)\\mbox{ if } \\lVert x\\rVert < 1 \\\\\n(1,0,\\ldots,0)\\mbox{ otherwise}\n\\end{cases}$$\nYou can easily check that it is continous (that's because of properties of stereographic projection, the closer arguments get to infinity in $\\mathbb{R}^n$ the closer values get to $(1,0,\\ldots,0)$) and \"onto\". Also\n$$f(x)=f(y)\\mbox{ if and only if }x=y\\mbox{ or }\\lVert x\\rVert = \\lVert y\\rVert = 1$$\nIn particular $f$ induces the mapping\n$$\\overline{f}:D^n\\ /\\sim\\to S^n$$\n$$\\overline{f}([x])=f(x)$$\nThis map is continous, one-to-one and \"onto\". Since $D^n$ is compact then so is $D^n\\ /\\sim$ (you only need to verify that it is Hausdorff) and thus $\\overline{f}$ is also closed. Therefore the inverse is continous so $\\overline{f}$ is a homeomorphism. $\\Box$\n",
    "tags": [
      "general-topology",
      "proof-verification",
      "proof-writing",
      "quotient-spaces"
    ],
    "score": 7,
    "answer_score": 6,
    "is_accepted": false,
    "question_id": 2114817,
    "answer_id": 2114866
  },
  {
    "theorem": "Show that if $x^2 y=2x+y$, then if $y \\neq 0$ then $x \\neq 0$",
    "context": "Prove if $x^2 y=2x+y$, then if $y \\neq 0$ then $x \\neq 0$. Obviously, $x,y \\in \\mathbb R$.\nI know this is rather simple. It is more about the process than this example.\nIs it logically correct to do the following:\nSuppose $x^2 y=2x+y$, and $x=0$. It follows that $y=0$. Therefore if $y \\neq 0$ then $x \\neq 0$. \nIs this correct use of the contrapositive?\n",
    "proof": "You are correct. What you've done is essentially a proof by contradiction (assume the opposite of what you're trying to prove, derive something that contradicts something you know to be true, and this implies that what you're trying to prove is true). \n",
    "tags": [
      "algebra-precalculus",
      "proof-writing"
    ],
    "score": 7,
    "answer_score": 6,
    "is_accepted": true,
    "question_id": 1147387,
    "answer_id": 1147391
  },
  {
    "theorem": "Maximum and average number of inversions in array by induction",
    "context": "Just for your information, an inversion in an array $a$ is any ordered pair of points $(i, j)$ where $i < j$ and $a_i > a_j$.\nI can prove the maximum and average number of inversions in an array easily, like so...\nThe max will be when the array is fully unsorted in which case the number of inversions would be $(n-1) + (n-2) + (n-3)$ all the way down to $(n-(n-1))$. This gets simplified to 0.5(n-1)n maximum inversions. Now the average number of inversions will just be half way between the best and worst case scenario. The worst case scenario is $0.5(n-1)n$ and the best case scenario is $0$ so the average would be $0.25(n-1)n$\nNow my question is: how would you go about proving this via induction instead? I'm having trouble knowing what the base case will be and I'm just having trouble with it in general.\n",
    "proof": "Since you insist on proof by induction, maybe you could try it like this:\nMaximum number of inversions\na) Prove by induction that the number of inversions is $\\le \\frac{n(n-1)}2$.\n$1^\\circ$ If $n=1$, we have only one permutation which has no inversions.\n$2^\\circ$ Suppose the claim is true for $n$ elements. If we add $(n+1)$-th element, then we have inversions coming from the first $n$ elements -- there is at most $\\frac{n(n-1)}2$ of them, and we also have inversions containing the $(n+1)$-th element -- there is at most $n$ of them.\n$$\\frac{n(n-1)}2 + n = \\frac{n(n+1)}2$$\nSo we obtained the same formula for $(n+1)$ instead if $n$.\nb) Prove by induction that the permutation $(n,n-1,\\dots,2,1)$ has exactly $\\frac{n(n-1)}2$ inversions.\n(Note that I am using one-line notation, not cycle notation for permutations.)\n$1^\\circ$ The permutation $(1)$ has no inversions.\n$2^\\circ$ The permutation $(n+1,n,\\dots,2,1)$ has $n$ inversions containing the element $(n+1)$. The remaining inversion are the same as in the permutation $(n,\\dots,2,1)$. So together we have \n$$\\frac{n(n-1)}2 + n = \\frac{n(n+1)}2$$\ninversions.\n\nAverage number of inversions\nWe claim that average number of inversions is $\\frac{n(n-1)}4$.\n$1^\\circ$ For $n=1$ we only have one permutation, which has no inversions.\n$2^\\circ$ Let $S_n$ denotes the set of all permutations of $\\{1,2,\\dots,n\\}$ and $i(\\varphi)$ denotes the number of inversions of permutation $\\varphi$.\nWe are interested in the average\n$$A_n = \\frac{\\sum_{\\varphi\\in S_n}i(\\varphi)}{n!}.$$\nWe assume that $A_n=\\frac{n(n-1)}4$ and we want to calculate $A_{n+1}$.\nFor a permutation $\\varphi$ let us denote $\\overline\\varphi$ the permutation, in which the order of elements is reversed. \nNotice that if $(n+1)$ is in $k$ inversions in $\\varphi\\in S_{n+1}$, then it is in $(n-k)$ inversions in $\\overline{\\varphi}$. This yields\n$$i(\\varphi)+i(\\overline\\varphi)=k+i(\\varphi')+(n-k)+i(\\overline{\\varphi'}) = n + i(\\varphi')+i(\\overline{\\varphi'}),$$\nwhere $\\varphi'$ denotes the permutation obtained by omitting the element $(n+1)$.\n(Note that $k$ depends on $\\varphi$. But in the above expression we were working only with one permutation, so we did not denote this by $k_\\varphi$ or some other notation stressing this dependence.)\nSo we get\n$$\n\\begin{align}\n2(n+1)!A_{n+1} = &\\sum_{\\varphi\\in S_{n+1}}(i(\\varphi)+i(\\overline{\\varphi})) =\\\\ \n=&\\sum_{\\varphi\\in S_{n+1}} (n+i(\\varphi')+i(\\overline{\\varphi'})) =\\\\\n=&(n+1)!n + \\sum_{\\varphi\\in S_{n+1}} (i(\\varphi')+i(\\overline{\\varphi'})) \\overset{(*)}= \\\\\n=&(n+1)!n + (n+1)\\left(\\sum_{\\varphi'\\in S_{n}} i(\\varphi')+ \\sum_{\\varphi'\\in S_{n}} i(\\overline{\\varphi'})\\right)=\\\\\n=&(n+1)!n +(n+1) \\left(\\sum_{\\varphi'\\in S_{n}} i(\\varphi')+ \\sum_{\\varphi'\\in S_{n}} i(\\varphi')\\right)=\\\\\n=&(n+1)!n + 2(n+1) \\cdot n! \\cdot A_n =\\\\\n=&(n+1)!n + 2 (n+1)! \\cdot A_n \n\\end{align}$$\n(In the equation denoted by $(*)$ we have the factor $(n+1)$ since there are $(n+1)$ possibilities how to obtain the same permutation $\\varphi'$ from a permutation of $(n+1)$ elements.)\nThe last equation yields\n$$A_{n+1} = \\frac n2 + A_n = \\frac n2 +\\frac{n(n-1)}4 = \\frac{n(n+1)}2$$\n\nHowever, if you have a look at other proofs about average number of inversions, the trick we used in this proof (using pairs of permutation and this reverse) can probably relatively easily transformed to a proof avoiding induction completely.\n\nLet me comment on one thing you wrote in your post:\n\nNow the average number of inversions will just be half way between the best and worst case scenario.\n\nIt is not necessarily true that average number over the whole sample is always simply arithmetic mean of the worst case and the best case. Just have a look at examples of best/average/worst time complexity on Wikipedia.\n",
    "tags": [
      "proof-writing",
      "induction",
      "permutations"
    ],
    "score": 7,
    "answer_score": 6,
    "is_accepted": true,
    "question_id": 905700,
    "answer_id": 905843
  },
  {
    "theorem": "Prove that $2+2=4$.",
    "context": "Before you might chastise this quesion, I understand that we all know $2+2=4$. But a while ago I just stumbled across this paper which formally proves that $2+2=4$: http://www.cs.yale.edu/homes/as2446/224.pdf\nHowever, seeing that I had never taken any courses in number theory, I had no exposure to congruence relations, so I have a hard time understanding those areas of the proof. Also, at 1.5 pages, I personally find the proof in the paper to be rather long.\nI would write my own proof here (e.g. maybe show that $2+2 \\le 4$ and $2+2 \\ge 4$ in order to achieve equality?), but I fear that it will be extremely faulty, so I had been wondering for a while: \n\nAre more concise ways to write a formal proof which shows that $2+2=4$, preferably without the use of any congruence relations? I would like to see different ways if possible.\n\n",
    "proof": "I don't know about more concise, but I will sketch the outline of a simpler proof starting with Peano's Axioms for the natural numbers $(N,0,S)$.\nWe will construct the function $add: N^2\\to N$ such that \n$\\forall a\\in N: add(a,0)=a$\n$\\forall a,b\\in N: add(a,S(b))=S(add(a,b))$\nBegin by constructing the set $add$ such that\n$\\forall a,b,c:[(a,b,c)\\in add \\iff (a,b,c)\\in N^3$\n$\\land \\forall d:[\\forall e,f,g: [(e,f,g)\\in d\\implies (e,f,g)\\in N^3]$\n$\\land \\forall e\\in N: (e,0,e)\\in d$\n$\\land \\forall e,f,g:[(e,f,g)\\in d \\implies (e,S(f),S(g))\\in d]$\n$\\implies (a,b,c)\\in d]]$\nThen prove that $add$ is the required function (see full formal proof in DC Proof format, 728 lines).\nThen define $1=S(0), 2=S(1), 3=S(2), 4=S(3).$\nThen prove, in turn, that $add(2,0)=2, add(2,1)=3, add(2,2)=4$ as required.\n",
    "tags": [
      "proof-writing",
      "arithmetic"
    ],
    "score": 7,
    "answer_score": 5,
    "is_accepted": true,
    "question_id": 842314,
    "answer_id": 843213
  },
  {
    "theorem": "How do I approach mathematical proofs involving nested if-then statements?",
    "context": "I'm new to proofs and trying to prove the following statement. I already have trouble writing the \"automatic parts\" of the proof at the beginning because I'm not sure what assumptions I can and should make given the nested if-then statements. I am finding that particularly confusing. Is there an algorithmic strategic approach to generally proving nested if-then-statements so that it's easier to see what should be defined and assumed in the beginning?\nHere is the statement:\n\nLet $A,B$ be sets and let $f: A \\rightarrow B$ be a function. If, for all $W,X \\subseteq A$, if $f(W) \\subseteq f(X)$, then $W \\subseteq X$, then $f$ is injective.\n\nI try breaking this down to the general form $(P \\Rightarrow Q)\\Rightarrow R$ such that\n\nP: \"for all $W,X \\subseteq A$, $f(W) \\subseteq f(X)$\"\nQ: \"$W \\subseteq X$\"\nR: \"$f$ is injective\"\n\nMy proof is then as follows (line-for-line):\n\n(1) Suppose for all $W,X \\subseteq A$, if $f(W) \\subseteq f(X)$, then\n$W \\subseteq X$.\n(2) Suppose $f(W) \\subseteq f(X)$.\n(3) Let $y \\in f(W)$.\n(4) Then, for all $y \\in f(W)$, there exists $w \\in W$ such that $f(w) = y$.\n(5) Since $f(W) \\subseteq f(X)$, it follows that $y \\in f(X)$ as well.\n(6) So, there exists $x \\in X$ such that $f(x) = y$.\n(7) Hence, $f(w)=f(x)$.\n(8) Based on the initial assumptions, from $f(W) \\subseteq f(X)$ follows that $W \\subseteq X$.\n(9) So, $w \\in X$.\n\nSo, I have $f(w)=f(x)$ and $x,w \\in X$. Since I'm trying to prove that $f$ is injective, I need to show that $x=w$ but I'm not seeing how to explicitly show this. My suspicion is that I might've already messed up at the very beginning because the nested if-then statements tend to give me trouble.\nAny help would be greatly appreciated.\n\nEDIT\nThanks to everyone for suggestions and corrections. I believe that I now have a decent proof. Here again the statement:\n\nLet $A,B$ be sets and let $f: A \\rightarrow B$ be a function. If, for all $W,X \\subseteq A$, if $f(W) \\subseteq f(X)$, then $W \\subseteq X$, then $f$ is injective.\n\nAnd the proof:\n\n(1) Suppose for all $W, X \\subseteq A$, if $f(W) \\subseteq f(X)$, then $W \\subseteq X$.\n(2) Let $x, w \\in A$ and suppose $f(x) = f(w)$.\n(3) Let $W = \\{w\\}$ and $X = \\{x\\}$.\n(4) Note that $f(W)=f(\\{w\\})=\\{f(w)\\}$ and $f(X)=f(\\{x\\})=\\{f(x)\\}$.\n(5) Since $f(w) = f(x)$, it follows that $\\{f(w)\\} \\subseteq \\{f(x)\\}$, i.e., $f(W) \\subseteq f(X)$.\n(6) So, by the initial assumption, $W \\subseteq X$, i.e., $\\{w\\} \\subseteq \\{x\\}$.\n(7) Hence, $w = x$ and so $f$ is injective. $\\square$\n\n",
    "proof": "The quantifier that you placed in $\\boldsymbol P$ applies to the entire inner conditional instead:\n$\\bigg($for each $W{,}X \\:{\\subseteq} A\\;\\Big(f(W)\\subseteq f(X)\\implies \\boldsymbol Q(W,X)\\Big)\\bigg)\\implies \\boldsymbol R.$\nThis checks the universality of the inner conditional ($W$ and $X$ here are dummy variables) instead of the universality of the subset relation between the two images.\nHow you originally structured the translation left the last instance of $W$ and $X$ orphaned and free (not dummy variables): you can see this by substituting the dummy variable $W$ (and likewise the $X)$ in your proposed $\\boldsymbol P$ with another letter.\n\nAddendum (corresponding to the OP's EDIT)\nHere's a clearer rewrite of your correction proof, where $W$ and $X$ are not confusingly arbitrary yet non-arbitrary:\n\n\n\nLet $A,B$ be sets and let $f: A \\rightarrow B$ be a function. Suppose that for all $W,X \\subseteq A,$ if $f(W) \\subseteq f(X),$ then $W \\subseteq X.$ Then $f$ is injective.\n\nLet $x$ and $y$ be arbitrary elements of $A$ such that $f(x)=f(y).$\nThen $\\{x\\},\\{y\\}\\subseteq A$ and $\\{f(x)\\}\\subseteq\\{f(y)\\},$ that is, $f(\\{x\\})\\subseteq f(\\{y\\}).$\nBy hypothesis, $\\{x\\}\\subseteq\\{y\\}.$\nBut they are singletons; so in fact $x=y.$\nHence, $f$ is injective.\n\n\n",
    "tags": [
      "analysis",
      "logic",
      "proof-writing"
    ],
    "score": 7,
    "answer_score": 5,
    "is_accepted": true,
    "question_id": 4682727,
    "answer_id": 4682741
  },
  {
    "theorem": "How to prove the function $b : \\mathbb Z &#215; \\mathbb Z → \\mathbb Z$ defined by $b(m, n) = 3m + n$ is surjective?",
    "context": "\nHow to prove the function $b : \\mathbb Z × \\mathbb Z → \\mathbb Z$ defined by $b(m, n) = 3m + n$ is surjective?\n\nI believe it is surjective and I can explain it using words but not proof talk. Give me any integer in the codomain, I can always input $m$ as $0$ and $n$ to be that integer to output that integer.\nAny tips on how to prove it? Correct me if my reasoning is incorrect.\nI tend to struggle with proving a function is surjective much more than injective since I usually prove the contra positive.\n",
    "proof": "For a more general solution (and to show that it wouldn't work for example with the function $b(m,n)=3m+6n$), the GCD of two integers is the smallest positive linear combination of them. Since the gcd of $3$ and $1$ (the coefficients of $m,n$) is $1$, there will be integers $m_0,n_0$ such that $3m_0+n_0=1$.\nThen for any $k\\in\\mathbb Z$, $b(km_0,kn_0)=k$.\n",
    "tags": [
      "functions",
      "proof-writing"
    ],
    "score": 7,
    "answer_score": 5,
    "is_accepted": true,
    "question_id": 4615918,
    "answer_id": 4615957
  },
  {
    "theorem": "Prove that $f$ must have an inflection point at $0$",
    "context": "\nSuppose $f$ is a function such that $f'(x)>0$ $\\forall x\\not=0$ and $f'(0)=0$. Also, $f''$ is a continuous $\\text{one to one}$ function on some open interval containing $0$. Prove that $f$ must have an inflection point at $0$.\n\nPf:\nLet a,b be arbitrary values in the open interval containing $0$ such that $a<0<b$\nSince $f''$ is a continuous $1-1$ on [a,b], then either $f'''>0$ $\\forall x \\in [a,b]$ or $f'''<0$ $\\forall x \\in [a,b]$. So, -$f''\n$ is continuous on $[a,b]$ and $f''$ is differentiable on $(a,b)$\nBy the mean value theorem, there exists $c$ with, $a<c<b$ such that $$f''(c)=\\frac{f'(b)-f'(a)}{b-a}$$\nAssuming I am even going the right direction with this, I do not know how to show that $f''(0)=0$ and that $f''$ changes sign at $0$. Most of what I am reading speaks in regards to a $\\delta$ neighborhood of $0$ where I may be able to prove a sign change. \nAny helpful hints or guidance is greatly appreciated. Preferably more leaning towards hints and NOT full blown spoilers on this question. :)\nEDIT:\n$f''$ is not known to be differentiable, $f'$ is known to be differentiable and we are applying the mean value theorem to it. \n",
    "proof": "Since $f''$ is continuous and injective it is either increasing or decreasing.\nConsider a sequence of points $x_k \\searrow 0$. For each $k$ there exists $y_k \\in (0,x_k)$ satisfying $$f''(y_k) = \\frac{f'(x_k) - f'(0)}{x_k - 0} = \\frac{f'(x_k)}{x_k} > 0.$$\nSince $f''$ is continuous you can then show $f''(0) \\ge 0$. Since $f''$ attains positive values along a sequence decreasing to $0$, it follows that $f''$ cannot be decreasing - it is thus increasing.\nCan you proceed from there?\n",
    "tags": [
      "calculus",
      "real-analysis",
      "derivatives",
      "proof-writing",
      "proof-explanation"
    ],
    "score": 7,
    "answer_score": 3,
    "is_accepted": true,
    "question_id": 2220885,
    "answer_id": 2220926
  },
  {
    "theorem": "Prove that every function f continuous on $\\mathbb{R}$ can be written as $f=E+O$, where $E$ is even and continuous and $O$ is odd and continuous.",
    "context": "I have been asked to prove the following in my Real Analysis class:\nProve that every function $f$ continuous on $\\mathbb{R}$ can be written as $f=E+O$, where $E$ is even and continuous and $O$ is odd and continuous. \nI am not quite sure where to begin. \nThanks!\n",
    "proof": "We have that a function $g : \\mathbb{R} \\to \\mathbb{R}$ is:\n\neven when $\\color{blue}{g(-x) = g(x)}$, for all $x \\in \\mathbb{R}$;\nodd when $\\color{red}{g(-x) = -g(x)}$, for all $x \\in \\mathbb{R}$.\n\nSuppose a function $f$ can be written as the sum of an even function $E$ and an odd function $O$, so:\n$$f(x) = \\color{blue}{E(x)}+\\color{red}{O(x)} \\tag{1}$$\nThen we also have:\n$$\\begin{align}f(-x) & = \\color{blue}{E(-x)}+\\color{red}{O(-x)} \\\\[3pt]\n & =  \\color{blue}{E(x)}\\color{red}{-O(x)} \\tag{2} \\end{align}$$\nNow add and subtract equations $\\rm (1)$ and $\\rm (2)$ to express $E$ and $O$ in terms of $f$.\nNotes:\n\nSince sums and differences of continuous functions remain continuous, the functions $E$ and $O$ constructed from $f$ will be continuous provided that $f$ is (which it is, by assumption).\nAlthough we assume that $f$ can be written in such a way, this only gives us an elegant way to come up with the correct formula(s). We can then easily verify that with these formulas, it is always possible to decompose $f$ into an even and an odd part.\n\n",
    "tags": [
      "real-analysis",
      "continuity",
      "proof-writing"
    ],
    "score": 7,
    "answer_score": 5,
    "is_accepted": false,
    "question_id": 2216037,
    "answer_id": 2216042
  },
  {
    "theorem": "Prime numbers to power 100",
    "context": "I made the following observations:\n$$ p^{100} \\equiv 1 \\pmod {1000}$$\n$p$ is a prime number other than $2$ and, $5$\nI checked the above till $p = 127$ and, want to know the reason for it and whether it is true for all prime numbers except 2 and 5 and if so, what is its proof?\n",
    "proof": "You can find this relationship captured in the Carmichael function $\\lambda(1000)=100$, representing the largest exponential cycle of any number $\\bmod 1000$. For numbers $a$ coprime to $1000$, this ensures that $a^{100}\\equiv 1 \\bmod 1000$, since cycles shorter than $100$ will nevertheless divide $100$.\nThis varies from the Euler totient function $\\phi(1000) = 400$ for two reasons; powers of $2$ are treated slightly differently and results from distinct prime powers (here $2^3$ and $5^3$) are combined by least common multiple, not by multiplication.\n",
    "tags": [
      "proof-writing",
      "prime-numbers",
      "modular-arithmetic"
    ],
    "score": 7,
    "answer_score": 3,
    "is_accepted": true,
    "question_id": 2184992,
    "answer_id": 2185026
  },
  {
    "theorem": "Rudin&#39;s &#39;Principle of Mathematical Analysis&#39; Exercise 3.14",
    "context": "Since I'm studying real analysis using this book by myself, I'm not sure whether or not my method to prove convergence of sequence is right. I'm working on the above question's (d), and my solution was kind of different from the solution provided by the following website. The solution in the website is much more beautiful and more concise than mine, but I believe my method also works.\nhttp://minds.wisconsin.edu/handle/1793/67009\n\n3.14 If $\\{s_n\\}$ is a complex sequence, define its arithmetic means $\\sigma_n$ by \n$$\\sigma_n = \\dfrac{s_0+s_1+...+s_n}{n+1} (n=0,1,2,...).$$\n(d) Put $a_n=s_n-s_{n-1}$, for $n\\geq 1$. Show that\n$$s_n-\\sigma_n=\\dfrac{1}{n+1}\\sum_{k=1}^n k a_n$...$(1)$$\nAssume that lim$(n a_n)=0$ and that ${\\sigma_n}$ converges. Prove that $\\{s_n\\}$ converges. \n\nMy solution:\nSince $\\sigma_n$ converges, it is a Cauchy sequence. So,\n$\\forall\\epsilon,\\forall\\delta>0, \\exists N\\in\\mathbb{N}$ s.t.$\\forall n, \\forall m> N , |n a_n-0|<\\epsilon$ and \n$|\\sigma_n-\\sigma_m|<\\delta$\nLet $M=\\max\\{k |a_k|:1\\leq k\\leq m\\}$\nFrom eq. $(1)$,\n\\begin{align}\n|s_n-s_m|&=|(\\sigma_n-\\sigma_m)+(\\dfrac{1}{n+1}\\sum_{k=1}^{n}k a_k-\\dfrac{1}{m+1}\\sum_{k=1}^{m}k a_n)|\\\\&\\leq|\\sigma_n-\\sigma_m|+\\dfrac{|(m+1)(a_1+2a_2+...+n a_n)-(n+1)(a_1+2a_2+...m a_m)|}{(n+1)(m+1)}\\\\&=|\\sigma_n-\\sigma_m|+\\dfrac{|(m+1)((m+1)a_{m+1}+(m+2)a_{m+2}+...+n a_n)+(m-n)(a_1+2a_2+...m a_m)|}{(n+1)(m+1)}\\\\&<\\delta+\\dfrac{(m+1)(n-m)\\epsilon+(m-n)mM}{(n+1)(m+1)}\\\\&=\\delta+\\dfrac{(n-m)\\epsilon}{n+1}+\\dfrac{(m-n)mM}{(n+1)(m+1)}\\end{align}\nAs $\\epsilon, \\delta \\to 0, n \\to \\infty$ and $m \\to n$, the right side converges to $0$; therefore, the Cauchy sequence $\\{s_n\\}$ converges. \n",
    "proof": "You're overcomplicating things. Suppose that $\\sigma_n\\to s$. I claim $s_n\\to s$. \nCesaro's theorem says that if $x_n\\to x$, then $\\widehat{x_n}\\to x$ where $\\widehat{x_n}=\\displaystyle \\frac{1}{n+1}\\sum_{k=0}^nx_k$.\nLet $x_k=ka_k$. Then $ka_k\\to 0$; so $\\widehat{x_k}=s_k-\\sigma_k\\to 0$. Since $\\sigma_k\\to s$, $s_n\\to s$.\nThus, all you have to do is prove Cesaro's theorem. You can assume that $x_n\\to 0$ (why?), and then it is all pretty easy.\n",
    "tags": [
      "real-analysis",
      "convergence-divergence",
      "proof-writing",
      "proof-verification"
    ],
    "score": 7,
    "answer_score": 5,
    "is_accepted": true,
    "question_id": 802232,
    "answer_id": 802245
  },
  {
    "theorem": "Increasing and bounded sequence proof",
    "context": "Prove that the sequence $a_n= 1+ \\frac 12+ \\frac 13+\\cdots+ \\frac 1n-\\ln(⁡n)$ is increasing and bounded above. Conclude that it’s convergent.\nThis what I got so far\nProof:\nPart 1: Proving $a_n$ is increasing by induction.\nBase: \n$a_1=1$\n$a_2=1+\\frac 12= \\frac 32$\n$a_1≤a_2$\nSo the base case is established.\nInduction step: We assume that $a_{n-1}≤a_n$. We will show that $a_n≤a_{n+1}$.\nSince \n$a_{n-1}≤a_n$\n$$1+ \\frac 12+ \\frac 13+\\cdots+ \\frac{1}{(n-1)}-\\ln(n-1) \\leq 1+ \\frac 12+ \\frac 13+\\cdots+ \\frac 1n-\\ln n$$\nHow should I continue?\n",
    "proof": "I think you should have been asked to show that\n$$\n1+\\frac12+\\frac13+\\cdots+\\frac1n-\\ln(n+1) \\tag 1\n$$\nincreases with $n$.  The previous term is\n$$\n1+\\frac12+\\frac13+\\cdots+\\frac{1}{n-1} - \\ln n. \\tag 2\n$$\nThen the problem is to show that when you subtract $(2)$ from $(1)$, the difference is nonnegative.  You get\n$$\n\\frac1n - \\ln(n+1) + \\ln n = \\int_n^{n+1} \\frac1n - \\frac1x \\, dx. \\tag 3\n$$\nIt is easy to show that that is nonnegative.\nTo show that $(1)$ is bounded above, first notice that $(1)$ is equal to the sum of expressions like the one in $(3)$:\n$$\n\\sum_{k=1}^n \\int_k^{k+1} \\frac1k - \\frac1x\\, dx. \\tag 4\n$$\nThen\n$$\n[\\text{expression in $(4)$}] \\ge \\sum_{k=1}^n \\int_k^{k+1} \\frac1k - \\frac{1}{k+1}\\, dx = \\sum_{k=1}^n \\frac1k - \\frac{1}{k+1},\n$$\nand this is a telescoping sum, and all its terms are non-negative, and it adds up to $1$.\n",
    "tags": [
      "calculus",
      "proof-writing"
    ],
    "score": 7,
    "answer_score": 1,
    "is_accepted": true,
    "question_id": 499733,
    "answer_id": 499775
  },
  {
    "theorem": "How do I formally show that the Zariski tangent space of the intersection of two closed subschemes is the intersection of the tangent spaces?",
    "context": "I am looking for help in writing down the following formally/mathematically:\n\nLet $(A, \\frak p)$ be a local ring with $I \\subset \\frak p$ and $J \\subset \\frak p$.\nThe Zariski cotangent space of $A/I$ at $\\frak p$ can be identified with $\\frac{\\frak p/\\frak p^2}{(I+\\frak p^2)/\\frak p^2}$ which tells us that the tangent space of $A/I$ at $\\frak p$ is a subspace of the tangent space of $A$ at $\\frak p$. Similarly for the  closed subscheme cut out by $J$.\nThe cotangent space of $A/(I+J)$ at $\\frak p$ can be identified with $\\frac{\\frak p/\\frak p^2}{(I+J+\\frak p^2)/\\frak p^2}$.\nInformally\n\nthe tangent space of $A/I$ at $\\frak p$ is cut out by the ideal $(I+\\frak p^2)/\\frak p^2$,\nthe tangent space of $A/J$ at $\\frak p$ is cut out by the ideal $(J+\\frak p^2)/\\frak p^2$,\nthe tangent space of $A/(I+J)$ at $\\frak p$ is cut out by the ideal $(I+J+\\frak p^2)/\\frak p^2$.\n\nFrom this I gather that the intersection of the tangent spaces of $A/I$ and $A/J$ at $\\frak p$ will be cut out by  the ideal $(I+J+\\frak p^2)/\\frak p^2$? Not sure how to really write this out.\nTherefore, the tangent space of the intersection $\\operatorname{Spec}A/I  \\cap \\operatorname{Spec} A/J=\\operatorname{Spec}A/(I+J)$ at $\\frak p$ is the intersection of the tangent spaces of $\\operatorname{Spec}A/I$ and $\\operatorname{Spec}A/J$ at $\\frak p$.\n",
    "proof": "Call these $X = \\mathrm{Spec}(A)$ and $Y = \\mathrm{Spec}(A/I)$ and $Z = \\mathrm{Spec}(A/J)$. Denote the point $\\mathfrak{p} \\in \\mathrm{Spec}(A)$ by $x \\in X$.\nDualizing with respect to the field $\\kappa(x)$, your quotients gives inclusions $T_x Y \\to T_x X$ and $T_x Y \\to T_xX$. By the same argument we get a diagram of inclusions,\n$\\require{AMScd}$\n\\begin{CD}\nT_x (Y \\cap Z) @>>> T_x Y\\\\\n@V V V @VV V\\\\\nT_x Z @>>> T_x X\n\\end{CD}\nand we want this to be an intersection. This is the same as asking for this diagram to be a pullback in vectorspaces but we won't really need this. Going back to the undualed versions (which I write as $T^*_x X = \\mathfrak{m}_x / \\mathfrak{m}_x^2$ etc to save space), the above diagram is an intersection if and only if the diagram,\n$\\require{AMScd}$\n\\begin{CD}\nT^*_x (Y \\cap Z) @<<< T^*_x Y\\\\\n@AAA @AAA\\\\\nT^*_x Z @<<< T^*_x X\n\\end{CD}\nis a ``gluing diagram'' or pushout of vector spaces. Explicitly, this means it identifies $T^*_x(Y \\cap Z)$ with $T_x^* Y \\oplus T^*_x Z$ modulo the subspace generated by $(v,-v)$ for $v \\in T_x^* X$.\nSee if you can show this explicitly (or verify the universal property if you prefer) in terms of ideals.\n",
    "tags": [
      "linear-algebra",
      "algebraic-geometry",
      "commutative-algebra",
      "proof-writing",
      "solution-verification"
    ],
    "score": 7,
    "answer_score": 4,
    "is_accepted": false,
    "question_id": 4325846,
    "answer_id": 4326964
  },
  {
    "theorem": "Euclidean Algorithm proof exercise Terence Tao &quot;Analysis I&quot;",
    "context": "I'm self-studying Analysis from Terence Tao's \"Analysis I\" and one of the exercises given is prove the following proposition\n\nProposition 2.3.9 (Euclidean Algorithm). Let $n$ be a natural number and let $q$ be a positive natural number. Then there exist natural numbers $m$, $r$ such that $0 \\leq r < q$ and $n = mq + r$.\n\nI'm fairly new to proving so I initially got stuck so I looked at the hint which was to fix $q$ and induct on $n$. And now I currently have the following:\n\nProof.   We fix $q$ and use induction on $n$. We first prove the base case $n=0$. If we set $m=0$ and $r=0$ then we have $n = 0 \\cdot q + 0 = 0$ but $0 \\leq 0 < q$, so we are done with the base case. Now suppose inductively that $n = m \\cdot q + r$ for some natural numbers $m$, $r$ such that $0 \\leq r < q$ and $n = mq + r$. We wish to show that there exist natural numbers $m'$ and $r'$ such that  $n+1= m' \\cdot  q + r'$ where $0\\leq r'< q$.  From the inductive hypothesis we have $n+1 = mq + (r+1)$.  Since $r<q$, $r+1 \\leq q$ that is $r+1 = q$ or $r+1 <q$.  If $r+1 = q$, we set $m' = m+1$ and $r'=0$ then $m' \\cdot q + r' = (m+1) \\cdot q + 0$ but $n+1 =(m+1) \\cdot q + 0$, so $n+1 = m' \\cdot  q + r'$ and\n$0\\leq r'< q$.  If however $r+1 <q$ then we set $m' = m$ and $r' = r+1$ then we have that $n+1 = m' \\cdot  q + r'$ and $0\\leq r'< q$.  This completes the induction. $$\\tag*{$\\Box$}$$\n\nI'd be grateful for any corrections or suggestions for improvement.\n",
    "proof": "The proof as it stands in the current form of your post is correct and well-written. Exceptions:\n\nIt is recommended to put the induction base and the induction step in their own paragraphs.\nIt is also good style to have any two formulas separated by a word (so replace \"Since $r<q$, $r+1\\leq q$\" by \"Since $r<q$, we have $r+1\\leq q$\").\nMoreover, I would switch the meanings of the letters $m$ and $q$, since $q$ usually denotes the quotient, which however is what $m$ stands for in your argument. Alas, this nonstandard choice of notation originates in the book itself.\n\n",
    "tags": [
      "analysis",
      "proof-writing",
      "solution-verification",
      "natural-numbers"
    ],
    "score": 7,
    "answer_score": 4,
    "is_accepted": true,
    "question_id": 3437334,
    "answer_id": 3437364
  },
  {
    "theorem": "Show $x^n \\geq x_1^n+x_2^n+\\ldots+x_k^n \\Bigg\\vert x_1+x_2+\\ldots+x_k = x, x \\geq 0, n \\geq 1, k\\in\\mathbb{Z}$",
    "context": "Hello StackExchange Community,\nThis is my first post on the forum. Please forgive me for any errors with formatting and my expressions. I am working on the following proof:\nShow $$x^n \\geq x_1^n+x_2^n+\\ldots+x_k^n \\Bigg\\vert x_1+x_2+\\ldots+x_k = x, x \\geq 0, n \\geq 1, k\\in\\mathbb{Z}$$\nThis is to say, I'd like to show that a positive number raised to $n\\geq1$ is greater than the sum of any set of addends each raised to $n$.\nWhat I have scribbled so far:\n\nLet's look at $x^n \\geq (\\frac{x}{2})^n+(\\frac{x}{2})^n$\n$x^n \\geq2 \\frac{x^n}{2^n}$\n$x^n \\geq 0$\n$1 \\geq \\frac{2}{2^n}$\n$2^n \\geq 2$\n$n \\geq 1$, thus true.\nIt is obvious that $x^n \\geq k(\\frac{x^n}{k^n})$ is also true (for $k$\n  $x_k$ that are all equal).\nCan I make $x_1^n+x_2^n+\\ldots+x_k^n$ look like $k(\\frac{x}{k})^n$?\n  Well if $x_1^n+x_2^n+\\ldots+x_k^n < k(\\frac{x}{k})^n$, then it is also\n  less than $x^n$ (proving original problem).\n$k \\geq 0$\n$\\frac{k^n}{k}(x_1^n+x_2^n+\\ldots+x_k^n) < x^n$\nI feel I need to use the information $x_1+x_2+\\ldots+x_k = x$\n$\\frac{k^n}{k}(x_1^n+x_2^n+\\ldots+x_k^n) < (x_1+x_2+\\ldots+x_k)^n$\n\nI am not sure how to revise my thoughts and continue. I greatly appreciate any comments and questions; thank you in advance for posting! :)\nWith regards,\nPolite Master\n",
    "proof": "Unfortunately your idea won't lead to a solution, because evenly distributing will give the least possible sum. Notice that the single $n$-th power is in fact still a sum of $k\\ $ $n$-th powers of numbers that have the same sum, and so essentially the problem is about maximizing the sum of the $n$-th powers given the sum of $k$ non-negative numbers. One simple approach would be to see that if you can prove that $a^n + b^n \\le (a+b)^n$ for any non-negative real $a,b$, then then you can use it $(k-1)$ times to prove the result. Do you see it?\n",
    "tags": [
      "algebra-precalculus",
      "proof-writing",
      "exponentiation"
    ],
    "score": 7,
    "answer_score": 2,
    "is_accepted": false,
    "question_id": 1279916,
    "answer_id": 1279924
  },
  {
    "theorem": "Prove that $\\sup(A+B) = \\sup(A) + \\sup(B)$ and why does $\\sup(A+B)$ exist?",
    "context": "We want to show that $\\sup(A)+\\sup(B)$ is the least upper bound\nof the set $A + B$. First, we need to show that $\\sup(A) + \\sup(B)$\nis an upper bound for the set $A + B$. Indeed, if $z\\in A + B$,\nthen there exists $a\\in A$ and $b\\in B$ such that $z = a + b$. But by\ndefinition of $\\sup(A)$ and $\\sup(B)$, $a \\leq \\sup(A)$ and $b \\leq \\sup(B)$, so\n$z = a + b \\leq \\sup(A) + \\sup(B)$. So, $\\sup(A) + \\sup(B)$ is an upper\nbound for $A + B$.\nWe now wish to show that $\\sup(A) + \\sup(B)$ is the least upper\nbound for the set $A + B$. So, if $u$ is an upper bound for $A + B$,\nwe need to show that $\\sup(A)+\\sup(B) \\leq u$. We will use part (i):\nthat is, we need to show that there exists $\\varepsilon > 0$, $\\sup(A) + \\sup(B) < u + \\varepsilon$.\nTo do this, note that since $\\sup(A)$ is the least upper bound for\n$A$, $\\sup(A) - \\varepsilon/2$ is not an upper bound for A, so there exists an\n$a\\in A$ so that $\\sup(A) - \\varepsilon/2 < a$. Similarly, there is a $b\\in B$ so that $\\sup(B) - \\varepsilon/2 < b$.\nAdding these two inequalities gives\n$$\n\\sup(A) + \\sup(B) - \\varepsilon  < a + b;\n$$\nin other words\n$$\n\\sup(A) + \\sup(B) < a + b + \\varepsilon.\n$$\nBut $u$ is an upper bound for the set $A + B$, so $a + b \\leq u$, and\nhence we have\n$$\n\\sup(A) + \\sup(B) < u + \\varepsilon.\n$$\nThus, by part (i), $\\sup(A) + \\sup(B) \\leq u$, so $\\sup(A) + \\sup(B)$ is\nthe least upper bound for $A + B$, as required.\nNow how do I show that $\\sup(A+B)$ exists? \n",
    "proof": "Show $A+B$ is nonempty and bounded above.\n",
    "tags": [
      "real-analysis",
      "proof-writing",
      "order-theory",
      "alternative-proof",
      "solution-verification"
    ],
    "score": 7,
    "answer_score": 2,
    "is_accepted": true,
    "question_id": 494502,
    "answer_id": 494505
  },
  {
    "theorem": "Help to understand the proof of partial derivatives of homogeneous functions",
    "context": "I found this short proof that says the partial derivaties of homogenous functions of degree $k$ is homogeneous of degree $k-1$. Here is the proof in its entirety:\n\nI am lost at the very first step of the proof which says to differentiate with respect to $x_i$ both sides of the equation:\n$f(tx_1,tx_2,\\dots,tx_n)=t^kf(x_1,x_2,\\dots,x_n)$\nBased on my understanding of partial derivates, if I were to differentiate the left hand side, I will get this:\n$tf'_i(x_1,x_2,\\dots,x_n)$\nWhich is not the same as what the proof says it should be.\nPlease advise.\n",
    "proof": "To be sure, do each step carefully. Write \n$$ \\phi_t(x)=(tx_1\\ldots tx_n).$$\nTherefore you are trying to compute \n$$\\frac{\\partial}{\\partial x_i} \\left( f\\circ \\phi_t\\right)(x_1\\ldots x_n).$$\nNow use the chain rule. \n",
    "tags": [
      "proof-writing",
      "partial-derivative"
    ],
    "score": 7,
    "answer_score": 3,
    "is_accepted": true,
    "question_id": 481966,
    "answer_id": 481971
  },
  {
    "theorem": "Would like a hint for proving $(\\forall x P(x)) \\to A \\Rightarrow \\exists x( P(x) \\to A)$ in graphical proof exercise on The Incredible Proof Machine",
    "context": "Update: Updated the title now that I've observed that we can use math in the title. I've also gone thru and removed dots. The tool expresses quantification using dots like this $\\forall x.P(x)$ rather than $\\forall x P(x)$. I originally used these dots in my post as well.\nI'm going through all of the proofs in The Incredible Proof Machine and need a hint for one of the proofs. (The Incredible Proof Machine is an online graphical proof tool.)\nGiven: $(\\forall x P(x)) \\to A $\nProve: $\\exists x (P(x) \\to A )$\nIt seems like a trivial proof and here's my hand-waving attempt: There are two cases to consider:  \n\n$\\forall(x) P(x)$: In this case its trivial to prove the conclusion since we can prove $A$. \n$\\neg \\forall(x) P(x) $  In this case it must be that there exists some $c$ such that $ \\neg P(c)$. Therefore trivially, $P(c) \\to A$ and therefore $\\exists x (P(x) \\to A)$.\n\nHowever, I get stuck trying to prove it using the actual logic connectors available in the tool. I'm not able to use the same approach in the second case of my case analysis (or at least I'm not sure how). This is my attempt: \n\nIf you look right in the middle of the diagram you'll see there are no connections (and a small red dot indicating an error). This approach doesn't seem to lead anywhere. \nI'm using two instances of TND to do case analysis. The first case is as described above. But I don't know how to handle the second case, so I used a TND in the second case to generate two sub-cases: $P(y_{10}) \\vee (P(y_{10}) \\to \\bot)$. The second case of this TND is again trivial, but the first case doesn't lead anywhere.\nIn the middle of the proof I have two facts $P(y_{10})$ and $(\\forall x P(X)) \\to \\bot$. These two facts don't seem like they can lead to the conclusion.\nI'm looking for a hint of an approach to try to solve this proof. \n",
    "proof": "For other interested parties, I did get some hints from the tool community. If you wish to review these they are on this issue at GitHub\nUPDATE: I finally finished the proof. The key to success was figuring out (with help) how to prove the identity $\\neg \\forall x P(x) \\Rightarrow \\exists x (\\neg P(x))$. That identity was not built into the tool and can be a little tricky/cumbersome to prove from axioms. If you attempt the proof using the tool, learn how to create custom blocks like \"proof by contradiction\" and \"case analysis\". Otherwise the proof gets very messy. Here's the finished proof (still a little messy).\nThe blocks with the snowmen on them are \"proof by contradiction\". The block with baseball is case analysis.\n\n",
    "tags": [
      "logic",
      "proof-writing"
    ],
    "score": 7,
    "answer_score": 3,
    "is_accepted": true,
    "question_id": 1575982,
    "answer_id": 1576853
  },
  {
    "theorem": "Groups - Prove that every element equals inverse of inverse of element",
    "context": "This is my first proof about groups. Please feed back and criticise in every way (including style & language).\nAxiom names (see Wikipedia) are italicised. We use $^{-1}$ to denote inverse elements; $e$ denotes the identity element.\n\nLet $(G, \\cdot)$ be a group.\nBy $\\textit{identity element}$, $G \\ne \\emptyset$.\nNow, let $a \\in G$.\nBy $\\textit{inverse element}$, $a^{-1} \\in G$ and $(a^{-1})^{-1} \\in G$.\nIt remains to prove that $(a^{-1})^{-1} = a$.\n\\begin{equation*}\n\\begin{split}\na &= a \\cdot e && \\text{by }\\textit{identity element} \\\\\n  &= a \\cdot \\Big(a^{-1} \\cdot (a^{-1})^{-1}\\Big) && \\text{by }\\textit{inverse element} \\\\\n  &= (a \\cdot a^{-1}) \\cdot (a^{-1})^{-1} && \\text{by }\\textit{associativity} \\\\\n  &= e \\cdot (a^{-1})^{-1} && \\text{by }\\textit{inverse element} \\\\\n  &= (a^{-1})^{-1} && \\text{by }\\textit{identity element}\n\\end{split}\n\\end{equation*}\nQED\n\nPS: What I do know is that I have a personal preference for details and explicitness :-(\n",
    "proof": "First of all, your proof is absolutely fine! There's a more elegant way to prove this fact though. Since inverses are unique in groups, $(a^{-1})^{-1}$ is the unique element of the group satisfying\n$$\n(a^{-1})^{-1} (a^{-1}) = (a^{-1}) (a^{-1})^{-1} = e.\n$$\nOn the other hand, we know that $a$ satisfies\n$$\na (a^{-1}) = (a^{-1}) a = e,\n$$\nso we conclude $a=(a^{-1})^{-1}$ from the uniqueness of inverse elements in a group.\n",
    "tags": [
      "abstract-algebra",
      "group-theory",
      "soft-question",
      "proof-writing",
      "proof-verification"
    ],
    "score": 7,
    "answer_score": 3,
    "is_accepted": true,
    "question_id": 910202,
    "answer_id": 910216
  },
  {
    "theorem": "Show that if some nontrivial linear combination of vectors $\\vec{u}$ and $\\vec{v}$ is $\\vec{0}$, then $\\vec{u}$ and $\\vec{v}$ are parallel.",
    "context": "I've never been that great at writing proofs, but I'm getting a bit better. I think I have the answer correct, but I don't know if I'm missing anything. My logic seems right but there may be some minute detail that I'm leaving out. Can anybody give any feedback on this? Thanks.\n\n$\\vec{0}$ being a nontrivial linear combination of $\\vec{u}$ and $\\vec{v}$ implies that there exists a non-zero $a$ or $b$ such that $a\\vec{u}=-b\\vec{v}$. Without loss of generality, assume $a\\neq 0$. Then divide by $a$ and the equality holds: $\\vec{u}=-\\frac{b}{a}\\vec{v}$. And since $-\\frac{b}{a}\\vec{v}$ is a scalar multiple of $\\vec{u}$, it remains that $\\vec{u}$ and $\\vec{v}$ are parallel.\nMore rigorous proof:\n\\begin{align*}\n\\vec{0}=a\\vec{u}+b\\vec{v}&\\Longrightarrow a\\neq 0\\vee b\\neq 0&&\\text{Given}\\\\\n&\\Longrightarrow a\\vec{u}=-b\\vec{v}\\\\\n&\\Longrightarrow \\vec{u}=-\\frac{b}{a}\\vec{v}&&\\text{WLOG assume $a\\neq 0$}\\\\\n&\\Longrightarrow \\vec{u}\\text{ and }\\vec{v}\\text{ are parallel.}\n\\end{align*}\n",
    "proof": "Almost perfect.\nBut, we don't have $a\\ne 0$ and $b\\ne 0$, only $a\\ne 0$ or $b\\ne 0$. So, one side might be $0$, but then the other is again parallel to it.\n",
    "tags": [
      "linear-algebra",
      "proof-writing"
    ],
    "score": 7,
    "answer_score": 3,
    "is_accepted": true,
    "question_id": 391953,
    "answer_id": 391959
  },
  {
    "theorem": "Latin phrase for &quot;accepting without proof&quot;",
    "context": "Is there a Latin phrase that would be used when accepting some statement without providing the proof of such a statement?\nFor example, say you are working on an elementary number theory proof, and you make the statement \"since $p$ is odd, $p^2$ is odd, which we accept [Latin phrase for 'without showing proof'].\" Obviously this is a simple thing to prove, but in some cases it might be nice to acknowledge that a proof exists and we do not wish to show it.\nCould ex facie be used in such a situation (\"we accept ex facie that $q$ odd implies $q^2$ odd\")? Or failing the existence of a Latin phrase, is there a way that sounds a little less crude than \"without proof\"?\n",
    "proof": "It appears as though ex facie bears the intended meaning. Prima facie has a similar meaning.\n",
    "tags": [
      "terminology",
      "proof-writing"
    ],
    "score": 7,
    "answer_score": 5,
    "is_accepted": true,
    "question_id": 180409,
    "answer_id": 455390
  },
  {
    "theorem": "Prove the existence of $\\frac{1}{2}$ from the following axioms",
    "context": "The question is how to prove that there exists an element $z$ such that $z+z = 1$ from the following axioms (assume we are talking about set $R$):\nA1: $x≠y$ implies $x < y$ or $x > y$;\nA2: $x < y$ implies not $y < x$\nA3: $x < y$ implies there exists $t$ such that $x < t < y$\nA4: For any two sets $S$ and $T$ which is a subset of $R$, if (any $x$ from $S$ and $y$ from $T$ implies $x < y$) then (there exists a $z$ such that for all $m$ from $S$ and $n$ from $T$, $m≠z$ and $n≠z$ implies $m < z < n$)\nA5: $x+(y+z)=(x+z)+y$\nA6: For all $x$ and $y$, there exists $z$ such that $x = y+z$\nA7: $x+z < y+t$ implies $x < y$ or $z < t$\nA8: $1$ is an element of $R$\nA9: $1 < 1+1$\nSo far following the textbook, I construct set $K$ containing all $x$ such that $x+x < 1$, and set $L$ containing all $y$ such that $1 < y+y$. Now by A4, there exists an element $z$ such that any $x$ from $K$ is smaller than or equal to $z$, and any $y$ from $L$ is larger than or equal to $z$. It is all good up to this point. Now I try to prove that z cannot belong to K nor L. \nAssume $z$ is an element of $K$. Then $z+z < 1$ and there exists an element $t$ such that $z+z < t < 1$ by A3. Define set $N$ containing all $p$ such that $p+p < t$. Then by A4, there exists $q$ such that any $p$ from $N$ is smaller than or equal to $q$, and any $y$ from $L$ is larger than or equal to $q$. But now I have trouble proving that $q ≠ z$. If $q ≠ z$ then the contradiction is immediate. \nFor anyone wondering, the textbook is \"Introduction to Logic and the Methodology of the Deductive Sciences,\" by none other than A. Tarski himself. Chapter 10, exercise 5.\n",
    "proof": "There is something fundamentally wrong with this question. Axiom A4 talks about sets, but that makes sense only within a meta-system that 'knows' what sets are, not within any axiomatization for $R$. In particular, within the axiomatization as stated, A4 is simply useless because there are no set specification axioms and so one cannot create anything that A4 can be applied to! Thus it makes no sense to ask for a proof of something from those axioms. One cannot just brush the issue aside and say that we can construct any set of objects that we wish, otherwise we can immediately get a contradiction via Russell's set. One can still work within the meta-system and ask whether every model (with full semantics) for the axiomatization satisfies \"$\\exists x\\ ( x+x = 1 )$\", which does have a positive answer...\n$\n\\def\\nn{\\mathbb{N}}\n\\def\\zz{\\mathbb{Z}}\n\\def\\rr{\\mathbb{R}}\n$\n\n\nActually it was proven in the textbook that A1,2,5,6,7 are equivalent to an abelian group with respect to the operation +. I assumed that I could use this.\n\nJust for completeness here is a sketch of the proof, in case future readers want to see how it is done. Thanks to Rutger Moody for finding how to get commutativity, which was the major part I could not figure out despite the simplicity of the solution. After that associativity immediately follows, and then it is not hard to get existence of additive identity.\nTake any $x,y,z \\in R$. Let $w \\in R$ such that $x = y+w$. Then $y+x = y+(y+w)$ $= (y+w)+y$ $= x+y$. Thus $z+(x+y) = z+(y+x) = z+(x+y)$.\nTherefore $+$ is commutative and associative on $R$ and we can omit brackets from now on.\nLet $o \\in R$ such that $1 = 1+o$. Take any $x \\in R$. Let $y \\in R$ such that $x = 1+y$. Then $x+o = 1+y+o = 1+o+y = 1+y = x$.\nTherefore $o$ is an identity for $+$ on $R$.\n\nFrom this we get that $R$ is an ordered abelian group. Specifically, for any $x,y,z \\in R$ such that $x < y$, we have $x \\ne y$ and $\\neg z > z$ by A2, and hence $x+z < y+z$ by A7 and the abelian group properties. Thus if $x+y = z$ and $x < z$, then $y > 0$, where $0$ denotes the additive identity of $R$. Therefore if $x < y$ and $y < z$, then letting $a,b \\in R$ such that $x+a = y$ and $y+b = z$, we get $x+(a+b) = z$ and $a+b > 0$ since $a,b > 0$, and hence $x < z$.\nYour attempt had used this property of transitivity in applying A4 to $K,M$ without justification!\nBut then the desired claim is not hard. For convenience let \"$x \\le y$\" be short for \"$x < y \\lor x = y$\".\n\nLet $K = \\{ x : x \\in R \\land x+x < 1 \\}$.\nLet $M = \\{ x : x \\in R \\land x+x > 1 \\}$.\nLet $z \\in R$ such that $x \\le z \\le y$ for any $x \\in K$ and $y \\in M$.\nIf $z+z < 1$:\n  Let $t \\in R$ such that $z+z+t = 1$. Then $t > 0$.\n  Let $u \\in R$ such that $0 < u < t$.\n  Let $v \\in R$ such that $u+v = t$. Then $v > 0$.\n  Also $v < t$, otherwise $t = u+v > 0+t = t$.\n  By A1 and symmetry we can assume that $u \\le v$.\n  Let $w \\in R$ such that $0 < w < u$.\n  Thus $(z+w)+(z+w) < z+z+u+v = 1$ and hence $z+w \\in K$.\n  But $z+w > z$, contradicting the definition of $z$.\nTherefore $\\neg z+z < 1$.\nBy symmetry $\\neg z+z > 1$ and hence $z+z = 1$.\n",
    "tags": [
      "abstract-algebra",
      "logic",
      "proof-writing"
    ],
    "score": 7,
    "answer_score": 2,
    "is_accepted": true,
    "question_id": 2169856,
    "answer_id": 2171911
  },
  {
    "theorem": "What to keep in mind when attempting proof of basic properties of divisibility/what techniques are useful/what&#39;s the intuition for showing them?",
    "context": "So I am currently trying to prove some basic divisibility relations, as follows.\n\n\nIf $a \\mid b$ and $a \\mid c$, then $a \\mid b + c$.\nIf $a \\mid b$ and $s \\in \\mathbb{Z}$, then $a \\mid sb$.\nIf $a \\mid b$ and $a \\mid c$ and $s$, $t \\in \\mathbb{Z}$, then $a \\mid sb + tc$.\nIf $a \\mid b$ and $b \\mid c$, then $a \\mid c$.\n$a \\mid 0$ for all $a \\neq 0$.\n$1 \\mid b$ for all $b \\in \\mathbb{Z}$.\nIf $a \\mid b$ and $b \\neq 0$, then $|a| \\le |b|$.\nIf $a \\mid b$, then $\\pm a \\mid \\pm b$.\n\n\nI frequently find myself having trouble showing these quite basic facts.\n\nWhat should I keep in mind when trying to prove these properties, i.e. what techniques are useful?\nWhat is the intuition for the proofs of these facts, or rather, morally why must these facts be true?\n\nThanks in advance.\n",
    "proof": "Good question.\nIt may be easiest to prove some of these facts straight from the definition. That is, recall that $a|b\\implies \\exists k\\in\\mathbb{Z}$ such that $ak=b$. For your first property, we have $ak=b$ and $al=c$ for some $k,l\\in\\mathbb{Z}$.\nWhen we add those two together we find $ak+al=b+c$. Then, by distributivity, $(k+l)a=b+c$. Since $k+l\\in\\mathbb{Z}$, we have that $a|(b+c)$ by definition.\nHopefully that provides some framework to prove some of these other statements since many amount to simple algebraic manipulation once you apply the definition of \"divides.\"\n",
    "tags": [
      "elementary-number-theory",
      "proof-writing",
      "divisibility"
    ],
    "score": 7,
    "answer_score": 2,
    "is_accepted": false,
    "question_id": 1851353,
    "answer_id": 1962918
  },
  {
    "theorem": "Limit superior inequalities proof: $\\limsup_{n\\to \\infty} \\left(\\frac{a_1+a_{n+1}}{a_n}\\right)^n\\ge e$",
    "context": "Let $a_n$ be a positive sequence. Prove that \n$$\\limsup_{n\\to \\infty} \\left(\\frac{a_1+a_{n+1}}{a_n}\\right)^n\\geqslant e.$$\n",
    "proof": "Since I solved this problem several years ago, I didn't write my solution immediately, so that others could think on this problem. Now I am writing my own solution:\nIt starts as the solution by Ju'x, i.e. we can safely assume that $a_1 = 1$ and suppose the converse inequality. Then there exists $N \\in \\mathbb{N}$ such that \n$$\\frac{1+a_{n+1}}{a_n} < e^{1/n}, \\qquad n \\ge N.$$\nHence\n$$a_N > \\frac{1}{e^{1/N}} + \\frac{a_{N+1}}{e^{1/N}} > \\frac{1}{e^{1/N}} + \\frac{1}{e^{(1/N)+(1/N+1)}} + \\frac{a_{N+2}}{e^{(1/N)+(1/N+1)}} > \\ldots,$$\ni.e.\n$$a_N > \\frac{1}{e^{1/N}} + \\frac{1}{e^{(1/N)+(1/N+1)}} + \\ldots + \\frac{1}{e^{(1/N)+\\ldots+(1/N+k)}}, \\qquad k \\in \\mathbb{N}.$$\nUsing $e^{1/n} < 1 + \\dfrac{1}{n-1} = \\dfrac{n}{n-1}$ we get\n$$a_N > (N-1)\\left( \\frac{1}{N} + \\frac{1}{N+1} + \\ldots + \\frac{1}{N+k}\\right), \\qquad k \\in \\mathbb{N},$$\nwhich is impossible, since the harmonic series diverges.\n",
    "tags": [
      "calculus",
      "inequality",
      "proof-writing",
      "limsup-and-liminf"
    ],
    "score": 7,
    "answer_score": 2,
    "is_accepted": true,
    "question_id": 507827,
    "answer_id": 507991
  },
  {
    "theorem": "Prove that the set $\\mathrm{Aut}(G)$ of all automorphisms of the group $G$ with the operation of taking the composition is a group",
    "context": "\nLet $G$ be a group. Say what it means for a map $\\varphi: G \\rightarrow G$ to be an automorphism. Show that the set-theoretic composition $\\varphi \\psi = \\varphi \\circ \\psi$ of any two automorphisms $\\varphi, \\psi$ is an automorphism. Prove that the set $\\mathrm{Aut}(G)$ of all automorphisms of the group $G$ with the operation of taking the composition is a group.\n\nI have said: A map is an automorphism of a group $G$ if it is an isomorphism to itself. \na) For the next bit, I want to show if $\\varphi, \\psi$ is bijective, then $\\varphi \\circ \\psi$ is bijective: For two elements $a, b \\in G$ we have\n$$\\varphi \\circ \\psi (ab) = \\varphi(\\psi(ab)) = \\varphi(\\psi(a)\\psi(b))$$\nas $\\psi$ is an isomorphism. Also, as $\\varphi$ is an isomorphism, we have\n$$\\varphi(\\psi(a) \\psi(b)) = \\varphi \\circ \\psi(a) \\varphi \\circ \\psi(b)$$\nShowing $\\varphi \\circ \\psi$ is an isomorphism iff $\\varphi, \\psi$ are isomorphisms.\nFor the group bit, we want to prove the 3 group axioms. \n1) Associativity: $\\varphi \\circ (\\psi \\circ \\zeta) = (\\varphi \\circ \\psi) \\circ \\zeta$. So for some $x \\in G$, we get:\n$$\\varphi \\circ (\\psi \\circ \\zeta)(x) = (\\varphi \\circ \\psi) \\zeta(x) = \\varphi(\\psi(\\zeta(x))) $$ \n2) Identity: If we let the identity automorphism, $e: G \\rightarrow G$, be the map $e(x) = x$, then clearly we get that $e \\circ \\psi = \\psi \\circ e = \\psi$.\n3) Inverse: As the automorphisms are bijective (already proved) then we know that  by definition of a bijection, there is well defined inverse such that $\\psi^{-1}: G \\rightarrow G$ exists.\n(Second edit to correct proof for inverse): For any two elements $a,b \\in G$, we want to see if $\\psi^{-1}(ab) = \\psi^{-1}(a)\\psi^{-1}(b)$. Apply $\\psi$ to both sides gives us\n$$\\psi \\circ \\psi^{-1}(ab) = \\psi(\\psi^{-1}(ab)) = ab$$\nDoing the same on RHS gives us $ab$ and so we have proved the inverse exists and is unique.\nIs this right and enough to prove this?\nEDIT: Actually, can I just say that by definition of two bijective maps, the composition is also bijective and this is enough?\n",
    "proof": "Best Answer:  Let Aut(G) be the set of all automorphisms φ: G --> G. In order to show that this is a group under the operation of composition, we must verify: \n(1) Is the set is closed under composition? Yes! If you are given isomorphisms φ, ψ: G --> G, then it is not too tough to show that ψ∘φ and φ∘ψ are isomorphisms. I can expand on this in more detail if you like, but you have probably seen a proof before that a composition of bijective functions is bijective. If a and b are elements of the group, ψ∘φ(ab) = ψ(φ(ab)) = ψ(φ(a)φ(b)), because φ is an isomorphism. Since ψ is also an isomorphism, ψ(φ(a)φ(b)) = ψ∘φ(a)ψ∘φ(b), so the composition ψ∘φ preserves products. Thus, ψ∘φ is an isomorphism if ψ and φ are. \n(2) Is the set associative? Yes! All you need to do is show that, for any three isomorphisms φ, ψ and ξ, φ∘(ψ∘ξ) = (φ∘ψ)∘ξ. To do that, just show that for each x in G, φ∘(ψ∘ξ)(x) = (φ∘ψ)∘ξ(x) = φ(ψ(ξ(x))). It's just pushing around definitions. \n(3) Does the set contain an identity element? Yes! Let the identity automorphism e: G --> G be the map e(x) = x. Clearly, e∘φ = φ∘e = φ. \n(4) Does each element of the set have an inverse under ∘? Yes! Since each isomorphism φ: G --> G is bijective, there is a well-defined inverse map φ^(-1): G --> G. You may have already seen a proof that the inverse of an isomorphism is an isomorphism. If not, it isn't too difficult to prove: I'll leave it to you, but I can expand on it if you need me to. Further, the composition φ^(-1) ∘ φ = φ ∘ φ^(-1) = e. \nSince Aut(G) satisfies all the group axioms, it forms a group under ∘, as needed.\n",
    "tags": [
      "group-theory",
      "proof-writing"
    ],
    "score": 7,
    "answer_score": 2,
    "is_accepted": false,
    "question_id": 267666,
    "answer_id": 2035170
  },
  {
    "theorem": "Showing a biconditional statement about function lim sups in $\\Bbb R^n$, and codifying the intuition into a proof",
    "context": "$\n\\newcommand{\\R}{\\mathbb{R}}\n\\newcommand{\\N}{\\mathbb{N}}\n\\newcommand{\\BB}{\\mathcal{B}}\n\\newcommand{\\ve}{\\varepsilon}\n\\newcommand{\\para}[1]{\\left( #1 \\right)}\n\\newcommand{\\set}[1]{\\left\\{ #1 \\right\\} }\n$The Statement: Let $f : E \\subseteq \\R^n \\to \\R$. The claim to prove:\n$$\\beta = \\limsup_{x \\to x_0} f(x) \\iff \\text{conditions (i) & (ii) below hold}$$\nThe conditions:\n\n(i) $\\exists \\set{x_n}_{n \\in \\N} \\subseteq \\R^n$ a sequence such that $f(x_n) \\to \\beta$\n(ii) $(\\forall b > \\beta)(\\exists \\delta > 0)(f(x) < b \\text{ for } x \\in B^* \\cap E)$\n\n\nNotable Definitions & Notations: I think the more relevant definitions to go over would be these two:\n\nWe denote the \"punctured ball\" centered at $x_0$ with radius $\\delta$ by $B^*$, and more explicitly may also write\n$$B^* := B^*(x_0;\\delta) := B(x_0;\\delta) \\setminus \\set{x_0}$$\n(Some of you may be more familiar with the notation $B_\\delta(x_0)$ to denote a ball of radius $\\delta$ centered at $x_0$.) Essentially the punctured ball $B^*$ is the usual ball with the center removed.\n\nWe define the limit supremum of $f$ as $x \\to x_0$ as so:\n\n\n$$\\limsup_{x \\to x_0} f(x) = \\lim_{\\delta \\to 0} \\left( \\sup_{x \\in B^* \\cap E} f(x) \\right)$$\n\nContext & Attempts: Ultimately this is a homework problem, so I would prefer to not have full proofs given out; moreso just nudges and such.\nNow, in the senses of geometry, visuals, and intuition, I think I have some ideas as to what is going on, though not quite the whole picture.\n\nIn the forward direction: let $\\beta$ be $\\limsup f(x)$. Then that means two things. Firstly, we can find a sequence $x_n$ in the domain where that sequence under $f$, $f(x_n)$, converges to $\\beta$. I think that helps get $\\beta \\le \\sup f(x)$ on the relevant set. Moreover, whenever we have a number $b$ larger than $\\beta$, then condition (ii) essentially constricts $b$ and $\\beta$ in a sense. The closer $b$ and $\\beta$, the smaller $\\delta$ is, and eventually I think that will help us get to equality.\n\nIn the backwards direction: we know there is a sequence $\\set{x_n}$ where $f(x_n) \\to \\beta$, and whenever $b > \\beta$, $x \\in B^*(x_0;\\delta) \\cap E$ have $f(x) < b$ (though where $\\beta$ lies between the two is not known). I think this would give that $\\sup f(x) \\le b$ for the relevant set, and as $\\delta \\to 0$, $b$ and $\\beta$ get closer and closer together until we somehow get equality.\n\n\nMy issues with these are more like ... trying to codify my intuitions. I have written down some things, but I'm not too certain they're on the right track.\nFor instance, for the forward direction:\n\nAttempt at the Forward Direction:\nSuppose $f : E \\subseteq \\R^n \\to \\R$ and $\\beta := \\displaystyle \\limsup_{x \\to x_0} f(x)$. Then by definition,\n$$\n\\beta = \\lim_{\\delta \\to 0} \\para{ \\sup_{x \\in B^*(x_0;\\delta) \\cap E} f(x) }\n$$\nLet $\\delta_k := 1/k$. Then define $\\BB_k := B^*(x_0;\\delta_k)$. Since $\\delta_k \\to 0$, it holds that\n$$\n\\sup_{x \\in \\BB_k \\cap E} f(x) \\to \\beta\n$$\nThus, define $x_k$ by $x_k \\in \\BB_k \\cap E \\subseteq E$, and then $\\set{x_k}_{k \\in \\N}$ is one such that $f(x_k) \\to \\beta$, satisfying condition (i).\nSuppose $b > \\beta$. To see condition (ii) holds, suppose otherwise, that $\\forall \\delta > 0$, $f(x) \\ge b$ for $x \\in B^* \\cap E$. This would mean that\n$$\n\\sup_{x \\in B^*(x_0;\\delta) \\cap E} f(x) \\ge b > \\beta\n$$\nfor all $\\delta > 0$ and in particular the case $\\delta \\to 0$. This would give that $ \\displaystyle\\limsup_{x \\to x_0} f(x) = b$ instead from the definition, contradicting that $\\beta$ is the lim sup. Thus, a contradiction is reached and condition (ii) holds.\n\nAttempt at the Backwards Direction: (which is moreso scratch work that stalled hard)\nWe're going to suppose (i) & (ii) hold. Since $\\exists \\set{x_k}_{k \\in \\N} \\subseteq E$ such that $f(x_k) \\to \\beta$, from definition, $\\forall \\ve > 0$, $\\exists N \\in \\N$ such that, $\\forall n \\ge N$, $|f(x_n) - \\beta| < \\ve$. In particular, we can consider the elements of $\\set{x_k}$ which are within a radius $\\delta > 0$ of $x_0$ and are different from $x_0$; these elements define a subsequence $\\set{x_{n_k}}_{k \\in \\N} \\subseteq B^* \\cap E$.\nFrom the fact that limits are unique, and if a sequence $z_k \\to L$, then all subsequences of $\\set{z_k}_{k \\in \\N}$ converge to $L$ as well. Thus, $\\set{x_{n_k}}_{k\\in\\N}$ is a sequence in $B^* \\cap E$. If we take the limit as $\\ve,\\delta \\to 0$, it then follows from these and the definition of supremum that\n$$\n\\lim_{\\delta \\to 0} \\para{ \\sup_{B^* \\cap E} f(x) } \\le \\beta\n$$\nConsider condition (ii) and that for a $b > \\beta$, $\\exists \\delta > 0$ such that $f(x) < b$ for $x \\in B^* \\cap E$.\n(That's where I get lost though...)\n\nQuestions & Concerns:\n\nIs my intuition for what's going on correct?\nIf not, what's the correct intuition and the general approach I should use?\nIs there anything remotely salvageable from my approaches?\nIf there are errors with my approach, what are they? How might I rectify them?\n\nThanks for any insight you can give me!\n\nEdit: (thoughts for converse as of 1/27/2021)\nI had a thought for the approach that seems to neatly tie conditions (i) and (ii) together.\nLet $\\set{b_k}_{k \\in \\N}$ be a monotone decreasing sequence with limit $\\beta$, i.e. $b_k > \\beta$ for every $k$, and $b_k \\searrow \\beta$. Define $\\ve_k := b_k - \\beta$.\nConsider the preimage $f^{-1}(\\beta,\\ve_k)$. This may consist of a set of disjoint sets, so consider only the one $\\mathcal{A_k}$ the point $x_0$ lies in. For each preimage determined by $\\ve_k$, we can get a $\\mathcal{A_k}$, containing a ball $B(x_0;\\delta_k)$.\nThus, each $\\ve_k$ determines a preimage, which determines a ball (a subset of the preimage), which determines a $\\delta_k$.\nDefine a set of points $x_k$ by being in that preimage and different from $x_0$, i.e. take $x_k \\in B^*(x_0;\\delta_k)$.\nObviously, as $b_k \\to \\beta$, then $\\delta_k \\to 0$. Moreover, $x_k \\to x_0$, and $f(x_k) \\to \\beta$.\nFor each $k \\in \\N$, we have that\n$$\\sup_{x \\in B^*(x_0;\\delta_k) \\cap E} f(x) = b_k$$\nand, as $\\delta_k \\to 0$, we have $b_k \\to \\beta$, giving the lim sup as desired.\n(I'm sure there are details to iron out in this, but I feel there's a grain of truth at the solution in there somewhere...)\n",
    "proof": "We assume that in (i) $\\{x_n\\}$ is a sequence of points of $E\\setminus\\{x_0\\}$, converging to $x_0$.\nThe statement is technical. To avoid to be lost in details, we shall follow a path, which is a suitable chosen auxiliary sequence. Namely, for each natural $n$ put $\\beta_n=\\sup_{x\\in B^*(x_0,1/n)\\cap E} f(x)\\in \\Bbb R\\cup\\{\\pm\\infty\\}$. Clearly, a sequence $\\{\\beta_n\\}$ is non-increasing, so there exists $\\beta^*=\\lim_{n\\to\\infty}\\beta_n\\in \\Bbb R\\cup\\{\\pm\\infty\\}$. Since the sequence $\\{\\beta_n\\}$ is non-increasing and for each $\\delta>0$ there exists natural $n$ such that $B^*(x_0,1/n)\\subset B^*(x_0,\\delta)$, $\\beta^*=\\lim\\sup_{x \\to x_0} f(x)$.\n$(\\Rightarrow)$ (i) I assume that $x_0$ is an accumulation point of the set $E$, that is $x_0\\in\\overline{E\\setminus \\{x_0\\}}$, otherwise the conclusion can fail. The following case are possible:\n1)) $\\beta^*=-\\infty$. Then for each natural $n$ we can pick $x_n\\in B^*(x_0,1/n)\\cap E$ such that $f(x_n)<-n$.\n2)) $\\beta^*=+\\infty$. Then for each natural $n$ we can pick $x_n\\in B^*(x_0,1/n)\\cap E$ such that $f(x_n)>n$.\n3)) $\\beta^*\\in\\Bbb R$. Then for each natural $n$ we can pick $x_n\\in B^*(x_0,1/n)\\cap E$ such that $|f(x_n)-\\beta^*|<1/n$.\n(ii) Since $b>\\beta^*$ and the sequence $\\{\\beta_n\\}$ tends to $\\beta^*$, there exists $n$ such that $\\beta_n<b$. We can pick $\\delta=1/n$.\n$(\\Leftarrow)$ Let $\\beta<b\\in \\Bbb R\\cup\\{+\\infty\\}$. By (ii), $\\beta_n\\le b$ for all sufficiently big $n$, so $\\beta^*\\le b$, thus $\\beta^*\\le\\beta$. Let $m$ be any number. Since $\\{x_n\\}$ is a sequence of points of $E\\setminus\\{x_0\\}$, converging to $x_0$,\nthere exist a natural number $N$ such that $x_n\\in B^*(x_0,1/m)$ for each $n\\ge N$, so $f(x_n)\\le\\beta_m$. Therefore $\\beta\\le\\beta_m$. Thus $\\beta\\le\\beta^*$ and so $\\beta=\\beta^*$.\n",
    "tags": [
      "real-analysis",
      "sequences-and-series",
      "proof-writing",
      "metric-spaces",
      "limsup-and-liminf"
    ],
    "score": 7,
    "answer_score": 1,
    "is_accepted": true,
    "question_id": 4001510,
    "answer_id": 4007841
  },
  {
    "theorem": "Arzela-Ascoli Theorem on metric spaces",
    "context": "I've been looking for a proof of one particular direction of this theorem for metric spaces. I've looked online, but everyone seems to use different terminology/notation to state the theorem, so I'd like to ask for an outline of a proof specific to my text's version, which my text \"leaves to the reader\".\nLet $A \\subset M$ where $A$ is compact and $M$ is a metric space.\nLet $B \\subset C_{b}(A,N)$ where $N$ is a metric space and $C_{b}(A,N)$ is the set of all bounded continuous functions from $A$ to $N$.\nThen $B$ is compact if and only if $B$ is equicontinuous, closed, and pointwise compact. \nI'm trying to prove the forward direction. The reverse direction (showing compactness) is based on the diagonalization argument, which is described well in the textbook, but the text makes no remarks on the forward direction. I already managed to prove pointwise compactness, and closure, which were trivial, but equicontinuity seems difficult. Could someone provide an outline of the proof, or link me to the proof of this particular version of the theorem?\nRemarks:\n- The metric on $B$ is defined by $d_{B}(f,g) = sup(d_{N}(f(x),g(x)|x \\in A) $ where $d_{N}$ is the metric on $N$. So compactness of $B$ is defined relative to this metric $d_{B}$. (Showing $d_{B}$ is a metric is trivial).\n",
    "proof": "This is a standard application of compactness.  Fix $\\epsilon>0$ and $x_0\\in A$.  By continuity, for each $f\\in B$ there exists $\\delta_f>0$ such that $d(f(x),f(x_0))<\\epsilon$ whenever $d(x,x_0)<\\delta_f$.\nOur aim is to show that $\\delta_f$ may be chosen independently of $f$.  For each $\\delta>0$, let $B_\\delta=\\{f\\in B\\;\\colon\\;d(f(x),f(x_0))<\\epsilon\\textrm{ whenever }d(x,x_0)<\\delta\\}$.  By what we have said above, the $B_\\delta$ cover $B$.  \nYou can show yourself that each $B_\\delta$ is open.  Then compactness of $B$ gives us a finite subcover $(B_{\\delta_j})_{j=1}^n$.  But since clearly $B_\\delta\\subset B_{\\delta'}$ whenever $\\delta'\\le\\delta$, we may conclude that $B=B_\\delta$, where $\\delta=\\min\\{\\delta_1,\\dots,\\delta_n\\}$.  This gives us equicontinuity.\n\nIf you would rather use the sequential definition of compactness, then you can do that too.  We suppose that $B$ is not equicontinuous.  Then there exist $\\epsilon>0,x_0\\in A$ such that for all $\\delta>0$ there is some $f\\in B,x\\in A$ such that $d(x_0,x)<\\delta$ and $d(f(x_0),f(x))\\ge\\epsilon$.  \nWe may therefore choose sequences $x_1,x_2,x_3,\\dots$ and $f_1,f_2,f_3,\\dots$ such that $d(x_0,x_n)<1/n$ and $d(f_n(x_0),f_n(x_n))\\ge\\epsilon$ for each $n$.  \nBy compactness of $B$, the sequence $f_1,f_2,\\dots$ must have a convergent subsequence.  After passing to this subsequence, we may assume without loss of generality that $(f_n)$ is a convergent sequence converging uniformly to some $f\\in B$.  \nWe claim that $f$ is not continuous at $x_0$.  Indeed, let $\\delta>0$.  We find $x\\in A$ such that $d(x_0,x)<\\delta$ but $d(f(x_0),f(x))\\ge\\epsilon/3$.  \nLet $n$ be chosen so that\n\n$1/n<\\delta$\n$d(f,f_n)<\\epsilon/3$\n\nThen we have $d(x_0,x_n)<\\delta$.  Lastly, observe that:\n\\begin{align}\n\\epsilon\\le d(f_n(x_0),f_n(x_n))&\\le d(f_n(x_0),f(x_0)) + d(f(x_0),f(x_n)) + d(f(x_n),f_n(x_n))\\\\\n&\\le d(f_n,f) + d(f(x_0),f(x_n)) + d(f_n,f)\\\\\n&\\le d(f(x_0),f(x_n))+2\\epsilon/3\n\\end{align}\nWe may conclude that $d(f(x_0),f(x_n))\\ge\\epsilon/3$.  Since $\\delta>0$ was arbitrary, this means that $f$ is not continuous at $x_0$.\n",
    "tags": [
      "general-topology",
      "analysis",
      "metric-spaces",
      "proof-writing",
      "arzela-ascoli"
    ],
    "score": 7,
    "answer_score": 1,
    "is_accepted": true,
    "question_id": 1717423,
    "answer_id": 1717488
  },
  {
    "theorem": "What to look for in a proof?",
    "context": "I am a physics undergrad, wishing to pursue a PhD in Math. I am mostly self taught in the typical math undergrad curriculum.\nI am looking for more input, in ways I can improve my mathematical thinking. So, my question is once you read a proof of a particular theorem, what should be the important things that you are looking for in the proof. How should you approach it?\nSo, one of the things I have learnt to do is to look at every hypothesis of the theorem, and see its effect on the proof, or how do the subsections of the proof correspond to parts of the hypothesis, and how they fit together. \nHowever, what I find difficult is to recognize how the proof fits together in the general scheme of things. What should I ask or explore after the proof? How can I learn to solve problems more quickly and effectively through proof - reading? I am very slow are problem solving and wish to improve that. \nLastly, I am terrible at coming up with new examples, and I don't even seem to remember examples beyond very typical ones. How can I improve this ability through reading of proofs?\nI hope this question is welcome here. Otherwise, please feel free to close it. I understand this is a vague and difficult question, but any kind of input (however small) will be highly appreciated. \n",
    "proof": "One thing that I find helpful when reading a proof (especially those in papers, where the authors often omit more details than in a textbook) is to take notes and try to restate in my own words the proof as I read it; imagine you are preparing a short lecture on the theorem to present to your classmates, and you expect them to ask \"why does that follow?\" after every step.\nIt is also possible that a proof you find in a paper or book simply isn't very well-written, or is simply written in a style that you have trouble following. In this case, see if you can find a different reference and follow the alternate proof (certainly as you're beginning graduate studies, you should be able to find a multitude of references for any well-known result). If nothing else, reading multiple proofs of the same theorem will give you good insights that you might not otherwise get.\nAnother question you might ask yourself after absorbing a result/proof is \"are the methods of proof interesting in and of themselves?\"\n",
    "tags": [
      "soft-question",
      "proof-writing",
      "self-learning"
    ],
    "score": 7,
    "answer_score": 1,
    "is_accepted": false,
    "question_id": 601623,
    "answer_id": 601642
  },
  {
    "theorem": "How to prove a function is not onto?",
    "context": "Let $f : Z\\to Z$ be the function defined by $f(x) = 3x + 1$. Prove that $f $ is not onto, using a proof by contradiction. (Choose an integer $n$, and then prove ($\\forall m \\in Z$)($f(m) ≠ n$) by contradiction.)\nso far I have:\n\\begin{gather*}\n    1.\\quad \\text{Let}\\quad y  = -1\t    \\qquad   \\text{assumption}\\\\\n    2. \\quad \\text{Let}\\quad f(x) = -1       \\qquad \\text{hypothesis}\\\\\n    3. \\quad 3x+1 = -1\t       \\qquad \\text{Definition of $f (2)$}\\\\\n    4. \\quad 3x  = -2            \\qquad \\text{Algebra}\\\\\n    5. \\quad x = -2/3          \\qquad \\text{Algebra}\n\\end{gather*}\nWhat else is needed? I don't know where to go from here?\n",
    "proof": "Your work shows that the only real number $x$ for which $f(x)=−1$ is $x=−2/3$. In particular, there is no integer $n$ such that $f(n)=−1$. Since the domain of $f$ is $\\mathbb Z$, the set of integers, you can conclude that $f$ is not surjective.\n",
    "tags": [
      "functions",
      "discrete-mathematics",
      "proof-verification",
      "proof-writing"
    ],
    "score": 7,
    "answer_score": 2,
    "is_accepted": false,
    "question_id": 943939,
    "answer_id": 943976
  },
  {
    "theorem": "Prove the following by two different methods, one combinatorial and one algebraic",
    "context": "Reading through my textbook I came across the following problem, and I am looking for some help solving it. I am asked to prove the following by two different methods, one combinatorial and one algebraic. If I could get help with either or both it would be great, thanks!\nProve that this identity is true,\n$$\\binom{n}{k} -\\binom{n-3}{k} =\\binom{n-1}{k-1} + \\binom{n-2}{k-1} + \\binom{n-3}{k-1}$$\n",
    "proof": "Repeatedly, use the identity (Pascal's Identity), namely\n$$\n\\binom{n}{k}=\\binom{n-1}{k}+\\binom{n-1}{k-1}.\n$$\nNote that \n$$\n\\left(\\binom{n}{k}-\\binom{n-1}{k-1}\\right)-\\binom{n-2}{k-1}-\\binom{n-3}{k-1}-\\binom{n-3}{k}\n$$\nequals\n$$\n\\binom{n-1}{k}-\\binom{n-2}{k-1}-\\binom{n-3}{k-1}-\\binom{n-3}{k}\n$$\nwhich equals\n$$\n\\binom{n-2}{k}-\\binom{n-3}{k-1}-\\binom{n-3}{k}\n$$\nwhich equals\n$$\n\\binom{n-3}{k}-\\binom{n-3}{k}=0\n$$\nas desired.\n",
    "tags": [
      "combinatorics",
      "proof-writing",
      "binomial-coefficients"
    ],
    "score": 6,
    "answer_score": 6,
    "is_accepted": true,
    "question_id": 2637240,
    "answer_id": 2637263
  },
  {
    "theorem": "What are some examples of subtle logical pitfalls?",
    "context": "Here's an example: \nDemonstrating that the assumption $A=B$ leads to a true statement is a vacuous truth. In order the show that $A=B$, prove that the difference $\\Delta =A-B$ is zero.  The subtle change being that $\\Delta$ is not assumed to be zero. \n\nWhat are some other examples of subtle logical pitfalls that the amateur Mathematician should be aware of?\n\nHere is an specific argument that shows how assuming $A=B$ leads to absurdity.\n$$\n\\begin{eqnarray}\n2&=&1\\\\ \n2-1&=&1-1\\\\ \n1&=&0\\\\ \n\\end{eqnarray} \n$$\n$$\n\\begin{eqnarray}\na+b&=&a+b\\\\ \na+1*b&=&1*a+b\\\\ \na+0*b&=&0*a+b\\\\ \na&=&b\n\\end{eqnarray} \n$$\nA falsity implies anything. Assuming that the false statement is true implies that the two undefined objects $a$ and $b$ are equal, absurd. However, if we define the difference as $\\Delta$, then a true statement is forced.\n$$\n\\begin{eqnarray}\n2-1&=&\\Delta\\\\ \n\\Delta &=&1\\\\ \n2&=&\\Delta +1\\\\ \n2&=&2\n\\end{eqnarray} \n$$\n",
    "proof": "This isn't necessarily subtle, but... \nI've encountered many students who mistakenly conclude that if an implication is true, then the converse must be true.\n\nThat is, mistakenly concluding that if $p \\rightarrow q$, then $q \\rightarrow p$.\nThe same error in reasoning comes when there is a chain of right-directional implications, and then assuming that then shows the equivalence of the original claim and the conclusion at the end of the chain of implications. \nA bit more subtly: I often encounter the erroneous conclusion that if $p\\rightarrow q$, then $\\lnot p\\rightarrow \\lnot q$.\nAlso, when asked to prove a biconditional \"if and only if\" statement like $p \\iff q$, some stop after proving only $p \\rightarrow q$, thinking they are done.\n\nAnd more subtly, when starting with an equation, then operating on each side of the equation (resulting in an equation), students often assume that whatever is the case about the end result also holds for the original. E.g. when given something of the form, $$\\begin{eqnarray} y &=& f(x)\\tag{1} \\\\ \\text{So} \\;\\;y^2 &=& (f(x))^2\\tag{2}\\end{eqnarray}$$ and then (mistakenly) concluding solutions to $(2)$ are solutions to $(1)$. Or, e.g., given $$\\begin{eqnarray}y^2 &=& x^2\\tag{3} \\\\ \\text{So} \\;\\;\\sqrt{y^2} &=& \\sqrt{x^2}\\tag{4}\\end{eqnarray},$$ then (mistakenly) concluding that the only solutions to $(3)$ are the solutions to $(4)$.\n\nAdditionally, one possible pitfall is not correctly applying DeMorgan's laws:\n\nAs it relates to the distribution of negation over conjunction and the distribution of negation over disjunction: Mistakenly equating $\\lnot (p \\land q)$ with $ \\lnot p \\land \\lnot q$ or $\\lnot (p \\lor q)$ with $\\lnot p \\lor \\lnot q$.\nAs it relates to containment in the complement of a union of sets and containment in the complement of the intersection of sets: E.g. Making the mistake of equating $\\neg(A \\cup B)$ with $\\neg A \\cup \\neg B$ (and similarly in the case of the complement of an intersection).\n\n\nAlso, the negation of a quantified proposition seems to be problematic for some: making the mistake of equating $\\lnot \\forall x, P(x)$ with $\\forall x, \\lnot P(x)$, and similarly, when negating an existentially quantified statement. \n",
    "tags": [
      "logic",
      "proof-writing",
      "propositional-calculus",
      "big-list"
    ],
    "score": 6,
    "answer_score": 11,
    "is_accepted": true,
    "question_id": 254721,
    "answer_id": 254741
  },
  {
    "theorem": "How can one prove that the cube root of 9 is irrational?",
    "context": "Of course, if you plug the cube root of 9 into a calculator, you get an endless stream of digits. However, how does one prove this on paper?\n",
    "proof": "Suppose that $9^{1/3}=m/n$ with $m$, $n$ integers with ${\\rm GCD}(m,n)=1$.\nThis may be assumed because if $d$ is an integer divisor of both $m$ and $n$, then $m/n=(m/d)/(n/d)$. \nThen\n$$\r\n9n^3=m^3\r\n$$\nso that $3$ divides $m^3$, hence $m$ since $3$ is prime. Thus $3^3=27$ divides both sides of the equality so that $3$ divides $n^3$, hence $n$ for the same reason as above. This contradicts the assumption that $m$ and $n$ are coprime.\nThis arguent generalizes immediately to showing that the $n$-th root of an integer which is not an $n$-th power of an integer is not rational.\n",
    "tags": [
      "elementary-number-theory",
      "proof-writing",
      "irrational-numbers"
    ],
    "score": 6,
    "answer_score": 15,
    "is_accepted": true,
    "question_id": 102348,
    "answer_id": 102349
  },
  {
    "theorem": "Prove $(A \\triangle B) \\cap (B\\triangle C) \\cap (C\\triangle A) = \\emptyset$",
    "context": "This can be proved by assuming that there exists some $x \\in (A \\triangle B) \\cap (B\\triangle C) \\cap (C\\triangle A) $ and then deriving a contradiction by considering each of the cases that arise. \n[$X\\triangle Y$ is the symmetric difference of $X$ and $Y$]\nNow, this proof ultimately breaks down into six cases which makes it a bit long considering the simple goal. So I was wondering if there was a better way of doing this?\nMy six case proof goes something like this: \nSuppose to the contrary that $x \\in (A \\triangle B) \\cap (B\\triangle C) \\cap (C\\triangle A) $. Then $x \\in (A \\triangle B)$ and $ x \\in (B \\triangle C)$ and $x \\in (C\\triangle A)$. Since $x \\in (C \\triangle A)$, $x \\in A\\backslash C$ or $x \\in C\\backslash A$.\nCase 1: $x \\in A\\backslash C$. Since $x \\in (B \\triangle C)$, $x \\in B\\backslash C$ or $x \\in C\\backslash B$. Case 1.1: $x\\in B\\backslash C$... \nCase 2: $x \\in C\\backslash A$. Since $x \\in (B \\triangle C)$, $x \\in B\\backslash C$ or $x \\in C\\backslash B$. Case 2.1: $x\\in B\\backslash C$...\n",
    "proof": "In principle there are two cases, but symmetry brings it down to one. \nSuppose $x$ is in our set. Then $x$ is in $A\\triangle B$. Without loss of generality we may assume that $x$ is in $A$ but not in $B$. \nSo since $x$ is in $B\\triangle C$, we conclude that $x$ must be in $C$. But then $x$ cannot be in $A\\triangle C$. So our set is empty. \n",
    "tags": [
      "elementary-set-theory",
      "proof-writing"
    ],
    "score": 6,
    "answer_score": 10,
    "is_accepted": true,
    "question_id": 353050,
    "answer_id": 353058
  },
  {
    "theorem": "Is my proof that $(A^n)^{-1} = (A^{-1})^n$ correct?",
    "context": "I am still learning Linear Algebra at it's basic levels, and I encountered a theorem about invertible matrices that stated that:\n\nIf $A$ is an invertible matrix, then for $n=0,1,2,3,..$. $A^n$ is invertible and $(A^n)^{-1} = (A^{-1})^n$.\n\nNow, in attempting to write my proof, I proceeded this way (note that it's not complete):\n$$A^n(A^{-1})^n=\\prod_{i=1}^nA\\prod_{i=1}^nA^{-1}=\\prod_{i=1}^n(AA^{-1})=\\prod_{i=1}^nI=I$$\nIs this line of thinking correct? Well, am just returning to math after a long time of little practice, so I could be wrong.\nBased on my comment to Dimitri's answer, would my use of this argument improve my proof?\n$$\\prod_{i=1}^{n-1}A.(AA^{-1}).\\prod_{i=1}^{n-1}=\\prod_{i=1}^{n-1}A.(I).\\prod_{i=1}^{n-1}=...=A.(AIA^{-1}).A^{-1}=AIA^{-1}=AA^{-1}=I$$\nAfter checking the comments, it seems this last argument gives me a correct proof eventually, and I now see that the problem with my original approach was making the argument that:\n\n$$\\prod_{i=1}^nA\\prod_{i=1}^nA^{-1}=\\prod_{i=1}^n(AA^{-1})$$\n\nWhich is not necessarily correct, but like @Srivatsan demonstrates, that approach is not at all wrong since :\n\nNotice that $A$ and $A^{−1}$ commute, so this justifies your proof now\n\nThanks to everyone for guidance, now I see why collaboration is going to make me love math :D\n",
    "proof": "As Dimitri points out, your proof is incomplete. You can make it work in two ways:\n\nYou can group the middle $AA^{-1}$ (remember that matrix product is associative). Noting that this equals $I$, the product simplifies to $A^{n-1} (A^{-1})^{n-1}$. You can then use induction to argue that $A^n (A^{-1})^{n}$ is $I$ for all $n$.\nThis is slightly more general variant of the above trick. Suppose $A$ and $B$ are commuting matrices (i.e., $AB = BA$), then you can indeed use \n$$\r\nA^n B^n = (AB)^n\r\n$$\nguilt-free! (Notice that $A$ and $A^{-1}$ commute, so this justifies your proof now so you can justify the proof this way as well. Keep in mind that some justification is necessary, otherwise the proof is wrong or incomplete.) The proof of this fact also uses similar ideas; see if you can figure it out yourself. \n\nIn fact, if you have an arbitrary product of matrices consisting of $m$ $A$'s and $n$ $B$'s (and no other matrices), then you can show that this product equals $A^m B^n$. For example, if $A$ and $B$ commute, then\n$$\r\nB^5ABA^2 B^{3}  = A^{1+2} B^{5+1+3} = A^3 B^9.\r\n$$\n\nA method by induction: \nI leave you to verify that the result is true for the base case $n=0$. For the induction step, assume that $$ (A^{n-1})^{-1} = (A^{-1})^{n-1} .$$\nWe must now prove the claim for $n$. This follows from the chain of equalities: \n$$\r\n\\begin{eqnarray*}\r\n(A^n)^{-1} &=& (A \\cdot A^{n-1})^{-1} \r\n\\\\ &\\stackrel{({a})}{=}& (A^{n-1})^{-1} \\cdot A^{-1} \r\n\\\\ &\\stackrel{({b})}{=}& (A^{-1})^{n-1} \\cdot A^{-1} \r\n\\\\ &=& (A^{-1})^{n}.\r\n\\end{eqnarray*}\r\n$$\nBe sure to justify each step, particularly the ones marked $(a)$ and $(b)$. \n",
    "tags": [
      "linear-algebra",
      "proof-writing"
    ],
    "score": 6,
    "answer_score": 7,
    "is_accepted": true,
    "question_id": 66062,
    "answer_id": 66078
  },
  {
    "theorem": "(Proof) If $f$ and $g$ are continuous, then $\\max\\{f(x),g(x)\\}$ is continuous",
    "context": "Consider the continuous functions $f,g:\\mathbb{R}\\rightarrow\\mathbb{R}$.\nShow that $F:\\mathbb{R}\\rightarrow\\mathbb{R}$ with $x\\mapsto \\max\\{f(x),g(x)\\}$ is continuous using the $\\epsilon - \\delta$ definition of continuity.\nI know there must be four cases.\nIf $f(x)\\leq g(x)$ and $f(x_0)\\leq g(x_0)$ or\nif $g(x)\\leq f(x)$ and $g(x_0)\\leq f(x_0)$ it is easy.\nHowever, assuming $f(x_0)\\neq g(x_0)$, what if \n$g(x)\\leq f(x)$ and $f(x_0)\\leq g(x_0)$ or\n$f(x)\\leq g(x)$ and $g(x_0)\\leq f(x_0)$?\nFor example:\n$|f(x)-g(x_0)|$... how do I get from here to $|x-x_0|$?\n",
    "proof": "Hint: The following identity may make the calculation more familiar.\n$$\\max(a,b)=\\frac{1}{2}\\left(a+b+|a-b|\\right).$$\n",
    "tags": [
      "real-analysis",
      "continuity",
      "proof-writing",
      "epsilon-delta"
    ],
    "score": 6,
    "answer_score": 13,
    "is_accepted": true,
    "question_id": 1564757,
    "answer_id": 1564778
  },
  {
    "theorem": "Prove that there exist no positive integers $m$ and $n$ for which $m^2+m+1=n^2$",
    "context": "The problem: Prove that there exist no positive integers $m$ and $n$ for which $m^2+m+1=n^2$.\nThis is part of an introductory course to proofs, so at this point, the mathematical machinery should not be too involved.  This is supposed to be proven by contradiction.  I've been messing around with this for a bit and can't help but feel that I'm missing something completely obvious.  My first instinct was to evaluate this case by case based off of combinations of even and odd for m and n, which led to contradictions in the case that both m and n are even or that m is odd and n is even (the contradiction being that zero equates to an odd number).  The problem comes when trying to find a contradiction where n is a positive odd number and m is either even or odd.     \nI then tried to approach it by showing that if $n^2=m^2+m+1$ and n is a positive integer, that $(m^2+m+1)^{\\frac{1}{2}}$ must be a positive integer, and that this leads to a contradiction.  It certainly looks like this will be the case, since, for the first few positive integer values of m, we get $3^{\\frac{1}{2}}$,$7^{\\frac{1}{2}}$,$13^{\\frac{1}{2}}$,$21^{\\frac{1}{2}}$, none of which are positive integers, but at this point at least, I don't know how to demonstrate this with a proof. \nA gentle nod in the correct direction would be greatly appreciated.  Just starting off with this stuff, so any help/insight would be great.  \n",
    "proof": "Hint: If such $n$ existed, then it would have to be between $m$ and $m+1$ since $$m^2<m^2+m+1<m^2+2m+1=(m+1)^2.$$\n",
    "tags": [
      "elementary-number-theory",
      "proof-writing"
    ],
    "score": 6,
    "answer_score": 17,
    "is_accepted": true,
    "question_id": 515531,
    "answer_id": 515534
  },
  {
    "theorem": "Prove: Number of derangement is odd if and only if number of items is even .",
    "context": "Let $D_n$ be a number of derangement of $n$ items. Prove that $D_n$ is odd if and only if $n$ is even. \nI was trying to use induction on the $!n=(n-1)(!(n-1)+!(n-2))$ recurrence relation but I can't derive the parity of the $!(n-2)$.\nThe other way I was thinking about is direct proof using the standard sum we get from  inclusion-exclusion principle $$n!\\sum_{i=0}^{n}\\frac{(-1)^i}{i!}$$ but I don't see how I can prove it's odd if and only if $n$ is even.\n",
    "proof": "Generally, derangements come in pairs: the inverse of a derangement is a derangement. The exceptions are the fixed-point-free involutions, i.e., the permutations in which every cycle is a $2$-cycle. Thus the parity of the number of derangements is the same as the parity of the number of fixed-point-free involutions.\nIf $n$ is odd, there are no fixed-point-free involutions, so the number of derangements is even.\nIf $n$ is even, say $n=2k$, then the number of fixed-point-free involutions is\n$(2k-1)(2k-3)(2k-5)\\cdots3\\cdot1$, an odd number.\nAlternatively: Let $A_n$ be the number of even derangements of $n$ items, i.e., derangements which are even permutations; and let $B_n$ be the number of odd derangements. Establish the identity $A_n-B_n=(-1)^{n-1}(n-1)$. (Note that this is the value of a certain determinant, namely, the determinant of an $n\\times n$ matrix with zeros on the main diagonal and ones everywhere else.) It follows that$$D_n=A_n+B_n=A_n-B_n+2B_n=(-1)^{n-1}(n-1)+2B_n\\equiv n-1\\pmod2.$$\nYet another way using the identity $D_n=nD_{n-1}+(-1)^n$ (which is easily derivable from the familiar inclusion-exclusion formula for $D_n)$. Clearly, if $n$ is even, then $D_n=nD_{n-1}+(-1)^n$ is odd. On the other hand, if $n$ is odd, then $nD_{n-1}$ is odd, and so $D_n=nD_{n-1}+(-1)^n$ is even.\n",
    "tags": [
      "combinatorics",
      "proof-writing",
      "inclusion-exclusion",
      "derangements"
    ],
    "score": 6,
    "answer_score": 14,
    "is_accepted": true,
    "question_id": 705104,
    "answer_id": 705126
  },
  {
    "theorem": "prove: if n is even, then n+1 is not even",
    "context": "This proof seems so simple that it's hard (if that makes any sense.)\nbased on the definition, n is even iff there exists k such that n = 2k.\nWhat I really want to say is (big picture)\nBy definition,  let $n = 2k.\\;$ Then $n+1 = 2k + 1$.\n$2k + 1$ is not divisible by $2$, therefore $n + 1$ is not even.\nI can't seem to figure out how to show the work.  Any help would be appreciated.\n",
    "proof": "I love contradiction. Here is how I would do it:\nLet n and n+1 both be even,\nTherefore, $n=2k$ for some k and $n+1=2j$ for some  $j,k \\in \\mathbb{I}$\nSubtracting, $n+1-n=2j-2k$.\n$$1 = 2(j-k)$$\n$$\\frac{1}{2} = j-k$$\nBut, 1<2 so, the fraction is not an integer and the difference of 2 integers is necessarily an integer. Thus, Contradiction!\n",
    "tags": [
      "proof-writing"
    ],
    "score": 6,
    "answer_score": 11,
    "is_accepted": true,
    "question_id": 234371,
    "answer_id": 234375
  },
  {
    "theorem": "For all integers $n$: if $7n+4$ is even, then $5n+6$ is even.",
    "context": "I am still new to the proof game so please be kind! This is my third time attempting a proof. Any feedback would be greatly appreciated. Thank you in advance.\nClaim: For all integers $n$: if $7n+4$ is even, then $5n+6$ is even.\nProof: Assume $5n+6$ is odd. If $5n+6$ is odd, then $5n+6=2k+7 \\Rightarrow 5n=2k+1$ for some $n,k \\in \\mathbb{Z}$. If $7n+4$ is even, then $7n+4=2k$ for some $k \\in \\mathbb{Z}$. Therefore, $5n=7n+4+1 \\Rightarrow 2n=-5$ for some $n \\in \\mathbb{Z}$. Clearly, $2n \\neq -5$, which contradicts our assumption that $5n+6$ is odd.\nI feel like I completely lost myself, but do not know where to go. Please be kind! Any words of wisdom and insight would be great. Thanks.\n",
    "proof": "Walking through your attempt:\n\nClaim: For all integers $n$: if $7n+4$ is even, then $5n+6$ is even.\nProof: Assume $5n+6$ is odd.\n\nNormally proving $P_a\\Rightarrow P_b$ you would start by assuming $P_a$. Here you're assuming $\\lnot P_b$, suggesting you want to prove the contrapositive, $\\ \\lnot P_b\\Rightarrow \\lnot P_a,$ which is equivalent to $P_a\\Rightarrow P_b$. But you need to understand where you are trying to get to.\n\nIf $5n+6$ is odd, then $5n+6=2k+7 \\ldots$\n\nThis is unusual - it would be more usual to say  $5n+6=2k+1 $\n\n$\\Rightarrow 5n=2k+1$ for some $n,k \\in \\mathbb{Z}$.\n\nWhich would then be $\\Rightarrow 5n=2k-5$.\n\nIf $7n+4$ is even,\n\nHaving started in on the contrapositive, you are now switched over... perhaps for the purpose of contradiction, but it is starting to get hard to follow.\n\nthen $7n+4=2k$ for some $k \\in \\mathbb{Z}$.\n\nYou shouldn't reuse $k$. We'll say $7n+4=2m$ for some $m\\in \\Bbb Z$. This was your main mistake.\n\nTherefore, $5n=7n+4+1$\n\nNot true. This is the result of re-using $k$ inappropriately.\n\n$\\Rightarrow 2n=-5$ for some $n \\in \\mathbb{Z}$. Clearly, $2n \\neq -5$, which contradicts our assumption that $5n+6$ is odd.\n\nActually - if this were valid - you should have it contradict your later statement that $7n+4$ is even, putting you on track to prove the contrapositive.\nYou don't need to come into the proof backwards on this occasion. It's easy enough to start with the the premise:\nTaking $\\mathit{ 7n+4}$ as even, we have $\\mathit{ 7n+4=2m}$ for some $\\mathit{ m\\in  \\Bbb Z}$. Then $\\mathit{ 5n+6 = (7n-2n+4+2) = 2m-2n+2  = 2(m-n+1)}$ and since $\\mathit{ (m-n+1)\\in \\Bbb Z}$ we have ${\\mathit 5n+6}$ is even.\n",
    "tags": [
      "elementary-number-theory",
      "proof-verification",
      "proof-writing"
    ],
    "score": 6,
    "answer_score": 6,
    "is_accepted": true,
    "question_id": 2864713,
    "answer_id": 2864746
  },
  {
    "theorem": "How to prove that every real number is the limit of a convergent sequence of rational numbers?",
    "context": "Here is my procedure:\nso we want to prove $\\forall r\\in \\mathbb{R},$ there exists a sequence $q_n$ of rationals such that $\\forall\\epsilon\\gt 0,$ there exists a $N$ such that $n\\gt N\\implies |q_n-r|\\lt\\epsilon$.\nI believe we can apply Denseness of $\\mathbb{Q}$, which states that if $a,b\\in \\mathbb{R}$ and $a\\lt b$, then $\\exists q\\in\\mathbb{Q}$ such that $a\\lt q \\lt b$.\nBy density theorem, we can claim: let $r\\in\\mathbb{R}$, since $r-\\epsilon\\lt r+\\epsilon$ for any $\\epsilon\\gt 0$, $\\exists q_n\\in\\mathbb{Q} $ (this is for any $n\\in \\mathbb{N}$) such that $r-\\epsilon\\lt q_n\\lt r+\\epsilon$. Then we can pick and ideal $N$  such that $|q_n-r|\\lt \\epsilon$.\nFirst, I am not quite confident about the proof as I felt I skipped or missed lots of elements. Second, my process doesn't seem to follow the \"let $r$ be..., let $q_n$ be..., let $\\epsilon$ be..., ...\". Could someone help me fix the proof?\n",
    "proof": "You have the right overall idea, but you haven't actually defined a sequence or proved it converges.  I think you're getting these two steps confused: the procedure you use to prove the sequence converges is a useful guide to how you want to define the sequence, but defining the sequence and proving it converges are separate tasks.\nTo define a sequence $(q_n)$, you need to say how to find $q_n$ if I give you $n$.  You've said to pick $q_n$ such that $r-\\epsilon<q_n<r+\\epsilon$, but what is $\\epsilon$?  You need to specify how $\\epsilon$ is defined in terms of $n$.  Keep in mind that we don't want to talk about an arbitrary $\\epsilon$ at this point--that will only come up when we are proving that $(q_n)$ converges to $r$.  In order to define $q_n$ in the first place, we need to be specific about exactly what properties we are defining it to have, and so have to specify all  our numbers.\nIntuitively, we want the $q_n$ to be getting closer and closer to $r$ as $n$ gets larger.  This suggests we want to be making your \"$\\epsilon$\" get smaller and smaller as $n$ gets larger.  There are lots of ways to do this; a simple way is to take $\\epsilon$ to be $1/n$.  So to be precise, we are defining $q_n$ to be some rational number such that $$r-\\frac{1}{n}<q_n<r+\\frac{1}{n}$$ for each $n$.  We know such a $q_n$ exists by the density of $\\mathbb{Q}$ in $\\mathbb{R}$.\nNow we need to prove that the sequence $(q_n)$ really converges to $r$.  Here's where the arbitrary $\\epsilon$ comes in.  Let $\\epsilon>0$.  We want to prove that there exists $N$ such that for all $n\\geq N$, $r-\\epsilon<q_n<r+\\epsilon$.\nSo, we need to somehow choose $N$ in terms of $\\epsilon$.  What do we need to know about $n$ in order to say that $r-\\epsilon<q_n<r+\\epsilon$?  Well, we know that $r-1/n<q_n<r+1/n$, so as long as $1/n\\leq \\epsilon$, we can conclude that $r-\\epsilon<q_n<r+\\epsilon$.  So we just need to choose $N$ to be such that $1/n\\leq\\epsilon$ whenever $n\\geq N$.  For this, we can just choose $N$ to be any integer such that $N\\geq 1/\\epsilon$.  And that completes the argument!\nI encourage you to try writing up the argument above more formally as a proof.  Hidden below is how I might write it:\n\n Let $r$ be a real number.  For each $n\\in\\mathbb{N}$, let $q_n$ be a rational number such that $$r-\\frac{1}{n}<q_n<r+\\frac{1}{n}.$$  Such a rational number exists because $\\mathbb{Q}$ is dense in $\\mathbb{R}$.  These numbers form a sequence $(q_n)$ of rational numbers which I claim converges to $r$.\n\n To prove that $(q_n)$ converges to $r$, let $\\epsilon>0$.  Let $N$ be any integer such that $N\\geq1/\\epsilon$.  Then for any $n\\geq N$, $1/n\\leq1/N\\leq\\epsilon$.  We thus have $$r-\\epsilon\\leq r-\\frac{1}{n} < q_n < r+\\frac{1}{n}\\leq r+\\epsilon.$$  That is, for any $n\\geq N$, $$|q_n-r|<\\epsilon.$$  Since $\\epsilon>0$ was arbitrary, this proves that $(q_n)$ converges to $r$.\n\n",
    "tags": [
      "calculus",
      "proof-verification",
      "proof-writing",
      "proof-explanation",
      "real-numbers"
    ],
    "score": 6,
    "answer_score": 10,
    "is_accepted": true,
    "question_id": 2270058,
    "answer_id": 2270152
  },
  {
    "theorem": "The smallest odd perfect number must exceed $10^{300}$.",
    "context": "I am studying about perfect numbers from last two week and have experienced so much adventure in studying such an interesting topic. The basic sources have been Wikipedia and the book Euler: Master of us all.\nAfter proving so many results and reading so much theory I m stuck on one of the results mentioned at the end of the the book \"Euler: Master of us all\".\nThe result is as follow:\n\nThe smallest odd perfect number must exceed $10^{300}$.\n\nSince the name of mathematician who gave the result is not given in the book so I can't even find it on internet. I shall be highly thankful if you can give me a hint to approach for this result or can supply a direct proof. Forgive me if this result is trivial and I m missing very common thing.\nThanks.\n",
    "proof": "From http://mathworld.wolfram.com/OddPerfectNumber.html \n\nTo this day, it is not known if any odd perfect numbers exist,\n  although numbers up to $10^{300}$ have been checked without success,\n  making the existence of odd perfect numbers appear unlikely (Brent et\n  al. 1991; Guy 1994, p. 44). The following table summarizes the\n  development of ever-higher bounds for the smallest possible odd\n  perfect number. There is a project underway at\n  http://www.oddperfect.org/ seeking to extend the limit beyond\n  $10^{300}$. \n\nauthor bound  \nKanold (1957)  $10^{20}$\nTuckerman (1973)   $10^{36}$ \nHagis    (1973)    $10^{50}$\nBrent and Cohen (1989) $10^{160}$\nBrent et al.   (1991) $10^{300}$\n\nBrent, R. P. and Cohen, G. L. \"A New Bound for Odd Perfect Numbers.\"\n  Math. Comput. 53, 431-437 and S7-S24, 1989.\nBrent, R. P.; Cohen, G. L.; te Riele, H. J. J. \"Improved Techniques\n  for Lower Bounds for Odd Perfect Numbers.\" Math. Comput. 57, 857-868,\n  1991.\nGuy, R. K. \"Perfect Numbers.\" §B1 in Unsolved Problems in Number\n  Theory, 2nd ed. New York: Springer-Verlag, pp. 44-45, 1994.\n\n",
    "tags": [
      "number-theory",
      "proof-writing"
    ],
    "score": 6,
    "answer_score": 8,
    "is_accepted": true,
    "question_id": 2036211,
    "answer_id": 2036222
  },
  {
    "theorem": "If a set is countable and infinite, there is a bijection between the set and $\\mathbb{N}$",
    "context": "I'm trying to show that if a set $S$ is infinite and countable then there is a bijection $\\varphi : S\\to \\mathbb{N}$. Since $S$ is countable, we know that there is an injection $f: S\\to \\mathbb{N}$. Now there are two things that can be done:\n\nWe can construct a bijection $\\varphi : S\\to \\mathbb{N}$ directly.\nWe can construct a surjection $g: S\\to \\mathbb{N}$ and use Cantor-Bernstein-Schröder theorem to show there is a bijection between $S$ and $\\mathbb{N}$.\n\nThe second way seems better, but I don't know how to start. I thought on the following: we want to build a surjection $g : S\\to \\mathbb{N}$. For that, fix an element $a_1\\in S$ and define $g(a_1)=1$. Now, since $S$ is infinite, $S\\setminus\\{a_1\\}$ is infinite, so we can pick $a_2\\in S\\setminus\\{a_1\\}$ and define $g(a_2)=2$.\nWe can continue this procedure arbitrarily so that given $n$ we will have $a_n\\in S\\setminus\\{a_1,\\dots,a_{n-1}\\}$ so that $g(a_n)=n$, so that $g$ is a surjection.\nAlthough the idea is clear to me, I recognize it lacks rigor. First of all, the statement \"continue this process arbitrarily\" when defining $g$ is not meaningful from a rigorous standpoint in my opinion. I believe there should be some argument with the axiom of choice to make this rigorous, but there might be a simpler way.\nIn that case, is this a correct way to prove the result? Also, how to make this idea I exposed rigorous and define $g$ correctly?\nEDIT: The definitions I'm using are as follows:\n\nA set $S$ is countable if there is an injective function $f : S\\to \\mathbb{N}$\nA set $S$ is infinite if for all $n\\in \\mathbb{N}$ there is no bijection between $S$ and $\\{1,\\dots,n\\}$.\n\nFrom the second definition then I've proved that if $S$ is infinite and $a\\in S$ then $S\\setminus\\{a\\}$ is non-empty and still infinite.\n",
    "proof": "Consider the range of the injection $f:S\\to\\Bbb N.$ It will necessarily be an infinite (why?) subset of $\\Bbb N,$ which can readily (and order-isomorphically!) be mapped onto $\\Bbb N$ by sending its least element to the least element of $\\Bbb N,$ its second-least element to the second-least element of $\\Bbb N,$ and so on.\nEdit: Basically, all you have to do is adapt your approach slightly. Let $A$ be the range of $f,$ so that $A\\subseteq\\Bbb N$ and $f:S\\to A$ is a bijection. (Why?) Then we define a function $g:A\\to\\Bbb N$ as follows: Let $g(\\min A)=1,$ and for all $n\\in\\Bbb N,$ let $$g\\Bigl(\\min\\bigl(A\\setminus \\{a\\in A\\mid g(a)\\le n\\}\\bigr)\\Bigl)=n+1.$$ You should try to show that $g$ is well-defined and that $g:A\\to\\Bbb N$ is a bijection.\nOnce everything above is proved, it is fairly straightforward to prove that $g\\circ f$ is the desired bijection $S\\to\\Bbb N.$ Let me know if you get stuck with any steps of the proof, and I'll do what I can to get you unstuck when I am able. Also, feel free to bounce your reasoning off me if you just want someone to check your thinking.\n",
    "tags": [
      "elementary-set-theory",
      "proof-writing"
    ],
    "score": 6,
    "answer_score": 6,
    "is_accepted": true,
    "question_id": 1411351,
    "answer_id": 1411372
  },
  {
    "theorem": "How to prove that $n\\log n = O(n^2)$?",
    "context": "How to prove that $n\\log n = O(n^2)$?\nI think I have the idea but need some help following through.\nI start by proving that if $f(n) = n \\log n$ then $f(n)$ is a member of $O(n\\log n)$. To show this I take a large value $k$ and show that $i\\geq k$ and $f(i) \\leq c_1\\cdot i\\log(i)$.\nNext I need to show that if $f(n)$ is a member of $O(n \\log n)$ then $f(n)$ is a member of $O(n^2)$ by taking a large value $k$ and showing that $i\\geq k$ and $f(i) \\leq c_2\\cdot i\\log i$ which turns out to be $f(i)=i^2\\log i$ which is a member of $O(n^2)$.\nIs that right? Could someone formalize this for me?  \n",
    "proof": "From your notation, it looks like we are assuming that $n\\in\\mathbb{N}$. In fact, I'll be more general and just say $n\\in(0,\\infty)$.\nThen $\\log n<n$, since $n < 1+n < e^n$ by its Taylor series.\nThus, $n\\log n<n^2$ for all $n\\in(0,\\infty)$.\nAs a consequence, $n\\log n = O(n^2)$.\n",
    "tags": [
      "asymptotics",
      "proof-writing"
    ],
    "score": 6,
    "answer_score": 12,
    "is_accepted": true,
    "question_id": 437769,
    "answer_id": 437771
  },
  {
    "theorem": "Prove Cauchy-Schwarz with AM-GM for three variables",
    "context": "I want to extend CS from two to three variables. Here's a Cauchy-Schwarz proof with two variables, which is proof 4 from here\nLet $A = \\sqrt{a_1^2 + a_2^2 + \\dots + a_n^2}$ and $B = \\sqrt{b_1^2 + b_2^2 + \\dots + b_n^2}$. By the arithmetic-geometric means inequality (AGI), we have\n$$\n\\sum_{i=1}^n \\frac{a_ib_i}{AB} \\leq \\sum_{i=1}^n \\frac{1}{2} \\left( \\frac{a_i^2}{A^2} + \\frac{b_i^2}{B^2} \\right) = 1\n$$\nso that\n$$\n\\sum_{i=1}^na_ib_i \\leq AB =\\sqrt{\\sum_{i=1}^na_i^2} \\sqrt{\\sum_{i=1}^n b_i^2}\n$$\nHow would I extend this method for three variables, i.e. to get the following?\n$$\n\\sum_{i=1}^na_ib_i c_i \\leq \\sqrt{\\sum_{i=1}^na_i^2} \\sqrt{\\sum_{i=1}^n b_i^2}  \\sqrt{\\sum_{i=1}^n c_i^2}\n$$\nSomehow I don't think it's as trivial as the first method, i.e. simply defining $C$ the same way does not seem to work. Maybe there is a better approach?\n",
    "proof": "$$\\sum_{i=1}^n(a_ib_i) c_i \\leq \\sqrt{(\\sum_{i=1}^{n}a_i^2b_i^2)(\\sum_{i=1}^{n}c_i^2) } \\leq \\sqrt{\\sum_{i=1}^{n}a_i^2} \\sqrt{\\sum_{i=1}^{n}b_i^2} \\sqrt{\\sum_{i=1}^{n}c_i^2} $$\nFirst inequality follows using AM $\\geq$ GM for two variables( $a_ib_i $'s as one variable, and $c_i$'s as another), and the second one follows as $\\sum_{i=1}^{n}a_i^2b_i^2\\leq (\\sum_{i=1}^{n}a_i^2)(\\sum_{i=1}^{n}b_i^2).$\n",
    "tags": [
      "linear-algebra",
      "inequality",
      "proof-writing",
      "cauchy-schwarz-inequality"
    ],
    "score": 6,
    "answer_score": 9,
    "is_accepted": true,
    "question_id": 2925940,
    "answer_id": 2925945
  },
  {
    "theorem": "Real function continuous on closed interval implies it is bounded - over-simple proof??",
    "context": "So here is my proof, which after looking up others seems to be too simple, or not rigourous enough, though I don't see why (hence I am asking!):\nWe take the contrapositive, and so prove that if $f$ is unbounded on $[a,b]$, then it is not continuous on this interval.\nBy hypothesis, $\\exists c \\in [a,b] : \\displaystyle\\lim_{x\\to c} f(x) = \\infty \\implies f(c)$ is undefined. (or, $ \\displaystyle\\lim_{x\\to c} f(x) \\not= f(c)$ so we get discontinuity instantly)\nSince $f(c)$ doesn't exist, the definition of continuity cannot be applied so $f$ must be discontinuous at at least $x=c$, as required.  \n",
    "proof": "The contrapositive you wrote down is incorrect, but the basic idea there can be adapted to give a proof, as I show below. \nTo be precise, what you wrote down is strictly stronger than the genuine contrapositive. To know that such a $c$ even exists, you need to use the sequential compactness of $[0,1]$. See the wiki on the Heine-Borel Theorem.\nThe original statement is that for a continuous map $f : [0,1] \\rightarrow \\mathbb{R}$, there exists a constant $M > 0$ such that $|f(x)| \\leq M$ for all $x \\in [0,1]$.\nThe structure of this sentence might seem peculiar from a natural-language standpoint, but it's precise enough that we can form a contrapositive: if a map $f : [0,1] \\rightarrow \\mathbb{R}$ has the property that for any $M > 0$ there exists $x \\in [0,1]$ such that $|f(x)| > M$, then $f$ is not continuous.\nLet's give a proof now. Suppose $f$ has this property. For each $M \\in \\mathbb{N}$, let $x_M$ denote any choice of an element of $[0,1]$ for which $|f(x_M)| > M$. The compactness of $[0,1]$ ensures the existence of a convergent subsequence $x_{M_k} \\rightarrow x^* \\in [0,1]$, where $M_k \\rightarrow \\infty$.\nApply the continuity of $f$ at $x^*$:\n$$\n\\infty > |f(x^*)| = \\lim_{k \\rightarrow \\infty} |f(x_{M_k})| \\geq \\lim_{k \\rightarrow \\infty} M_k = \\infty\n$$\nwhich is a contradiction.\n",
    "tags": [
      "real-analysis",
      "proof-writing"
    ],
    "score": 6,
    "answer_score": 10,
    "is_accepted": true,
    "question_id": 465936,
    "answer_id": 465987
  },
  {
    "theorem": "Prove $X$ is compact if every family of closed sets with the finite intersection property have non-empty intersection",
    "context": "\nLet $X$ a metric space, and $\\mathcal M$ a family of closed subsets of $X$ with the finite intersection property, i.e. with the property that every finite intersection of elements of $\\mathcal M$ is non-empty.\nShow that $X$ is compact if and only if every family of closed sets with the finite intersection property have non-empty intersection.\n\nLet define $\\mathcal M:=\\{A_\\alpha\\subset  X:A_\\alpha=\\overline A_\\alpha\\land \\alpha\\in I\\}$ with the properties defined above, i.e.\n$$\\bigcap \\mathcal M\\neq\\emptyset\\iff\\bigcap_{\\alpha\\in J} A_\\alpha\\neq\\emptyset,\\;\\forall J\\subseteq I:|J|\\in\\Bbb N$$\n(The right implication is obvious.)\nMy strategy for this proof is to show that $X$ is complete and totally bounded, what is equivalent to be compact.\n\nProof:\n$X$ is complete: for every Cauchy sequence $(x_n)$ in $X$ start choosing $X$ as a closed neighborhood for $x_0$. Because $(x_n)$ is Cauchy for every $\\epsilon=1/k$ exists some $N\\in\\Bbb N$ such that $d(x_n,x_m)<1/k$ whenever $n,m\\ge N$.\nThen for every Cauchy sequence on $X$ we can define the family of sets\n$$\\mathcal B:=\\{B_k:B_k=\\overline{\\Bbb B}(x_N,1/k), k\\in\\Bbb N_{>0}\\land B_0=X\\}$$\nwhat is a family of nested closed sets with the property of non-empty finite intersection. Then if $\\bigcap\\mathcal B\\neq\\emptyset$ then $X$ is complete, i.e every Cauchy sequence converges in $X$.\n$X$ is totally bounded: if $X$ is not totally bounded then exists some $\\epsilon>0$ such that doesnt exists a finite open cover of $X$ composed of open balls with centers in $X$. Then for some $\\epsilon>0$ the family\n$$C_\\epsilon :=\\{\\Bbb B(x,\\epsilon):x\\in X\\}$$\ndoesn't contain a finite subcover. Now define the family composed by the complements of $C_\\epsilon$:\n$$C_\\epsilon^*:=\\{(\\Bbb B(x,\\epsilon))^\\complement:x\\in X\\}$$\nthen observe that the family $C_\\epsilon^*$ have the finite intersection property because in other case $C_\\epsilon$ would have a finite subcover.\nBut we have that\n$$\\bigcap C_\\epsilon^*=\\bigcap (\\Bbb B(x,\\epsilon))^\\complement=\\left(\\bigcup \\Bbb B(x,\\epsilon)\\right)^\\complement=X^\\complement=\\emptyset$$\nwhat is a contradiction about the assumptions on $X$, then $X$ is totally bounded.\nBecause $X$ is totally bounded and complete then is compact.$\\Box$\n\n\nCan you check this proof please and comment any dubious or wrong step?\n\nIf you know some alternative proof easier to this approach it will be very interesting to see it. Thank you.\n\n\n",
    "proof": "This theorem holds also for topological spaces that are not metric, so using Cauchy sequences might not be the most suitable approach.\nLet $X$ be a topological space and assume that the intersection of any family of closed subsets having the finite intersection property is non-empty. Let $\\{O_i\\}_{i\\in I}$ be some open cover of $X$ and assume that it has no finite subcover. Hence for each finite $F\\subseteq I$ there is some $x\\in X$ such that $x\\notin \\bigcup_{i\\in F}O_i$. Equivalently, for each finite $F\\subseteq I$ there is some $x\\in\\bigcap_{i\\in I}(X\\setminus O_i)$. Hence the family $\\{X\\setminus O_i\\}_{i\\in I}$ is a family of closed subsets with the finite intersection property. It follows that there is some $x\\in\\bigcap_{i\\in I}(X\\setminus O_i)$, i.e., $x\\notin\\bigcup_{i\\in I}O_i$, contradicting that $\\{O_i\\}_{i\\in I}$ is a cover of $X$. We conclude that $\\{O_i\\}_{i\\in I}$ must have a finite subcover, so $X$ is compact. \nConversely, assume that $X$ is compact and let $\\{C_i\\}_{i\\in I}$ be some family of closed subsets with the finite intersection property. Assume that $\\bigcap_{i\\in I}C_i$ is empty. Then $\\{X\\setminus C_i\\}_{i\\in I}$ is an open cover of $X$. By compactness, it has a finite subcover, so $\\bigcup_{i\\in F}X\\setminus C_i=X$ for some finite $F\\subseteq I$. But then $\\bigcap_{i\\in F}C_i=\\emptyset$, contradicting that $\\{C_i\\}_{i\\in I}$ has the finite intersection property. We conclude that $\\bigcap_{i\\in I}C_i\\neq\\emptyset$.\n",
    "tags": [
      "general-topology",
      "solution-verification",
      "proof-writing",
      "compactness",
      "alternative-proof"
    ],
    "score": 6,
    "answer_score": 17,
    "is_accepted": true,
    "question_id": 2017587,
    "answer_id": 2017601
  },
  {
    "theorem": "Uniqueness proof for $\\forall A\\in\\mathcal{P}(U)\\ \\exists!B\\in\\mathcal{P}(U)\\ \\forall C\\in\\mathcal{P}(U)\\ (C\\setminus A=C\\cap B)$",
    "context": "I managed to prove existence for the following theorem: \n$$\\forall A\\in\\mathcal{P}(U)\\ \\exists!B\\in\\mathcal{P}(U)\\ \\forall C\\in\\mathcal{P}(U)\\ (C\\setminus A=C\\cap B)$$\nwhere U is any set. My assumption is that $B=U\\setminus A$, and it works for existence, but I'm stuck with proving uniqueness part with $B$ being defined this way.\nFor uniqueness we need to prove\n$$\\forall B'\\in\\mathcal{P}(U)\\ \\forall C\\in\\mathcal{P}(U)\\ (\\ (C\\setminus A=C\\cap B')\\rightarrow B'=B)$$\nwhere A is arbitrary element of $\\mathcal{P}(U)$ but I don't how to connect $x\\in B'$ or $x\\in B$ to the assusmed identitiy $C\\setminus A=C\\cap B'$.\nAny pointers are much appreciated.\nEDIT\nHere is my attempt of proof.\nProof: Let $A$ be an arbitrary element of $\\mathcal{P}(U)$ and let $B=U\\setminus A$.\nExistence: Let $C$ be an arbitrary element of $\\mathcal{P}(U)$. $(\\rightarrow)$ Let $x$ be an arbitrary element of $C\\setminus A$. Since $C\\subseteq U$, then $x\\in U\\setminus A$. Therefore $x\\in C\\cap B$. $(\\leftarrow)$ Let x be an arbitrary element of $C\\cap B$. Then $x\\in C\\cap (U\\setminus A)$, so we can conclude $x\\in C\\setminus A$.\nUniqueness: Let $B'$ be an arbitrary element of $\\mathcal{P}(U)$ and suppose $\\forall C\\in\\mathcal{P}(U)(C\\setminus A=C\\cap B')$.\n$(B'\\subseteq B)$ Since $B'\\in\\mathcal{P}(U)$, then in particular $B'\\setminus A=B'\\cap B'=B'$, so clearly $B'\\cap A=B'\\cap (U\\setminus B)=\\varnothing$. Then $\\forall x(x\\not\\in B'\\lor x\\not\\in U\\lor x\\in B)$, which is equivalent to $\\forall x(x\\in B'\\cap U\\rightarrow x\\in B$). Since $B'\\subseteq U$, $B'\\cap U=B'$, we now have $\\forall x(x\\in B'\\rightarrow x\\in B)$, and therefore $B'\\subseteq B$.\n$(B\\subseteq B')$ Let $C=B$. Then $B\\setminus A=B\\cap B'$, and because $B\\cap A=\\varnothing$, we have $B=B\\cap B'$, so we can conclude $B\\subseteq B'$.  \n",
    "proof": "Hint: Plug $B$ into the $C$ from the $B'$ assertion to get that $B\\setminus A\\subseteq B'$, and similarly for $B'$ into $C$ from the $B$ assertion, to have $B'\\setminus A\\subseteq B$.\nNow show that $B'\\setminus A=B'$ and $B\\setminus A=B$ (plug $C=\\varnothing$ into both assertions) and you are done.\n",
    "tags": [
      "elementary-set-theory",
      "proof-writing"
    ],
    "score": 6,
    "answer_score": 4,
    "is_accepted": true,
    "question_id": 253446,
    "answer_id": 253452
  },
  {
    "theorem": "Euclid&#39;s Lemma and $\\gcd(a,b)=1\\Rightarrow \\gcd(a^n,b^k)= 1$",
    "context": "I'm solving some problems in Apostol's Intro to Analytic Number Theory. He asks to prove\n$$(a,b)=1 \\wedge c|a \\wedge d|b\\Rightarrow (c,d)=1$$\nMy take is\n$$\\tag 1(a,b)=1 \\Rightarrow 1=ax+by$$\n$$ \\tag 2 c|a \\Rightarrow a=kc$$\n$$\\tag 3d|b \\Rightarrow b=jd$$\n$$(1),(2),(3) \\Rightarrow 1=ckx+djy=cx'+dy'\\Rightarrow (c,d)=1$$\nIs this correct or is something missing?\nI intend to use something similar to prove \n$$\\eqalign{\n  & \\left( {a,b} \\right) = 1 \\wedge \\left( {a,c} \\right) = 1 \\Rightarrow \\left( {a,bc} \\right) = 1  \\cr \n  & \\left( {a,b} \\right) = 1 \\Rightarrow \\left( {{a^n},{b^k}} \\right) = 1;n \\geqslant 1,k \\geqslant 1 \\cr} $$\nso I'd like to know.\n",
    "proof": "Hint $\\ \\ \\begin{eqnarray}\\rm (c,d)&\\mid c\\mid a\\\\ \n\\rm(c,d)&\\mid d\\mid b\\end{eqnarray}\\ \\Rightarrow\\rm\\ (c,d)\\mid a,b\\:\\Rightarrow\\: (c,d)\\mid (a,b)\\ \\ \\ $ QED\nEL = Euclid's Lemma $\\rm\\:(a,b)=1=(a,c)\\:\\Rightarrow\\:(a,bc) = 1,\\:$ is a special case of\n$$\\rm\\ (a,b,c) = 1\\ \\Rightarrow\\ (a,b)(a,c) = (a(a,b,c),bc) = (a,bc) $$\nThus iterating EL: $\\rm\\:\\ (a_i,b_j) = 1\\:\\Rightarrow\\:(a_i,\\prod b_j)=1\\:\\Rightarrow\\:(\\prod a_i,\\prod b_j) = 1.$\nSpecializing $\\rm\\:a_i = a,\\ b_j = b\\:$ we deduce $\\rm\\:(a^n,b^k) = 1.\\:$\n",
    "tags": [
      "elementary-number-theory",
      "proof-writing"
    ],
    "score": 6,
    "answer_score": 4,
    "is_accepted": true,
    "question_id": 152671,
    "answer_id": 152692
  },
  {
    "theorem": "Prove With Three Real Numbers Prove That We Can Pick Two And Their Product is Non Negative",
    "context": "Consider three real numbers $a$, $b$, and $c$. Prove that we can pick two of them such that their product is non-negative.\nMy Proof:\nUsing proof by cases:\nCase 1:\n$a >0,\\space b>0,\\space c>0$\nAbove, we can see that if we pick ANY two pairs that their product is alway positive.\nEx: $$a\\cdot b = ab \\qquad (ab>0)$$\nCase 2:\nLets say $a <0, b<0,c>0$\nThere is only one way to get a positive product out of this group. You need to pick two variables that have the same \"sign\"(The variables must both be positive or negative). If the two variables are not the same sign then their product will always be negative.\nEx:\n$$a \\cdot b = ab \\qquad (ab>0, a<0, b<0)$$\n$$a \\cdot c = ac \\qquad (ac <0, a<0, b >0)$$\n$\\therefore$ You can pick two variables with like signs from this group and their producgt will always be positive. $\\square$\nMy Question:\nIs this the correct way to go about this proof? I don't see another way with the small set of proof methods we have. I don't know if this is enough to prove this though. It seems too simple... Any thoughts?\n",
    "proof": "Proof by contradiction.\nWe want to prove that given any triple of numbers $a,b,c\\in\\mathbb{R}$ we can extract two such that their product is non negative.\nSuppose that there exists a triple $(x,y,z)$ such that any of the three possible products\n$$xy,\\;xz,\\;yz$$\nis negative. So their product should be negative as well. But\n$$(xy)(xz)(yz)=(xyz)^2$$ Contradiction.\nThanks to @Servaes for the collaboration.\nEdit.\nThis property can be easily generalized to any set $S \\subseteq\\mathbb{R}$ containing more than two elements.\nWe want to prove that from any of those subsets  it can be chosen a pair of elements $(x,y)\\in S\\times S$ such that $xy\\ge 0$. Indeed suppose all pairs $(a,b)\\in S\\times S$ are such that $ab<0$ we could chose $(p,q)$ and $(p,r)$ such that $pq<0,\\;pr<0$. So $(pq)(pr)>0$ which means $p^2(qr)>0$ and as $p^2>0$ we would have $qr>0$ in contradiction with the assumpt that all pair gave a negative product.\n",
    "tags": [
      "discrete-mathematics",
      "proof-writing",
      "solution-verification"
    ],
    "score": 6,
    "answer_score": 10,
    "is_accepted": true,
    "question_id": 3832672,
    "answer_id": 3832709
  },
  {
    "theorem": "Where is the flaw in my proof?",
    "context": "Possible proof of the conjecture that there always exists a prime between $n^2$ and $(n+1)^2$?\n\nAssume the statement is invalid. Therefore, for some $n \\in \\mathbb{N}$, there does not exist a prime that ranges from $n^2$ and $(n+1)^2$ exclusive. Let $\\pi (x)$ denote the amount of prime numbers less than or equal to $x$, then we have that if $\\pi ((n + 1)^2) - \\pi (n^2) = k$, there exists $k$ primes that range between $n^2$ and $(n + 1)^2$ exclusive. Therefore, by assuming the conjecture is false, we consider $k = 0$, which consequently gives us the following: $$\\begin{align} \\pi ((n + 1)^2) - \\pi (n^2) &= 0 \\\\ \\Leftrightarrow \\pi ((n+1)^2) &= \\pi (n^2). \\end{align}$$ It is well known that $\\pi (x) \\approx \\dfrac{x}{\\ln x}$ so by order of substitution, $$\\frac{n^2}{\\ln n^2} \\approx \\frac{(n + 1)^2}{\\ln (n + 1)^2}.$$ For some $m$, one can see that $\\ln x^2 = m \\Rightarrow e^m = x^2 \\Rightarrow \\sqrt {e^m} = e^{m/2} = x \\Rightarrow \\ln x = m/2$. Therefore, we conclude that $\\ln x^2 = 2\\ln x$. $$\\begin{align} \\therefore \\frac{n^2}{2\\ln n} &\\approx \\frac{(n + 1)^2}{2\\ln (n + 1)} \\\\ \\\\ \\Leftrightarrow \\bigg(\\frac n2\\bigg)\\bigg(\\frac{n}{\\ln n}\\bigg) &\\approx \\bigg(\\frac{n + 1}{2}\\bigg)\\bigg(\\frac{n + 1}{\\ln (n + 1)}\\bigg) \\\\ \\\\ \\Leftrightarrow \\bigg(\\frac n2\\bigg)\\pi (n) &\\approx \\bigg(\\frac{n + 1}{2}\\bigg)\\pi (n + 1) \\\\ \\\\ \\Leftrightarrow \\frac{n}{n + 1} &\\approx \\frac{\\pi (n + 1)}{\\pi (n)}. \\end{align}$$\nNow, by taking the reciprocal of the equation, it follows then that, $$\\frac{n + 1}{n } = 1 + \\frac 1n \\approx \\frac{\\pi (n)}{\\pi (n + 1)}.$$ However, although true, $$\\square \\ \\dfrac{n + 1}{n} > \\dfrac{\\pi (n)}{\\pi (n + 1)}.$$\nBy definition of the function $\\pi$, we have that $\\pi (n) = \\pi (n + 1)$ if and only if $n$ and $n + 1$ are not prime and $\\pi (n) < \\pi (n + 1)$ if and only if $n + 1$ is prime, therefore $\\pi (n) \\leqslant \\pi (n + 1)$. We thus arrive at the following conclusion: $$\\begin{align} \\frac{\\pi (n)}{\\pi (n + 1)} &\\leqslant 1 \\\\ \\\\ \\Leftrightarrow \\frac{n + 1}{n} &> \\frac{\\pi (n)}{\\pi (n + 1)} \\\\ \\\\ \\Leftrightarrow \\pi ((n + 1)^2) &> \\pi (n^2) \\\\ \\\\ \\Leftrightarrow \\pi ((n + 1)^2) - \\pi (n^2) &\\geqslant 1 \\\\ \\\\ \\therefore k &= 1. \\end{align}$$ Thus, the conjecture is true.$\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\,\\,\\,\\bigcirc$\n\nThis looks correct to me, but this is a famous conjecture because it is apparently very difficult to prove and if true, it will tell us more about how the prime numbers are distributed across the positive integers. Therefore, I must have some kind of flaw in my proof, because it was too easy (only one page long). Surely this cannot be the case.\nThank you in advance.\n",
    "proof": "You write $1+\\frac{1}{n} \\approx \\frac{\\pi(n)}{\\pi(n+1)}$, which is equivalent to\n$$\n\\lim_{n\\to \\infty}\\frac{1+\\frac{1}{n}}{\\frac{\\pi(n)}{\\pi(n+1)}}=1,\n$$\nwhich is true because both numerator and denominator have limit $1$.\n",
    "tags": [
      "proof-writing",
      "prime-numbers",
      "conjectures"
    ],
    "score": 6,
    "answer_score": 9,
    "is_accepted": true,
    "question_id": 2603021,
    "answer_id": 2603027
  },
  {
    "theorem": "To what extent do you explain a proof?",
    "context": "What previous proofs are assumed and which are referenced? Is it based on the target audience of the proof (if that is a thing)? Level's of intuition may also vary drastically, a logical step for one person may need a paragraph of explanation for another. \nFor example, I can't imagine quoting the definition of a derivative and the standard rules of differentiation, when finding the minimum of a function. In some cases would you even need to show the process, could you just write the minimum?\n",
    "proof": "Yes, all proofs are written for a particular audience, each with its own level of expertise and its own peculiar body of knowledge, which may be assumed without explanation. One has to tailor the mathematical writing to the audience. When I give a proof to a graduate seminar, it will look very different than when I give a proof to an undergraduate analysis class, or when I am speaking at a conference of experts.\nWhen writing a mathematics paper for publication, one doesn't always know exactly the audience you will get, and so one should err on the side of extra explanation. A math talk amongst people whose background you know well will be very different. \nWhen I teach a math class and have my students write proofs, then I always tell them to write for the rest of the class as the audience, rather than writing to me. They should explain the things that they think the rest of the class would want explained, and can assume the things that the rest of the class would freely assume along with them. \n",
    "tags": [
      "proof-writing",
      "proof-explanation"
    ],
    "score": 6,
    "answer_score": 16,
    "is_accepted": true,
    "question_id": 2312874,
    "answer_id": 2312878
  },
  {
    "theorem": "Dividing a Checkerboard into L-Shaped Regions",
    "context": "In preparation for the GRE Math-Subject test, and honestly for the fun of it, I've been working through a select number of my texts. The first of which is Saracino's Abstract Algebra text. I was hoping this wouldn't happen, but I've been very quickly stumped.\nThe following is Exercise 0.22, and I'm looking for a few hints.\n\"Prove that the follow statement is true for any integer $n \\geq 1$: If the number of squares in a \"checkerboard\" is $2^n \\times 2^n$ and we remove any one square, then the remaining part of the board can be broken up into L-shaped regions each consisting of 3 squares.\"\nBefore I wrote this proof, I went ahead and pulled out a sheet of paper and did a little testing, because I'm not exactly sure if all of these have to be correct. I found that I could remove pretty much any square on the $2^2 \\times 2^2$ board and not be able to break it up. This leads me to believe that the statement is false. Any advice?\n",
    "proof": "You can prove this using induction.\nOur trivial base case is $n=2$.\nNow, assume that the statement is true for $n-1$ \n\nWe can split a $2^n\\times 2^n$ board into four $2^{n-1} \\times 2^{n-1}$ boards, as shown above. W.L.O.G. the square that we choose to take out is in the upper right quadrant. Since a $2^{n-1} \\times 2^{n-1}$ board can be filled with Ls if one square is taken out, the upper right quadrant can be filled without any issue. Now, suppose we take out the L shape in the middle of the board (which is highlighted in red in the picture above). Then each of the other quadrants will have one square missing, and thus can be filled with Ls. Since the shape in the middle itself is an L, the whole board can be filled with Ls. (Sorry for the bad english.)\n",
    "tags": [
      "elementary-set-theory",
      "proof-writing",
      "induction"
    ],
    "score": 6,
    "answer_score": 10,
    "is_accepted": true,
    "question_id": 1805114,
    "answer_id": 1805127
  },
  {
    "theorem": "Proof of the infinite descent principle",
    "context": "Hi everyone I wonder to myself if the next proof is correct. I would appreciate any suggestion. \nProposition: There is not a sequence of natural numbers which is infinite descent.\nProof: Suppose for contradiction that there exists a sequence of natural numbers which is infinite descent. Let $(a_n)$ be such sequence, i.e., $a_n>a_{n+1}$ for all natural numbers n.\nWe claim that if the sequence exists, then $a_n\\ge k$ for all $k, n \\in N$. \nWe induct on $k$. Clearly the base case holds, since  each $a_n$ is a natural number and then $a_n \\ge 0$ for all $n$. Now suppose inductively that the claim holds for $k\\ge 0$, i.e., $a_n\\ge k$ for all $n \\in N$; we wish to show that also holds for $k+1$ and thus close the induction. Furthermore, we get a contradiction since $a_n \\ge k$ for all $k, n \\in N$, implies that the natural numbers are bounded.\n$a_n>a_{n+1}$ since $(a_n)$ is an infinite descent. By the inductive hypothesis we know that $a_{n+1}\\ge k$, so we have $a_n>k$ and then $a_n\\ge k+1$.\nTo conclude we have to show that the claim holds for every $n$. Suppose there is some $n_0$ such that $a_{n_0}<k+1$ so, $\\,a_{n_0}\\le k$. Then either $\\,a_{n_0} = k$ or $\\,a_{n_0} < k$ but both cases contradicts our hypothesis. Thus $a_n\\ge k+1$ for all $n$, which close the induction. \nThanks :)\n",
    "proof": "I would argue a different way.\nBy assumption,\nfor all $n$,\n$a_n > a_{n+1}$,\nor\n$a_n \\ge a_{n+1}+1$.\nTherefore,\nsince\n$a_{n+1} \\ge a_{n+2}+1$,\n$a_n \\ge a_{n+2}+2$.\nProceeding by induction,\nfor any $k$,\n$a_n \\ge a_{n+k}+k$.\nBut,\nset $k = a_n+1$.\nWe get\n$a_n \\ge a_{n+a_n+1}+a_n+1\n> a_n$.\nThis is the desired contradiction.\nThis can be stated in this form:\nWe can only go down as\nfar as we are up.\nNote:\nThis sort of reminds me\nof some of the\nfixed point theorems\nin recursive function theory.\n",
    "tags": [
      "proof-writing",
      "self-learning"
    ],
    "score": 6,
    "answer_score": 14,
    "is_accepted": true,
    "question_id": 522822,
    "answer_id": 522875
  },
  {
    "theorem": "How can I prove $\\lim \\limits_{n \\to \\infty}\\left(1+\\frac{1}{a_n}\\right)^{a_n}=e$ without involving function limit?",
    "context": "If I already know that\n$$\\lim \\limits_{n \\to \\infty} a_n=+\\infty$$\nThen how can I prove\n$$\\lim \\limits_{n \\to \\infty}\\left(1+\\frac{1}{a_n}\\right)^{a_n}=e$$ \nwithout involving function limit?\nThis question comes because you may find some books on calculus or analysis (maybe they are badly written) require you to prove something like\n$$\\lim \\limits_{n \\to \\infty}\\left(1+\\frac{2}{n}\\right)^{n}=e^2$$\nor something more complex even before they formally introduce the definition of limit of a function, They are hard to prove because you can't simply take something like $\\frac{n}{2}$ as a subsequence of $n$. \nThe definition of limit of a function (at infinity) here mean:\n\nFor a real function $f$ which is well-defined on $[a, +\\infty)$, if for any $\\epsilon >0$, there is a positive number $M \\geq a$ such that when $x>M$ we can say $|f(x)-A|<\\epsilon$, then\n  $$\\lim \\limits_{x \\to \\infty}f(x)=A.$$\n\nWhile the definition of limit of a sequence here mean:\n\nFor a sequence $\\{a_n\\}$, if for any $\\epsilon>0$, there is a positive integer $N$ such that when $n>N$ we can say $|a_n-A|<\\epsilon$, then $$\\lim \\limits_{n \\to \\infty}a_n=A.$$\n\nI know sequence is a \"special\" kind of function whose domain is $\\mathbb{N}$ and thus sequence limit is but a special case of function limit. Here I say avoid involving the idea of function limit means not to use the idea above but only to prove it by the \"special case\" below. After all, $(1+\\frac{1}{a_n})^{a_n}$ is still a \"special\" function - a sequence.\np.s. $e$ is defined by\n$$\\lim \\limits_{n \\to \\infty}\\left(1+\\frac{1}{n}\\right)^n=e.$$\nMy try so far:\nSince $$\\lim \\limits_{n \\to \\infty}\\left(1+\\frac{1}{n}\\right)^n=e,$$ for every $\\epsilon>0$, there is $N \\in \\mathbb{N}$ s.t. for all $n>N$,$$|\\left(1+\\frac{1}{n}\\right)^n-e|<\\epsilon.$$\nMeanwhile, since $$\\lim \\limits_{n \\to \\infty} a_n=+\\infty,$$ for $N' \\in \\mathbb{N}$ and $N'>N$, there is $N'' \\in \\mathbb{N}$ s.t. for all $n>N''$, $a_n>N'>N.$\nHowever, if $a_n$ become bigger then $1+\\frac{1}{a_n}$ will be smaller, and vise versa, so I don't know how to deal with $\\left(1+\\frac{1}{a_n}\\right)^{a_n}.$\n",
    "proof": "Any subsequence of a Cauchy sequence is a Cauchy sequence with the same limit point, hence\n$$ \\lim_{n\\to +\\infty}\\left(1+\\frac{1}{a_n}\\right)^{a_n}=e $$\nas soon as $\\{a_n\\}_{n\\in\\mathbb{N}}$ is a diverging sequence of natural numbers. The very last assumption can be dropped by noticing that $\\frac{\\log(1+x)}{x}$ is a positive, decreasing and convex function on $[0,1]$, hence if $a_m\\in\\mathbb{R}$ is between $n$ and $n+1$,\n$$\\left(1+\\frac{1}{a_m}\\right)^{a_m}$$\nis between $\\left(1+\\frac{1}{n}\\right)^n$ and $\\left(1+\\frac{1}{n+1}\\right)^{n+1}$, so the same conclusion as above follows by squeezing.\n",
    "tags": [
      "real-analysis",
      "limits",
      "proof-writing",
      "exponential-function"
    ],
    "score": 6,
    "answer_score": 12,
    "is_accepted": true,
    "question_id": 1450946,
    "answer_id": 1450968
  },
  {
    "theorem": "Help in proving $ \\left( 1 + \\frac{1}{n} \\right)^{n} \\leq \\sum\\limits_{k=0}^{n} \\frac{1}{k!} &lt; 3 $.",
    "context": "I am trying to prove this statement for all $ n \\geq 1 $ using induction:\n$$\n\\left( 1 + \\frac{1}{n} \\right)^{n} \\leq \\sum_{k=0}^{n} \\frac{1}{k!} < 3.\n$$\nI said:\n\nBase case $ n = 1 $:\n$$\n  \\left( 1 + \\frac{1}{1} \\right)^{1} \\leq \\sum_{k=0}^{1} \\frac{1}{k!} < 3,\n  $$\nwhich is okay.\nInduction step: Suppose that $ \\displaystyle \\left( 1 + \\frac{1}{n}\n  \\right)^{n} \\leq \\sum_{k=0}^{n} \\frac{1}{k!} < 3 $ for a given $ n \\in\n  \\mathbb{N} $.\nTransition from $ n \\to n + 1 $:\n$$\n    \\displaystyle \\left( 1 + \\frac{1}{n + 1} \\right)^{n+1}\n  = \\left( 1 + \\frac{1}{n + 1} \\right)^{n} \\left( 1 + \\frac{1}{n + 1} \\right)\n  = \\ldots \\text{Help} \\ldots\n  < 3.\n  $$\n\nI need some guidance for proof-writing (-thinking) in orders.\n",
    "proof": "For all $ n \\in \\mathbb{N} $, we have\n\\begin{align}\n      \\left( 1 + \\frac{1}{n} \\right)^{n}\n&=    \\sum_{k=0}^{n} \\binom{n}{k} \\left( \\frac{1}{n} \\right)^{k} \\quad (\\text{By the Binomial Theorem.}) \\\\\n&=    \\sum_{k=0}^{n} \\frac{n!}{k!(n - k)!} \\cdot \\frac{1}{n^{k}} \\quad (\\text{By the definition of the binomial coefficient.}) \\\\\n&=    \\sum_{k=0}^{n} \\frac{1}{k!} \\cdot \\frac{n!}{(n - k)!} \\cdot \\frac{1}{n^{k}} \\\\\n&=    \\sum_{k=0}^{n} \\frac{1}{k!} \\left( \\prod_{i=n-k+1}^{n} i \\right) \\frac{1}{n^{k}} \\quad (\\text{By cancellation of terms.}) \\\\\n&\\leq \\sum_{k=0}^{n} \\frac{1}{k!} \\left( \\prod_{i=n-k+1}^{n} n \\right) \\frac{1}{n^{k}} \\quad (\\text{As $ i \\leq n $ for all $ i \\in \\{ n - k + 1,\\ldots,n \\} $.}) \\\\\n&=    \\sum_{k=0}^{n} \\frac{1}{k!} \\cdot n^{k} \\cdot \\frac{1}{n^{k}} \\\\\n&=    \\sum_{k=0}^{n} \\frac{1}{k!} \\\\\n&\\leq 1 + \\sum_{k=0}^{n-1} \\frac{1}{2^{k}} \\quad (\\text{By comparison of terms.}) \\\\\n&<    1 + \\sum_{k=0}^{\\infty} \\frac{1}{2^{k}} \\\\\n&=    1 + 2 \\quad (\\text{Sum of a well-known convergent geometric series.}) \\\\\n&=    3. \\quad (\\text{Voilà!})\n\\end{align}\n",
    "tags": [
      "sequences-and-series",
      "inequality",
      "proof-writing",
      "induction",
      "summation"
    ],
    "score": 6,
    "answer_score": 10,
    "is_accepted": true,
    "question_id": 264458,
    "answer_id": 264472
  },
  {
    "theorem": "Image of a basis forms a basis, if and only if matrix is invertible",
    "context": "Suppose $B_1=\\{v_1,v_2,...,v_n\\}$ is a basis of $\\mathbb{R}^n$, and $M$ is an $n*n$ matrix. Prove that $B_2=\\{Mv_1,Mv_2,...,Mv_n\\}$ is also a basis of $\\mathbb{R}^n$ if and only if $M$ is invertible.\nFollowing is what I have so far:\nAssume $B_2$ is basis of $\\mathbb{R}^n$. \nThen, $B_2$ is a set of linearly independent vectors, and $B_2$ spans $\\mathbb{R}^n$.\nSince $B_1$ is also a basis of $\\mathbb{R}^n$, then any element(vector) of $B_2$ is a linear combination of elements(vectors) of $B_1$ and vice-versa. \n$Mv_1= a_{11}v_1+a_{21}v_2+...+a_{n1}v_n$ , where $a_{11},a_{21},...,a_{n1}\\in \\mathbb{R}$\nLikewise, $Mv_2= a_{12}v_1+a_{22}v_2+...+a_{n2}v_n$ , where $a_{12},a_{22},...,a_{n2}\\in \\mathbb{R}$\n$\\begin{bmatrix}Mv_1&Mv_2&...&Mv_n\\end{bmatrix}=\\begin{bmatrix}v_1&v_2&...&v_n\\end{bmatrix}\\begin{bmatrix}a_{11}&a_{12}&...&a_{1n}\\\\a_{21}&a_{22}&...&a_{2n}\\\\ \\vdots&\\vdots&\\vdots&\\vdots\\\\a_{n1}&a_{n2}&...&a_{nn}\\end{bmatrix}$\nNot sure what to do next ...\n",
    "proof": "Using the determinant is a very elegant solution. But if you can't use the determinant, you still can prove it.\nLet us denote $\\tilde{B_1}=[v_1~v_2~\\ldots~v_n]$ and $\\tilde{B}_2=[Mv_1~Mv_2~\\ldots~Mv_n]$. Then your last equation says $\\tilde B_2=\\tilde B_1M$.\nHints:\nTo finish $B_2$ basis implies $M$ invertible:\nSince $B_2$ is a basis, the matrix $\\tilde B_2$ is invertible.\nOn the RHS the matrix $\\tilde B_1$ is invertible, because $B_1$ is a basis. \nTherefore, you can manipulate the equation to get $M=\\ldots$, where $\\ldots$ is invertible, hence $M$ is invertible.\nFor $M$ invertible implies $B_2$ basis:\nSuppose $B_2$ is not a basis, then you can find a nontrivial linear combintion of $B_2$ which is $0$. You can rewrite the equation and get a nontrivial solution of $Mx=0$. Hence $M$ is not invertible and we are done.\n",
    "tags": [
      "linear-algebra",
      "matrices",
      "proof-writing",
      "hamel-basis"
    ],
    "score": 6,
    "answer_score": 5,
    "is_accepted": true,
    "question_id": 2722955,
    "answer_id": 2722961
  },
  {
    "theorem": "A field has no &quot;zero divisors&quot;",
    "context": "Question:\nAssume $x, y$ are elements of a field $F$. Prove that if $xy = 0$, then $x = 0$ or $y = 0$.\nMy thinking:\nI am not sure how to prove this. I can only use basic field axioms. Should I assume that both x and y are not equal to 0 and then prove by contradiction or should I assume one of x and y is not 0 and then prove the other one has to equal 0?\nThanks \n",
    "proof": "Hint:\nIf $x=0$, then we are done.\nIf $x\\ne 0$, then $x^{-1}$ exists. Multiply $x^{-1}$ to both sides of $xy=0$.\n",
    "tags": [
      "proof-writing",
      "field-theory"
    ],
    "score": 6,
    "answer_score": 13,
    "is_accepted": true,
    "question_id": 2299678,
    "answer_id": 2299681
  },
  {
    "theorem": "Finding the dimension of $S = \\{B \\in M_n \\,|\\, AB = BA\\}$, where $A$ is a diagonalizable matrix",
    "context": "I need some help proving the following:\n\nLet $A \\in M_{n \\times n}$ be a diagonalizable matrix with distinct eigenvalues $\\lambda_1, \\ldots , \\lambda_k$ and corresponding multiplicities $d_1, \\ldots, d_k$. Given that $S = \\{B \\in M_n \\,|\\, AB = BA\\}$, prove that $\\dim (S) = d_1^2 + \\cdots + d_k^2$.\n\nHere's what I've done:\nSince $A$ is diagonalizable, we have that $D = P^{-1}AP$, where $D$ is a diagonal matrix. We may then write, $$P^{-1}AP = \\begin{bmatrix}\\Lambda_1 & & &  \\\\ & \\Lambda_2 & & \\\\ & & \\ddots & \\\\ & & & \\Lambda_k\\end{bmatrix} = D,$$ where $$\\Lambda_i = \\underbrace{\\begin{bmatrix}\\lambda_i & & & \\\\ & \\lambda_i & & \\\\ & & \\ddots & \\\\ & & & \\lambda_i\\end{bmatrix}}_{d_i \\, \\text {columns}}.$$\nObviously the size of each $\\Lambda_i$ is $d_i \\times d_i = d_i^2$, which I'm thinking we need to show that each block contributes this to the total dimension as we work along the diagonal, but I'm having a little trouble coming to this conclusion via the why and how. \nClearly each column vector of $D$ and consequently each column vector of each $\\Lambda_i$ is linearly independent, but I am unable to see what comes next to receive the squared part. Can someone provide a hint as to how I can make this conclusion?\n",
    "proof": "Hint: Write $D$ as a block matrix\n$$\nD=\n\\pmatrix{\n\\lambda_1 I \\\\\n& \\lambda_2 I \\\\\n&& \\ddots \\\\\n&&& \\lambda_k I\n}\n$$\nNoting that\n$$\nP^{-1}[AB]P= D(P^{-1} BP), \\qquad\nP^{-1}[BA]P= (P^{-1} BP)D\n$$\nIt suffices to determine which matrices $M$ satisfy $MD=DM$.\nTo that end, write $M$ as a block matrix (partitioned in the same way as $D$), and compute the products $DM$ and $MD$. Determine that all blocks of $M$ must be zero except those on the diagonal.\n",
    "tags": [
      "linear-algebra",
      "matrices",
      "vector-spaces",
      "proof-writing",
      "eigenvalues-eigenvectors"
    ],
    "score": 6,
    "answer_score": 7,
    "is_accepted": true,
    "question_id": 1923522,
    "answer_id": 1923804
  },
  {
    "theorem": "Prove that $f : [-1, 1] \\rightarrow \\mathbb{R}$, $x \\mapsto x^2 + 3x + 2$ is strictly increasing.",
    "context": "\nProve that $f : [-1, 1] \\rightarrow \\mathbb{R}$, $x \\mapsto x^2 + 3x + 2$ is strictly increasing.\n\nI do not have use derivatives, so I decided to apply the definition of being a strictly increasing function, which should be: \n\nIf we pick 2 numbers $a$ and $b$ from the domain of a function $f$, where $a < b$, then $f(a) < f(b)$.\n\nNow (if that is a correct definition), I have tried to apply it to my case:\n\nLet $a$, $b \\in [-1, 1]$ and $a < b$. We want to show that $f(a) < f(b)$ or that $f(b) - f(a) > 0$.\nWe know that $x^2 + 3x + 2 = (x + 2)(x + 1)$, thus we have that $f(a) = (a + 2)(a + 1)$ and $f(b) = (b + 2)(b + 1)$ , therefore we need to show that:\n$$(b + 2)(b + 1) - (a + 2)(a + 1) > 0$$\nWe can see that $(b + 2)$ and $(a + 2)$ will always be positive, and that $(b + 2) > (a + 2)$ , since $b > a$ (by assumption).\nSince $b > a$, we know that $b > -1$ (otherwise $a \\geq b$), thus $(b + 1) > 0$ (so we know that $(b + 2)(b + 1) > 0$ .  We also have at most $(a + 1) = 0$, and thus  we have that $(a + 2)(a + 1) \\ge 0$.\nSo, here is the proof that  $f(b) - f(a) > 0$ .  \n\nAm I correct? If yes, what can I improve it? If not, where are the erros and possible solutions?\n",
    "proof": "It looks right. Another possible approach is the following one - since:\n$$ f(x)=x^2+3x+2 = \\left(x+\\frac{3}{2}\\right)^2-\\frac{1}{4} $$\nwe have that $f(x)$ attains its minimum in $x=-\\frac{3}{2}$ (the abscissa of the vertex) and for every $r>-\\frac{1}{4}$ the equation\n$$ f(x)=r $$\nhas two solutions, symmetric with respect to $x=-\\frac{3}{2}$. \nIt follows that $f(x)$ is increasing over $\\left(-\\frac{3}{2},+\\infty\\right)$ and $[-1,1]$ is just a subset of it.\n\nStill another way - for every $\\varepsilon>0$ we have:\n$$ f(x+\\varepsilon)-f(x) = \\varepsilon(2x+\\varepsilon)+3\\varepsilon>\\varepsilon(2x+3) $$\nhence $x\\in[-1,1]$ implies $(2x+3)>0$, then $f(x+\\varepsilon)>f(x)$, i.e. $f$ increasing.\n",
    "tags": [
      "calculus",
      "functions",
      "proof-verification",
      "proof-writing"
    ],
    "score": 6,
    "answer_score": 5,
    "is_accepted": false,
    "question_id": 1253120,
    "answer_id": 1253145
  },
  {
    "theorem": "how to prove $\\left(\\frac{n}{3}\\right)^n\\leq\\frac{1}{3}n!$",
    "context": "i am asked to prove this statement:\n\n$$\\left(\\frac{n}{3}\\right)^n\\leq\\frac{1}{3}n!$$ \n\nNow after several attempts, i am lost not knowing where and how to start. if I use induction, i am stuck on the way. how can i solve this in an easy way? i have huge difficulties when i have to find something which is bigger than the latter term or so, because i lack some important source of maths in my brain. i am very likely to give up on the way if it becomes tough. for any guidance how to overcome this phase i will be very very thankful. i am trying for proficiency in math. Thanks\n",
    "proof": "We would like to prove\n$$\\left(\\frac{n}{3}\\right)^n \\leq \\frac{1}{3}n!.$$\nThis is equivalent to\n$$n^{n-1} \\leq 3^{n-1}(n-1)!.$$\nIt is obviosly true for $n = 1$. However, as $n$ increases, left side rises slower that the right-hand side (thus the inequality will still hold). To put this formally, consider the ratio\n$$\\frac{(n+1)^{n}}{(n)^{n-1}} \\leq \\frac{3^{n}n!}{3^{n-1}(n-1)!} = 3n$$\nwhich simplifies to\n$$\\left(\\frac{n+1}{n}\\right)^{n} \\leq 3.$$\nand is true because $\\left(1+\\frac1n\\right)^n$ is increasing in $n$ (e.g. see this post) and converges to $e < 3$.\nCheers!\n",
    "tags": [
      "proof-writing",
      "induction",
      "problem-solving"
    ],
    "score": 6,
    "answer_score": 3,
    "is_accepted": true,
    "question_id": 264512,
    "answer_id": 264558
  },
  {
    "theorem": "Counterexample regarding basic properties of limits",
    "context": "Let $f, g$ be two functions defined on a deleted neighbourhood of\n$a ∈ R$. If $g(x) \\neq 0$ for every x on the deleted neighbourhood and $$\\lim_{x\\to a}{f(x)\\over g(x)} = 1$$\nthen\n$$\\lim_{x\\to a}{(f(x) - g(x))} = 0$$\nI'm asked to prove or disprove this claim, along with others. My instinct in these type of questions is to start figuring out if there are any piecewise function that contradict the statements but I almost feel like I'm searching in the dark with no clear strategy.. is there an agreed upon process regarding searching for counter examples?\n",
    "proof": "What about\n$$\\begin{cases}\nf(x) &= \\frac{1}{(x-a)^2}+1\\\\\ng(x) &= \\frac{1}{(x-a)^2}\n\\end{cases}$$\nThe idea to find this counterexample is that if $\\lim\\limits_{x \\to a} f(x)$ or $\\lim\\limits_{x \\to a} g(x)$ exists and is finite, then the two limits are equal and therefore the limit of $f-g$ vanishes. So in the counterexample above, I looked at maps having non finite limits.\n",
    "tags": [
      "limits",
      "proof-writing",
      "examples-counterexamples"
    ],
    "score": 6,
    "answer_score": 11,
    "is_accepted": true,
    "question_id": 3944475,
    "answer_id": 3944482
  },
  {
    "theorem": "When proving a biconditional, how to say the &quot;I&#39;ll prove left-to-right first&quot; in words?",
    "context": "When proving a biconditional, sometimes the writer says \"I'll prove the converse first\".  I want to say I'll prove left-to-right first.  How to say that?  I don't want to put a left to right arrow.  I prefer words.\nAlso, how can I say I'll prove the converse of the statement but my strategy is to prove the contrapositive of the converse.  Should I just say \"I'll prove the contrapositive of the converse\"?  I'd like to use the standard lingo, but I'm not actually used to reading such statements.  I can't remember seeing anyone using these descriptions.\n",
    "proof": "\nWhen proving a biconditional, sometimes the writer says \"I'll prove the converse first\". I want to say I'll prove left-to-right first. How to say that? I don't want to put a left to right arrow. I prefer words.\n\nEevee Trainer suggests \"forward implication\" in their answer, and I think that sounds good: \"I'll prove the forward implication first.\" The opposite, of course, would be \"I'll prove the reverse implication first.\"\nAn alternative which I think also sounds good is to say \"forward direction\" instead of \"forward implication.\"\n\nAlso, how can I say I'll prove the converse of the statement but my strategy is to prove the contrapositive of the converse. Should I just say \"I'll prove the contrapositive of the converse\"?\n\nThe contrapositive of the converse is called the inverse. Don't call it the contrapositive of the converse, since that sounds redundant (just like if you called it \"the inverse of the converse of the converse\").\nPutting all this together, you might write something like: \"First, I'll prove the forward implication. Next, I'll prove the inverse of the forward implication.\"\n",
    "tags": [
      "proof-writing",
      "terminology"
    ],
    "score": 6,
    "answer_score": 4,
    "is_accepted": true,
    "question_id": 3547121,
    "answer_id": 3547171
  },
  {
    "theorem": "What does &#39;imply&#39; mean in maths?",
    "context": "What does it mean for something to imply something else in maths? I don't think I've grasped this concept and it's making understanding theorems and proofs really difficult.\nI think it might be the word 'imply' that's throwing me off. I've searched for definitions and examples, but they don't seem to make anything clearer.\n",
    "proof": "A statement $A$ implies another statement $B$ (written as $A\\Rightarrow B$), if from the truth of the former, it necessarily follows the truth of the latter.\nExample. If I am in London, I am necessarily in England. So the statement \"I am in London\" implies the statement \"I am in England\".\nOn the other hand, if it is possible for $B$ to be true, while simultaneously $A$ is false, then $A$ is not implying $B$.\nExample. If I am in England, I am not necessarily in London (I could be in Oxford). Hence, the statement \"I am in England\" does not imply the statement \"I am in London\".\n",
    "tags": [
      "logic",
      "proof-writing",
      "first-order-logic",
      "propositional-calculus",
      "proof-theory"
    ],
    "score": 6,
    "answer_score": 7,
    "is_accepted": false,
    "question_id": 2531122,
    "answer_id": 2609134
  },
  {
    "theorem": "Contraposition of &quot;P if and only if Q&quot;",
    "context": "I'm understanding the basic idea of contraposition, when it comes to propositional logic and writing proofs, but I'm having trouble figuring out what the contraposition of \"P if and only if Q\" would be. It seems simple enough that the contraposition of \"If P then Q\" would be \"If not Q then not P\", but that seems too simple to be the case with if and only if. Anyone able to help?\n",
    "proof": "$P \\Leftrightarrow Q$ is the same as $P \\Rightarrow Q$ and $Q \\Rightarrow P$.\nSo contrapose those two and you get $Not(Q) \\Rightarrow Not(P)$ and $Not(P) \\Rightarrow Not(Q)$, otherwise written as $Not(P) \\Leftrightarrow Not(Q)$\n",
    "tags": [
      "logic",
      "proof-writing",
      "propositional-calculus"
    ],
    "score": 6,
    "answer_score": 8,
    "is_accepted": true,
    "question_id": 2134093,
    "answer_id": 2134114
  },
  {
    "theorem": "How would I prove $|x + y| \\le |x| + |y|$?",
    "context": "How would I write a detailed structured proof for:\n\nfor all real numbers $x$ and $y$, $|x + y| \\le |x| + |y|$\n\nI'm planning on breaking it up into four cases, where both $x,y < 0$, $x \\ge 0$ and $y<0$, $x<0$ and $y \\ge0$, and $x,y \\ge 0$. But I'm not sure how I'd go about writing it formally.\nThanks!\n",
    "proof": "You are absolutely on the right track. I'll model one case for you, and you can try the other cases on your own.\nCase 1: $x,y\\geq 0$. Then $x+y\\geq 0$, so $|x+y|=x+y$. Similarly, $|x|=x$ because $x\\geq 0$, and $|y|=y$ because $y\\geq 0$. Thus $|x+y|=x+y=|x|+|y|$.\n",
    "tags": [
      "real-analysis",
      "inequality",
      "proof-writing",
      "absolute-value"
    ],
    "score": 6,
    "answer_score": 5,
    "is_accepted": true,
    "question_id": 554926,
    "answer_id": 554938
  },
  {
    "theorem": "Prove that: $(1+a_1)(1+a_2)...(1+a_n) \\le 1 + S_n + (S_n)^2/2! + ... + (S_n)^n/n!$",
    "context": "I am currently working on this problem from Hardy's Course of Pure Mathematics and have gotten stuck near the end. I was wondering if someone could help me determine what to go next.\nQuestion\nIf $a_1, a_2, ...,a_n$ are all positive and $S_n=a_1+a_2+...+a_n$ then:\n$(1+a_1)(1+a_2)...(1+a_n) \\le 1 + S_n + \\dfrac{(S_n)^2}{2!} + ... + \\dfrac{(S_n)^n}{n!}$\nMy Attempt\nProof by induction:\nI had first shown that it was true for $n=1$ and $n=2$.\nNow suppose that it is true for n. Then it must also be true for $n+1$\n$(1+a_1)(1+a_2)...(1+a_n)(1+a_{n+1}) \\le 1 + S_n + \\dfrac{(S_n)^2}{2!} + ... + \\dfrac{(S_n)^n}{n!} + \\dfrac{(S_{n})^{n+1}}{(n+1)!}$\nDefine: $(1+a_1)(1+a_2)...(1+a_n)=x$ and $1 + S_n + \\dfrac{(S_n)^2}{2!} + ... + \\dfrac{(S_n)^n}{n!}=y$\nThen: $x+xa_{n+1} \\le y+\\dfrac{(S_{n})^{n+1}}{(n+1)!}$\nBy the induction assumption, we know that $x \\le y$.\nWhat I can't figure out\nFrom this point, what I think the next natural step would be is to show that\n$xa_{n+1} \\le \\dfrac{(S_{n})^{n+1}}{(n+1)!}$\nI know this rearranges to:\n$xa_{n+1} \\le \\dfrac{(S_{n})^{n}}{n!} \\times \\dfrac{S_n}{(n+1)} $\nAnd feel there may be something I can do here, but haven't been able to figure anything out.\n",
    "proof": "I apologize ahead of time for not being interested in an inductive proof, this just seems simpler to me personally and it's the argument that jumps out to my sensibilities.\nIt suffices to show that every monomial on the left (viewed simply as a polynomial on both sides of the equation) also appears on the right, since everything here is positive. For each $k$ and each string of distinct indices $i_1,\\cdots,i_k$, the expansion of $S_n^k$ will have $k!$ copies of $a_{i_1}\\cdots a_{i_k}$ corresponding to all the possible ways of ordering the terms, so when the coefficient $k!$ is divided by $k!$ it will be $1$, and so every monomial on the left also appears on the right.\n",
    "tags": [
      "proof-writing"
    ],
    "score": 6,
    "answer_score": 7,
    "is_accepted": true,
    "question_id": 326721,
    "answer_id": 326759
  },
  {
    "theorem": "Proving $|x|+|y|+|z| ≤ |x+y-z|+|y+z-x|+|z+x-y|$ for all real $x$",
    "context": "\nWe need to prove that for all real $x,y,z$\n$$|x|+|y|+|z| ≤ |x+y-z|+|y+z-x|+|z+x-y|$$\nSource ISI entrance examination sample questions\n\nI don't know how to solve in mod form so I thought about squaring and removing the mod, but the mod on RHS still persists.\nI believe there can be a different approach to it, I would like to have some hints on how to do approach it.\n",
    "proof": "Hint:\n$$2x=(x+y-z)+(x+z-y) \\implies x=\\frac{x+y-z}{2}+\\frac{x+z-y}{2}$$\nNow repeat the process for $y$ and $z$ and use the triangle inequality trice\n",
    "tags": [
      "inequality",
      "proof-writing"
    ],
    "score": 6,
    "answer_score": 8,
    "is_accepted": true,
    "question_id": 3781629,
    "answer_id": 3781641
  },
  {
    "theorem": "Proof of $n!\\geq2^{n-1}$ by mathematical induction",
    "context": "I am trying to make a proof\n$$n!\\geq2^{n-1}\\;\\;\\forall \\;n\\in N$$\nHere's what I've done!\n$ \\text{When}\\;\\; n=1,\\;\\; 2^{0}\\leq 1!$,\n$ \\text{when}\\;\\; n=2,\\;\\; 2^{1}\\leq 2!$,\n$ \\text{when}\\;\\; n=3,\\;\\; 2^{2}\\leq 3!$,\n$\\vdots$\nAssume it is true for $n=k$, then\n$$2^{k-1}\\leq k!$$.\nNow, we want to prove for $n=k+1$.\nI got stuck at this point. I need help! Thanks!\n",
    "proof": "Then for $n = k+1$, we have\n$$2^k = 2\\cdot2^{k-1} \\le 2 \\cdot k!\\ \\text{ by inductive argument}$$\nFrom here, it is easy to see that $2\\cdot k! \\le (k+1)k! = (k+1)!$\n",
    "tags": [
      "algebra-precalculus",
      "proof-writing",
      "induction"
    ],
    "score": 6,
    "answer_score": 3,
    "is_accepted": true,
    "question_id": 2705478,
    "answer_id": 2705487
  },
  {
    "theorem": "Product of two convergent sequences is convergent",
    "context": "I want to show that if I have $\\lim_{n\\to\\infty}a_n = a$ and $\\lim_{n\\to\\infty}b_n = b$ then $\\lim_{n\\to\\infty}a_nb_n = ab$\nI want to do it in a slightly different way than by book but I don't know if it is correct.\n$(a_n)$ converges to $a$ means $$\\forall \\epsilon >0, \\exists n_1 \\in\\mathbb{N}: |a_n - a| < \\frac{\\epsilon}{2|b|} \\,\\,\\,\\ \\forall n\\in\\mathbb{N}, n>n_1$$\n$(b_n)$ converges to $b$ means $$\\forall \\epsilon >0, \\exists n_2 \\in\\mathbb{N}: |b_n - b| < \\frac{\\epsilon}{2|a|} \\,\\,\\,\\ \\forall n\\in\\mathbb{N}, n>n_2$$\nHence take $n_0 := \\max\\{n_1,n_2\\}$ and we have $$|a_nb_n - ab| = |a_nb_n -a_nb+a_nb-ab| = |a_n(b_n-b)+b(a_n-a)| \\leq |a_n|\\frac{\\epsilon}{2|a|}+|b|\\frac{\\epsilon}{2|b|}=\\frac{\\epsilon}{2} +\\frac{\\epsilon}{2} = \\epsilon$$\nso it's proven.\n\nCan I do that trick in the penultimate step, where I cancel out $|a_n|$ and $|a|$ ? I know that for large $n$ they will be the same, but I think this proof right now is not very rigorous!\n\n",
    "proof": "Let $\\epsilon > 0$. There exist $n_1 \\in \\mathbb{N}$ such that\n$$|a_n - a| < \\frac{\\epsilon}{2(|a|+1)} \\,\\,\\,\\ \\forall n\\in\\mathbb{N}, n>n_1$$\nThere exist $n_2 \\in \\mathbb{N}$ such that\n$$|b_n - b| < \\frac{\\epsilon}{2(|b|+1)} \\,\\,\\,\\ \\forall n\\in\\mathbb{N}, n>n_2$$\nAlso, there exists $n_3$ such that (reverse triangle inequality)\n$$ |a_n|-|a| \\leq ||a_n|-|a||\\leq |a_n-a|<1$$\nfor all $n > n_3$, so that $|a_n|<|a|+1$, i.e, $\\frac{|a_n|}{|a|+1}<1$.\nHence, for all  $n > \\max\\{n_1,n_2,n_3\\}$ we have \n$$|a_nb_n - ab| = |a_nb_n -a_nb+a_nb-ab| = |a_n(b_n-b)+b(a_n-a)| \\leq |a_n|\\frac{\\epsilon}{2(|a|+1)}+|b|\\frac{\\epsilon}{2(|b|+1)}<\\frac{\\epsilon}{2} +\\frac{\\epsilon}{2} = \\epsilon.$$\n",
    "tags": [
      "real-analysis",
      "sequences-and-series",
      "proof-verification",
      "proof-writing"
    ],
    "score": 6,
    "answer_score": 12,
    "is_accepted": true,
    "question_id": 2109183,
    "answer_id": 2109214
  },
  {
    "theorem": "How to prove Disjunction Elimination rule of inference",
    "context": "I've looked at the tableau proofs of many rules of inference (double-negation, disjunction is commutative, modus tollendo ponens, and others), and they all seem to use the so-called \"or-elimination\" (Disjunction Elimination) rule:\n$$(P\\vdash R), (Q\\vdash R), (P \\lor Q) \\vdash R$$\n(If $P\\implies R$ and $Q\\implies R$, and either $P$ or $Q$ (or both) are true, then $R$ must be true.)\nIt's often called the \"proof by cases\" rule, it makes sense, and I've seen the principle used in many mathematical proofs.\nI'm trying to figure out how to logically prove this rule (using other rules of inference and/or replacement), however the proof offered is self-reliant! Is this an axiom?\n(There's also the Constructive Dilemma rule, which looks like a more generalized version of Disjunction Elimination. Maybe the proof of D.E. depends on C.D.? or maybe C.D. is an extension of D.E.?)\n",
    "proof": "I agree that the link you provide to the proof offered by Proof Wiki is \"self-reliant\", and seems, to me at least, to invoke the very rule in which we are interested. In a sense, it can be seen as a \"model\" proof exemplifying the general case of invocation of Disjunction Elimination, $\\lor \\mathcal E$.\nSo, yes, this can sort of be thought of as an \"axiom\": a basic rule of inference we take to be valid, and from which other rules of inference may be derived. It does rely on modus ponens, twice. But the principle that, if, given the truth of $p \\lor q$, and some conclusion $r$ follows from $p$, and the same conclusion $r$ follows from $q$, then we can conclude that $r$ is true.\nIt might be reassuring to note that the argument characterizing \"Disjunction Elimination\" can be expressed as follows:\n$$(((P \\to Q) \\land (R \\to Q)) \\land (P \\lor R)) \\to Q\\tag{Tautology}$$\nwhich proves to be tautologically (necessarily) true $(\\dagger)$, and so we have very good reason to accept the rule of inference as valid.\n$(\\dagger)$: Exercise - Confirm, using a truth-table, that the given tautology is, in fact, tautologically true.\n",
    "tags": [
      "logic",
      "proof-writing",
      "propositional-calculus"
    ],
    "score": 6,
    "answer_score": 7,
    "is_accepted": true,
    "question_id": 461030,
    "answer_id": 461046
  },
  {
    "theorem": "Proof of the Mean Value Theorem for Integrals",
    "context": "My Single Variable Calc Textbook asked me to prove the Mean Value Theorem for Integrals by applying the Mean Value Theorem for Derivatives to the function $F(x)=\\displaystyle\\int_a^xf(t)dt$. I'm pretty sure that my proof is correct, but a correct proof is not listed in the textbook. Also, it would be helpful to critique my proof's format because I've been trying to format my proofs more professionally. \n$$\\text{Theorem: If $f$ is continuous on [a,b], then there exists a number $c$ in [a,b] such that}$$\n$$f(c)(b-a)=\\int_a^b f(t)dt$$\n$$\\text{Proof:}$$\n$$F(x)=\\int_a^xf(t)dt$$\n$$\\text{By the Fundamental Theorem of Calculus, we have}$$\n$$F'(x)=f(x)$$\n$$\\text{By the Mean Value Theorem for Derivatives}$$\n$$F'(c)=\\frac{F(b)-F(a)}{b-a}$$\n$$f(c)=\\frac{F(b)-F(a)}{b-a}$$\n$$f(c)(b-a)=F(b)-F(a)$$\n$$\\text{By the Fundamental Theorem of Calculus}$$\n$$f(c)(b-a)=\\int_a^bf(t)dt$$\n",
    "proof": "The proof is correct, but can be made shorter and more accurate (there should be “there exists $c$” somewhere).\nConsider $F(x)=\\int_a^x f(t)\\,dt$; then, by the fundamental theorem of calculus, $F'(x)=f(x)$, for every $x\\in(a,b)$; moreover, $F(b)-F(a)=\\int_a^b f(t)\\,dt$.\nSince $F$ is continuous over $[a,b]$ and differentiable over $(a,b)$, the mean value theorem applies and there exists $c\\in(a,b)$ such that\n$$\n\\frac{F(b)-F(a)}{b-a}=F'(c)\n$$\nthat is,\n$$\n\\int_a^b f(t)\\,dt=(b-a)f(c)\n$$\n\nBy the way, the proof can be given without mentioning the mean value theorem. Since $f$ is continuous over the interval $[a,b]$, it has a maximum value $M$ and a minimum value $m$. Then, by definition of integral and from $m\\le f(t)\\le M$, we have\n$$\nm(b-a)\\le\\int_a^b f(t)\\,dt\\le M(b-a)\n$$\nBy the intermediate value theorem, there exists $c\\in[a,b]$ such that\n$$\nf(c)=\\frac{1}{b-a}\\int_a^b f(t)\\,dt\n$$\nThis is somewhat less precise than the other version, because we cannot state, without further work, that $c$ can be chosen in $(a,b)$.\n",
    "tags": [
      "real-analysis",
      "calculus",
      "proof-writing"
    ],
    "score": 6,
    "answer_score": 11,
    "is_accepted": true,
    "question_id": 3290410,
    "answer_id": 3290415
  },
  {
    "theorem": "Proving Trigonometric “Definitions”",
    "context": "The expression trigonometric “definitions” refers here, rather narrowly, to statements expressing stable relations between the sides of right triangle. \nThus, for instance, the traditional definition of sine supposes that one has a demonstration that the ratios between opposite side and hypothenuses are independent of the size of the right triangle and are dependent instead on the amplitude of the angles of the right triangle.\nMy question is the following: How these stable relations, simply assumed by most trigonometric “definitions,” can actually be demonstrated?\nA very similar question has been posted here but I remain unsatisfied by most answers — which seem to convoke either unnecessary complex mathematical objects or simplistic historical accounts. \nI guess, what I am looking for, is something like a geometrical proof — but am open to others!\n",
    "proof": "For proving that the sine function is well-defined, one uses two theorems from Euclidean geometry combined with a tiny bit of algebra. The two theorems are:\n\nTheorem 1: In any triangle, the sum of the angles equals $\\pi$.\n\nI don't actually care about the numerical value of the sum, so perhaps one can state Theorem 1 more classically: the sum of the angles of any triangle is equal to the sum of two right angles. In any case, all that I'll use is that the sums of the angles of any two triangles are equal.\n\nTheorem 2 (The \"Angle-Angle-Angle\" theorem): For any two triangles $\\triangle ABC$ and $\\triangle A'B'C'$, if $\\angle ABC = \\angle A'B'C'$ are congruent, and if $\\angle BCA = \\angle B'C'A'$ are congruent, and if $\\angle CAB = \\angle C'A'B'$ are congruent, then the triangles are similar. In more detail, this means that we have equality of ratios\n  $$\\text{Length}(\\overline{AB}) \\bigm/ \\text{Length}(\\overline{A'B'}) = \\text{Length}(\\overline{BC}) \\bigm/ \\text{Length}(\\overline{B'C'}) = \\text{Length}(\\overline{CA}) \\bigm/ \\text{Length}(\\overline{C'A'})\n$$\n\nSo now let's consider two right triangles $\\triangle ABC$ and $\\triangle A'B'C'$, such that the $\\angle ABC$ and $\\angle A'B'C'$ are right angles. It follows that $\\angle ABC = \\angle A'B'C'$.\nSuppose also that $\\angle CAB = \\angle C'A'B'$. By applying Theorem 1, it follows that $\\angle BCA = \\angle B'C'A'$. The hypotheses of Theorem 2 have therefore been verified, so its conclusions are true. From the equation\n$$\\text{Length}(\\overline{BC}) \\bigm/ \\text{Length}(\\overline{B'C'}) = \\text{Length}(\\overline{CA}) \\bigm/ \\text{Length}(\\overline{C'A'})\n$$\nwe deduce, by a tiny bit of algebra, that\n$$\\text{Length}(\\overline{BC}) \\bigm/ \\text{Length}(\\overline{CA}) = \\text{Length}(\\overline{B'C'}) \\bigm/ \\text{Length}(\\overline{C'A'})\n$$\nIn words, this says that if in triangle $ABC$ we divide the length of the side opposite angle $A$ by the length of the hypotenuse, and in triangle $A'B'C'$ we divide the length of the side opposite angle $A$ by the length of the hypotenuse, we get the same number. That number is the sine of the angle $A$.\nThis proves that the sine of an angle is well-defined no matter what right triangle we use for its calculation.\n",
    "tags": [
      "trigonometry",
      "proof-writing",
      "definition"
    ],
    "score": 6,
    "answer_score": 9,
    "is_accepted": true,
    "question_id": 3183728,
    "answer_id": 3183757
  },
  {
    "theorem": "Using proof by contradiction and a counter example.",
    "context": "I have a vague question about proof by contradiction. The main goal of proofs by contradiction is to show that if some statement was true, then we would end up by a logical contradiction. My question would: in order to achieve the logical contradiction, would it be enough to just show a counter-example? That is we suppose the false statement, and then just give an example where the statement gives an illogical/absurd result. Would that be correct?\n",
    "proof": "No that's not enought (if i understood what you mean). Think about it this way:\nYou want to prove A => B and you want to do it by contradiction. That means you suppose B is false even with the hypothesis A. Giving a counterexample to this means giving an example of A => B, and this doesn't prove A=>B in general.\nFor example:\nI try to prove that every odd number is prime (which is false);\nBy contradiction suppose n is odd but not prime. I can find a counterexample to this (3 is odd and prime) but this doesn't give me the thesis (in fact 9 is odd but not prime)\n",
    "tags": [
      "logic",
      "proof-writing"
    ],
    "score": 6,
    "answer_score": 9,
    "is_accepted": true,
    "question_id": 2357148,
    "answer_id": 2357166
  },
  {
    "theorem": "Prove that $a * b = a + b - ab$ defines a group operation on $\\Bbb R \\setminus \\{1\\}$",
    "context": "So, basically I'm taking an intro into proofs class, and we're given homework to prove something in abstract algebra. Being one that hasn't yet taken an abstract algebra course I really don't know if what I'm doing is correct here.\n\nProve: The set $\\mathbb{R} \\backslash \\left\\{ 1 \\right\\}$ is a group under the operation $*$, where:\n  $$a * b = a + b - ab, \\quad \\forall  \\,\\, a,b \\in \\mathbb{R} \\backslash \\left\\{ 1 \\right\\} .$$\n\nMy proof structure:\nAfter reading about abstract algebra for a while, it looks like what I need to show is that if this set is a group, it has to satisfy associativity with the operation, and the existence of an identity and inverse element.\nSo what I did was that I assumed that there exists an element in the set $\\mathbb{R} \\backslash \\left\\{ 1 \\right\\}$ such that it satisfies the identity property for the set and another element that satisfies the inverse property for all the elements in the set. However I'm having trouble trying to show that the operation is indeed associative through algebra since\n$$\\begin{align}\na(b * c) & = a(b+c) - abc \\\\\n& \\ne (a+b)c - abc = (a*b)c \\\\\n\\end{align}$$\nSo in short, I want to ask if it's correct to assume that an element for the set exists that would satisfy the identity and inverse property for the group. Also, is this even a group at all since the operation doesn't seem to satisfy the associativity requirements.\n",
    "proof": "The statement of the associativity condition is wrong; it should be\n$$(a \\ast b) \\ast c = a \\ast (b \\ast c) .$$\nExpanding the l.h.s. gives\n\\begin{align}(a \\ast b) \\ast c &= (a + b - ab) \\ast c \\\\ &= (a + b - ab) + c - (a + b - ab) c \\\\ &= a + b + c - bc - ca - ab + abc .\\end{align}\nNow, do the same thing for the r.h.s. and verify that the expressions agree.\n\nWe cannot simply assume the existence of an identity and inverse. There are plenty of associative binary operations which lack one or both!\nWe can pick out the identity element using the definition: We must have, for example, that\n$$a \\ast e = a$$ for all $a$, and expanding gives\n$$a + e - ae = a.$$\nCan you find which $e$ makes this true for all $a$?\nLikewise, to show that the operation admits inverses, it's enough to produce a formula for the inverse $a^{-1}$ of an arbitrary element $a$, that is, an element that satisfies $a \\ast a^{-1} = e = a^{-1} \\ast a$. As with the identity, use the definition of $\\ast$ and solve for $a^{-1}$ in terms of $a$.\n\nRemark If we glance at the triple product, which by dint of associativity we may as well write $a \\ast b \\ast c$, we can see the occurrence of the elementary symmetric polynomials in $a, b, c$, which motivates writing that product as $(a - 1) (b - 1) (c - 1) + 1$. Then, glancing back we can see that we can similarly write $a \\ast b = (a - 1) (b - 1) + 1,$ which is just the conjugation of the usual multiplication (on $\\Bbb R - \\{0\\}$) by the shift $s : x \\mapsto x + 1$, that is, $a \\ast b := s(s^{-1}(a) s^{-1}(b))$. Since multiplication defines a group structure on $\\Bbb R - \\{0\\}$, that $\\ast$ defines a group structure follows by unwinding definitions. For example, to show associativity, we have\n$$(a \\ast b) \\ast c = s(s^{-1}(s(s^{-1}(a) s^{-1}(b))) s^{-1}(c)) = s(s^{-1}(a) s^{-1}(b) s^{-1}(c)) = s(s^{-1}(a)s^{-1}(s(s^{-1}(b) s^{-1}(c))) = a \\ast (b \\ast c) .$$\n(Note that in writing the third expression without parentheses we have implicitly used the associativity of the usual multiplication.) You can just as well use this characterization to determine the group identity $e$ and the formula for the inverse of an element under $\\ast$.\nIndeed, this together with analogous arguments for the other group axioms shows that conjugating any group operation by a set bijection defines an operation on the other set.\n",
    "tags": [
      "abstract-algebra",
      "group-theory",
      "solution-verification",
      "proof-writing"
    ],
    "score": 6,
    "answer_score": 5,
    "is_accepted": true,
    "question_id": 2184693,
    "answer_id": 2184719
  },
  {
    "theorem": "Prove if $f(a)&lt;g(a)$ and $f(b)&gt;g(b)$, then there exists $c$ such that $g(c)=f(c)$.",
    "context": "First of all, let me write the statement properly:\nTheorem :\n\nLet $f(x)$ and $g(x)$ are continuous on a closed interval $[a,b]$. If $f(a)< g(a)$ and $f(b)>g(b)$, then there exists a $c$ in the interval $[a,b]$ such that $f(c)=g(c)$.\n\n\nI am new at proofs, so I wanted ask if the proof below correct? I feel like Ive jumped logical step. If so please let me know, thanks. \nProof: If $f$ is continuous on $[a,b]$, then by the Intermediate Value Theorem, there exists a $c$ such that $f(c)= L$ where $L$ is between $f(a)$ and $f(b)$. And similarly, by the same theorem there exists a $c$ such that $g(c)= K$ where $K$ is between $g(a)$ and $g(b)$. \nWe must prove that there is at least one value in the interval such that $K=L$. \nSince we know $f(a)< g(a)$, therefore at some point \n$$ f(a)- g(a) < 0 \\tag{1}\\label{eqn1} $$ \nand at a distinct point in the interval we know the following will be true: \n$$f(b)-g(b)>0 \\tag{2}\\label{eqn2} $$\nNow define a function $h(c) = g(c)-f(c)$.\nIt follows from (1) and (2) that $h(c)>0$ at one point and $h(c)<0$ at another point then by the intermediate value theorem at some point on the interval $h(c)$ must equal to zero. So,\n$$h(c)= f(c)- g(c) = 0$$\nTherefore,\n$$f(c)=g(c)$$\nFor reference to the Intermediate Value Theorem see the following links:\nhttps://www.mathsisfun.com/algebra/intermediate-value-theorem.html\nhttps://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=9&cad=rja&uact=8&ved=0CFsQFjAIahUKEwjdppOg66HHAhUFjw0KHUQvCOM&url=http%3A%2F%2Fwww.cut-the-knot.org%2FGeneralization%2Fivt.shtml&ei=3lbKVd2_H4WeNsTeoJgO&usg=AFQjCNE4cTobZvz7jim1bUQPx1A35H3jzw&sig2=rsKlZooQCf-Cw0_TWTqMTg&bvm=bv.99804247,d.eXY\n",
    "proof": "You got off to a great start! Rather, since $f-g$ is continuous on $[a,b]$ (why?) and $$(f-g)(a)<0<(f-g)(b),$$ then at some point $c$ between $a$ and $b$, you know that $(f-g)(c)=0,$ meaning $f(c)=g(c),$ and you're done!\n",
    "tags": [
      "calculus",
      "algebra-precalculus",
      "proof-verification",
      "proof-writing"
    ],
    "score": 6,
    "answer_score": 2,
    "is_accepted": true,
    "question_id": 1386070,
    "answer_id": 1386125
  },
  {
    "theorem": "Let $f:A\\rightarrow B$. Prove that if $ X\\subseteq A$, and $f$ is one to one, then $f(A)-f(X) \\subseteq f(A-X)$.",
    "context": "Let $f:A\\rightarrow B$. Prove that if $ X\\subseteq A$, and $f$ is one to one, then $f(A)-f(X) \\subseteq f(A-X)$.\nCould anyone please guide me through this problem? I got stuck and don't know if what I'm doing is right. Thanks.\n",
    "proof": "My attempt: assume that $y\\in f(A)-f(X)$. Then $y\\in f(A)$, $y\\not\\in f(X)$. Since $f$ is surjective over $f(A)$ and injective by definition, there exists only one $x\\in A$ such that $f(x)=y$. Now, if $x\\in X$, $f(x)=y\\in f(X)$ which is false. Therefore $x\\not\\in X$, that is, $x\\in A-X$ and thus $f(x)\\in f(A-X)$. This shows that $f(A)-f(X)\\subset f(A-X)$.\n",
    "tags": [
      "elementary-set-theory",
      "proof-writing"
    ],
    "score": 6,
    "answer_score": 5,
    "is_accepted": true,
    "question_id": 455376,
    "answer_id": 455381
  },
  {
    "theorem": "$ֿ\\sqrt{2}$ is irrational. Proof by contradiction or Proof of Negation?",
    "context": "I am just learning about proofs in an introductory course. I came across an example of \"proof by contradiction\" (see attachment) about $ֿ\\sqrt{2}$ being irrational. Some online sources have said that this is in fact a proof of negation. Is this correct? If so, how can you spot the difference between the two?\nThank you.\n\n",
    "proof": "tl;dr The distinction is real. Nonetheless, you can safely call this a proof by contradiction in most introductory courses. Later on, in advanced and specialized courses, people might expect you to use more precise terminology, and distinguish between proofs of negation and proofs by contradiction.\n\nWelcome to Math.SE!\nI. To prove that the negation $\\neg P$ of a proposition $P$ holds, we can use the following strategy: we tentatively assume that the proposition $P$ holds, and using this tentative assumption, we derive a contradiction.\nFor example, to prove that the negation of the proposition \"there is a rational number $x$ such that $x^2 = 2$\" holds, we tentatively assume that such an $x$ exists, and using this tentative assumption we show that there is an integer (the denominator of the same $x$, written in lowest terms) that is both even and odd, a contradiction.\nSince assuming that \"there is a rational number $x$ such that $x^2 = 2$\" leads to a contradiction, we can conclude that the negation of this statement, \"there is no rational number $x$ such that $x^2 = 2$\" must in fact hold.\nCommon mathematical parlance calls this sort of reasoning a proof by contradiction (or reductio ad absurdum). It is perfectly appropriate to use the term \"proof by contradiction\" to refer to this sort of argument in an introductory course. Your instructors will understand what you mean.\n\nII. Now, the sort of reasoning demonstrated above is often combined with a different logical principle, the so-called law of double negation, i.e. the fact that every proposition is equivalent to the negation of its negation.\nTo prove that a proposition $Q$ holds, we can tentatively assume that its negation $\\neg Q$ holds, then use this assumption to reach a contradiction. By the principle above, this proves that the negation of $\\neg Q$, i.e. $\\neg\\neg Q$, must hold.\nBut by the law of double negation, $\\neg \\neg Q$ is logically equivalent to $Q$, so we have in fact managed to prove that the proposition $Q$ holds.\nAn example would be the following proposition: \"there are at least two different digits which appear infinitely many times in the decimal expansion of $\\sqrt{2}$\". We can tentatively assume the negation of this statement, \"there are no two different digits which appear infinitely many times in the decimal expansion of $\\sqrt{2}$\". If this was the case, then at most one digit would occur infinitely often in the decimal expansion: from some point onwards, $\\sqrt{2}$ would consist of the repetition of that one  digit. So $\\sqrt{2}$ would have a recurring decimal expansion, contradicting the fact that $\\sqrt{2}$ is irrational.\nWe have shown that \"there are no two different digits that appear infinitely many times in the decimal expansion of $\\sqrt{2}$\" leads to a contradiction. Therefore, the negation of this statement, \"it is not the case that there are no two different digits which appear infinitely often in the decimal expansion of $\\sqrt{2}$\" must hold.\nUsing the law of double-negation, we can conclude that equivalently, \"there are at least two different digits that appear infinitely many times in the decimal expansion of $\\sqrt{2}$\".\nSurprising fact: as of 2023, nobody managed to give an actual example of two digits that appear infinitely many times in the decimal expansion $\\sqrt{2}$, even though (by the simple proof above) we know that there definitely exist two such digits. The simple proof above is non-constructive: while it proves that an object satisfying some property exists, it does not give an example of such an object.\n\nIII. In certain advanced branches of mathematics (among them computer science, topos theory, type theory, proof theory), and among adherents of constructivist mathematical philosophy (which forbids non-constructive existence proofs and only allows existence proofs that provide specific examples) we have to avoid appeals to double negation.\nPractitioners of these fields find it important to distinguish between the proofs by contradiction that do not appeal to the law of double negation (our first example, the irrationality of $\\sqrt{2}$) and the proofs by contradiction that do (our second example, about the digits).\nSo in the 1980s, some practitioners of these fields devised an alternative nomenclature, where they call only proofs of this second sort proof by contradiction, and instead call proofs of the first sort proof of negation.\nDespite efforts to evangelize this nomenclature, it never caught on among people who are neither computer scientists, nor topos theorists, nor type theorist, nor proof theorists. (NB it didn't fully catch on among proof theorists either! Many of us think that this newer naming scheme is dumb, since it focuses on the contradiction, even though the double negation is the real culprit).\nOnce you start learning about topos theory, type theory or proof theory, it will be important to know the distinction between proof of negation and proof by contradiction in this alternative nomenclature. Until then, and in any other math course, don't expect your instructors to be familiar with this alternative nomenclature, and don't expect them to care about it either.\n",
    "tags": [
      "logic",
      "proof-writing",
      "rationality-testing"
    ],
    "score": 6,
    "answer_score": 7,
    "is_accepted": true,
    "question_id": 4619445,
    "answer_id": 4619477
  },
  {
    "theorem": "Pigeon Hole Principle (most probably)",
    "context": "Prove that any set of 46 distinct 2-digit numbers contains two distinct numbers which are relatively prime.\nThis is what I am trying to prove. I have a feeling that it would be using the pigeon hole principle but I just cannot figure it out.\nThis is what I found interesting so far:\nThere are 90 2-digits numbers ([10,99]), which means that there are exactly 45 even numbers, which means that in a set with 46 numbers, there must be at least one odd number. Though I do not know what to make of that...\nAlso, 46 is optimal, in the snse that there exists a set of 45 distinct 2-digit numbers so that no two distinct numbers are relatively prime.\n",
    "proof": "Suppose by contradiction that $S$ is a set of $46$ integers in $\\{10, \\ldots, 99\\}$ such that no two distinct elements $a, b \\in S$ are relatively prime; i.e., for all $a, b \\in S$ such that $a \\ne b$, $\\gcd(a,b) > 1$.\nA few key observations are needed.  First is that if $b = a+1$, then $\\gcd(a,b) = 1$.  So $S$ cannot contain any consecutive elements.  Second, what is the maximal size of the set $S$ under this condition?  Since there are $99 - 10 + 1 = 90$ two-digit numbers, one can choose at most half, or $45$ of these numbers in such a way that there are no two consecutive elements.  But this contradicts the assumption that $|S| = 46$; therefore, no such $S$ exists.\nIn short, the proof such a set doesn't exist merely relies on the fact that any two consecutive integers are relatively prime.\n",
    "tags": [
      "discrete-mathematics",
      "elementary-set-theory",
      "proof-writing",
      "pigeonhole-principle"
    ],
    "score": 6,
    "answer_score": 10,
    "is_accepted": false,
    "question_id": 3418629,
    "answer_id": 3418647
  },
  {
    "theorem": "proving transcendental numbers are irrational",
    "context": "I don't understand how every transcendental number is irrational, is there a way to prove that? I know it just means it's a non-algebraic number, but how does that correlate to irrationality?\n",
    "proof": "If $x$ is transcendental but not irrational, then $x = a/b$, with $a,b$ integers, and so $x$ solves the rational equation $b t - a = 0$, but then $x$ is algebraic and hence not transcendental.\n",
    "tags": [
      "proof-writing",
      "irrational-numbers",
      "transcendental-numbers"
    ],
    "score": 6,
    "answer_score": 8,
    "is_accepted": false,
    "question_id": 2972345,
    "answer_id": 2972360
  },
  {
    "theorem": "Proving area of triangle formed at parallelogram midpoint is 1/4 of the parallelogram?",
    "context": "ABCD is a parallelogram . X is the midpoint of AD & Y is the midpoint of BC. Show that the area of $\\triangle {ABX}$ is $\\frac{1}{4}$ the area of ABCD\n\nCan you help me with this proof? Where should I start? I think It should be by proving\n$\\triangle{DBC} \\cong \\triangle{DBA} $ using SAS as DB is a common side DC= AB as ABCD is a parallelogram, $\\angle {BDC} = \\angle{DBA} $ alternate angles\nAnd I can also predict that the use of the midpoint theorem here.\nMany thanks!\n\n",
    "proof": "The length of perpendicular for the triangle and parallelogram is the same.\n",
    "tags": [
      "geometry",
      "proof-writing"
    ],
    "score": 6,
    "answer_score": 4,
    "is_accepted": false,
    "question_id": 1975187,
    "answer_id": 1975197
  },
  {
    "theorem": "Prove that if $B$ is similar to $A$, then $B^T$ is similar to $A^T$ .",
    "context": "If two matrix ($A$ and $B$) are similar if there exists an invertible matrix $P$, such that:\n$$ B=P^{-1} A P $$\nI'm thinking if I can prove that $A$, $B$ , $A^T$ and $B^T$ have the same characteristic polynomial then that would prove the above statement. However, I am a little unsure how to put the whole proof together.\n",
    "proof": "Since\n$B = P^{-1} A P, \\tag{1}$\n$B^T = P^T A^T (P^{-1})^T; \\tag{2}$\nNow \n$PP^{-1} = I, \\tag{3}$\nwhence\n$(P^{-1})^T P^T = I \\tag{4}$\nas well; but (4) implies\n$(P^{-1})^T = (P^T)^{-1}; \\tag{5}$\nthus (2) becomes\n$B^T = P^T A^T (P^T)^{-1}, \\tag{6}$\nso $A^T$ is similar to $B^T$,\nputting the whole proof together! QED.\n",
    "tags": [
      "linear-algebra",
      "matrices",
      "proof-writing"
    ],
    "score": 6,
    "answer_score": 5,
    "is_accepted": true,
    "question_id": 1288806,
    "answer_id": 1288815
  },
  {
    "theorem": "Proving that $\\sin1 $(radian) is irrational without using Taylor Series Expansion.",
    "context": "In university last semester I was asked to prove that $\\sin1$ (1 radian that is) is irrational,  and ended up simply using the Taylor Series Expansion. This method provides a very quick solution, but I am curious as to whether anyone has a method for proving this without making use of the Taylor Series Expansion. I feel as though doing so must be possible using some number theory, but am low on ideas as to an alternative approach to the question.\n\nNote: \nIf anyone is interested in my solution using Taylor Series Expansion (although it is not the focus of my question), here it is : \n\nFrom \n  $$ \\sin x = x - \\frac{x^3}{3!} + \\frac{x^5}{5!} -  \\dots$$\n  We see that \n  $$ \\alpha = \\sin 1 = 1 - \\frac{1^3}{3!} + \\frac{1^5}{5!} -  \\dots\n$$\n  Given integers $a$ and $b$, if $\\alpha = \\frac{a}{b}$ then it follows that $b!\\alpha \\in \\mathbb{Z}$, and $b!\\alpha = C + D$ where $C \\in \\mathbb{Z}$ and we have :\n  $$\nD = \\begin{cases}\n\\pm(\\frac{1}{b+1} - \\frac{1}{(b+1)(b+2)(b+3)} + \\dots) \\text{    if $b$ is even}\\\\\n\\pm(\\frac{1}{(b+1)(b+2)} - \\frac{1}{(b+1)\\dots(b+4)} + \\dots ) \\text{ if $b$ is odd}\\\\\n\\end{cases} $$\n  In each case we can see that $0 < D < 1$, giving us a contradiction. \n  Thus, we have that $\\sin 1 $ is irrational. \n  $$ \\blacksquare $$\n\n",
    "proof": "An overkill proof.\nBy the Lindemann--Weierstrass Theorem, $e^i$ is transcendental.  I can get $e^i$ from $\\sin(1) = (e^i-e^{-i})/(2i)$ by solving a quadratic equation.  Thus, if $\\sin(1)$ were algebraic, then also $e^i$ would be algebraic.\n",
    "tags": [
      "number-theory",
      "proof-writing"
    ],
    "score": 6,
    "answer_score": 9,
    "is_accepted": true,
    "question_id": 1124435,
    "answer_id": 1124449
  },
  {
    "theorem": "How to prove $4(n!)&gt;2^{n+2}$ for $ n\\geq 4$ with induction",
    "context": "I've done the base step, but how do I prove it is true for $n+1$ without using a fallacy?\n$$4(n!)>2^{n+2}\\quad \\text{for } n\\geq 4$$\nPlease help.\n",
    "proof": "As AWertheim correctly noted, the claim you are asked to prove:\n\n$$4(n!)>2^{n+2}\\quad \\text{for } n\\geq 4$$\n\nis equivalent to proving: \n\n$$ n! > 2^{n}\\quad \\text{for } n\\geq 4\\tag{1}$$  \n\n$$\\text{since}\\quad 4(n!) > 2^{n+2} \\iff 2^2\\cdot n! > 2^2 \\cdot 2^n   \\iff n! > 2^n\\quad \\text{for } n\\geq 4$$\n\n$(1): P(n) \\quad  n! > 2^n\\quad \\text{for } n\\geq 4$\n$P(4)$ I'll assume you've shown this to be true for the base case: $ 4! = 24 > 16 = 2^4 \\quad \\large \\checkmark$\nAssume the inductive hypothesis is true. $P(k):\\quad$ There exist a $k \\in \\mathbb N, \\;\\; k\\geq 4$ such that $\\;k! > 2^k$. \nWe need to show that $P(k+1)$ is true: $\\quad(k+1)! > 2^{k+1}$. \n$$(k+1)! \\;\\; = \\;\\; \\underbrace{(k+1)\\,k! \\;\\;>\\;\\; \\color{blue}{\\bf (k+1)}2^{k}}_{P(k)}\\;\\; > \\;\\;\\color{blue}{\\bf 2}\\cdot2^{k} \\;\\;= \\;\\;2^{k+1}$$ \nas desired. Note that we make use of the fact that $\\color{blue}{\\bf k+1 > 2}\\;$ for all $\\,k\\geq 4$.\n",
    "tags": [
      "proof-writing",
      "induction"
    ],
    "score": 6,
    "answer_score": 4,
    "is_accepted": false,
    "question_id": 409609,
    "answer_id": 409654
  },
  {
    "theorem": "How to show that $f(x)=|x|/x$ does not have any limit as $x\\to0$?",
    "context": "\n$f(x)$ does not converge to any $L$ as $x\\to a$ if for every $L$ there is $\\epsilon>0$ such that for all $\\delta>0$ there is $x$ such that $0<|x-a|<\\delta$ and $|f(x)-L|\\geq\\epsilon$.\n\nI wish to prove that\n$$\nf(x)=\\frac{|x|}{x}=\\begin{cases}\n1,&x>0\\\\\n-1,&x<0\n\\end{cases}\n$$\ndoes not converge to any $L$ as $x\\to0$ using the above definition.\nThis is what I did: Fix $L$ and take $\\epsilon=\\frac{1}{2}$. For any $\\delta>0$ there is $x$ such that $0<|x-0|<\\delta$ and\n$$\n|f(x)-L|=\\begin{cases}\n|1-L|,&x>0\\\\\n|-1-L|,&x<0\n\\end{cases}=\n\\begin{cases}\n|1-L|,&x>0\\\\\n|1+L|,&x<0\n\\end{cases}\n$$\nbut I am not sure how I should show that $|f(x)-L|\\geq\\frac{1}{2}=\\epsilon$.\n",
    "proof": "If $L \\ge 0$ we can choose $x\\in(-\\delta/2,0)$ so that $|f(x)-L| = |-1-L| = L+1 > 1/2$\nIf $L < 0$ we can choose $x \\in (0,\\delta/2)$ so that $|f(x)-L| = |1-L| > |1-0| = 1 > 1/2$\n",
    "tags": [
      "real-analysis",
      "limits",
      "proof-writing"
    ],
    "score": 6,
    "answer_score": 3,
    "is_accepted": true,
    "question_id": 2370827,
    "answer_id": 2370835
  },
  {
    "theorem": "Are there any invalid $S5$-formulas $\\psi$ such that $\\diamond\\psi$ is valid?",
    "context": "I cannot find an example for an invalid $S5$ formula $\\psi$ (i.e. $\\nvDash\\psi$), such that $\\diamond\\psi$ is valid (i.e. $\\vDash\\diamond\\psi$).\nIf there is none, then $\\vDash\\diamond\\psi\\Rightarrow\\,\\vDash\\psi$ is the case, but I cannot find a proof for that.\nSo far, I only managed to prove it for propositional $\\psi$ (i.e. $\\psi$ that contain no $\\diamond$ or $\\square$), based on the idea that when $\\psi$ is propositional and $S5$-valid, then for a set of worlds that contains only one element, $\\psi$ is true, and since it is independent of other worlds (because it is propositional), it is also true for every set of worlds.\nDo you know an example or can you prove that no such example exists in the general case?\n",
    "proof": "Consider $\\psi = (P \\lor \\square \\lnot P)$. \n$\\psi$ is not valid in $S5$, since it's possible to have a world satisfying both $\\lnot P$ and $\\lozenge P$. \nBut $\\lozenge \\psi$ is valid in $S5$. Indeed, consider any world $w$ in a Kripke model of $S5$ (actually, all we need is that the accessibility relation $R$ is reflexive). Case 1: $P$ holds at some world $v$ accessible from $w$. Then $\\lozenge \\psi$ holds at $w$, since $\\psi$ holds at $v$. Case 2: $\\lnot P$ holds at every world accessible from $w$, i.e. $\\square \\lnot P$ holds at $w$. Then $\\lozenge \\psi$ holds at $w$, since $\\psi$ holds at $w$ and $R$ is reflexive. \n",
    "tags": [
      "logic",
      "proof-writing",
      "proof-theory",
      "modal-logic",
      "kripke-models"
    ],
    "score": 6,
    "answer_score": 9,
    "is_accepted": true,
    "question_id": 2267726,
    "answer_id": 2267896
  },
  {
    "theorem": "Prove $(3x^2+3) \\geq (x+1)^2+1$",
    "context": "$(3x^2+3) \\geq (x+1)^2+1$\nI tried using a direct proof but I think I got stumped along the way. \n$3x^2+3 \\geq x^2+2x+2$\n$2x^2+1 \\geq 2x$\n$2(x^2) +1 \\geq 2x$\n$x^2 + (1/2) \\geq x$\nHow can I make this appear more clear? I don't think this is evident that it is true.\n",
    "proof": "Try completing the square:\n\\begin{align*}\nx^2 + 1/2 \\geq x\n&\\iff x^2 - x + 1/2 \\geq 0 \\\\\n&\\iff (x^2 - x + 1/4) - 1/4 + 1/2 \\geq 0 \\\\\n&\\iff (x - 1/2)^2 + 1/4 \\geq 0 \\\\\n\\end{align*}\nwhich is always true for any $x \\in \\mathbb R$. So the original inequality must also always be true for all $x \\in \\mathbb R$.\n",
    "tags": [
      "proof-writing",
      "proof-verification"
    ],
    "score": 6,
    "answer_score": 8,
    "is_accepted": true,
    "question_id": 570872,
    "answer_id": 570873
  },
  {
    "theorem": "Suppose $R$ and $S$ are transitive relations on $A$. Prove that if $S \\circ R \\subseteq R \\circ S$ then $R \\circ S$ is transitive.",
    "context": "Suppose $R$ and $S$ are transitive relations on $A$. Prove that if $S \\circ R \\subseteq R \\circ S$ then $R \\circ S$ is transitive. \nFirst, I'm wondering if my proof is correct? Second, I'm really curious as to if there are any more elegant ways of proving this statement? It took me a long time of playing around with different possibilities to find this argument. Are there any shortcuts I am missing, or any related theorems about relations that people find are helpful when trying to prove statements like the one above? Here is my proof: \nSuppose $S \\circ R \\subseteq R \\circ S$. Let $x,y,z \\in A$. Suppose $(x,y) \\in R \\circ S$ and $(y,z) \\in R \\circ S$. Since $(x,y) \\in R \\circ S$, we can choose some $a \\in A$ such that $(x,a) \\in S$ and $(a,y) \\in R$. Similarly, since $(y,z) \\in R \\circ S$, we can choose some $b \\in B$ such that $(y,b) \\in S$ and $(b,z) \\in R$. We have $(a,y) \\in R$ and $(y,b) \\in S$, so $(a,b) \\in S \\circ R$. Since $S \\circ R \\subseteq R \\circ S$, $(a,b) \\in R \\circ S$, so we can choose some $c \\in A$ such that $(a,c) \\in S$ and $(c,b) \\in R$. We have $(x,a) \\in S$ and $(a,c) \\in S$, so since $S$ is transitive, $(x,c) \\in S$. We also have $(c,b) \\in R$ and $(b,z) \\in R$, so since $R$ is transitive, $(c,z) \\in R$. Hence, $(x,c) \\in S$ and $(c,z) \\in R$, so $(x,z) \\in R \\circ S$. \n",
    "proof": "Your proof is OK. But there is a more concise way to say the same thing:\n$$\n(R \\circ S) \\circ (R \\circ S) = R \\circ (S \\circ R) \\circ S \\subseteq\nR \\circ (R \\circ S) \\circ S = (R \\circ R) \\circ (S \\circ S) \\subseteq R \\circ S.\n$$\nOf course, we use several things here implicitly. First, we use the associativity of $\\circ$, i.e. that $(X \\circ Y) \\circ Z = X \\circ (Y \\circ Z)$. Second, we use its \"monotonicity\", i.e. that if $X \\subseteq X'$ and $Y \\subseteq Y'$, then $X \\circ Y \\subseteq X' \\circ Y'$.\n",
    "tags": [
      "elementary-set-theory",
      "proof-writing",
      "relations",
      "function-and-relation-composition"
    ],
    "score": 6,
    "answer_score": 9,
    "is_accepted": true,
    "question_id": 373200,
    "answer_id": 373204
  },
  {
    "theorem": "Proving the sum of the first $n$ natural numbers by induction",
    "context": "I am currently studying proving by induction but I am faced with a problem.\nI need to solve by induction the following question.\n$$1+2+3+\\ldots+n=\\frac{1}{2}n(n+1)$$    \nfor all $n > 1$.\nAny help on how to solve this would be appreciated.\n\nThis is what I have done so far.\nShow truth for $N = 1$ \nLeft Hand Side = 1\nRight Hand Side = $\\frac{1}{2} (1) (1+1) = 1$\nSuppose truth for $N = k$\n$$1 + 2 + 3 + ... + k = \\frac{1}{2} k(k+1)$$\nProof that the equation is true for $N = k + 1$\n$$1 + 2 + 3 + ... + k + (k + 1)$$\nWhich is Equal To\n$$\\frac{1}{2} k (k + 1) + (k + 1)$$\nThis is where I'm stuck, I don't know what else to do. The answer should be:\n$$\\frac{1}{2} (k+1) (k+1+1)$$\nWhich is equal to:\n$$\\frac{1}{2} (k+1) (k+2)$$\nRight?\nBy the way sorry about the formatting, I'm still new.\n",
    "proof": "Basic algebra is what's causing the problems: you reached the point\n$$\\frac{1}{2}K\\color{red}{(K+1)}+\\color{red}{(K+1)}\\;\\;\\;\\:(**)$$\nNow just factor out the red terms:\n$$(**)\\;\\;\\;=\\color{red}{(K+1)}\\left(\\frac{1}{2}K+1\\right)=\\color{red}{(K+1)}\\left(\\frac{K+2}{2}\\right)=\\frac{1}{2}(K+1)(K+2)$$\n",
    "tags": [
      "discrete-mathematics",
      "summation",
      "proof-writing",
      "induction"
    ],
    "score": 6,
    "answer_score": 5,
    "is_accepted": true,
    "question_id": 292423,
    "answer_id": 292499
  },
  {
    "theorem": "Can you prove why Popsicle Stick Multiplication works?",
    "context": "This is a unique way of multiplying numbers by using sticks.  Let's call it \"Popsicle Stick Multiplication\".  Or maybe \"Linear Algebra\" quite literally.  Take a look at both images that I've drawn below.  Can someone explain why this works out?  Notice how you must carry the 10's digit when the sum of the intersections is greater than 9, and if you have more than 99, then carry the hundreds digit just like any other multiplication problem.  I'm not sure if it's faster to do it the long way or to do it this way.  Perhaps this is the way the Egyptians did it while building the great pyramids?  This works for multiplying any amount of sticks.\n\n\n",
    "proof": "$$\\begin{matrix}\n& 1 & 2 \\\\\n\\times & 3 & 4 \\\\\n\\hline\n& 4 & 8 \\\\\n3 & 6 \\\\\n\\hline\n3 & 10 & 8 \\\\\n\\hline\n4 & 0 & 8\n\\end{matrix}$$\nIf you tilt your head a little it's exactly like popsicle stick multiplication. You're just doing the carries later rather than sooner, compared to the usual way to do long multiplication.\nUpdate:\n$$\\begin{matrix}\n& & & 4 & 1 & 2 \\\\\n\\times & & 1 & 2 & 3 & 1 \\\\\n\\hline\n& & & 4 & 1 & 2 \\\\\n& & 12 & 3 & 6 \\\\\n& 8 & 2 & 4 \\\\\n4 & 1 & 2 \\\\\n\\hline\n4 & 9 & 16 & 11 & 7 & 2 \\\\\n\\hline\n5 & 0 & 7 & 1 & 7 & 2\n\\end{matrix}$$\n",
    "tags": [
      "arithmetic",
      "proof-writing"
    ],
    "score": 6,
    "answer_score": 6,
    "is_accepted": true,
    "question_id": 279093,
    "answer_id": 279099
  },
  {
    "theorem": "prove $\\lceil{x}\\rceil=-\\lfloor-x\\rfloor$",
    "context": "i am trying to prove that $\\lceil{x}\\rceil=-\\lfloor-x\\rfloor$, but having difficulties to prove. the definitions are: \n$\\lceil{x}\\rceil:=m-1<x\\leq m$ and $\\lfloor{x}\\rfloor:=n\\leq x<n+1$.  \nhow can i prove the statement above, if needed, by using the definitions? i dont know how to change the inequalities when i negate the $x$ back and forth. i am thankful for any hand for help\n",
    "proof": "If $m < x \\leq m+1$, where $m \\in \\mathbb{Z}$, then $\\lceil x \\rceil = m+1$. Now $-m-1 \\leq -x <-m$.\nHence, $\\lfloor -x \\rfloor = -m-1$.\nHence, $-\\lfloor -x \\rfloor = m+1 = \\lceil x \\rceil$\n",
    "tags": [
      "proof-writing",
      "problem-solving"
    ],
    "score": 6,
    "answer_score": 5,
    "is_accepted": true,
    "question_id": 264744,
    "answer_id": 264748
  },
  {
    "theorem": "Is this equivalence of the compactness theorem true?",
    "context": "In my mathematical logic class the teacher asked us the following question: ¿is the compactness theorem equivalent to: let $\\Gamma$ be a set of propositional formulas, and let $\\sigma$ be a formula. If $\\Gamma \\models \\sigma$  then there is finite ${ \\Gamma  }_{ 0 }\\subseteq \\Gamma $ such that ${ \\Gamma  }_{ 0 }\\models \\sigma $?, I have tried several times but can't find the correct way to prove it, I appreciate any help.\nThe statement my teacher gave was: $\\Gamma$ is finitely satisfiable if and only if $\\Gamma$ is satisfiable.\n",
    "proof": "Yes, the two statements below are equivalent. Let us prove it. Let $\\Gamma$ be a set of formulas.\n\nFor any  formula $\\sigma$,  $\\Gamma \\models \\sigma$  if and only if there is a finite $\\Gamma_0 \\subseteq \\Gamma $ such that $\\Gamma_0 \\models \\sigma $;\n$\\Gamma$ is finitely satisfiable if and only if $\\Gamma$ is satisfiable.\n\nProof that 1. implies 2. We suppose that 1. holds and we prove that the equivalence in 2. holds\n\n$\\Rightarrow$: Assume that $\\Gamma$ is finitely satisfiable. Then, for every finite $\\Gamma_0 \\subseteq \\Gamma$, there is a structure that satisfies $\\Gamma_0$. Let $\\sigma$ be a contradiction formula (i.e. a formula that is not satisfied by any structure, for instance $\\bot$ or $\\alpha \\land \\lnot \\alpha$). So, for every finite $\\Gamma_0 \\subseteq \\Gamma$, there is a structure that satisfies $\\Gamma_0$ and not $\\sigma$, i.e. $\\Gamma_0 \\not\\models \\sigma$.\nBy 1. (direction left-to-right), $\\Gamma \\not\\models \\sigma$ and hence there exists a structure that satisfies $\\Gamma$ but not $ \\sigma$.\nIn particular, $\\Gamma$ is satisfiable.\n\n$\\Leftarrow$:  Assume that $\\Gamma$ is satisfiable. Then, there exists a structure that satisfies all the formulas in $\\Gamma$. In particular, it satisfies every finite $\\Gamma_0 \\subseteq \\Gamma$. Therefore, $\\Gamma$ is finitely satisfiable. (Note that here we do not use the hypothesis that 1. holds)\n\n\nProof that 2. implies 1. We suppose that 2. holds and we prove that the equivalence in 1. holds. Let $\\sigma$ be a formula.\n\n$\\Rightarrow$: Assume that $\\Gamma \\models \\sigma$. Then, every structure that satisfies $\\Gamma$ does not satisfies $\\lnot \\sigma$, i.e. $\\Gamma \\cup \\{\\lnot \\sigma\\}$ is unsatisfiable. By 2. (direction left-to-right), $\\Gamma \\cup \\{\\lnot \\sigma\\}$ is not finitely satisfiable, hence there is a finite $\\Gamma_0 \\cup \\{\\lnot \\sigma\\} \\subseteq \\Gamma \\cup \\{\\lnot \\sigma\\}$ that is unsatisfiable. Therefore, for some  finite $\\Gamma_0 \\cup \\{\\lnot \\sigma\\} \\subseteq \\Gamma \\cup \\{\\lnot \\sigma\\}$, every structure that satisfies $\\Gamma_0$ satisfies $\\sigma$ too, i.e. $\\Gamma_0 \\models \\sigma$.\n\n$\\Leftarrow$: Assume that $\\Gamma_0 \\models \\sigma$ for some finite $\\Gamma_0 \\subseteq \\Gamma$. Then,  every structure that satisfies $\\Gamma$ in particular satisfies $\\Gamma_0$ and hence $\\sigma$, since $\\Gamma_0 \\models \\sigma$. Therefore, $\\Gamma \\models \\sigma$. (Note that here we do not use the hypothesis that 2. holds)\n\n\n\nIn both statements 1. and 2., the interesting part is the left-to right direction. Indeed, in statement 1., the right-to-left direction is trivial and holds independently from the hypothesis that 2. holds.  And similarly, in statement 2., the right-to-left direction is trivial and holds independently from the hypothesis that 1. holds.\nThe left-to-right direction of the compactness theorem (in both formulations 1. and 2.) has the following logical form, at the meta-level:\n$$\\tag{*}\\forall \\exists \\implies \\exists \\forall$$\nWhat do I mean? Take the left-to-right direction of statement 2. By making explicit the meaning of \"satisfiable\" and \"finitely satisfiable\", it says: if for all finite subsets of $\\Gamma$ there exists a structure that satisfies it, then there is a structure that satisfies all formulas in $\\Gamma$. This is surprinsing, because $\\Gamma$ can be an infinite set of formulas and so compactness theorem says that we can somehow transfer a property (satisfiability) from finite to infinite.\nUsually, implications of the form $(*)$ does not hold. Think of natural numbers: the fact that for every natural number $n$ there is a natural number  greater than  $n$, does not imply that there is a natural number  greater all other natural numbers.\nCompactness theorem says that when we talk about satisfiability of sets of formulas, the implication $(*)$ holds, even when the set $\\Gamma$ of formulas is infinite.\nThe other direction (the right-to-left one) is of the form\n$$\\exists \\forall \\implies \\forall \\exists$$\nwhich is obvious and indeed it corresponds to the less interesting part of statements 1. and 2.\n",
    "tags": [
      "logic",
      "proof-writing",
      "predicate-logic",
      "model-theory"
    ],
    "score": 6,
    "answer_score": 5,
    "is_accepted": true,
    "question_id": 3912146,
    "answer_id": 3912537
  },
  {
    "theorem": "Show that the inequality $\\left|\\int_{0}^{1} f(x)\\,dx\\right| \\leq \\frac{1}{12}$ holds for certain initial conditions",
    "context": "\nGiven that a function $f$ has a continuous second derivative on the interval $[0,1]$,  $f(0)=f(1)=0$, and $|f''(x)|\\leq 1$, show that $$\\left|\\int_{0}^{1}f(x)\\,dx\\right|\\leq \\frac{1}{12}\\,.$$\n\nMy attempt: This looks to be a maximization/minimization problem. Since the largest value $f''(x)$ can take on is $1$, then the first case will be to assume $f''(x)=1$. This is because it is the maximum concavity and covers the most amount of area from $[0,1]$ while still maintaining the given conditions.\nEdit: Because of the MVT and Rolle's Theorem, there exists extrema on the interval $[0,1]$ satisfying $f'(c)=0$ for some $c\\in[0,1]$. These extrema could occur at endpoints.\nThen $f'(x)=x+b$ and $f(x)=\\frac{x^2}{2}+bx+c$. Since $f(0)=0$, then $c=0$ and $f(1)=0$, then $b=-\\frac{1}{2}$. Remark: Any function with a continuous, constant second derivative will be of the form $ax^2+bx+c$ and in this case, $a=-b$ and $c=0$. Now,  $$\\begin{align*}\\int_{0}^{1}f(x)\\,dx&=\\frac{1}{2}\\int_{0}^{1}(x^2-x)\\,dx\\\\&=\\frac{1}{2}\\bigg[\\frac{x^3}{3}-\\frac{x^2}{2}\\bigg]_{x=0}^{x=1}\\\\&=-\\frac{1}{12}\\end{align*}$$\nNext, we assume that $f''(x)=-1$ and repeating the process yields $$ \\begin{align*}\\int_{0}^{1}f(x)\\,dx&=\\frac{1}{2}\\int_{0}^{1}(-x^2+x)\\,dx\\\\&=\\frac{1}{2}\\bigg[\\frac{-x^3}{3}+\\frac{x^2}{2}\\bigg]_{x=0}^{x=1}\\\\&=\\frac{1}{12}\\end{align*}$$ Thus we have shown that at the upper and lower bounds for $f''(x)$ that $\\frac{-1}{12}\\leq\\int_{0}^{1}f(x)\\,dx\\leq \\frac{1}{12} \n \\Longleftrightarrow \\left|\\int_{0}^{1}f(x)\\,dx\\right|\\leq\\frac{1}{12}$ because $f''(x)$ is continuous on $[0,1]$.\nI was wondering if this was 'rigorous' enough to be considered a full proof and solution to the problem.\n",
    "proof": "Consider the following integral:\n$$\\int_{0}^{1}\\left(\\frac{x^{2}}{2}-\\frac{x}{2}\\right)f^{\\prime\\prime}(x)\\, dx. $$\nBy integrating by parts twice, you get\n$$\\int_{0}^{1}\\left(\\frac{x^{2}}{2}-\\frac{x}{2}\\right)f^{\\prime\\prime}(x)\\, dx = \\underbrace{\\left(\\frac{x^{2}}{2}-\\frac{x}{2}\\right)f'(x)\\bigg|_0^1}_{0} - \\int_0^1\\bigg(x-\\frac{1}{2}\\bigg)f'(x)dx=$$$$= - \\int_0^1\\bigg(x-\\frac{1}{2}\\bigg)f'(x)dx= \\underbrace{- \\bigg(x-\\frac{1}{2}\\bigg)f(x)\\bigg|_0^1}_{0} + \\int_0^1f(x)dx$$\nTherefore,\n$$\\boxed{\\int_{0}^{1}f(x)\\, dx = \\int_{0}^{1}\\left(\\frac{x^{2}}{2}-\\frac{x}{2}\\right)f^{\\prime\\prime}(x)\\, dx}$$\nNow use the following inequality:\n$$\\left|\\int_{a}^{b}f(x)g(x)\\,dx\\right| \\leq \\int_{a}^{b}|f(x)||g(x)|\\, dx$$\nSince $g(x)=\\frac{x^{2}}{2}-\\frac{x}{2}$\nis the expression you got, this should yield the desired result.\n$$\\left|\\int_0^1 f(x)\\,dx\\right|=\\left| \\int_{0}^{1}\\left(\\frac{x^{2}}{2}-\\frac{x}{2}\\right)f^{\\prime\\prime}(x)\\, dx\\right|\\le\\frac{1}{2}\\int_{0}^{1}|x^2-x|\\,dx=\\frac{1}{12}$$\n",
    "tags": [
      "real-analysis",
      "definite-integrals",
      "proof-writing",
      "solution-verification",
      "integral-inequality"
    ],
    "score": 6,
    "answer_score": 8,
    "is_accepted": true,
    "question_id": 3777387,
    "answer_id": 3777413
  },
  {
    "theorem": "How to prove that expessions like $\\sqrt{93+63\\sqrt{85}} - \\sqrt{143} \\notin \\Bbb{Z}$?",
    "context": "The Problem:\nThere are multiple \"rooty\" equations that can be simplified to a whole number, for example:\n$$\\sqrt{19 + 6\\sqrt{2}} - \\sqrt{18} = 1$$\nBecause:\n$$\\sqrt{19 + 6\\sqrt{2}} - \\sqrt{18} = \\sqrt{18+\\sqrt{72} + 1} - \\sqrt{18} = \\\\ = \\sqrt{18+2\\sqrt{18}+1} - \\sqrt{18} = \\sqrt{(\\sqrt{18}+1)^2}-\\sqrt{18} = \\\\ = \\sqrt{18}+1-\\sqrt{18} = 1$$\nHowever, in the title, we have the expression of:\n$$\\sqrt{93+63\\sqrt{85}} - \\sqrt{143} \\approx 14$$\nAnd using a scientific calculator you indeed get $14$ as the answer. But using a high precision online calculator you get the true answer of:\n$$\\sqrt{93+63\\sqrt{85}} - \\sqrt{143} = 14.00000000005032...$$\nThe Question:\nIs there a general way to prove that a \"rooty\" expression (like the one in the title) $\\notin \\Bbb{Z}$? Even if the expression is really close to a whole number, and even high precision calculators can't give you the correct answer?\nIs there a general procedure or algorithm which tells you for sure that the number is or isn't $\\in \\Bbb{Z}$?\n",
    "proof": "Let $K_0:=\\Bbb{Q}$ and for $n\\geq0$ define the field $K_{n+1}$ as\n$$K_{n+1}:=K_n(\\{\\sqrt{x}:\\ x\\in K_n\\})=K_n^{1/2},$$\nand set $K:=\\bigcup_{n\\geq0}K_n$. Let $R\\in K$ be a rooty expression, and let $n\\in\\Bbb{N}$ be such that $R\\in K_n$. Let $x\\in K_{n-1}$ be non-square. Then the map\n$$\\sigma_x:\\ K_n\\ \\longrightarrow\\ K_n:\\ \\sqrt{x}\\ \\longmapsto\\ -\\sqrt{x},$$\nextends to a $K_{n-1}$-linear automorphism of $K_n$ that fixes $K_{n-1}$ pointwise. So if $\\sigma_x(R)\\neq R$ then $R\\notin\\Bbb{Z}$. On the other hand, if $\\sigma_x(R)=R$ then $R\\in K_{n-1}$, and we can repeat the same for some $x'\\in K_{n-1}$ that is not a square. Eventually this will yield the minimal $n\\in\\Bbb{N}$ such that $R\\in K_n$.\nFor example, for $R_1:=\\sqrt{19 + 6\\sqrt{2}} - \\sqrt{18}$ we have $R_1\\in K_2$ and we would like to check whether $x:=19+6\\sqrt{2}$ is a square in $K_1$ (clearly $18$ is a square in $K_1$ because $18\\in K_0$). If it is, then it is a square in $\\Bbb{Q}(\\sqrt{2})$, and\n$$19+6\\sqrt{2}=(a+b\\sqrt{2})^2=(a^2+2b^2)+2ab\\sqrt{2},$$\nquickly shows that $(a,b)=(1,3)$ is a solution. Hence\n$$R_1=\\sqrt{19 + 6\\sqrt{2}} - \\sqrt{18}=1+3\\sqrt{2}-3\\sqrt{2}=1.$$\nWe can try the same for $R_2:=\\sqrt{93+63\\sqrt{85}} - \\sqrt{143}\\in K_2$. We would like to check whether $x:=93+63\\sqrt{85}$ is a square in $K_1$. If it is, then it is a square in $\\Bbb{Q}(\\sqrt{85})$, but\n$$93+63\\sqrt{85}=(a+b\\sqrt{85})^2=(a^2+85b^2)+2ab\\sqrt{85},$$\nis quickly checked to have no rational solutions. Then the automorphism of $K_2$ defined by\n$$\\varepsilon_x:\\ K_2\\ \\longrightarrow\\ K_2:\n\\ \\sqrt{93+63\\sqrt{85}}\\ \\longmapsto\\ -\\sqrt{93+63\\sqrt{85}},$$\nfixes $K_1$ pointwise, and hence\n$$\\sigma_x(R_2)=-\\sqrt{93+63\\sqrt{85}}+\\sqrt{143}\\neq R_2,$$\nso $R_2\\notin K_1$. In particular $R_2$ is not an integer.\n",
    "tags": [
      "algebra-precalculus",
      "proof-writing",
      "algorithms",
      "roots",
      "rounding-error"
    ],
    "score": 6,
    "answer_score": 1,
    "is_accepted": true,
    "question_id": 3169255,
    "answer_id": 3169281
  },
  {
    "theorem": "Why is $x^n\\approx \\left(n(x^{1/4096}-1)+1\\right)^{4096}$?",
    "context": "There's an old-school pocket calculator trick to calculate $x^n$ on a pocket calculator, where both, $x$ and $n$ are real numbers. So, things like $\\,0.751^{3.2131}$ can be calculated, which is awesome.\nThis provides endless possibilities, including calculating nth roots on a simple pocket calculator.\nThe trick goes like this:\n\nType $x$ in the calculator\nTake the square root twelve times\nSubtract one\nMultiply by $n$\nAdd one\nRaise the number to the 2nd power twelve times (press * and = key eleven times)\n\nExample:\nI want to calculate $\\sqrt[3]{20}$ which is the same as $20^{1/3}$. So $x=20$ and $n=0.3333333$. After each of the six steps, the display on the calculator will look like this:\n\n$\\;\\;\\;20$\n$\\;\\;\\;1.0007315$\n$\\;\\;\\;0.0007315$\n$\\;\\;\\;0.0002438$\n$\\;\\;\\;1.0002438$\n$\\;\\;\\;2.7136203$\n\nThe actual answer is $20^{1/3}\\approx2.7144176$. So, our trick worked to three significant figures. It's not perfect, because of the errors accumulated from the calculator's 8 digit limit, but it's good enough for most situations.\n\nQuestion:\nSo the question is now, why does this trick work? More specifically, how do we prove that:\n$$x^n\\approx \\Big(n(x^{1/4096}-1)+1\\Big)^{4096}$$\n\nNote: $4096=2^{12}$.\nI sat in front of a piece of paper trying to manipulate the expression in different ways, but got nowhere.\nI also noticed that if we take the square root in step 1 more than twelve times, but on a better-precision calculator, and respectively square the number more than twelve times in the sixth step, then the result tends to the actual value we are trying to get, i.e.:\n\n$$\\lim_{a\\to\\infty}\\Big(n(x^{1/2^a}-1)+1\\Big)^{(2^a)}=x^n$$\n\nThis, of course doesn't mean that doing this more times is encouraged on a pocket calculator, because the error from the limited precision propagates with every operation. $a=12$ is found to be the optimal value for most calculations of this type i.e. the best possible answer taking all errors into consideration. Even though 12 is the optimal value on a pocket calculator, taking the limit with $a\\to\\infty$ can be useful in proving why this trick works, however I still can't of think a formal proof for this.\nThank you for your time :)\n",
    "proof": "A standard trick is to calculate the natural logarithm first to get the exponent under control:\n$$\\log(\\lim_{a\\to\\infty}(n(x^{1/a}-1)+1)^a)=\\lim_{a\\to\\infty}a\\log(nx^{1/a}-n+1)$$\nSet $u=1/a$.\nWe get\n$$\\lim_{u\\to 0}\\frac{\\log (nx^u-n+1)}{u}$$\nUse L'Hopital:\n$$\\lim_{u\\to 0}\\frac{nx^u\\log x}{nx^u-n+1}=n\\log x=\\log x^n$$\nHere we just plugged in $u=0$ to calculate the limit!\nSo the original limit goes to $x^n$ as desired.\n",
    "tags": [
      "proof-writing",
      "recreational-mathematics",
      "approximation"
    ],
    "score": 6,
    "answer_score": 3,
    "is_accepted": true,
    "question_id": 2908186,
    "answer_id": 2908200
  },
  {
    "theorem": "Relation of trace and determinant",
    "context": "\nLet $A$ be a $3\\times3$ non-diagonal matrix with $A=A^{-1}$. Prove that $\\det A = \\operatorname{tr} A =\\pm1$\n\n\nI have already proved $\\det A = \\pm1$, but I have no idea about how to proceed with $\\operatorname{tr} A$. Thank you.\n",
    "proof": "Since\n$A = A^{-1}, \\tag 1$\nit follows that\n$A^2 = I, \\tag 2$\nor\n$A^2 - I = 0; \\tag 3$\nif $\\lambda$ is an eigenvector of $A$ with eigenvector $v$, $Av = \\lambda v$ with $v \\ne 0$, we have\n$(\\lambda^2 - 1)v = \\lambda^2 v - v = A^2 v - Iv = (A^2 - I)v = 0, \\tag 4$\nso $v \\ne 0$ yields\n$\\lambda^2 - 1 = 0; \\tag 5$\nit follows that every eigenvalue of $A$ is $\\pm 1$.  Since\n$A^2 = I, \\tag 6$\nwe also have\n$(\\det A)^2 = \\det A^2 = \\det I = 1, \\tag 7$\nso\n$\\det A = \\pm 1. \\tag 8$\nWe next invoke the hypothesis that $A$ is not a diagonal matrix to show that the eigenvalues of $A$ cannot all be the same; this will rule out the cases in which the eigenvalues are $1, 1, 1$ or $-1, -1, -1$.  In so doing we will in fact formally validate copper.hat's statement, made without proof in his answer, that \"If all eigenvalues were the same then $A$ would be $\\pm I$,\" and also lhf's assertion that \" the minimal polynomial is $(X-1)(X+1)$ because $A$ is not diagonal\"; here we will show why both these affirmations are in fact correct.  \nSo let $\\lambda = \\pm 1$, and assume the eigenvalues of $A$ are all $\\lambda$.  Consider then the characteristic polynomial of the matrix $A$, \n$p_A(x) = \\det (A - xI); \\tag 9$\nwe know that\n$p_A(\\lambda) = \\det(A - \\lambda I) = 0; \\tag{10}$\nin addition, the Cayley-Hamilton theorem gives us\n$p_A(A) = 0; \\tag{11}$\nwe also have that $p_A(x)$ splits into $3$ linear factors $x - \\lambda$, whence\n$p_A(x) = (x - \\lambda)(x - \\lambda)(x - \\lambda) = (x - \\lambda)^3; \\tag{12}$\ncombining (11) and (12) we find\n$(A - \\lambda I)^3 = p_A(A) = 0; \\tag{13}$\nif we set\n$N = A - \\lambda I, \\tag{14}$\nsince $A$ is not diagonal, it follows that\n$N \\ne 0; \\tag{15}$\nfrom (13), we see that $N$ is nilpotent:\n$N^3 = 0. \\tag{16}$\nWe do not have enough information to determine directly whether $N^2 = 0$ or not in the above, but we can prove that \n$N^2 = 0 \\Longrightarrow I, N \\; \\text{are linearly independent}; \\tag{17}$\n$N^2 \\ne 0 \\Longrightarrow I, N, N^2 \\; \\text{are linearly independent}; \\tag{18}$\nfor instance, to see that (18) holds we assume\n$\\alpha I + \\beta N + \\gamma N^2 = 0, \\tag{19}$\nand multiply by $N^2$,\n$\\alpha N^2 + \\beta N^3 + \\gamma N^4 = 0; \\tag {20}$\nnow using (16) we see that\n$\\alpha N^2 = 0 \\Longrightarrow \\alpha = 0, \\tag{21}$\nso that (19) becomes\n$\\beta N + \\gamma N^2 = 0, \\tag{22}$\nwhich if multiplied by $N$ yields\n$\\beta N^2 + \\gamma N^3 = \\beta N^2 = 0, \\tag{23}$\nwhence \n$\\beta = 0; \\tag{24}$\nnow inserting $\\alpha = \\beta = 0$ in (19) we see that\n$\\gamma = 0 \\tag{25}$\nas well; thus $I$, $N$, and $N^2$ are linearly independent as asserted.  In the case $N^2 = 0$ we start from the assumption that \n$\\alpha I + \\beta N = 0, \\tag{26}$\nmultiply by $N$,\n$\\alpha N = \\alpha N + \\beta N^2 = 0, \\tag{27}$\nwhich again implies $\\alpha = 0$; $\\beta = 0$ is then immeditate from (26).  So we see that (17)-(18) bind.\nWe return to (14) and write\n$A = \\lambda I + N; \\tag{28}$\nthen with (2) we have\n$I = A^2 = (\\lambda I + N)^2 = \\lambda^2 I^2 + 2\\lambda N + N^2 = I \\pm 2 N + N^2, \\tag{29}$\nusing the fact that $\\lambda = \\pm 1$.  (29) yields\n$\\pm N + N^2 = 0; \\tag{30}$\nif $N^2 = 0$, (30) asserts $N = 0$, which is clearly not possible; if $N^2 \\ne 0$, (30) contradicts the linear independence of $N$, $N^2$.  Since either of these conclusions follows directly from the hypothesis $A$ has three identical eigenvalues, we see that such an assumption is disallowed; thus, the eigenvalues of $A$ are either $1, 1, -1$ or $1, -1, -1$.  Int the former case, $\\text{Tr}(A) = 1$: in the latter, $\\text{Tr}(A) = -1$; in any case, we see that \n$\\text{Tr}(A) = \\pm 1 \\tag{31}$\nbinds.\nNote Added in Edit, Thursday 26 October 2017 2:36 PM PST:  The above argument reveals general principles which may be proved along lines similar to the above:  If $A$ is an $n \\times n$ non-diagonal matrix all of the eigenvalues $\\lambda$ of which are equal, then $N = A - \\lambda I \\ne 0$ is in fact a nilpotent matrix with $N^n = 0$; if $N$ also satisfies $N^m = 0$ with $1 < m \\le n$, but $A^l \\ne 0$ for $l < m$ (so that $m$ is the least positive integer for which $A^m = 0$), then the matrices $I, N, N^2, \\ldots, N^{m - 1}$ are in fact linearly independent.  Using such principles, it may be possible to address certain generalizaions of the present problem, such as the case $A^k = rI$ for non-diagonal $A$.  But I will leave such questions for the present time.  End of Note.\n",
    "tags": [
      "linear-algebra",
      "matrices",
      "proof-writing",
      "determinant",
      "trace"
    ],
    "score": 6,
    "answer_score": 3,
    "is_accepted": true,
    "question_id": 2487774,
    "answer_id": 2491468
  },
  {
    "theorem": "Prove that there are no positive integers $n&gt;1$ and $k&gt;1$ such that $n!=k^k$.",
    "context": "\nProve that there are no positive integers $n>1$ and $k>1$ such that $n!=k^k$.\n\nCan anybody help me?\nif $n=k$ then $n!\\neq k^k$,\nif $n<k$, then $n!\\neq k^k$,\nif $n\\geqslant2k$, then $n!\\neq k^k$,\nif $k<n<2k$ i don't know what to do. Maybe it is wrong way.\n",
    "proof": "Assume $n!=k^k$ with $n>1$.\nLet $p$ be a prime $\\le n$. Then $p\\mid n!$ implies $p\\mid k^k$, hence $p\\mid k$.\nSo let $p_1=2,p_2=3,\\ldots, p_r$ be all the primes $\\le n$.\nThen from the above, we have $p_1p_2\\cdots p_r\\mid k$ and in particular $k\\ge p_1p_2\\cdots p_r$.\nFrom Euklid's proof of the infinitude of primes, we remember that there exists a prime $q\\le p_1p_2\\cdots p_r+1$ that differs from all $p_i$. Thus in our situation we conclude $n<q$ and in particular $n\\le p_1p_2\\cdots p_r\\le k$.\nBut then $k^k\\ge n^n>n!$, contradiction.\n",
    "tags": [
      "elementary-number-theory",
      "proof-writing"
    ],
    "score": 6,
    "answer_score": 8,
    "is_accepted": true,
    "question_id": 2315270,
    "answer_id": 2315311
  },
  {
    "theorem": "Please critique my proof that $\\sqrt{12}$ is irrational",
    "context": "I would like critiques on correctness, conciseness, and clarity. Thanks!\nProposition: There is no rational number whose square is 12\nProof: Suppose there were such a number, $a = \\in \\mathbb{Q}$ s.t. $a^2 = 12$.\nThis implies that $\\exists$ $m, n \\in \\mathbb{Z}$ s.t. $\\frac{m^2}{n^2} = 12.$ Assume without loss of generality that $m,~ n$ have no factors in common.\n$\\Rightarrow m^2 = 12n^2$. \nThis implies that $m^2$ is even, and therefore that $m$ is even; it can thus be written $2k = m$ for some $k \\in \\mathbb{Z}$.\nThus $m^2 = 12n^2 $\n$\\Rightarrow 4k^2 = 12n^2 $\n$\\Rightarrow \\frac{k^2}{3} = n^2$\nBecause $n^2$ is an integer, it is clear that $3$ divides $k^2$ which imples that $k$ has $3$ or $\\frac{k}{n}$ has a factor (because $\\frac{k^2}{n^2}= 3$)\nSuppose that the former is true, and $3$ is a factor of $k$. Then $k = 3j$ for some integer j, which implies that $(3j)^2 = 3n^2$\n$\\Rightarrow 9j^2 = 3n^2 $\n$\\Rightarrow n^2 = 3j^2 $\n$\\Rightarrow n^2 = \\frac{k^2}{n^2}j^2$\n$\\Rightarrow k = \\frac{n^2}{j}$ but this implies that $j$ divides $n^2$, but $j$ divides $m$, and by initial assumption $n$ and $m$ have no factors in common, so this is a contradiction.\nSuppose now that $\\frac{k}{n}$ is a factor of k. Then $k = \\frac{k}{n}j$ for some integer $j$. Then $(\\frac{k}{n}j)^2 = 3n^2$ which implies that $3j^2 = 3n^2 \\Rightarrow j^2 = n^2 \\Rightarrow j = n$. But this means that $n$ divides $m$, which again is a contradiction. Thus any rational representation of the number whose square equals $12$ leads to a contradiction and this number must therefore have no rational representation.\n",
    "proof": "Proof.  \nAssume $\\sqrt{12} \\in \\mathbb{Q}$ is rational, then it can be written as $\\sqrt{12}=\\cfrac{m}{n}$ with $m,n \\in \\mathbb{Z}$ coprime.\nSquaring the equality gives $m^2 = 12 n^2 = 3 \\cdot 4 \\cdot n^2\\,$. Therefore $3 \\mid m^2 = m \\cdot m$ and, since $3$ is a prime, it follows by Euclid's Lemma that $3 \\mid m\\,$.\nThen $m = 3k$ for some $k \\in \\mathbb{Z}$ and substituting back gives $9 k^2 = 12 n^2 \\iff 3 k^2 = 4 n^2\\,$. Therefore $3 \\mid 4 n^2$ and, since $3 \\not \\mid 4$ it follows that $3 \\mid n^2$ then, again by Euclid's Lemma, $3 \\mid n\\,$.\nBut $3 \\mid m$ and $3 \\mid n$ contradicts the assumption that $m,n$ are coprime, so the premise that $\\sqrt{12} \\in \\mathbb{Q}$ must be false, therefore $\\sqrt{12}$ is irrational.\n\nCritique of the posted proof.\n\nProof: Suppose there were such a number, $a = \\in \\mathbb{Q}$ s.t. $a^2 = 12$.\nThis implies that $\\exists$ $m, n \\in \\mathbb{Z}$ s.t. $\\frac{m^2}{n^2} = 12.$ Assume without loss of generality that $m,~ n$ have no factors in common.\n$\\Rightarrow m^2 = 12n^2$. \n\nSo far so good.\n\nThis implies that $m^2$ is even, and therefore that $m$ is even;\n\nThe fact that $2 \\mid m^2 \\implies 2 \\mid m$ may sound obvious, but still needs some justification. You could argue by contradiction, or use Euclid's Lemma.\n\nit can thus be written $2k = m$ for some $k \\in \\mathbb{Z}$.\nThus $m^2 = 12n^2 $\n$\\Rightarrow 4k^2 = 12n^2 $\n\nCorrect. As an observation, $k^2 = 3 n^2$ just eliminated the perfect square factor of $4$ and reduced the problem to proving that $\\sqrt{3}$ is irrational.\n\n$\\Rightarrow \\frac{k^2}{3} = n^2$\nBecause $n^2$ is an integer, it is clear that $3$ divides $k^2$ which imples that $k$ has $3$\n\nYou should generally avoid fractions where they are not necessary. The previous line gave $k^2 = 3 n^2\\,$, which directly implies that $3 \\mid k^2\\,$.\n\nor $\\frac{k}{n}$ has a factor (because $\\frac{k^2}{n^2}= 3$)\n\nThis makes no sense, and it is in fact not needed to complete the proof.\n\nSuppose that the former is true, and $3$ is a factor of $k$. Then $k = 3j$ for some integer j, which implies that $(3j)^2 = 3n^2$\n$\\Rightarrow 9j^2 = 3n^2 $\n$\\Rightarrow n^2 = 3j^2 $\n\nThe proof is complete right here at this point, if you just note that the last equality implies that $3 \\mid n^2\\,$, and therefore $3 \\mid n$ which contradicts the assumption that $m,n$ are coprime.\n\n[ rest of post snipped ]\n\n",
    "tags": [
      "real-analysis",
      "elementary-number-theory",
      "solution-verification",
      "proof-writing",
      "rationality-testing"
    ],
    "score": 6,
    "answer_score": 6,
    "is_accepted": true,
    "question_id": 2115380,
    "answer_id": 2115472
  },
  {
    "theorem": "Prove that there exists only one prime number of the form $p^2−1$ where $p≥2$ is an integer",
    "context": "By factoring  $p^2 − 1$, we have $(p + 1)(p - 1)$. \nI know that $p = 2$ which gives $3$ is the only solution.\nHowever, how do I prove that $p = 2$ is the only integer which gives a prime?\n",
    "proof": "If $p>2$, both  $p+1$ and $p-1$ are $>1$, hence $p^2-1$ is composite.\n",
    "tags": [
      "elementary-number-theory",
      "discrete-mathematics",
      "proof-verification",
      "proof-writing"
    ],
    "score": 6,
    "answer_score": 8,
    "is_accepted": false,
    "question_id": 1557483,
    "answer_id": 1557487
  },
  {
    "theorem": "proof detail concerning bijection between a set and its power set",
    "context": "Theorem: If $X$ is a set, then $X$ is not equivalent to its power set.\nProof: suppose for a contradiction that $f:X\\to P(X)$ is a bijection. Define $B:=\\{x \\in X, x\\not\\in f(x)\\}$. Because $f$ is surjective (onto), there is a $b$ in $X$ with $B=f(b)$. Now $b$ in $B$ would imply $b$ in $B=f(b)$. By definition of $B$, this would mean that $b$ is not an element of $f(b)=B$. Thus we infer $b$ is not an element of $B$. But then $b$ is not an element of $B=f(b)$, which by definition of $B$ forces $b$ in $B$, a contradiction.\nI understood this proof until he reached the part that says \"Now $b$ in $B$ would imply $b$ in $B=f(b)$...\"\nThis seems problematic to me because the assumption $b$ in $B$ seems unwarranted. Since $B$ is a subset of $X$ it makes sense if we had $b$ in $B$ and then arrived at $b$ in $X$ but the reverse does not seem like a valid step and I'm left wondering if the resulting contradiction that arised came simply from the assumption $b$ is in $B$ and therefore only means that $b$ is not not an element of $B$, And that this has no bearing on the bijection part of the argument. In fact I dont even see how this connects to the part of the argument that used the onto property of bijectiveness. \n",
    "proof": "You start with $X$ and with $f$. Now you define $B$, and you say that $b\\in X$ is such that $f(b)=B$.\nSince $B$ is a subset of $X$ and $b$ is an element of $X$, it has to be that either $b\\in B$ or $b\\notin B$. Deriving $b\\in X$ from $b\\in B$ is meaningless, it tells us nothing new. But asking whether or not $b$ is also an element of $B$ or not, that is progress! So we have two cases:\n\nIf $b\\in B$, then $b\\in f(b)$. Then by definition of $B$, $b\\notin B$. So this can't be.\nIf $b\\notin B$, then $b\\notin f(b)$. Then by definition of $B$, $b\\in B$. Again, this cannot be.\n\nSo it is impossible that there is some $b\\in X$ such that $f(b)=\\{x\\in X\\mid x\\notin f(x)\\}$.\n",
    "tags": [
      "logic",
      "proof-verification",
      "proof-writing"
    ],
    "score": 6,
    "answer_score": 8,
    "is_accepted": true,
    "question_id": 1127149,
    "answer_id": 1127153
  },
  {
    "theorem": "Construction of an equilateral triangle from two equilateral triangles with a shared vertex",
    "context": "Problem\nGiven that $\\triangle ABC$ and $\\triangle CDE$ are both equilateral triangles. Connect $AE$, $BE$ to get segments, take the midpoint of $BE$ as $O$, connect $AO$ and extend $AO$ to $F$ where $|BF|=|AE|$. How to prove that $\\triangle BDF$ is a equilateral triangle.\n\nAttempt\n\nI've noticed that $\\triangle BCD \\cong \\triangle ACE$ so that $|AE|=|BD|$, but I was stuck when proving $\\triangle AOE \\cong \\triangle FOB$.\nEven though I assume that $ABFE$ is a parallelogram, I can neither prove one of three angles of $\\triangle BDF$ is 60 degree nor prove $|DB|=|DF|$ through proving $\\triangle DEF \\cong \\triangle DCB$ (which I think is right.)\n\nCould anybody give me a hand?\n",
    "proof": "Embed the construction in the complex plane.\nLet $\\omega=\\exp\\left(\\frac{\\pi i}{3}\\right),B=0,C=1,E=1+v$.\nThen $A=\\omega$ and $D=1+\\omega v$, hence $F=B+E-A$ implies:\n$$ F = 1-\\omega+v,$$\nhence:\n$$ \\omega F = \\omega -\\omega^2 + \\omega v = 1+\\omega v = D, $$\nso $BFD$ is equilateral.\n",
    "tags": [
      "geometry",
      "proof-writing",
      "triangles",
      "geometric-transformation"
    ],
    "score": 6,
    "answer_score": 4,
    "is_accepted": true,
    "question_id": 891319,
    "answer_id": 891327
  },
  {
    "theorem": "When does one proof of one direction of an If and Only If proof suffice?",
    "context": "Would someone please explain when this is admissible (please expound on $\\color{darkred}{sometimes}$)? \nIn advance of starting an Iff proof, how would one divine/previse if this convenience (of a string of equivalences) can be applied? If so, which direction should be proven manually? \n\nVelleman, 2nd Ed, P129-130:\n  $\\color{darkred}{Sometimes}$ in a proof of a goal of the form P ↔ Q the steps in the proof of\n  Q → P are the same as the steps used to prove P → Q, but in reverse order.\n  In this case you may be able to simplify the proof by writing it as a string of\n  equivalences, starting with P and ending with Q...\nP130: The technique of figuring out a sequence of equivalences in one order and\n  then writing it in the reverse order is used quite often in proofs...In particular,\n  if you are trying to prove P ↔ Q, it is wrong to start your write-up of the\n  proof with the unjustified statement P ↔ Q and then work out the meanings\n  of the two sides P and Q, showing that they are the same. You should instead\n  start with equivalences you can justify and string them together to produce a\n  justification of the goal P ↔ Q before you assert this goal.\n\n",
    "proof": "In my experience, there is no plan of making a proof bidirectional. Really, you just try to prove $P\\implies Q$ and see what happens. When you have your proof that $P$ implies $Q$, you take a good look at it and try to reverse every step of it. The usual case is that you can reverse some of the steps, but not all of them, but sometimes you get lucky and can reverse every step.\nHowever, you must always be sure that you know why a step can be reversed. For example, if you go from $x>1$ to $2x>2$, you can just as easily go from $2x>2$ to $x>1$, however, if you go from $x>1$ to $x^2>1$, you cannot reverse the step.\n",
    "tags": [
      "proof-writing",
      "intuition"
    ],
    "score": 6,
    "answer_score": 4,
    "is_accepted": false,
    "question_id": 701473,
    "answer_id": 701476
  },
  {
    "theorem": "Supposed proof of dirichlets theorem on primes",
    "context": "I think theirs somthing wrong with this proof as it was not hard to create, if someone could find a mistake I would greatly appreiciate it:\nDefine a function $[k\\equiv b \\bmod a]$, to be equal to zero if $k$ isn't congruent to $b \\mod a$, and 1 if it is.\nFrom that definition we have:\n$$\\sum _{k=1}^{\\infty }\\frac{\\ln(k)}{k^s}[k\\equiv b.mod. a]=\\sum _{k=1}^{\\infty }\\frac{\\Lambda(k)}{k^s}\\sum _{j=1}^{\\infty} \\frac{[jk\\equiv \\bmod a]}{j^s}$$\n(assume the above statement is true ^, its the only lemma I will ask for, and the proof takes 2 long to show here)\nBut we can brake that sum into parts, with the identity, $$\\sum _{j=1}^{\\infty}f(j)=\\sum_{r=1}^{a}\\sum_{j=0}^{\\infty}f(aj+r)$$\n(were just breaking it up into congruences which still form a basis for all the integers, for example the even and odd integers are the case where a=2)\nso we have $$\\sum _{k=1}^{\\infty }\\frac{\\ln(k)}{k^s}[k\\equiv \\bmod a]=\\sum _{k=1}^{\\infty }\\frac{\\Lambda(k)}{k^s}\\sum _{j=1}^{\\infty} \\frac{[jk\\equiv b.mod. a]}{j^s}$$$$=\\sum _{k=1}^{\\infty }\\frac{\\Lambda(k)}{k^s}\\sum_{r=1}^{a}\\sum _{j=0}^{\\infty} \\frac{[(aj+r)k\\equiv b.mod. a]}{(aj+r)^s}=\\sum _{k=1}^{\\infty }\\frac{\\Lambda(k)}{k^s}\\sum_{r=1}^{a}\\sum _{j=0}^{\\infty} \\frac{[rk\\equiv b.mod. a]}{(aj+r)^s}$$\nbut also note $\\sum_{j=1}^{\\infty}\\frac{1}{(aj+r)^s}=\\frac{\\zeta(s)}{a^s}+o(1)$,  so we get\n$$\\sum _{k=1}^{\\infty }\\frac{\\Lambda(k)}{k^s}\\sum_{r=1}^{a}\\sum _{j=0}^{\\infty} \\frac{[rk\\equiv \\bmod a]}{(aj+r)^s}=\\sum _{k=1}^{\\infty }\\frac{\\Lambda(k)}{k^s}\\sum_{r=1}^{a} [rk\\equiv \\bmod a](\\frac{\\zeta(s)}{a^s}+o(1))))=(\\frac{\\zeta(s)}{a^s}+o(1))\\sum _{k=1}^{\\infty }\\frac{\\Lambda(k)}{k^s}\\sum_{r=1}^{a} [rk\\equiv \\bmod a]$$\nback tracking a bit we have, $$\\sum _{k=1}^{\\infty }\\frac{\\ln(k)}{k^s}[k\\equiv \\bmod a]=(\\frac{\\zeta(s)}{a^s}+o(1))\\sum _{k=1}^{\\infty }\\frac{\\Lambda(k)}{k^s}\\sum_{r=1}^{a} [rk\\equiv \\bmod a]$$ and thus we have,\n$$\\sum _{j=0}^{\\infty }\\frac{\\ln(aj+b)}{(aj+b)^s}=(\\frac{\\zeta(s)}{a^s}+o(1))\\sum _{k=1}^{\\infty }\\frac{\\Lambda(k)}{k^s}\\sum_{r=1}^{a} [rk\\equiv \\bmod a]$$\n(sense all the solutions to $k\\equiv b$ mod a, take on the form aj+b for all integers 'j')\nand so $$\\frac{a^s}{\\zeta(s)}\\sum _{j=0}^{\\infty }\\frac{\\ln(aj+b)}{(aj+b)^s}=(1+o(\\frac{a^s}{\\zeta(s)}))\\sum _{k=1}^{\\infty }\\frac{\\Lambda(k)}{k^s}\\sum_{r=1}^{a} [rk\\equiv \\bmod a]$$\nnow taking the limit as s->1, we see\n$$\\infty=\\sum _{k=1}^{\\infty }\\frac{\\Lambda(k)}{k}\\sum_{r=1}^{a} [rk\\equiv \\bmod a]=\\sum _{k=1}^{\\infty }\\frac{\\Lambda(k)}{k}[k\\equiv b.mod. a]+\\sum _{k=1}^{\\infty }\\frac{\\Lambda(k)}{k}[2k\\equiv b.mod. a]+\\sum _{k=1}^{\\infty }\\frac{\\Lambda(k)}{k}[3k\\equiv b.mod. a]+...\\sum _{k=1}^{\\infty }\\frac{\\Lambda(k)}{k}[(a-1)k\\equiv b.mod. a]$$\nand in order for a finite sequence of postive terms to diverge, atleast one of the terms must diverge, so picking out any $0<c<a$, we see there must be some c, greater then zero, and less then a, such that for all b, \n$$\\infty=\\sum _{k=1}^{\\infty }\\frac{\\Lambda(k)}{k}[ck\\equiv b.mod. a]$$\nand sense $ck\\equiv b$ mod a, only has solutions if c and a are coprime, we see that $[ck\\equiv b.mod. a]=0$, if c,a arn't coprime, and sense our series diverges, we see that the c we chose must be coprime to a.\nSo to reiderate weve shown that for some c coprime to a, and less then a, we have\n$$\\infty=\\sum _{k=1}^{\\infty }\\frac{\\Lambda(k)}{k}[ck\\equiv b.mod. a]$$, now sense we havn't explictly defined the integer b, we can make it a multiple of c at this point,\n say $b=c*d$, thus we have $$\\infty=\\sum _{k=1}^{\\infty }\\frac{\\Lambda(k)}{k}[ck\\equiv cd.mod. a]$$\nbut sense c is coprime to a, we may cancle it from both sides of the congruence, giving,\n$$\\infty=\\sum _{k=1}^{\\infty }\\frac{\\Lambda(k)}{k}[k\\equiv d.mod. a]$$\nbut sense the vonmangoldt sum is a proxyed sum over the prime powers, and any power greater then 2 is neglible we see,\n$$\\infty=\\sum _p\\frac{\\ln(p)}{p}[p\\equiv d.mod. a]$$\nand sense if there were a finite number of primes congruent to d mod a, the series wouldn't diverge, we can conclude dirichlets theorem is true.\n(I understand the entire proof is based on the first statement, but I can prove that it is true using only some elementry algebra, and some other ideas I have worked on, and although its too long to post here, if interested you can email me, the proof of the first statrement is about a page long, but it can be condensed)\n",
    "proof": "You say that we can pick b to be a multiple of c, but your choice of c certainly depends on b. You need to justify the statement that one c works for all b. This is the most likely spot for an error I can find.\n",
    "tags": [
      "prime-numbers",
      "proof-writing"
    ],
    "score": 6,
    "answer_score": 8,
    "is_accepted": true,
    "question_id": 256444,
    "answer_id": 261209
  },
  {
    "theorem": "Is there an analysis book with a proper introduction to mathematical proofs?",
    "context": "I am taking my first analysis course in university. Since I am studying in Germany, there is no real distinction between calculus and real analysis, meaning this is the first university course on anything analysis that can be taken here. Therefore, I am new to the concepts of mathematical proofs. However, they are regularly used in the course and I often have problems following them. Since I am taking classes on mathematical principles and logic as well, my initial idea was to buy a dedicated textbook on proofs, e.g. Hammack's Book of Proof, Velleman's How to Prove It or Chartrand's Mathematical Proofs: A Transition to Advanced Mathematics, which I could use for analysis as well as the other classes.\nHowever, some answers to this question imply that this might not be the best way to go and I should rather buy a book on a certain mathematical field, in my case analysis, that contains a rigorous introduction to proofs in itself and applies them. Additionally, since time is limited, I am not quite sure whether it makes sense to focus on a book on mathematical proofs (which sometimes have several hundred pages) rather than the course itself.\nSince I am missing an introductory text to analysis anyway, I thought about following the recommendations in the answers and getting a book that teaches introductory analysis with a focus on formal notation as well as writing and understanding mathematical proofs. From what I have read so far, Abbott's Understanding Analysis and Tao's Analysis I + II might be great options for that purpose, however, one answer on the question I was referring to earlier suggests there might be a specific book which is specifically designed that way.\nShould I get a separate book on mathematical proofs or would I be better off with one of the analysis books I mentioned (or a completely different one)?\n",
    "proof": "Analysis $1$ by Tao has an appendix on the basics of mathematical logic. So you can first read that before you begin to attempt the exercises.\nBut before you read that appendix, I would advise you to read the first chapter(technically the second) of the book on the construction of the natural numbers. This is where you will get introduced to the Principle of mathematical induction and use that principle to prove many statements regarding the natural numbers such as addition and multiplication for natural numbers is commutative, associative and so on.\nThe only benefit of these introductory proof courses or books is that you won't be wasting time looking those things up again. For example, I didn't read the whole book \"How to prove it\" but only some of it. And I didn't know what the sentence \"if and only if\" meant in terms of proving a mathematical statement. I only understood it when I went to check if my proof was correct.\nHonestly speaking, I only ever learnt all the things in proving mathematical statements by reading proofs in the book I was learning from and solutions for those exercises.\n",
    "tags": [
      "real-analysis",
      "calculus",
      "reference-request",
      "proof-writing",
      "book-recommendation"
    ],
    "score": 6,
    "answer_score": 4,
    "is_accepted": true,
    "question_id": 4566198,
    "answer_id": 4566209
  },
  {
    "theorem": "I&#39;m trying to prove $ M \\subset N \\Rightarrow M \\cup(N \\setminus M) = N $",
    "context": "I'm trying to prove:\n$$\nM \\subset N \\Rightarrow M \\cup(N \\setminus M) = N\n$$\nMy trial:\n$\n\\{x|x\\in M \\cup(N \\setminus M)\\} \\Leftrightarrow \\{x|x\\notin \\overline{M \\cup(N \\setminus M)}\\}\n$\nusing de Morgan\n$ \\Leftrightarrow \\{x|x\\notin \\overline{M} \\cap \\overline{(N \\setminus M)}\\}$\n$ \\Leftrightarrow \\{x|x\\notin \\overline{M} \\wedge x \\notin \\overline{(N \\setminus M)}\\}$\n$ \\Leftrightarrow \\{x|x\\notin \\overline{M} \\wedge x \\in (N \\setminus M)\\}$\n$ \\Leftrightarrow \\{x|x\\notin \\overline{M} \\wedge x \\in N \\wedge x\\notin M\\}$\naand we got a problem here. $x$ cannot be both $x\\notin \\overline{M}$ and  $x\\notin M$.\nMy steps seems logical to me. Can someone please spot my mistake. Thanks a lot!\n",
    "proof": "Your error: $x\\notin A\\cap B$ is the negation of $x\\in A\\cap B$. $$x\\in A\\cap B \\text{ iff }x\\in A \\land x\\in B$$\nSo,$$x\\notin A\\cap B \\text{ iff }x\\notin A \\lor x\\notin B$$\nPlease notice how the $\\land$ becomes $\\lor$ on negation! In general, $\\lnot(p\\land q) = \\lnot p\\lor \\lnot q$.\n\nAlternative proof strategy: Whenever I want to show $A = B$, where $A,B$ are two sets, I aim to establish $A\\subset B$ and $B\\subset A$. Works on most occasions!\n\nSuppose $M\\subset N$. Then, for every $x\\in M$, $x\\in N$. Note that\n$$N\\setminus M = \\{x\\in N: x\\notin M\\}$$\nSuppose $x\\in M \\cup N\\setminus M$. Then, $x\\in M$ or $x\\in N\\setminus M$. If $x\\in M$, then $x\\in N$ by $M\\subset N$. If $x\\in N\\setminus M$, then $x\\in N$ by definition of $N\\setminus M$. So, $M\\cup N\\setminus M \\subset N$.\nNext, suppose $x\\in N$. Then, either $x\\in M$ or $x\\notin M$. If $x\\in M$, $x\\in M\\cup N\\setminus M$ by definition of the union. If $x\\notin M$, then clearly $x\\in N\\setminus M$, since we assumed $x\\in N$, to begin with (refer to the definition of $N\\setminus M$ again). As a result, $x\\in M\\cup N\\setminus M$ by definition of the union, once again. So, we have $N\\subset M\\cup N\\setminus M$.\n$M\\cup N\\setminus M \\subset N$ and $N\\subset M\\cup N\\setminus M$ together, yield $M\\cup N\\setminus M = N$. So we have shown $$M\\subset N\\implies M\\cup N\\setminus M = N$$ Done!\n\nLastly, the following image says more than words ever can:\n\n",
    "tags": [
      "elementary-set-theory",
      "proof-writing",
      "solution-verification"
    ],
    "score": 6,
    "answer_score": 4,
    "is_accepted": true,
    "question_id": 4067944,
    "answer_id": 4067970
  },
  {
    "theorem": "A New Proof of Pythagorean theorem?",
    "context": "A right triangle with side lengths $a,b,c$  where the hypotenuse($c$) intercepts the height($h$) such that $c$ is the sum of $2$ side lengths, $c = m + n$.\nImage here: \nI found an elegant proof of Pythagorean theorem that seems to be related to Einstein's proof:\nFirst, slopes of $a$ and $b$ give:\n$$(\\frac{h}{m})(\\frac{h}{n}) = 1\\implies h^2 = mn$$\nThan with area formula, we find the intercept $(x,y)$ of the height and hypotenuse:\n$$bx = hm\\implies x = \\frac{am}{c}$$\n$$ay = hn\\implies y = \\frac{bn}{c}$$\nNext plugging this into the height's equation:\n$$\\frac{a}{b}(\\frac{am}{c}) = \n\\frac{bn}{c}\\implies a^2m = b^2n$$\nUsing $c = m  + n$:\n$$\\frac{c}{\\sqrt{mn}} = \\sqrt{\\frac{m}{n}} + \\sqrt{\\frac{n}{m}}$$\n$$\\frac{c^2}{ab} = \\frac{a}{b} + \\frac{b}{a}$$\n$$a^2 + b^2 = c^2$$\nQ.E.D.\nThoughts?\n",
    "proof": "This is really just an overly complex use of similar triangles to show$$m/a=h/b,\\,n/b=h/a\\implies c=m+n=(a^2+b^2)/c,$$where in the last $=$ I, like you, have used $ab=ch$. As with your other questions I've answered about Pythagorean proofs, the strategy works but is intimately related, once extraneous Cartesian coordinates have been stripped away, to old approaches. In particular, computing $x,\\,y$ is equivalent to computing $m,\\,n$, so there's no need to worry about how axes are oriented.\n",
    "tags": [
      "geometry",
      "proof-writing",
      "solution-verification"
    ],
    "score": 6,
    "answer_score": 5,
    "is_accepted": true,
    "question_id": 4030549,
    "answer_id": 4030572
  },
  {
    "theorem": "Show that $\\sqrt{1+x}&lt;1+\\frac{x}{2}$ for all $x&gt;0$",
    "context": "I am a little stuck on this question and would appreciate some help. The question asks me to prove that $\\sqrt{1+x}<1+\\frac{x}{2}$ for all $x>0$. \nI squared both sides of the question to get $1+x<\\frac{x^2}{4}+x+1$ for all $x>0$. Then, I multiplied both sides by $4$ to get $4+4x<x^2+4x+4$ for all $x>0$. \nI am a little stuck and was wondering what to do after this step and how to actually provide sufficient proof to say that this statement is true.\n",
    "proof": "You did well. Let's finish it:\nwe have $x^2>0$ thus $\\dfrac{x^2}{4}>0$. Adding $x+1$ to both sides, we have\n$$\\dfrac{x^2}{4}+x+1> x+1$$\nor $$(\\frac{x}{2}+1)^2>x+1$$\nor\n$$\\frac{x}{2}+1>\\sqrt{x+1}$$\n",
    "tags": [
      "real-analysis",
      "proof-writing",
      "proof-explanation"
    ],
    "score": 6,
    "answer_score": 6,
    "is_accepted": false,
    "question_id": 3600695,
    "answer_id": 3600705
  },
  {
    "theorem": "Exercise about primes in the ring of Gaussian integers",
    "context": "\nLet $p$ be a prime in $\\mathbb{Z}$ of the form $4n + 1, n \\in \\mathbb{N}$. Show that $\\left(\\frac{-1}{p}\\right) = 1$ (here $\\left(\\frac{\\#}{p}\\right)$ is the Legendre symbol). Hence prove that $p$ is not a prime in the ring $\\mathbb{Z}[i]$.\n\nHere is my solution:\nSince $p > 2$, we have $\\left(\\frac{-1}{p}\\right) = 1$ if and only if $(-1)^{\\frac{p - 1}{2}} \\equiv_p 1$ if and only $(-1)^{2n} \\equiv_p 1$ which is true.\nNow suppose $p$ is prime in $\\mathbb{Z}[i]$, which means that there exists $x \\in \\mathbb{Z}$ such that $-1 \\equiv_p x^2$, from which $p \\mid (x^2 + 1) = (x - i)(x + i)$ and, since $p$ is prime, $p \\mid (x - i)$ or $p \\mid (x + i)$. In either case we have $m + ni \\in \\mathbb{Z}[i]$ such that $p(m + ni) = x \\pm i$, which implies $pn = x$, that is $p \\mid x$, and $x^2 + 1 \\equiv_p 1$, which is not congruent to $0$, contradiction.\nIs it correct? thanks in advance\nEdit: (I've tried to write it better using Robert Soupe advice)\nSince $p>2$ we have $(-1)^{(p-1)/2}\\equiv_p (-1)^{2n} \\equiv_p 1$, that is $\\left(\\frac{-1}{p} \\right)= 1$.\nNow suppose $p$ is prime in $\\mathbb{Z}[i]$, this means that there exists $x \\in \\mathbb{Z}$ such that $x^2 \\equiv_p -1$, hence $p \\mid (x^2 + 1) = (x - i)(x + i)$ and, since $p$ is prime, $p \\mid (x + i)$. Therefore there exists $m + ni \\in \\mathbb{Z}[i]$ such that $p(m + ni) = x + i$, but this is absurd because $p$ does not divide $1$. We can conclude that $p$ is not prime in $\\mathbb{Z}[i]$.\n",
    "proof": "Yes, it's correct, though I'd like to unpack it a little, and work it out with a specific prime.\n\nSince $p > 2$, we have $$\\left(\\frac{-1}{p}\\right) = 1$$ if and only if $$(-1)^{\\frac{p - 1}{2}} \\equiv 1 \\pmod p$$ if and only $(-1)^{2n} \\equiv 1 \\pmod p$, which is true.\n\nSo far so good. The wording is a bit clunky, but I should be able to just get past that.\n\nNow suppose $p$ is prime in $\\mathbb Z[i]$, which means that there exists $x \\in \\mathbb{Z}$ such that $-1 \\equiv x^2 \\pmod p$, \n\nI'm sorry, I'm not liking your style of writing congruences at all. I left it alone when I edited your question, but now that I'm quoting you in my answer I really have to change it to something I like better.\nI'm also having a bit of a problem with $-1 \\equiv x^2 \\pmod p$, I'd much rather write $x^2 \\equiv -1 \\pmod p$ even though it means the very same thing.\nMaybe it's because it really obscures the meaning of $x^2 \\equiv -1 \\pmod p$. And that meaning is that the equation $x^2 = kp - 1$ has solutions in integers.\n\nfrom which $p \\mid (x^2 + 1) = (x - i)(x + i)$ and, since $p$ is prime, $p \\mid (x - i)$ or $p \\mid (x + i)$. In either case we have $m + ni \\in \\mathbb Z[i]$ such that $p(m + ni) = x \\pm i$, which implies $pn = x$, that is $p \\mid x$, and $x^2 + 1 \\equiv 1 \\pmod p$, which is not congruent to $0$, contradiction.\n\nThis is perfectly lucid and correct, but I can't shake the feeling that it's possible to be more elegant without getting too advanced (i.e., cyclic groups). Maybe Max will elaborate his comment into an answer.\nOkay, now to work out an example with a specific prime. I choose 41. We have $$\\left(\\frac{-1}{41}\\right) = (-1)^{20} = 1,$$ just as we expected.\nThis tells us that the congruence $x^2 \\equiv 40 \\pmod{41}$ has solutions, and likewise the equation $x^2 = 41k - 1$. This should immediately lead us to find $9^2 = 2 \\times 41 - 1$. Then $(9 - i)(9 + i) = 82$.\nAt this point we're saying that 41 is prime in $\\mathbb Z[i]$, which means that either 41 divides $9 - i$ or it divides $9 + i$. However, before carrying out either division, notice that $9 + i$ has even norm. It must be divisible by either $1 - i$ or $1 + i$, maybe both.\nSo then we compute $$\\frac{9 + i}{1 - i} = \\frac{(9 + i)(1 + i)}{2} = \\frac{8 + 10i}{2} = 4 + 5i$$ and we verify that $N(4 + 5i) = 41$.\nSince 41's norm is 1681 but $N(9 \\pm i)$ is just 82, we should have realized earlier that $41 \\nmid (9 \\pm i)$. I admit I don't know what is the most elegant way to get from $9 \\pm i$ to $4 \\pm 5i$.\n",
    "tags": [
      "abstract-algebra",
      "group-theory",
      "proof-writing",
      "prime-numbers",
      "gaussian-integers"
    ],
    "score": 6,
    "answer_score": 3,
    "is_accepted": true,
    "question_id": 2839945,
    "answer_id": 2842559
  },
  {
    "theorem": "Prove that $| \\operatorname{Aut}(D_n)|= n\\phi(n)$",
    "context": "\nProve that for $n\\gt 2$, $| \\operatorname{Aut}(D_n)|\\le n\\,\\phi(n)$ where $D_n$ is the dihedral group with $2n$ elements and $\\phi$ is Euler phi function.\n\nLet $\\rho$ be a rotation such that $o(\\rho)=n$, that is $R_n = \\langle\\rho\\rangle$, and $\\psi$ an automorphism of $D_n$; then $\\psi(\\rho)$ must have order $n$ and there are only $\\phi(n)$ such elements in $D_n$ and they are all rotations, therefore $\\psi(R_n)=R_n$. Now let $\\iota$ be the reflection through the $x$-axis, we have to send it in one of the $n$ reflection and since $D_n = \\langle\\rho, \\iota \\rangle$, $\\psi$ is univocally determined.\nIn conclusion we have at most $n\\phi(n)$ choices for $\\psi$.\n\nI could have also considered all the sets with two elements that generate $D_n$ which are of the form $\\{\\rho, \\iota\\}$ with $\\rho$ a rotation of order $n$ and $\\iota$ a reflection (there are $n\\phi(n)$ of them); since an automorphism sends a set of generators into a set of generators we have again at most $n\\phi(n)$ automorphisms (a rotation will be sent in a rotation) obtained by extending to a homomorphism the various choices (which give us bijective functions). Moreover there are exactly $n\\phi(n)$ of them because every choice give us a different automorphism.\nAre both solutions, and my remark, correct?\nThanks in advance\nEdit: I was wrong saying that the only sets of two elements generating $D_n$ are of the form $\\{\\rho, \\iota\\}$, because there are also sets formed by two reflections, but since a rotation must be sent in a rotation my second proof should be correct.\n",
    "proof": "Your explanation is correct.  I have a remark that the inequality you want to prove is indeed an equality.  \nLet $C_n$ denote the cyclic group of order $n$.   The group $G:=\\text{Aut}\\left(D_n\\right)$ is isomorphic to the semidirect product $H:=C_n\\rtimes \\text{Aut}\\left(C_n\\right)$, where \n$$\\left(c_1,f_1\\right)\\cdot \\left(c_2,f_2\\right):=\\big(c_1\\,f_1\\left(c_2\\right),f_1\\circ f_2\\big)$$\nfor all $c_1,c_2\\in C_n$ and $f_1,f_2\\in\\text{Aut}\\left(C_n\\right)$. If $C_n$ is generated by $c$, then each element of $\\text{Aut}\\left(C_n\\right)$ sends $c$ to $c^k$ for some $k=1,2,\\ldots,n$ with $\\gcd(k,n)=1$, and we write $\\gamma_k$ for this element of $\\text{Aut}\\left(C_n\\right)$.\nThe reason that $G$ is isomorphic to $H$ is as follows.  Let $$D_n=\\left\\langle r,s \\,|\\,r^n=s^2=1\\text{ and }rs=sr^{-1}\\right\\rangle=\\left\\{1,r,r^2,\\ldots,r^{n-1},s,rs,r^2s,\\ldots,r^{n-1}s\\right\\}\\,.$$\nHence, for each $\\tau\\in G$, it suffices to look at $r_\\tau:=\\tau(r)$ and $s_\\tau:=\\tau(s)$.  We have $r_\\tau=r^{k}$ and $s_\\tau=r^{j}s$ for some $k=1,2,\\ldots,n$ with $\\gcd(k,n)=1$ and for any $j=0,1,2,\\ldots,n-1$.  Thus, we write $\\tau_{j,k}$ for this automorphism $\\tau$.  Then, the map $\\psi:G\\to H$ sending $\\tau_{j,k}$ to $\\left(c^j,\\gamma_k\\right)$ is a group isomorphism.  That is,\n$$\\big|\\text{Aut}\\left(D_n\\right)\\big|=|G|=|H|=\\left|C_n\\right|\\,\\big|\\text{Aut}\\left(C_n\\right)\\big|=n\\,\\phi(n)\\,.$$\nYou have to fill in a lot of gaps I intentionally left out, of course.\nP.S.  Interestingly, the condition $n>2$ is important.  It turns out that $D_2\\cong C_2\\times C_2$ is extraordinary.  That is, $$\\text{Aut}\\left(D_2\\right)\\cong\\text{Aut}\\left(C_2\\times C_2\\right)\\cong \\text{GL}_2\\left(\\mathbb{F}_2\\right)\\cong S_3\\,,$$\nwhere $S_3$ is the symmetric group on $3$ symbols.\n",
    "tags": [
      "group-theory",
      "proof-writing",
      "finite-groups",
      "automorphism-group",
      "dihedral-groups"
    ],
    "score": 6,
    "answer_score": 7,
    "is_accepted": true,
    "question_id": 2836900,
    "answer_id": 2837048
  },
  {
    "theorem": "The use of modulo in the mathematical induction proof of $7^{2n}-9$ being divisible by 2",
    "context": "I tried proving that $7^{2n}-9$ is divisible by 2 in the following way:\nBase case: When $n=1$,\n$$7^{2}-9\\equiv 0\\;(\\text{mod}\\,2)$$\nInduction step: Assume $n=k$,\n$$7^{2k}-9\\equiv 0\\;(\\text{mod}\\,2)$$\nWe want to prove that it is true for $n=k+1$,\n$$7^{2(k+1)}-9=7^{2k+2}-9=49\\cdot7^{2k}-9$$\n$$=49\\cdot7^{2k}-9=49\\cdot[0\\;(\\text{mod}\\,2)+9]-9.$$\nSince I couldn't continue, I used the following instead:\nInduction step: Assume $n=k$,\n$$7^{2k}-9=2a$$\nFor $n=k+1$,\n$$7^{2(k+1)}-9=7^{2k+2}-9=49\\cdot7^{2k}-9$$\n$$=49\\cdot7^{2k}-9=49\\cdot(2a+9)-9=2(49a+216)=2b.$$\nCan anyone show me how to prove this by modulo instead?\n",
    "proof": "Yet another proof:\n$7^{2n}-9 \\pmod 2$ is the difference of two squares — $(7^n+9)(7^n-9)$. Both brackets are even (prove it!), so the original expression is not just even, it is divisible by $4$ (Robert Z)!\n",
    "tags": [
      "algebra-precalculus",
      "proof-writing",
      "induction",
      "proof-explanation"
    ],
    "score": 6,
    "answer_score": 1,
    "is_accepted": true,
    "question_id": 2705840,
    "answer_id": 2705855
  },
  {
    "theorem": "A $\\lambda$-system $\\mathcal{L}$ that is a $\\pi$-system is automatically a $\\sigma$-field.",
    "context": "Let $\\Omega$ be a set.  Let $\\mathcal{L}$ be a $\\lambda$-system, that is:\n\n$\\Omega \\in \\mathcal{L}$.\n$A \\in \\mathcal{L} \\implies A^c \\in \\mathcal{L}$.\n$A_n \\in \\mathcal{L}, n\\geq 1$ and $A_m \\cap A_n = \\varnothing$ when $n \\neq m \\ \\implies \\cup_{n} A_n \\in \\mathcal{L}$.\n\nA $\\pi$-system just means $\\mathcal{L}$ is also closed under finite intersection.\nA $\\sigma$-field is a set of subsets of $\\Omega$ that contains $\\varnothing$ and is closed under complement and countable union.\nClearly, I only have to show the last property (countable union) as the first two are immediate from the definitions.\nLet $A_n \\in \\mathcal{L}, n \\geq 1$ be a countable collection of sets in $\\mathcal{L}$.  I've tried this out:\n$B_n = A_n \\setminus (\\cup_{n \\neq m} A_m)$ satisfies property (3) but doesn't lead to a proof, nor does $\\cup A_n = \\cup B_n$.\nYeah... sort of stuck here.\n",
    "proof": "If $\\{E_n\\}$ is a countable collection in $\\mathcal{L}$, then define the sets\n\\begin{align*}\nF_1 &\\doteq E_1 \\\\\nF_n &\\doteq E_n \\backslash (E_1 \\cup E_2 \\cup \\dots \\cup E_{n-1})\n\\end{align*}\nThen $\\{F_n\\}$ are disjoint and $\\cup_1^{\\infty} E_n = \\cup_1^{\\infty} F_n$. We also have each $F_n \\in \\mathcal{L}$ since $\\mathcal{L}$ is closed under complementation and finite intersection. Hence, $\\cup_1^{\\infty} E_n \\in \\mathcal{L}$.\n",
    "tags": [
      "probability-theory",
      "measure-theory",
      "elementary-set-theory",
      "proof-writing"
    ],
    "score": 6,
    "answer_score": 7,
    "is_accepted": true,
    "question_id": 2369531,
    "answer_id": 2369542
  },
  {
    "theorem": "Proof of determinants for matrices of any order",
    "context": "I was told that the determinant of a square matrix can be expanded along any row or column and given a proof by expanding in all possible ways, but only for square matrices of order 2 and 3.\n\nIs a general proof for any order even possible ?\nIf so, how is this done ?\nOn a similar note, how can we prove the various properties of determinants for square matrices for any order like the following:\n\nSwap two rows/columns and all we get is a minus sign as a result.\n$R_1 \\to  R_1+ aR_2$ does not change the determinant.\nDeterminant of the transpose is the same as the determinant of the original matrix.\n\n\n",
    "proof": "Here is one possible path. We define the determinant recursively:\n\nif $A$ is $1\\times 1$, let $\\det A=A$;\n\nIf $A$ is $(n+1)\\times (n+1)$, let\n$$\n\\det A=\\sum_{k=1}^{n+1} (-1)^{k+1}A_{1k}\\,M_{1k}^A,\n$$\nwhere $M_{st}^A$ is the determinant of the $n\\times n$ matrix obtained by removing the $s^{\\rm th}$ row and the $t^{\\rm th}$  column  of $A$.\n\n\nNow,\n\nShow by induction that\n$$\n\\det A=\\sum_{\\sigma\\in\\mathbb S_n}(\\operatorname{sgn}\\sigma)\\,A_{1,\\sigma(1)}\\cdots A_{n,\\sigma(n)}. \n$$\nThis implies that $\\det$, when seen as a function of the rows of $A$, is a skew-symmetric multilinear map.\n\nThe next properties follow directly from the multilinearity and skew-linearity of $\\det$.\n\nShow that if $B$ is obtained from $A$ by multiplying a row by $\\alpha$, then $$\\det B=\\alpha\\,\\det A.$$ This is done by induction very easily.\n\nShow that if we have $A,B,C$ with $A_{rj}=B_{rj}+C_{rj}$ for all $j$, and $A_{kj}=B_{kj}=C_{kj}$ when $k\\ne r$ and for all $j$, then\n$$\\det A=\\det B+\\det C.$$ Again this is done by induction. When $r=1$ the equality follows trivially from the definition of determinant (as the minors of $A,B,C$ will be all equal) and when $r\\ne 1$ we use induction.\n\nShow that if $B$ is obtained from $A$ by swapping two rows, then $$\\det B=-\\det A.$$ Here one first swaps rows $1$ and $r$, and then any other swapping of two rows $r$ and $s$ can be achieved by three swaps ($r$ to $1$, $s$ to $1$, $r$ to $1$). This can be used to show that one can calculate the determinant along any row (swap it with row 1, calculate, unswap).\n\nIt now follows that if $A$ has two equal rows, then $\\det A=0$ (because $\\det A=-\\det A$).\n\nIf $B_{rj}=A_{rj}+\\alpha A_{sj}$, and $B_{kj}=A_{kj}$ when $k\\ne r$, then by 1. and 2.,\n$$\\det B=\\det A+\\alpha\\det C,$$ where $C$ is the matrix equal to $A$ but with the $s$ row in place of the $r$ row; by 4., $\\det C=0$, so $\\det B=\\det A$.\n\nNow one considers the elementary matrices, and checks directly (using the above properties) that for any elementary matrix $E$, $$\\det EA=\\det E\\,\\det A.$$\n\nIf $B$ is invertible, then $B$ can be written as a product of elementary matrices, $B=E_1E_2\\cdots E_m$, and so\n\\begin{align}\n\\det BA&=\\det E_1E_2\\cdots E_m A=\\det E_1\\det E_2\\cdot\\det E_m\\det A\\\\ \\ \\\\\n&=\\det (E_1\\cdots E_m)\\det A=\\det B\\det A.\n\\end{align}\nSimilarly, $\\det AB=\\det A\\det B$.\n\nIf neither $A$ nor $B$ are invertible: then $AB$ is not invertible either. For a non-invertible matrix, its Reduced Row Echelon form has a row of zeroes, and so its determinant is zero; as we can move to $A$ by row operations, it follows that $\\det A=0$; similarly,  $\\det AB=0$. So $$\\det AB=\\det A\\det B$$ also when one of them is not invertible.\n\nKnowing that det is multiplicative, we immediately get that, when $A$ is invertible, $$\\det A^{-1}=\\frac1{\\det A}.$$\n\nFor an arbitrary matrix $A$, it is similar to its Jordan form: $A=PJP^{-1}$. Then\n$$\n\\det A=\\det (PJP^{-1})=\\det P\\,\\det J\\,\\frac1{\\det P}=\\det J.\n$$\nAs $J$ is triangular with the eigenvalues of $A$ (counting multiplicities) in its diagonal, we get that\n$$\n\\det A=\\lambda_1\\cdots\\lambda_n,\n$$\nwhere $\\lambda_1,\\ldots,\\lambda_n$ are the eigenvalues of $A$, counting multiplicities.\n\nSince the eigenvalues of $A^T$ are the same as those from $A$, we get\n$$\n\\det A^T=\\det A. \n$$\n\nIf $B$ is the matrix obtained by exchanging rows $1$ and $2$ in $A$, then\n$$\n\\det A=-\\det B=-\\sum_{j=1}^n (-1)^{1+j}b_{1j}\\,M_{1j}^B\n=\\sum_{j=1}^n (-1)^{2+j}a_{2j}\\,M_{2j}^B.\n$$\n\nNow, everything we did for rows, we can do for columns by working on the transpose. In particular, we can calculate the determinant along any column. Exchanging with other rows we get by induction\n$$\n\\det A=\\sum_{j=1}^n (-1)^{k+j}a_{kj}\\,M_{kj}^A.\n$$\n\n\n",
    "tags": [
      "matrices",
      "proof-writing",
      "determinant"
    ],
    "score": 6,
    "answer_score": 5,
    "is_accepted": true,
    "question_id": 2254620,
    "answer_id": 2255753
  },
  {
    "theorem": "prove/disprove that if $f\\circ g$ injective and g is surjective, then f is injective",
    "context": "Question would be: prove/disprove that if $f\\circ g$ injective and g is surjective, then f is injective.\nafter thinking, I came to the conclusion that it's a proof. tried to prove it but it looks not that valid. Would appreciate your feedback and corrections.\nProof:\n\nbecause $f\\circ g$ is injective, then g is injective as well.\nbecause it's given that g is surjective, and we came to conclusion it's also injective -> it's reversible by $g^{-1}$\nif $f\\circ g$ is injective and $g^{-1}$ is injective, then $f\\circ g\\circ g^{-1}$ injective as well.\n\nLet there be $a_1,a_2$. $a_1=a_2 \\iff f\\circ g\\circ g^{-1}(a_1)=f\\circ g\\circ g^{-1}(a_2) \\iff f\\circ i(a_1) = f\\circ i(a_2) \\iff f(a_1)=f(a_2)$\nWhat do you think??\n",
    "proof": "I do not think your proof is wrong per se, but I would go about things a little more directly.\nSuppose $f(x_1)=f(x_2)$. There exist $y_1,y_2$ such that $x_1=g(y_1)$ and $x_2=g(y_2)$. We have $f\\circ g (y_1)=f\\circ g (y_2)$. So $y_1=y_2$. So $x_1=x_2$.\n",
    "tags": [
      "functions",
      "logic",
      "proof-verification",
      "proof-writing"
    ],
    "score": 6,
    "answer_score": 7,
    "is_accepted": false,
    "question_id": 1590974,
    "answer_id": 1590978
  },
  {
    "theorem": "Show that sup$AB$=(sup$A$)(sup$B$)",
    "context": "Where $AB$ is the product of the sets and $A,B \\in \\mathbb{R^+}$.\nSince $A,B$ are bounded above sup $A$ and sup $B$ exist. Let $\\alpha = $ sup $A$ and $\\beta = $ sup $B$. This implies $\\forall a \\in A$ and $\\forall b \\in B$ $a \\leq \\alpha$ and $b \\leq \\beta$. Then $ab \\leq \\alpha\\beta$ because $a,b > 0$. Thus $ab$ is bounded above and sup $AB$ exists and sup $AB \\leq \\alpha\\beta$. \\\n    We now show sup $AB \\geq \\alpha\\beta$. \\\n    Let $\\varepsilon > 0$ then $\\exists a \\in A$ s.t. $\\alpha - \\varepsilon < a \\leq \\alpha$ and $\\exists b \\in B$ s.t. $\\beta - \\varepsilon < b \\leq \\beta$. So:\n    \\begin{equation*}\n      (\\alpha-\\varepsilon)(\\beta-\\varepsilon) < ab \\leq \\alpha\\beta \\text{ since } a,b,\\varepsilon > 0\n    \\end{equation*}\n    \\begin{equation*}\n      = \\alpha\\beta-\\varepsilon(\\alpha+\\beta-\\varepsilon) < ab \\leq \\alpha\\beta\n    \\end{equation*}\nI spoke with my professor today about this and he suggested I show that $\\varepsilon$ is sufficiently small to proceed. I'm not sure exactly how to write this detail.\nEDIT: I was meant to show what $\\varepsilon$ was bounded by to proceed. The proof below realizes this idea. Feedback is welcome and appreciated.\nSince $A,B$ are bounded above sup $A$ and sup $B$ exist. Let $\\alpha = $ sup $A$ and $\\beta = $ sup $B$. This implies $\\forall a \\in A$ and $\\forall b \\in B$ $a \\leq \\alpha$ and $b \\leq \\beta$. Then $ab \\leq \\alpha\\beta$ because $a,b > 0$. Thus $ab$ is bounded above, sup $AB$ exists and sup $AB \\leq \\alpha\\beta$.\n    Let $\\varepsilon > 0$ then $\\exists a \\in A$ s.t. $\\alpha - \\varepsilon < a \\leq \\alpha$ and $\\exists b \\in B$ s.t. $\\beta - \\varepsilon < b \\leq \\beta$. So:\n    \\begin{equation*}\n      (\\alpha-\\varepsilon)(\\beta-\\varepsilon) < ab \\leq \\alpha\\beta\n    \\end{equation*}\n    \\begin{equation*}\n      = \\alpha\\beta-(\\varepsilon\\alpha+\\varepsilon\\beta-\\varepsilon^2) < ab \\leq \\alpha\\beta\n    \\end{equation*}\n    Since $ab$ is bounded above by $\\alpha\\beta$ we have $ab \\leq \\text{ sup}(AB)$. We let $\\varepsilon' = \\varepsilon\\alpha+\\varepsilon\\beta-\\varepsilon^2 > 0 $ so $\\forall(0 < \\varepsilon' < \\alpha+\\beta)$ we have $\\alpha\\beta-\\varepsilon'< ab < \\text{ sup}(AB) \\implies \\alpha\\beta \\leq \\text{ sup}(AB) + \\varepsilon' \\implies \\alpha\\beta \\leq \\text{ sup}(AB)$ by ``elbow room''.\n",
    "proof": "I will try to be as clearly as posible.\nFor $A, B \\subseteq \\mathbb{R}^+$, we will prove that $\\sup(A)\\cdot \\sup(B) = \\sup(AB)$. For this, we will prove that $\\sup(A)\\cdot \\sup(B) \\leqslant \\sup(AB)$ and $\\sup(AB) \\leqslant \\sup(A)\\cdot \\sup(B)$. Of course, $AB=\\{xy:x\\in A~\\wedge~y\\in B\\}$.\nLet, $$a=\\sup(A), b=\\sup(B), M=\\sup(AB)$$\nProof ($\\sup(A)\\cdot \\sup(B) \\leqslant \\sup(AB)$).\nBy definition, $\\forall x\\in A(x\\leqslant a)$ and $\\forall y\\in B(y\\leqslant b)$. Since $A, B \\subseteq \\mathbb{R}^+$, $x, y>0$ thus $0<x\\leqslant a$ and $0<y\\leqslant b$ where it follows that $xy\\leqslant ab$, where $xy\\in AB$. This means that $AB\\leqslant ab$ which implies that $M\\leqslant ab$.\nNow, by definition, $\\forall z\\in AB (z\\leqslant M)$. Since $z\\in AB$, $z=xy$ where $x\\in A$ and $y\\in B$ thus $xy\\leqslant M$. Since $x, y>0$, $xy>0$, this way we can define the quotient  $x\\leqslant \\frac{M}{y}$ which means that $\\frac{M}{y}$ is an upper bound for $A$ which implies that $a\\leqslant \\frac{M}{y}$. It follows that $ay\\leqslant M$ thus $y\\leqslant \\frac{M}{a}$ which means that $\\frac{M}{a}$ is an upper bound for $B$ which implies that $b\\leqslant \\frac{M}{a}$. Therefore, $ab\\leqslant M$.\n$$\\therefore \\sup(A)\\cdot \\sup(B) = \\sup(AB)$$\n",
    "tags": [
      "real-analysis",
      "proof-verification",
      "proof-writing",
      "supremum-and-infimum"
    ],
    "score": 6,
    "answer_score": 6,
    "is_accepted": false,
    "question_id": 1468022,
    "answer_id": 3782603
  },
  {
    "theorem": "Prove that if $A \\mathop \\triangle B \\subseteq A$ then $B\\subseteq A$",
    "context": "I'm having trouble with the logic in this proof and was wondering if anyone could point me in the right direction (if I'm wrong)?\nProve that if $A\\mathop\\triangle B\\subseteq A$ then $B\\subseteq A$. (Here $\\triangle$ refers to the symmetric difference).\nI started by using the definition of symmetric difference that $A\\mathop\\triangle B = (A\\setminus B)\\mathop\\cup \\mathop(B\\setminus A)$. So $A\\mathop\\triangle B\\subseteq A$ = $\\forall\\psi[(\\psi\\in A \\wedge \\psi \\notin B) \\vee (\\psi \\in B \\wedge \\psi \\notin A) \\rightarrow \\psi \\in A$].\nHere is what I have for my proof:\nSuppose $x \\in B$. Suppose $x \\notin A.$ Then since $x \\in B$ and $A\\mathop\\triangle B\\subseteq A$, it follows that $x \\in A$. But this contradicts the fact that $x \\notin A$, so we can conclude that $x \\in A$. Since $x$ was an arbitrary element of $B$, it follows that $B\\subseteq A$.\nWhat I'm wondering is, is it enough to use universal instantiation on $x$ from the statement $\\forall\\psi[(\\psi\\in A \\wedge \\psi \\notin B) \\vee (\\psi \\in B \\wedge \\psi \\notin A) \\rightarrow \\psi \\in A$] given that $x \\in B$ and $ x \\notin A$ to get my contradiction? Also, should I be giving more information about the logic used in the proof, or is it ok to leave it to the reader? Thanks for the help!\n",
    "proof": "Your proof is fine: you need only show the inclusion you've shown. You chose (any) arbitrary $x$ such that $x \\in B, x\\notin A$, and you've reached a contradiction through your assumption that $x \\in B \\land x\\notin A$. This implies (by definition, and perhaps you want to make this explicit) that $x \\in A\\triangle B$. But then since $A\\triangle B \\subseteq A, x\\in A$. This is a contradiction which is realized whatever the $x$ satisfying the initial assumption is chosen. So the proof already shows that for any (all) $x$ such that $x \\in B \\land x\\notin A \\rightarrow x\\in A \\triangle B$, and since $A\\triangle B \\subseteq A,\\;$ then $\\; x\\in A$. \nUniversal instantiation would be redundant.\n",
    "tags": [
      "elementary-set-theory",
      "logic",
      "proof-writing"
    ],
    "score": 6,
    "answer_score": 3,
    "is_accepted": true,
    "question_id": 344393,
    "answer_id": 344399
  },
  {
    "theorem": "Prove that a set consisting of a sequence and its limit point is closed",
    "context": "Can someone please check whether the following simple proof is \"mathematical\"? Is it correct, complete, rigid? Can it be simplified? I'm a complete autodidact so I'm looking for someone to give me feedback to gain experience in writing proofs... This is also my first question on MSE.\nThe proposition:\nLet $(X, d)$ be a metric space and $x_n \\to x$ where each $x_n \\in X$ and $x \\in X$. Let $A$ be the subset of $X$ which consists of $x$ and all of the points $x_n$. Prove that $A$ is closed in $(X, d)$.\nMy tentative to prove this:\nWe first show that all infinite sequences in $A$ converge to $x$: Let $y \\in X$, $y \\ne x$. Then there is some open ball $B_\\epsilon(x)$ with $\\epsilon < d(x,y)$ containing all but finitely many elements of $A$. As $y \\notin B_\\epsilon(x)$ there can be no infinite sequence in $A$ converging to $y$. Consequently all infinite series in $A$ converge to a point in $A$ which therefore must be a closed set.\nEdited: As rightly pointed out in the comments, I should have written in the first sentence \"...sequences with infinitely many distinct terms and which converge to some point of $X$\" and the last sentence should be \"Consequently all infinite sequences...\".\n",
    "proof": "As I said in my comment, your proof is ok. But here is the way I would have done it. I'll write the proof in a more formal way, because in math you can only talk loose after you master writing properly.\nLet $y\\in X\\setminus A$. Let $\\varepsilon=d(y,x)/2$. Then, by the convergence $x_n\\to x$, there exists $n_0$ such that $x_n\\in B_\\varepsilon(x)$ for all $n\\geq n_0$. So, for $n\\geq n_0$, \n$$\\tag{1}\r\nd(x_n,y)> d(x,y)-d(x_n,x)>d(x,y)-\\varepsilon=d(x,y)/2.\r\n$$\nLet $\\delta=\\min\\{d(x,y),d(x_1,y),\\ldots,d(x_{n_0},y)\\}/2$. Then\n$d(y,x_n)\\geq\\delta$ if $n\\leq n_0$, and by ($1$) $d(y,x_n)\\geq\\delta$ if $n\\geq n_0$. This shows that $B_\\delta(y)$ has no intersection with $A$, i.e. $B_\\delta(y)$ is contained in $X\\setminus A$. As $y$ was arbitrary, this shows that $X\\setminus A$ is open, i.e. $A$ is closed. \n",
    "tags": [
      "general-topology",
      "sequences-and-series",
      "proof-writing"
    ],
    "score": 6,
    "answer_score": 7,
    "is_accepted": true,
    "question_id": 125299,
    "answer_id": 125314
  },
  {
    "theorem": "How do I prove exercise 1.15 from Proofs by Jay Cummings",
    "context": "I am self studying proofwriting in preparation for an algebra class, and I though it would be a good idea to work through an introductory textbook. I ended up choosing Proofs by Jay Cummings due to all the endorsements, and after speeding through the first chapter and first dozen or so exercises, I’ve really hit a wall with number $1.15$.\n\nAssume that $n$ is a positive integer. Prove that if one chooses any $n+1$ distinct odd integrs from\n{$1,2,3,...,3n$}, then at least one of these numbers will divide another.\n\nMy main issue is not in getting the right answer, but in that my proof seems too clunky and assumption-based to be good enough.\nHere is the outline of my line of thought:\nFor my testing, I used $n = 7 \\implies  \\{1,2,3,...,21\\}$\n$\\boldsymbol {1)}$ Firstly, I removed all even numbers from the set. They are irrelevant.\n$\\boldsymbol{2)}$ I observed that I can composed a new set from the odd numbers wherein each index is the result of counting the number of odds which the number at that index divides. Example: $${1,3,5,7,9} \\implies\\{4,1,0,0,0\\}$$\n$\\boldsymbol{3)}$ I noticed that the cutoff happened $n$ odds from the end of the set.\n$\\boldsymbol{4)}$ I then came up with this idea:   Worst case scenario, all the zero-divisor odds would be chosen first. We, however, need to choose $n+1$, so we have to pick a number that divides into one of the larger odds.\nI think I am (at least a bit) on the right track, I just don’t know how I am supposed to present this in a rigorous proof. Any help would be much appreciated.\n",
    "proof": "This is an answer using the pigeonhole principle.\nLet us call the pigeonhole which contains all the numbers of the form $3^ab$ \"pigeonhole $b$\" where $a$ is a non-negative integer and $b$ is an odd integer not divisible by $3$.\nFor example, for $n=5$, we have the followings :\n\npigeonhole 1 contains $1,3,9$.\n\npigeonhole 5 contains $5,15$.\n\npigeonhole 7 contains $7$.\n\npigeonhole 11 contains $11$.\n\npigeonhole 13 contains $13$.\n\n\nThe number of pigeonhole is $n$. The reason is as follows :\n\nIf $n$ is odd, then there are $\\frac{3n+1}2$ odd integers, and $\\frac{n+1}2$ of them are divisible by $3$. So, there are $\\frac{3n+1}{2}-\\frac{n+1}{2}=n$ pigeonholes.\n\nIf $n$ is even, then there are $\\frac{3n}2$ odd integers, and $\\frac n2$ of them are divisible by $3$, so there are $\\frac{3n}{2}-\\frac n2=n$ pigeonholes.\n\n\nWe have $n$ pigeonholes and $n+1$ pigeons, so at least one pigeonhole has more than one pigeon.\nThis means that we have two numbers of the form $3^cb$ and $3^db$. So, we can say that one divides another.\n",
    "tags": [
      "proof-writing",
      "self-learning"
    ],
    "score": 6,
    "answer_score": 6,
    "is_accepted": true,
    "question_id": 4905484,
    "answer_id": 4907271
  },
  {
    "theorem": "Disproving a statement by showing that its negation doesn&#39;t lead to a contradiction?",
    "context": "Proof by contradiction (or the contradiction rule in logic) uses the first row of the following truth table to assert that if $¬p\\Rightarrow{}\\textbf{c}$ is true, then $p$ is true:\n\n\n\n\n$p$\n$¬p$\n$\\textbf{c}$\n$¬p\\Rightarrow{}\\textbf{c}$\n$(¬p\\Rightarrow{}\\textbf{c})\\Rightarrow{}p$\n\n\n\n\nT\nF\nF\nT\nT\n\n\nF\nT\nF\nF\nT\n\n\n\n\n(let $\\textbf{c}$ and $\\textbf{t}$ denote a contradiction and tautology respectively)\nWhat I have not seen discussed anywhere is that the second row shows that when $¬p\\Rightarrow{}\\textbf{c}$ is false, $p$ is false. The potential validity of this argument is further hinted by the equivalence $(¬p\\Rightarrow{}\\textbf{c})\\equiv(p\\vee{}\\textbf{c})\\equiv{p}$, which means $(¬p\\Rightarrow{}\\textbf{c})\\iff{p}$. Is it possible to use this argument form, that is, to show that $¬p\\Rightarrow{}\\textbf{c}$ is false and conclude that $p$ is false? (opposite of the proof by contradiction, which concludes $p$ is true.)\nI feel that this argument form is not completely logical/practical, but I am not 100% why. My guess is that you would have to show $¬p$ does not lead to any contradictions, which is not practical as you would have to cover all (infinite) cases. Or you could show that $¬p$ leads to a tautology, but suddenly I'm not sure how any statement implies a tautology. Any help that would point me in the right direction is appreciated!\n",
    "proof": "\n$$(¬p\\Rightarrow{}\\textbf{c})\\equiv{p}$$\n\nAlso: $\\quad(\\lnot p{\\kern.6em\\not\\kern-.6em\\implies}\\bot)\\;\\equiv\\;(\\lnot p\\land\\lnot \\bot)\\;\\equiv\\;(\\lnot p\\land\\top)\\;\\equiv\\;\\lnot p.$\n\nTo conclude that $p$ is false,\n\n\nis it possible to show that $(¬p\\Rightarrow{}\\textbf{c})$ is false?\n\nThis requires establishing that the antecedent $¬p$ is true (since a false antecedent automatically makes the implication true).\n\nMy guess is that you would have to show that $¬p$ does not lead to any contradictions\n\nThis also requires establishing that the antecedent $¬p$ is true (since a false antecedent can always lead to a contradiction).\nBut $¬p$ being true directly means that $p$ is false, without going through the above detours.\n\nOr you could show that $¬p$ leads to a tautology\n\nThis does not entail that $p$ is false; for example: $¬(A\\lor\\lnot A)\\implies\\top.$\n\nI feel that my argument is not completely logical\n\nSummary: your proposal to disprove $p$ by showing that $\\lnot p$ leads to no contradiction is not illogical—merely a circuitous way of disproving $p$ by proving $\\lnot p.$\n",
    "tags": [
      "logic",
      "proof-writing",
      "propositional-calculus"
    ],
    "score": 6,
    "answer_score": 6,
    "is_accepted": true,
    "question_id": 4731322,
    "answer_id": 4731400
  },
  {
    "theorem": "What is the maximum number of warriors one can put on a chess board so that no two warriors attack each other?",
    "context": "\nIn chess, a normal knight goes two steps forward and one step to the side, in some orientation. Thanic thought that he should spice the game up a bit, so he introduced a new kind of piece called a warrior. A warrior can either go three steps forward and one step to the side, or two steps forward and two steps to the side in some orientation.\nGiven a $2020\\times2020$ chess board. Find, with proof, the maximum number of warriors one can put on its cells such that no two warriors attack each other.\n\nThe question is a modified version of a problem from Bangladesh Mathematical Olympiad 2019. For more clarity, here is a picture that shows example moves of a warrior:\n\nThis is my first time solving this kind of problem. I've made the following progress in solving the question:\nWe place the warriors in each cell of $n$-th column where $n\\equiv1\\ (\\bmod 4)$. The following picture shows this strategy in an $8\\times8$ board:\n\nIt can be seen that no two warriors can\nattack each other. Hence, the answer to our original problem should be $2020\\times505$.\nThough this result matches with the original answer, I have still some confusions. Firstly, the optimal strategy is that in the $2020\\times2020$ board, we place a warrior in each cell of $n$-th column. But what if we don't place them with that strategy or we just randomly place the warriors so that they cannot attack each other? How will I know other strategies would not give a result greater than $2020\\times 505$? More specifically, how do I write a formal proof for this kind of problems?\n",
    "proof": "$505\\times 2020=1,\\!020,\\!100$ is certainly not optimal. By tiling a $2019\\times 2020$ board with a rectangular pattern of the following $3\\times 5$ rectangle, you can fit $$4\\times  \\frac{2019}3\\times \\frac{2020}{5}=1,\\!087,\\!568\\newcommand{\\W}{\\mathsf{W}}$$ warriors onto a $2019\\times 2020$ board alone.\n$$\n\\begin{array}{|c|c|c|c|c|}\n\\hline\n&&\\;\\W\\;&&\\phantom{\\Big(\\;\\;}\n\\\\\\hline\n&\\;\\W\\,&&\\;\\W\\;&\\phantom{\\Big(\\;\\;\\;\\,}\n\\\\\\hline\n\\phantom{\\Big(\\;\\;\\;}&&\\W&&\n\\\\\\hline\n\\end{array}\n$$\nYou strategy achieves a density of $1/4$ warriors/square, while mine has a density of $4/15$, so the latter should always be better for large enough boards. I do not know if this can be improved at all.\n\nLet $D$ be the optimal packing density for warriors. In addition to the lower bound of $D\\ge 4/15$, I can prove the upper bound $D\\le 1/3$.\nFor each warrior, imagine placing a token on the $12$ squares that the warrior can attack. Some squares will have multiple tokens. However, you can show that every square will have at most $6$ tokens. Indeed, for any unoccupied square $\\mathsf X$, if we partition the $12$ squares that can attack $\\mathsf X$ into $6$ attacking pairs as shown in this table, (pairs are labeled $\\mathsf A$ through $\\mathsf F$), then we see that $\\mathsf X$ can be attacked from at most one square in each pair.\n$$\n\\begin{array}{|c|c|c|c|c|c|c|}\n\\hline \n & &\\mathsf C& &\\mathsf D& &  \\\\\\hline\n &\\mathsf B& & & &\\mathsf C&  \\\\\\hline\n\\mathsf A& & & & & &\\mathsf D \\\\\\hline\n & & &\\mathsf X& & &  \\\\\\hline\n\\mathsf B& & & & & &\\mathsf E \\\\\\hline\n &\\mathsf A& & & &\\mathsf F&  \\\\\\hline\n & &\\mathsf F& &\\mathsf E& &  \\\\\\hline\n\\end{array}\n$$\nThis means that each warrior effectively occupies $1+12\\times \\frac16=3$ squares, so you can have no more than $1/3$ warriors per square.\nThis is only a \"long-run\" result, since warriors at the boundary of a grid will place fewer than $12$ tokens. However, this effect is negligible in the long run.\n",
    "tags": [
      "combinatorics",
      "optimization",
      "proof-writing",
      "contest-math",
      "chessboard"
    ],
    "score": 6,
    "answer_score": 6,
    "is_accepted": false,
    "question_id": 4259114,
    "answer_id": 4259310
  },
  {
    "theorem": "$L^p$ space, simple function and density.",
    "context": "The measurable simple functions $\\mathcal{S}$ are dense in $L^p(X,\\mathcal{A,\\mu}).$ The key result on which this is based is the following theorem.\nTheorem. Let $(X,\\mathcal{A})$ a measurable space, $f\\colon X\\to \\mathbb{\\overline{R}}$. Then exists a sequence $\\{s_n\\}$ of simple functions on $X$ such that $\\lim_{n\\to\\infty} s_n(x)=f(x)$ in $X$. Moreover,\n$(i)\\;$ if $f$ is measurable, then $s_n$ is measurable for all $n$;\n$(ii)\\;$ if $f$ is nonegative, the sequence $\\{s_n\\}$ is increasing and we have $$0\\le s_n\\le f\\quad n\\in\\mathbb{N}.$$\n$(iii)\\;$ if $f$ is bounded, then the convergence is uniform.\nNow, in the proof it is supposed that, at first, that $f$ is bounded and nonnegative; then  it is built $$s_n:=\\sum_{k=0}^{2^n-1}\\frac{k}{2^n}\\chi_{E_k^n},$$ where $$E_k^n:=\\bigg\\{x\\in X\\;\\bigg|\\; \\frac{k}{2^n}\\le f(x)<\\frac{k+1}{2^n}\\bigg\\}.$$\nIf $f$ it is not bounded(non negative), then it is built $$s_n=n\\chi_{E_I^n}+\\sum_{k=0}^{n2^n-1}\\frac{k}{2^n}\\chi_{E_k^n},$$ where $$E_I^n:=\\{x\\in X\\;|\\; f(x)\\ge n\\}.$$\n\nQuestion. In light of the above, since the $s_n$ coefficients are always rational, can I conclude in the same way that the simple functions with rational coefficients (obviously with limited support) are dense in $L^p(X)$? \n  If I can't finish this, how do I show it then?\n\n",
    "proof": "Yes, the theorem you cite leads to the result on density. Suppose $1\\le p <\\infty$ and $f\\in L^p.$ Write $f= f^+-f^-.$ Then there exist simple nonnegative $s_n,t_n$ with rational coefficients such that $0\\le s_n \\le f^+,$ $0\\le t_n \\le f^-$ with $ s_n \\to f^+,$ $ t_n \\to f^-$ pointwise everywhere. Note that\n$$|s_n-f^+|^p = (f^+ -s_n)^p \\le (f^+)^p\\,\\,\\text{everywhere}.$$\nThus by the DCT, $\\|s_n-f^+\\|_p \\to 0,$ i.e., $s_n\\to f^+$ in $L^p.$ Similarly, $t_n\\to f^-$ in $L^p.$ It follows that $s_n-t_n \\to f^+ - f^-=f$ in $L^p.$ Since each $ s_n+t_n$ is a simple function with rational coefficients, we're done.\nI'll leave the $p=\\infty$ case to you for now. Ask if you have any questions.\n",
    "tags": [
      "real-analysis",
      "proof-writing",
      "lp-spaces"
    ],
    "score": 6,
    "answer_score": 6,
    "is_accepted": true,
    "question_id": 3680568,
    "answer_id": 3688311
  },
  {
    "theorem": "Prove that $R$ is reflexive, symmetric, and transitive.",
    "context": "Define a relation $R$ on $\\Bbb Z$ by declaring that $xRy$ if and only if $x^2\\equiv y^2\\pmod{4}$. Prove that $R$ is reflexive, symmetric, and transitive.\nSuppose $x\\in\\Bbb Z$. Then $x^2\\equiv x^2\\pmod {4}$ means that $4\\mid (x^2-x^2)$, so $x^2-x^2=4a$ where $a=0\\in\\Bbb Z$. Therefore $R$ is reflexive. \nNow suppose $x^2\\equiv y^2\\pmod {4}$. This means $4\\mid (x^2-y^2)$ and so $x^2-y^2=4a$, for some $a\\in\\Bbb Z$. Multiplying by $-1$ we have $-1(x^2-y^2=4a)\\\\\\rightarrow -x^2+y^2=-4a\\\\\\rightarrow y^2-x^2=4(-a)$\nso $4\\mid(y^2-x^2)$ and $y^2\\equiv x^2\\pmod{4}$. This shows that $R$ is symmetric. \nNow we assume that $x^2\\equiv y^2\\pmod{4}$ and $y^2\\equiv z^2\\pmod{4}$. This means $4\\mid(x^2-y^2)$ and $4\\mid(y^2-z^2)$. Then we have $x^2-y^2=4a$ and $y^2-z^2=4b$ for some $a,b\\in\\Bbb Z$. Rearranging we get $x^2=4a+y^2$ and $z^2=y^2-4b$. \nThen\n$\\begin{align*}x^2-z^2&=(4a+y^2)-(y^2-4b)\\\\&=4a+4b\\\\&=4(a+b)\\end{align*}$\nThis shows that $4\\mid(x^2-z^2)$ and $x^2\\equiv z^2\\pmod{4}$, therefore $R$ is transitive.\n$\\blacksquare$\nPlease forgive my rushed formatting, just wondering if my arguments here work and if this is a valid proof. Any feedback is appreciated, thanks!\n",
    "proof": "I agree with J. W. Tanner's question comment that what you've done looks all right.\nI have just one small suggestion. With your $x^2−y^2=4a$ and $y^2−z^2=4b$ equations, you don't need to do any rearranging. Instead, you can just add these $2$ equations, as the $y^2$ terms cancel, to more directly get your result of $x^2−z^2=4a+4b=4(a+b)$. This will make your proof a bit shorter & more succinct.\n",
    "tags": [
      "elementary-number-theory",
      "proof-writing",
      "proof-explanation",
      "solution-verification"
    ],
    "score": 6,
    "answer_score": 4,
    "is_accepted": true,
    "question_id": 3488525,
    "answer_id": 3488552
  },
  {
    "theorem": "Why is the expectation of a trace equal to the trace of the expectation?",
    "context": "Some textbooks use the property $$\\mathbb{E}\\left[\\operatorname{tr}\\left(X\\right)\\right]=\\operatorname{tr}\\left(\\mathbb{E}\\left[X\\right]\\right)$$\nBut why? I would really appreciate it if someone could prove this.\n",
    "proof": "I am assuming that $X$ is a random matrix, with finite dimensions. Then\n$$\n\\textrm{tr}\\ X=\\sum_i X_{ii}.\n$$\nHence, the claim follows by linearity of expectation, since\n$$\n\\mathbb E(\\textrm{tr}\\ X)=\\mathbb E\\sum_i X_{ii}=\\sum_i \\mathbb E X_{ii}=\\sum_i (\\mathbb E X)_{ii}=\\textrm{tr}\\ (\\mathbb EX).\n$$\n",
    "tags": [
      "linear-algebra",
      "statistics",
      "proof-writing"
    ],
    "score": 6,
    "answer_score": 6,
    "is_accepted": true,
    "question_id": 3268268,
    "answer_id": 3268274
  },
  {
    "theorem": "Proof that $ \\frac{3\\pi}{8}&lt; \\int_{0}^{\\pi/2} \\cos{\\sin{x}} dx &lt; \\frac{49\\pi}{128}$",
    "context": "Proof that\n$$\n\\frac{3\\pi}{8} <\n\\int_{0}^{\\pi/2} \\cos\\left(\\sin\\left(x\\right)\\right)\\,\\mathrm{d}x <\n\\frac{49\\pi}{128}\n$$\nCan somebody give me some instruction how to deal with inequality like that? My current idea is:\nI see $\\frac{3\\pi}{8}$ on the left. So I think that I can prove that\n$$ \\frac{3}{4}<\\cos{\\sin{x}} $$\nAnd after take integral:\n$$ \\frac{3x}{4} \\rightarrow \\frac{3}{4} \\cdot \\frac{\\pi}{2} = \\frac{3\\pi}{8} $$\nBut it is not true because\n$$ \\cos{\\sin{x}} \\geqslant \\cos{1} \\approx 0.5403 < 3/4$$\nWhat have I do in such situation?\n",
    "proof": "Hint for one part, using this inequality and this one\n$$\\cos{x} \\geq 1 - \\frac{x^2}{2}$$\nwe have\n$$\\int\\limits_{0}^{\\frac{\\pi}{2}}\\cos{\\sin{x}} dx > \\int\\limits_{0}^{\\frac{\\pi}{2}} \\left(1-\\frac{\\sin^2{x}}{2}\\right)dx=\\frac{3 \\pi}{8}$$\n",
    "tags": [
      "integration",
      "trigonometry",
      "inequality",
      "proof-writing"
    ],
    "score": 6,
    "answer_score": 3,
    "is_accepted": true,
    "question_id": 3240437,
    "answer_id": 3240477
  },
  {
    "theorem": "Prove that $f(x) = \\sum_{n=1}^{\\infty} x^n/n^2$ is continuous on $[0,1]$",
    "context": "I have a general idea of how to prove this but I could use some help with the details. \nBasically I see that $f(x)$ is the uniform limit of $f_k(x) = \\sum_{n=1}^{k} x^n/n^2$ on $[0,1]$.\nEach $f_k$ is continuous, so $f$ is as well since uniform convergence preserves continuity.\nIs this proof correct/does it seem sufficient? \nThanks ahead of time.\n",
    "proof": "Note that $|x^n/n^2|\\leq 1/n^2$ on  $[0,1]$ and $\\sum_{n=1}^{\\infty} 1/n^2 \\lt \\infty$. Then by Weierstrass M-test,$\\sum_{n=1}^{\\infty} x^n/n^2$ converges uniformly. Hence $f$ is continuous.\n",
    "tags": [
      "real-analysis",
      "proof-verification",
      "proof-writing"
    ],
    "score": 6,
    "answer_score": 5,
    "is_accepted": true,
    "question_id": 3019664,
    "answer_id": 3019677
  },
  {
    "theorem": "Prove that $2\\text{Area}(\\omega) \\leq \\text{length}(\\gamma)\\,\\text{distance}(\\gamma)$",
    "context": "So basically, I am given the following to prove:\n\nLet $+\\gamma$ be a positively oriented smooth Jordan arc, and let $\\omega$ denote the interior of $+\\gamma$. Recall that if $F = (F_1, F_2):D \\to \\mathbb{R}^2$ is a continuously differentiable vector field in an open set $D$ containing $\\omega \\cup(+\\gamma)$, then\n$$\\iint_\\omega \\left(\\frac{\\partial F_2}{\\partial x} - \\frac{\\partial F_1}{\\partial y}\\right) dxdy = \\oint_{+\\gamma}F \\cdot \\overrightarrow{ds} $$\nwhere the right hand-side is the line-integral of $F$ along the path $+\\gamma$.\nBy suitably choosing $F$, prove that $$ \\DeclareMathOperator{\\Area}{Area} \\DeclareMathOperator{\\diameter}{diameter} \\DeclareMathOperator{\\length}{length} 2\\Area(\\omega) \\leq \\diameter(+\\gamma) \\length(+\\gamma)$$\nwhere\n$\\diameter(+\\gamma) = \\sup\\{|z(s)-z(t)| : s,t \\in [a,b]\\}$ and $\\length(+\\gamma) = \\int_{a}^{b} |z'(t)| dt$.\n\nThe only thing I know so far is that I need to find an $F$ such that  $\\frac{\\partial F_2}{\\partial x} - \\frac{\\partial F_1}{\\partial y} =1$ because then $$\\iint_{\\omega}  dxdy = \\Area(\\omega).$$ However, I do not know where to proceed from there!\n",
    "proof": "I assume that $z$ is $\\gamma$ in the definition of $\\operatorname{diameter}(+\\gamma)$.  Without loss of generality, we can assume that $\\gamma(a)=(0,0)$ (otherwise, we can translate $\\gamma$ until the point $\\gamma(a)$ hits the origin).\nI would take $\\vec{F}=(F_1,F_2)$ with $F_1=-y$ and $F_2=x$,\nso that\n$$\\frac{\\partial F_2}{\\partial x}-\\frac{\\partial F_1}{\\partial y}=2.$$\nBy Green's theorem, we have\n$$2\\operatorname{Area}(\\omega)=\\iint_\\omega \\left(\\frac{\\partial F_2}{\\partial x}-\\frac{\\partial F_1}{\\partial y}\\right)\\ dx\\ dy=\\oint_{+\\gamma}\\vec{F}\\cdot \\vec{dr}.$$\nHence,\n$$2\\operatorname{Area}(\\omega)\\leq \\oint_{+\\gamma}\\left\\vert\\vec{F}\\cdot \\vec{dr}\\right\\vert\\leq \\oint_{+\\gamma}\\left\\Vert\\vec{F}\\right\\Vert\\ dr.$$\nBecause $\\left\\Vert\\vec{F}\\right\\Vert\\leq \\operatorname{diameter}(+\\gamma)$, we conclude that\n$$2\\operatorname{Area}(\\omega)\\leq \\oint_{+\\gamma}\\operatorname{diameter}(+\\gamma)\\ dr=\\operatorname{diameter}(+\\gamma)\\oint_{+\\gamma}dr=\\operatorname{diameter}(+\\gamma)\\operatorname{length}(+\\gamma).$$\n\nThis inequality is a quite weak.  If we define $\\operatorname{radius}(+\\gamma)$ to be \n$$\\inf\\Big\\{r>0:\\exists p\\in\\mathbb{R}^2,\\ \\forall u\\in[a,b],\\ \\big\\vert\\gamma(u)-p\\big\\vert< r\\Big\\}\\,,$$\nthen we have\n$$2\\operatorname{Area}(\\omega)\\leq \\operatorname{radius}(+\\gamma)\\operatorname{length}(+\\gamma).$$\nThe equality holds iff $\\gamma$ traces a circle (once).  You can use Jung's theorem to show that\n$$2\\operatorname{Area}(\\omega)\\leq \\frac{1}{\\sqrt{3}}\\operatorname{diameter}(+\\gamma)\\operatorname{length}(+\\gamma),$$ which is stronger than the required result, but is still weak.  So, it is an interesting question to find the infimum $\\lambda_{\\min}$ of all $\\lambda>0$ such that \n$$\\operatorname{Area}(\\omega)\\leq \\lambda\\operatorname{diameter}(+\\gamma)\\operatorname{length}(+\\gamma).$$\nWe already know that $\\lambda_{\\min} \\leq \\frac1{2\\sqrt{3}}$.\n",
    "tags": [
      "integration",
      "complex-analysis",
      "multivariable-calculus",
      "proof-writing",
      "vector-analysis"
    ],
    "score": 6,
    "answer_score": 4,
    "is_accepted": true,
    "question_id": 2960734,
    "answer_id": 2960777
  },
  {
    "theorem": "Show that arbitrary $A$ and $A^T$ have same eigenvalue, algebraic and geometric multiplicity",
    "context": "\nShow that an arbitrary $n \\times n$ matrix $A$ and its transpose $A^T$\n  have the same eigenvalues, algebraic multiplicity and geometric\n  multiplicity.\n\nI'm not sure if I did it correctly and especially how to show that they have same geometric multiplicity?\n\nsame eigen values\n\nAssume $A$ and $A^T$ have same eigenvalues, then they have the same chracteristic polynomial. So we need to show that $p_A(\\lambda)=\\det(A-\\lambda I)$ is same as $p_{A^T}(\\lambda)=\\det(A^T-\\lambda I)$.\nSo we have $$p_{A^T}(\\lambda)=\\det(A^T-\\lambda I) = \\det(A^T-\\lambda I^T) = \\det\\left((A-\\lambda I)^T\\right) = \\det(A-\\lambda I)=p_A(\\lambda)$$\nWe see their characteristic polynomials are same so their eigenvalues are same as well.\n\nsame algebraic multiplicity\n\nI'm not sure if this is a correct reason proof but: Because the characteristic polynomials are same, we have that the algebraic multiplicities of the eigenvalues of $A$ nd $A^T$ sre the same.\n\nsame geometric multiplicity\n\nI don't know? :/\n",
    "proof": "Geometric Multiplicity of $\\lambda$ in $A=\\text{rank} (A-\\lambda I)=\\text{rank }(A-\\lambda I)^T=\\text{rank } (A^T-\\lambda I)=$Geometric multiplicity of $\\lambda$ in $A^T$\n",
    "tags": [
      "linear-algebra",
      "matrices",
      "proof-writing",
      "eigenvalues-eigenvectors",
      "transpose"
    ],
    "score": 6,
    "answer_score": 6,
    "is_accepted": false,
    "question_id": 2798918,
    "answer_id": 2798923
  },
  {
    "theorem": "Amateur proof verification: Between two consecutive roots of $f&#39;$ there is at most one root of $f$",
    "context": "i need someone to verify if i'm doing anything wrong on proving the following theorem (i'm new to real analysis and formal proofs). Also, suggestions on how to write it better would be appreciated.\n$f: I\\rightarrow R$ differentiable. Beteween two consecutives roots of $f'$ there is at most one root of $f$\nI think i can see why this is true.\nInformal Attempt: Let $g$ be a constraint of $f$ to the interval $[a,b]$, $a<b$, $g: [a,b]\\rightarrow R$. if $f'(a)=g'(a)$ and $f'(b)=g'(b)$ are consecutive zeros of $f'$, they are the sole zeros of $g'$. By the Weierstrass extreme value theorem, since $g$ is continuous and $[a,b]$ is a compact set, we know $g$ obtains its extreme values. Since the only two zeroes of $g'$ are, by its very definition, $a$ and $b$, they must be these extreme values. \nFrom this point on, i know that, by \"looking\" at the graph of $g$, it must intersect the $x$ axis at most once, otherwise there would be other  $g'(x)=0$. How can i write this down formally? Have i missed anything? After proving that result for $g$, I intended to apply it to $f$ and get to the final result.\nThanks for your attention.\n",
    "proof": "Assume there are $2$ or more roots of $f$ in between the $2$ consecutive roots of $f'$. Rolle's theorem says there must be another stationary point between $2$ of those roots of $f$ which leads to a contradiction.\n\nDepending on context, you may need to prove Rolle's theorem, of course.\n",
    "tags": [
      "calculus",
      "real-analysis",
      "functions",
      "proof-verification",
      "proof-writing"
    ],
    "score": 6,
    "answer_score": 6,
    "is_accepted": true,
    "question_id": 2364295,
    "answer_id": 2364301
  },
  {
    "theorem": "How can I prove whether a $9\\times 9$ square can be filled with L-shaped pieces in a completely &quot;regular&quot; way?",
    "context": "There are a great many ways to fill a $9\\times 9$ square with L-shaped pieces. One of them is below.\n\nNow, note that there are eleven $2\\times 3$ rectangles that are formed, as well as a larger L shape. There is one \"irregular\" piece, which has been colored in green. What I think of as being \"regular\" is a bit subjective (as in aesthetically appealing), but I think it suffices to define a regular piece as being part of a rectangle or larger L-shape.\nI conjecture that there must be at least one irregular piece. How can I prove this? (Alternatively, if I'm wrong, what would be a counterexample?)\nThe proof I'm thinking of is that all rectangles that can fit in a $9\\times 9$ and can be constructed from L-pieces must have at least one side with even length, and that larger L-shapes also have dimensions that are of even lengths. Hence, as $9 \\cdot 9 = 81$ is odd and all \"regular\" formations have even numbers of squares in them, there must be at least one square that does not fit into a \"regular\" formation, which them requires at least one \"irregular\" L-piece, which completes the proof. Is this rigorous enough (after adding mini-proofs that show why regular shapes must have even numbers of squares in them), or am I lacking important details?\n",
    "proof": "Actually if you consider a \"larger L shape\" as regular, you can take your green L\nas part of such a larger L shape:\n\n",
    "tags": [
      "proof-verification",
      "proof-writing",
      "recreational-mathematics"
    ],
    "score": 6,
    "answer_score": 4,
    "is_accepted": true,
    "question_id": 1095903,
    "answer_id": 1095927
  },
  {
    "theorem": "Proving a Subset Identity: If $A\\subseteq B$ and $C\\subseteq D$, then $A\\cap C\\subseteq C\\cap D$ and $A\\cup C\\subseteq C\\cup D$",
    "context": "Working on part A of this problem:\n\nProve each of the following results without using Venn diagrams of membership tables. (Assume a universe $\\mathscr U$.)\na) If $A\\subseteq B$ and $C\\subseteq D$, then $A\\cap C\\subseteq C\\cap D$ and $A\\cup C\\subseteq C\\cup D$.\nb) $A\\subseteq B$ if and only of $A\\cap\\overline B=\\emptyset$.\nb) $A\\subseteq B$ if and only of $\\overline A\\cup B=\\emptyset$.\n\nI worked out  the first part like this:\n\nIf $A$ is a subset of $B$ then $\\forall~x~[x\\in A \\implies x\\in B]$\n\nSame goes for $C$ being a subset of $D$ (If $x$ is in $C$ it is in $D$)\n\nIf $A\\cap C$, then $x\\in A \\wedge x\\in C$\n\n$x\\in B ~\\wedge x\\in D$ (From steps $1$ and $2$)\n\nSince $A\\cap B \\implies B\\cap D$ we can say $A\\cap C \\subseteq B\\cap D $\n\n\nI'm wondering if I've made any errors in proving the first part of the consequent?\n",
    "proof": "\nStarting with $(3)$ I would instead assume $x \\in A\\cap C$.\n\nThen argue as you did (...so $x \\in A \\land x\\in C$; using this together with the implications in $(1), (2)$ from modus ponens we get $x\\in B$ and $x\\in D$.\nConclude $x \\in B\\cap D$.\n\nTherefore $x \\in A \\cap C \\rightarrow x \\in B\\cap D$. (Having assumed $x \\in A\\cap C$, we derived $x \\in B\\cap D.$, Hence this implication is justified.)\nFinally we conclude $A\\cap C \\subseteq B\\cap D$.\n\n",
    "tags": [
      "discrete-mathematics",
      "elementary-set-theory",
      "proof-writing",
      "solution-verification"
    ],
    "score": 6,
    "answer_score": 6,
    "is_accepted": true,
    "question_id": 860535,
    "answer_id": 860541
  },
  {
    "theorem": "showing $a_n = \\frac{\\tan(1)}{2^1} + \\frac{\\tan(2)}{2^2} + \\dots + \\frac{\\tan(n)}{2^n}$ is not Cauchy",
    "context": "My gut telling me that the following sequence is not Cauchy, but I don't know how to show that.\n$$a_n = \\frac{\\tan(1)}{2^1} + \\frac{\\tan(2)}{2^2} + \\dots + \\frac{\\tan(n)}{2^n}$$\n",
    "proof": "In 2008, Shalikhov has improved the upper bound of irrationality measure of $\\pi$ to about 7.6063. This means for any $\\mu > 7.6063$, there are at most finitely many pairs of relative prime integers $(p,q)$ such that\n$$|\\pi - \\frac{p}{q}| < \\frac{1}{q^{\\mu}}$$\nA consequence of this is if one choose a small enough $C_\\mu > 0$, then for any pair of \npositive integers $(p,q)$, we have\n$$|\\pi - \\frac{p}{q}| > \\frac{C_\\mu}{q^{\\mu}}$$\nIn order for $\\tan n$ to blow up, $n$ need to be very close to some half integer multiple ,  $(\\ell + \\frac12)\\pi$, of $\\pi$. Using the bound of irrational measure above and let $\\mu = 8$, we find \n$$\\left|n - (\\ell + \\frac12)\\pi\\right| \n= \\frac{2\\ell+1}{2}\\left| \\frac{2n}{2\\ell+1} - \\pi \\right|\n> \\frac{C_8}{2(2\\ell+1)^7} \n\\sim  \\frac{\\pi^7C_8}{2^8n^7}\n$$\nNotice\n$$|\\tan x| \\sim \\frac{1}{|x - (\\ell+\\frac12)\\pi|}\\quad\\text{ for } x \\sim (\\ell+\\frac12)\\pi $$\nThis gives us an approximate bound for $\\tan n$\n$$|\\tan n| \\,\\lesssim\\,\\frac{2^8 n^7}{\\pi^7C_8} \\quad\\implies\\quad \\sum_{n=1}^{\\infty} \\frac{\\tan n}{2^n}\\quad\\text{ converges absolutely.}$$\n",
    "tags": [
      "calculus",
      "sequences-and-series",
      "proof-writing",
      "cauchy-sequences"
    ],
    "score": 6,
    "answer_score": 6,
    "is_accepted": true,
    "question_id": 501182,
    "answer_id": 501293
  },
  {
    "theorem": "Proving that a square can be divided into $n$ smaller squares for $n \\ge 6$",
    "context": "I'm trying to prove that for all natural numbers $n \\ge 6$, a square can be divided into $n$ smaller squares.\nThe smaller squares do not need to be of the same size.\nSo for induction, the base case is $P(6)$, which is that a square can be broken into $6$ squares (I can draw a picture to prove this). six squares out of one big square\n",
    "proof": "Hint: You only need to do it for $6$, $7$, and $8$. For these, you need to produce explicit splittings. \nBut after that, anything differs by $3$ from an earlier case. and adding $3$ squares is easy, we just do the natural splitting of an existing square. \nIf one wants to do a formal induction,  let $n \\gt 8$. Suppose the result is true for all $i$ such that $6\\le i \\lt n$. We want to show it holds at $n$. By the induction assumption, it holds at $n-3$. Split one of the squares of the splitting into $n-3$ squares into $4$ squares. That gives us a splitting into $n$ squares.\n",
    "tags": [
      "logic",
      "proof-writing",
      "induction"
    ],
    "score": 6,
    "answer_score": 6,
    "is_accepted": true,
    "question_id": 452252,
    "answer_id": 452256
  },
  {
    "theorem": "Any commutative associative operation can be extended to a function on nonempty finite sets",
    "context": "This is a fact we use very frequently in general mathematics when we write such notations as $1+2+3+4$: since we know that $+$ is commutative and associative, we can just \"drop the parentheses\" and not worry about order of operations. Of course I believe this, but how does one prove this in full generality? Even stating it is giving me trouble. Here's my attempt:\n\nAssume an operation $\\oplus:S\\times S\\to S$ is provided satisfying $x\\oplus y=y\\oplus x$ and $x\\oplus(y\\oplus z)=(x\\oplus y)\\oplus z$ for all $x,y,z\\in S$.\nClaim: Given any finite set $\\emptyset\\subset A\\subseteq S$, there exists a unique $z\\in S$ such that for any function $f:{\\cal P}(A)\\to{\\cal P}(A)$ which satisfies $\\emptyset \\subset f(B)\\subset B$ for all $B\\subseteq A$ with $|B|\\ge 2$ and any function $g:{\\cal P}(A)\\to S$ which satisfies $g(\\{x\\})=x$ for all $x\\in S$ and $g(B)=g(f(B))\\oplus g(B-f(B))$ for all $|B|\\ge 2$, $g(A)=z$.\n\nThe operation $\\oplus$ does not necessarily have an identity element, so we do not attempt to define an empty sum. Intuitively, this element $z$ represents the finite sum of the elements in $A$, so if $A=\\{1,2,3\\}$ and $f(\\{1,2,3\\})=\\{1\\}$ and $f(\\{2,3\\})=\\{3\\}$, then\n$$z=g(\\{1,2,3\\})=g(\\{1\\})\\oplus g(\\{2,3\\})=g(\\{1\\})\\oplus (g(\\{3\\})\\oplus g(\\{2\\}))=1\\oplus(3\\oplus 2).$$\nThere has got to be a better way to say that, but this is the only way I can think of to capture all the possibilities of parenthesization, and still be amenable to a formal proof. And now that I've stated it, how should I prove it? I suppose I should induct on something, but I've no idea what.\nEdit: The goal here is to be able to define an operation $F$ such that $F(\\{x_1,\\dots,x_n\\})=x_1\\oplus\\cdots\\oplus x_n$ and be assured that the operation is well defined and satisfies $F(A\\cup B)=F(A)\\oplus F(B)$, when $A$ and $B$ are disjoint finite nonempty subsets of $S$.\n",
    "proof": "So you want to prove that if $S$ is a set and $+$ is a binary commutative associative law : $S \\times S \\to S$, then there is a unique function $\\sum$ from non empty finite subsets of $S$ to $S$ satisfying :\n- if $x \\in S, \\sum \\{x\\} = x$\n- if $A \\cap B = \\emptyset, \\sum A \\cup B = \\sum A + \\sum B$.\nProve this by induction of the size of the subset : if $|A| = 1$ then we have no choice.\nIf $|A| \\ge 2$, suppose we can define $\\sum A$ in several ways : $A = B_1 \\cup B_2 = C_1 \\cup C_2$, where the pairs are disjoint. Using the induction hypothesis, the sums of the subsets $B_i$ and $C_j$ are well-defined and we need to show that $\\sum B_1 + \\sum B_2 = \\sum C_1 + \\sum C_2$.\nLet $D_{ij} = B_i \\cap C_j$. Suppose for now that none of them is empty. Then $\\sum B_1 = \\sum D_{11} + \\sum D_{12}$ and so on, and the claim boils down to proving $\\forall a,b,c,d \\in S, (a+b)+(c+d) = (a+c)+(b+d)$.\nAnd this is easy : $(a+b)+(c+d) = a+(b+(c+d)) = a+((c+d)+b) = a+(c+(d+b)) = (a+c)+(d+b) = (a+c)+(b+d)$.\nThe cases where one or more of the $D_{ij}$ is empty are done in a similar way (and are even easier). Or you could add an element $\\star$ to $S$ and extend $+$ by defining $\\star + x = x + \\star = x$, prove that $+$ still is commutative and associative, and finally define $\\sum \\emptyset = \\star$\n",
    "tags": [
      "abstract-algebra",
      "notation",
      "proof-writing",
      "universal-algebra"
    ],
    "score": 6,
    "answer_score": 1,
    "is_accepted": true,
    "question_id": 395253,
    "answer_id": 395367
  },
  {
    "theorem": "Is this proof, that $\\sqrt{n}$ is irrational for all non-square $n \\in \\mathbb{N}$, correct or not?",
    "context": "Prove that the square root of all non-square numbers $n \\in \\mathbb{N}$ is irrational\nI have made an attempt to prove this, I don't know if it's correct though:\nTake a non-square number $n \\in \\mathbb{N}$, and we'll assume that $\\sqrt{n}$ is rational.\n$\\sqrt{n} = \\dfrac{p}{q}$ , $p,q \\in \\mathbb{N}$ and they have no common factors.\n$$nq^2=p^2$$ Lets say that $z$ is a prime factor of $q$, it must also be a prime factor of $q^2$. However, it then must ALSO be a prime factor of $p^2$ because of the equality above, and this is a contradiction.\n",
    "proof": "Check carefully what you've done so far and you'll realize that you've \"proved\" that $\\,\\sqrt n\\,$ is always irrational, which of course is false.\nWhat thus is lacking? For example, take \n$$\\sqrt {25}=\\frac{p}{q}\\Longrightarrow 25q^2=p^2 \\,\\,,\\,\\,\\text{ so...what?!}$$\nTrue, if there's some prime factor of $\\,q\\,$ then it must also be a prime factor of $\\,q^2\\,$ and thus of $\\,p^2\\,$ (see Andre comments!) and we get a contradiction...unless...Can you see what little correction goes here?\nIn general, your direction is pretty good.\n",
    "tags": [
      "elementary-number-theory",
      "proof-writing"
    ],
    "score": 6,
    "answer_score": 5,
    "is_accepted": false,
    "question_id": 265616,
    "answer_id": 265624
  },
  {
    "theorem": "Is there anyway to quantify how difficult a statement is to prove in Mathematics?",
    "context": "Suppose we have an axiom system and theorems derived out of that axiom system, is there any way to rigorously speak about a theorem being more difficult to prove than others?\nIn my personal thoughts, I think maybe we can take the difficulty to prove the theorem as in the number of steps in the shortest proof of it... but a disadvantage of this way is that it maybe that a longer proof is actually a psychologically easier to understand.\nAre there other ways?\n\nMotivation\nSuppose in the future that for some reason that we  derive a contradiction from the existing foundation of mathematics then I was contemplating how one could try to salvage it into something which is still contradiction free.\nA thought which come in this contemplation was, Would the theorems of lesser difficulty, for a suitable definition of difficulty, be more possible to retains than others in the salvaged axiom system?\nExample: I am thinking of a situation like the day when Russel Paradox was discovered. If I have understood it correctly, the issue was stemmed out of how the axiom of comprehension was written.\nThe interesting thing was that even when ZFC came, we were able to implement many of the conceptual ideas we had in Set theory in the Naive set theory back in it. So, the thought was, could there be a way to quantify the difficulty so that we could say think things like \"hmm these theorems would could survive even if the foundations were a bit different\".\nTangentially related question on Philosophy.SE and Reverse Mathematics: figuring out what axioms a theorem is equivalent too\n",
    "proof": "There are several ways you can quantify how difficult a theorem is, each having its own advantages and disadvantages.\nProof complexity\nIn your question, you suggested looking at the number of \"steps\" it takes to prove a theorem as a measure of the difficulty of proving that theorem. However, the number of steps it takes to prove a given theorem depends on which proof system you are using to formalize your proofs. In fact, the field of proof complexity is more commonly used to compare entire proof systems to each other, and not to compare specific theorems to each other. For example, an open problem in proof complexity is to determine whether or not there exists a \"polynomially bounded\" proof system (i.e., a proof system that can prove all propositional tautologies via proofs whose sizes are polynomial in the sizes of the tautologies).\nSo, it might not be sensible to speak of \"the\" proof complexity of a given theorem, because it depends on which formal system you are using. Additionally, like you said, the size of a proof might not reflect how difficult the proof is for a human brain to come up with.\nReverse mathematical strength\nThe program of reverse mathematics aims to determine which axioms are necessary to prove which theorems. It works like this:\n\nStart with a \"base theory\", which is not strong enough to prove most of the theorems you are interested in. A base theory should be a strict subset of the axioms that are generally accepted by mathematicians. One common base theory used is $\\text{RCA}_0$.\n\nSelect a theorem that you would like to analyze.\n\nSee which other axioms you are able to prove assuming this theorem (over the base theory).\n\nSee if you can prove the selected theorem assuming these other axioms (over the base theory).\n\n\nIf you are successful, you will have proven that a given theorem is equivalent to some certain axioms (over the base theory). Here's an example: over $\\text{RCA}_0$, the Heine-Borel Theorem is equivalent to the axiom Weak König's Lemma, but the Bolzano-Weierstrass Theorem is equivalent to the Arithmetical Comprehension Axiom. It happens that the Arithmetical Comprehension Axiom implies Weak König's Lemma but that the converse is not true. So, we can say that the Bolzano-Weierstrass Theorem is \"stronger\" than the Heine-Borel Theorem, since it requires axioms that are strictly more powerful.\nSo, reverse mathematical strength is a measure of how \"high-powered\" the machinery needed to prove a theorem is. However, like proof complexity, it doesn't say anything about how hard a proof of it is for a human brain come up with.\nSubjective analysis\nUltimately, if you want to talk about how difficult a theorem is for humans to prove, you will need some kind of subjective measure, since the difficulty of a theorem depends on how we think, what kind of strategies and techniques we are taught in school, what mathematical facts we happen to know, etc.\nOne subjective scale for analyzing the difficulty of proofs is the Math Olympiad Hardness Scale (MOHS) created by Evan Chen. However, this scale is specifically for Math Olympiad problems, and not for theorems in general. Additionally, it is based on precedent (what types of problems have been included on the International Mathematical Olympiad in the past), so it could change over time.\nIf you want to assess how difficult theorems in general are, you could consider where they appear in standardized curricula and assume that the designers of these curricula organized them by difficulty.\nAdditionally, you might want to take a look a John Conway and Joseph Shipman's paper Extreme Proofs I: The Irrationality of $\\sqrt{2}$. In this paper, they consider different proofs of the fact that the square root of $2$ is irrational, and they explain why they are \"extreme\" in various senses. For example, one proof is the \"most general,\" another depends on the \"simplest concepts,\" and another is \"purely geometrical.\"\nRegarding your dream for salvaging inconsistency\nThe project of reverse mathematics has done much to classify foundational theories based on how strong they are. If one of these foundational theories is proven to be inconsistent, we could certainly \"fall back\" on a strictly weaker theory, and the program of reverse mathematics would tell us which theorems we would \"lose\" by abandoning our strong system.\nMost mathematicians are extremely confident that all of the theories we use are consistent. However, we can never be completely certain that a given theory is consistent. In fact, it is somewhat unclear what it even means to know that a given theory is consistent. If to \"know\" something means to have a proof of it, you must specify in which formal system you want a proof. Suppose that we have a proof in system $G$ that system $F$ is consistent. If we want to trust this proof, we should want system $G$ to be consistent. However, in order to \"know\" that system $G$ is consistent, we must have a proof in some other formal system $H$ that $G$ is consistent. See Lewis Carroll's dialogue \"What the Tortoise Said to Achilles\" to find out where this rabbit hole leads.\nPerhaps you might want a system to be able to prove its own consistency. However, since everything is provable in an inconsistent theory, such a consistency proof would be meaningless, since any inconsistent theory can prove its own consistency. In fact, Gödel's Second Incompleteness Theorem tells us that any consistent theory that can  formalize \"enough\" elementary arithmetic cannot prove its own consistency. So, such a consistency proof would be bad news; if a sufficiently strong system proves its own consistency, then it must be inconsistent.\n",
    "tags": [
      "proof-writing",
      "foundations",
      "philosophy"
    ],
    "score": 6,
    "answer_score": 5,
    "is_accepted": true,
    "question_id": 4496559,
    "answer_id": 4688409
  },
  {
    "theorem": "Prove contrapositive of a statement after proving its converse. (If $n$ is prime then $2^n-1$ is also prime)",
    "context": "I have been given the following problem:\n\nLet n > 1 be a positive integer. Let P be the following statement:\nIf n is a prime number then $2^n - 1$ is a prime number\nWrite down the converse of the statement P. Is it always true? Justify your answer.\n\nI have solved this part by using a proof by contradiction, in the same way that is shown in this question. However, this problem is followed up with the following:\n\nWrite down the contrapositive of the statement P.\nIs it always true? Justify your answer.\n\nFrom my understanding the contrapositive of P is:\nIf $2^n-1$ is not prime, then n is not prime\nWhat I'm struggling with is how to go about proving this. Could I just use the same method as the previous part of the question (again, see here)? Or would I have to get the contrapositive of the contrapositive (resulting back in P) and prove that in some way?\n",
    "proof": "The converse statement to statement (A) below:\n\n(A) If $n$ is a prime number, then $2^n-1$ is a prime number.\n\nis statement (B):\n\n(B) If $2^n-1$ is a prime number, then $n$ is a prime number.\n\nLikwise, statement (A) is the converse to statement (B).\nStatement (A) is false but statement (B) is true.\nTo see that (A) is false, note that $2047=2^{11}-1$ as per lulu's comment, and $11$ is prime but $2047$ is infact not. So (A) [and thus the contrapositive to (A)] is not true.\nOn the other hand, (B) is true. We show this by working with the contrapositive of (B), which we will write as (B$'$):\n\n(B$'$) If $n$ is not prime, then $2^n-1$ is not prime.\n\nTo establish (B$'$), first suppose that $n$ is not prime. Then write $n=pq$ where both $p$ and $q$ are integers at least $2$. Then $$2^n-1=2^{pq}-1$$ $$=$$ $$(1+2^q+2^{2q}+\\ldots +2^{(p-1)q})(2^p-1),$$ and both\n$1+2^q+\\ldots + 2^{(p-1)q}$ and $2^p-1$ are at least $2$ [actually at least $3$], because $p$ and $q$ are both integers at least $2$, and so $2^p$, $2^{(p-1)q}$ are each at least $2^2=4$. So $2^n-1$ is a product of $2$ integers each at least $2$, so $n$ is not prime. Thus (B$'$) is true.\nAs (B) is precisely the contrapositive of (B$'$) and (B$'$) is true, it follows that (B) must be true as well.\n",
    "tags": [
      "proof-writing",
      "prime-numbers"
    ],
    "score": 6,
    "answer_score": 1,
    "is_accepted": true,
    "question_id": 4440035,
    "answer_id": 4440056
  },
  {
    "theorem": "Proving (disproving?) a statement when its condition is not satisfied",
    "context": "Consider the following implication.\n\nLet $k\\in \\mathbb{Z}$. If $k^{2} + 5k$ is odd, then $k^{2}+5k+1$ is odd.\n\nAt first it seems to be false, and one could proceed easily to prove it false directly by assuming that the premise ($k^{2} + 5k$ is odd) is true. However, there is no integer $k$ such that $k^{2} + 5k = k(k+5)$ is odd, and the premise is false in all posible cases, and so the implication follows vacuously (true).\nI do have a lot of questions about this issue. But I want to  be concise. Does the statement is true or false? and why?\n",
    "proof": "\n\nLet $k\\in \\mathbb{Z}$. If $k^{2} + 5k$ is odd, then $k^{2}+5k+1$ is odd.\n\nSo the process of direct proof is correct but it assumes the\npremise to be true and since it is impossible for the premise to be\ntrue, it follows that the implication is false.\nHowever, there is no integer $k$ such that $k^{2} + 5k =\n> k(k+5)$ is odd, and the premise is false in all possible cases, and so\nthe implication follows vacuously (true).\nBut the\nprocess of direct proof (which proves it wrong) is correct\n\nTo be clear,  formally, the implication here is\n\nif $\\bigg(k\\in\\mathbb Z\\:\\:$ and  $\\:\\:k^{2} + 5k$ is odd$\\bigg),$ then $\\bigg[k^{2}+5k+1$ is odd$\\bigg]$\n\nrather than\n\n$\\bigg[k^{2}+5k+1$ is odd$\\bigg],$\n\nwhich is just its consequent/conclusion. And the statement to be proved is this:\n\nfor each $k$, if $\\bigg(k\\in\\mathbb Z\\:\\:$ and $\\:\\:k^{2} + 5k$ is odd$\\bigg),$ then $\\bigg[k^{2}+5k+1$ is odd$\\bigg].$\n\n(Informally, “implication” has a second meaning as a synonym of consequent, and conflating these two meanings is causing confusion, I think.)\n\nSo this has to do with\nproof methods focusing more on the processes of deduction than in the\ntruth of the premises.\n\nYes, proving an implication is typically a sequence of derivations that begins by assuming that its premises are true and ends by deducing its conclusion. In other words, the process is showing that whenever its premises are true, its conclusion must be true. There are two exceptions:\n\nshowing that its conclusion is true immediately (logically) derives the implication, even if none or only some of its premises are assumed: \\begin{align}∀xCx &⊨ ∀x\\big(Px → Cx\\big)\\\\C&⊨P→C ;\\end{align}\nshowing that its premise is false immediately (logically) derives the implication as being vacuously true: \\begin{align}¬∃xPx &⊨ ∀x\\big(Px → Cx\\big)\\\\¬P &⊨ P→C.\\end{align}\n\nThus, when we know that an implication's premise is false, we can just show this, even when it's possible to assume (against the facts) its premise as true then using axioms to deduce its conclusion, that is, even when we can, for example, exhibit the (vacuously true) chain $$-2=2\\implies(-2)^2=2^2\\implies4=4$$ or the (vacuously true) chain $$-2=2\\implies-2+5=2+5\\implies3=7.$$\n\nthe statement is true or false? and why?\n\nYour given statement is vacuously true.\n",
    "tags": [
      "logic",
      "proof-writing",
      "proof-explanation",
      "propositional-calculus",
      "predicate-logic"
    ],
    "score": 6,
    "answer_score": 5,
    "is_accepted": true,
    "question_id": 4432953,
    "answer_id": 4433349
  },
  {
    "theorem": "Different definitions of the archimedean property",
    "context": "In some textbooks I have seen the archimedean property defined as:\nfor some positive real $x$, real number $y$, there exists a natural $n$ such that $nx>y$.\nIn other textbooks the archimedean property is defined as:\nfor any real $x$, we can find a natural $n$ such that $x \\leq n$\nI'm guessing I can prove that the two definitions are equivalent, but its just my guess, so if my reasoning is incorrect please correct me. Also if there are any other ways of looking at how the two definitions are equivalent I would love to learn more.\nMy attempt at proving:\nTo see how the first definition implies the second, we let $x=1$ in the first definition, then we have $n>y$. But then this means for any real $y$, we can find n such that n satisfies $n \\geq y$(since n satisfies $n>y$), which is exactly the second definition.\nTo see how the the second definition implies the first, for any positive real $x$, real $y$, we pick a real $z$ such that $z=y/x$. Then we can find $n$ such that $n \\geq z=y/x$. Since $n+1>n$, we have $n+1>y/x$ . Since $x>0$, we multiply both sides of the inequality and it does not change the order, to get $x(n+1)>y$. Since $n+1$ is a positive integer, this is equivalent to the first definition.\n",
    "proof": "You've shown that they are equivalent under the assumption of the properties of the real numbers that you used to prove the equivalence. Luckily, the properties of the order on the real numbers do not depend on the Archimedean property for their proof, so your proof has some value.\nIn some books I have seen the Archimedean property stated as \"for any $x \\in \\mathbb{R}$, there exists $n \\in \\mathbb{Z}$ such that $n \\leq x < n + 1$\". This one is also equivalent to the two you have there. To prove the equivalence, I used the fact that every non-empty subset of $\\mathbb{N}$ has a minimum.\n",
    "tags": [
      "real-analysis",
      "proof-writing",
      "solution-verification",
      "real-numbers"
    ],
    "score": 6,
    "answer_score": 5,
    "is_accepted": true,
    "question_id": 4361237,
    "answer_id": 4361325
  },
  {
    "theorem": "The eigenvalue of Lie bracket",
    "context": "Set $R = M_n(\\mathbb{C})$ and let $f_A : R \\to R$ be a $\\mathbb{C}$-linear map such that \n$$\nf_A(X) = [X,A] = XA - AX.\n$$\nObvious fact:\nNote that there are $a_{ij} \\in \\mathbb{C}$ such that \n$$\nf_A^n (X) = \\sum_{i+j=n} a_{ij} A^i X A^j.\n$$\nThus if $A$ is nilpotent then $f_A$ is nilpotent, obviously. But the assumption \"$A$ is nilpotent\" is too strong. I want to extend this fact. \nMy prediction:\nAssume that $f_A$ has a non-zero eigenvalue $\\lambda \\in \\mathbb{C}$. Then there are  $\\beta,\\gamma \\in \\mathbb{C}$ which are eigenvalues of $A$ such that $\\beta - \\gamma = \\lambda$ \nMy effort \nAssume that $n=2$. Let $X$ be a non-zero element of $E(\\lambda,f_A)$. Then $X^k \\in E(k\\lambda, f_A)$. So we get $X$ is nilpotent. So there is a $P \\in GL_2(\\mathbb{C})$ such that \n$$\n\\Lambda := PXP^{-1} =  \\begin{pmatrix} 0 &1 \\\\ 0& 0 \\end{pmatrix}.\n$$\nSet \n$$\nB=PAP^{-1} = \\begin{pmatrix} a & b \\\\ c & d \\end{pmatrix}.\n$$ \nThen $[\\Lambda, B] = \\lambda \\Lambda$ implies that $c=0$ and $d-a = \\lambda$. So we obtain \n$$\n\\Phi_A(x) = \\Phi_B(x) = (x-a)(x-a-\\lambda).\n$$\nMy question:\nWhat is a proper extension of the \"obvious fact\"? Is my prediction true? If so, how to prove ?\n",
    "proof": "This is a direct consequence of the fact that the spectrum of $f_A$ is $\\{\\mu_i-\\mu_j:\\ 1\\le i,j\\le n\\}$ when the spectrum of $A$ is $\\{\\mu_1,\\ldots,\\mu_n\\}$.\nThis fact can be proved as follows. By a continuity argument, we may assume that $A$ is diagonalisable. Let $u_i$ and $v_i$ be respectively a right eigenvector and a left eigenvector of $A$ corresponding to the eigenvalue $\\mu_i$. Then $[u_jv_i^T,A]=u_jv_i^TA-Au_jv_i^T=(\\mu_i-\\mu_j)u_jv_i^T$. Hence $\\{u_jv_i^T:\\ 1\\le i,j\\le n\\}$ is an eigenbasis and $\\{\\mu_i-\\mu_j:\\ 1\\le i,j\\le n\\}$ is the spectrum of $f_A$.\nAlternatively, the matrix representation of $f_A$ is $A^T\\otimes I-I\\otimes A$. By transforming $A$ into its Jordan form, it is obvious that the eigenvalues of $f_A$ are $\\mu_i-\\mu_j$ for all $i,j\\in\\{1,2,\\ldots,n\\}$.\nAnother consequence of the aforementioned fact is that $f_A$ is nilpotent if and only if all eigenvalues of $A$ are the same.\n",
    "tags": [
      "linear-algebra",
      "proof-writing",
      "eigenvalues-eigenvectors"
    ],
    "score": 6,
    "answer_score": 5,
    "is_accepted": true,
    "question_id": 3314810,
    "answer_id": 3314882
  },
  {
    "theorem": "Proving a Subset - $(A∪B)∩C⊆A∪(B∩C).$",
    "context": "I'm having a hard time with my subset proof. I think I'm skipping over some steps.\n\nLet $A$, $B$, and $C$ be sets. Prove that $(A ∪ B) ∩ C ⊆ A ∪ (B ∩ C).$\n\nTheorem: $(A ∪ B) ∩ C ⊆ A ∪ (B ∩ C).$\nProof:\nLet $x ∈ (A ∪ B) ∩ C$\nAssume: \nIf $x ∈ A$ or $x ∈ B$, then $x ∈ C$; since $(A ∪ B) ∩ C$\nIf $x ∈ A$, then $x ∈ C$\nIf $x ∈ B$, then $x ∈ C$\n$∴ x ∈ A ∪ (B ∩ C)$\n",
    "proof": "I'm not really sure why that \"assume\" line is there. It should probably be \"by our assumption that $x \\in \\cdots$\".\n\nAnyhow, I feel like you get the general idea of how this proof is meant to go - if you want to prove $A \\subseteq B$, you want to show $x \\in A \\implies x \\in B$. However, these things are a bit more complicated than that when you have multiple sets and such on each side.\nI like to think of this in two steps - \"unraveling\" the left-hand side to figure out what sets $x$ is in, and what it isn't in, and trying to \"ravel it back up\" to make the right-hand side.\nSome of your wording obscures this idea but you get the idea, I believe. Rewriting it would help your clarity come through.\n\nAssumption: $x \\in (A \\cup B) \\cap C$\nThus: $x \\in (A \\cup B)$ and $x \\in C$ (to be in the intersection, it must be in both)\nThus: $x \\in A$ or $x \\in B$ (to be in the union, it must be in one or the other, possibly both)\n\nSo, we know for sure $x\\in C$, and $x$ is in one of (or both) $A,B$. We have \"unraveled\" this half of the proof, so to speak.\nAt this point it gets a bit tricky. It's handy here to take this by \"cases\" where $x \\in A$ or $x \\in B$.\n\nSuppose $x \\in A$. Then $x \\in A \\cup (B \\cap C)$\nSuppose $x \\in B$ instead. Then since $x \\in C$, $x \\in B \\cap C$ and thus $x \\in A \\cup (B \\cap C)$\n\nThus, $x \\in (A \\cup B) \\cap C \\implies x \\in A \\cup (B \\cap C)$, showing $(A \\cup B) \\cap C \\subseteq A \\cup (B \\cap C)$, completing the proof.\n\nI feel like you get the idea of what's going on and the basic idea - your writing simply obscures that fact. It's important to keep in mind why everything follows from one step to the next; writing that explanation out would be very helpful, both for your professor to follow your proof, and for yourself to justify what's going on.\n",
    "tags": [
      "proof-verification",
      "elementary-set-theory",
      "proof-writing"
    ],
    "score": 6,
    "answer_score": 2,
    "is_accepted": true,
    "question_id": 3129621,
    "answer_id": 3129633
  },
  {
    "theorem": "The limit of the nth root of a to the n plus b to the n is the maximum of (a,b)",
    "context": "I've been asked to prove the following from Spivak's Calculus\n$$\\lim_{n\\to\\infty}\\sqrt[n]{a^n+b^n}=\\max(a,b); a,b > 0$$\nI understand that this is a proof by cases, and that our cases are $a=b$, $a>b$, and $b>a$. I have done the $a=b$ case, but I am stuck on the $a>b$ and $b>a$ cases.\nSome hints would be appreciated-\nThanks!\n",
    "proof": "Hint: \nIf $a \\leqslant b$\n$$b \\leqslant (a^n + b^n)^{1/n} \\leqslant 2^{1/n} b$$\n",
    "tags": [
      "real-analysis",
      "limits",
      "proof-writing"
    ],
    "score": 6,
    "answer_score": 2,
    "is_accepted": false,
    "question_id": 2282248,
    "answer_id": 2282254
  },
  {
    "theorem": "Why is this proof that a circular cone is not a surface not rigorous?",
    "context": "In example $4.1.5$, page $73$ of Pressley's Elementary Differential Geometry, a \"heuristic\" argument is given to prove that the circular cone with vertex the origin and angle $\\pi/4$, is not a surface. Here are the exact words:\n\nTo see that it is not a surface, suppose that $\\sigma: U \\to S \\cap W$ is a surface patch containing the vertex $(0,0,0)$ of the cone, and let $a\\in U$ correspond to the vertex. We can assume that $U$ is an open ball with center $a$, since any open set $U$ containing $a$ must contain such an open ball. The open set $W$ must obviously contain a point $p$ in the lower half $S_{-}$ of $S$ where $z < 0$ and a point $q$ in the upper half $S^{+}$ where $z>0$; let $b$ and $c$ be the corresponding points in $U$. It is clear that there's a curve $\\pi$ in $U$ passing through $b$ and $c$, but not passing through $a$. This is mapped by $\\sigma$ into a curve $\\gamma = \\sigma \\circ \\pi$ lying entirely in $S$, passing through $p$ and $q$, and not passing through the vertex. (It is true that $\\gamma$ will, in general, only be continuous, and not smooth, but this does not affect the argument.) This is clearly impossible. (Readers familiar with point set topology will be able to make this heuristic argument rigorous).\n\nWhy is this argument not considered rigorous? Can someone give an outline of how a rigorous argument should be?\n",
    "proof": "This question hasn't been answered and as I just happen to work out this example I will try to present a different idea to prove that it is not a surface. I follow Do Carmo's notation and ideas closely.\nThe problem is at the center of coordinates, that is at the point $(0,0,0)$. The usual parametrization for a cone of one sheet is the following, $z=\\sqrt{x^2 +y^2}$ which is not differentiable at $(0,0,0)$. But could there be another parametrization such that the cone is a regular surface? Here comes to rescue the following result which is a converse to the usual result that the graph of a differentiable function is a regular surface. In this case the converse is local.\n\nNow if the cone where to be a regular surface, there should be a neighborhood $V$ of $(0,0,0)$ such that $V$ is the graph of a differentiable function which has one of those forms. If you haven't seen the proof of this proposition, what you actually use to construct the function is (not surprisingly) the inverse function theorem applied to the composition of the parametrization of the surface $X$ and the projection $\\pi$ to one of the planes  $\\{z=0\\} \\hspace{0.2cm} \\text {or} \\hspace{0.2cm} \\{y=0\\} \\hspace{0.2cm} \\text {or} \\hspace{0.2cm} \\{x=0\\} $. \nIn this case in particular of the cone, it can be seen that a neighborhood of $(0,0,0)$ whenever is projected to any other plane different than $\\{z=0\\}$, the projection can never be a bijection much less a diffeomorphism, so that you wouldn't be able to use the inverse function theorem. This tells us that the graph of the differentiable function ought to be of the type $z=f(x,y)$. This finishes the proof because now you know from the usual parametrization of the cone that $f(x,y)= \\sqrt{x^2 +y^2}$ in a neighborhood of $(0,0,0)$. This function is not differentiable at $(0,0)$ so that you have arrived at a contradiction. This contradiction arises from supposing that the cone was a regular surface.\n",
    "tags": [
      "differential-geometry",
      "proof-writing"
    ],
    "score": 6,
    "answer_score": 5,
    "is_accepted": false,
    "question_id": 1754171,
    "answer_id": 2975326
  },
  {
    "theorem": "Let $X$ an infinite $T_1$ space, then exist some subspace homeomorphic to $(\\Bbb N,\\tau)$ where $\\tau$ is discrete or cofinite",
    "context": "My attempt to prove the statement of the title: if $X$ is infinite and $T_1$ with topology $\\tau_1$ then any basis for $X$ is infinite too.\nIf $X$ is $T_1$ then for any $x, y\\in X$ with $x\\ne y$ exists some open sets $U$ and $V$ such that $x\\in U \\land y\\notin U$ and $x\\notin V \\land y\\in V$. \n$\\color{red}{(1)}$ If you extend this analysis for a finite subset $F\\subset X$ you can create a collection $\\mathcal{U}$ of open sets where $\\forall x_i,x_j\\in F,\\ i\\ne j,\\ \\exists U_i\\in\\mathcal{U}:\\ (x_i\\in U_i)\\land (x_j\\notin U_i)$. Then $F\\bigcap(\\bigcap_{i=0}^{n} U_i)=\\varnothing$ and $\\mathcal {U}=\\{U_i\\}$ but $\\bigcap_{i=0}^{n} U_i\\in\\tau_1$ due the definition of topology (and remember $F$ is finite).\nThen:\n\nIf exists some countable infinite collection of disjoint open sets for the basis of $X$ then if we take some point belonging to any of these disjoint basic open sets then the resulting subspace is obviously homeomorphic to $\\Bbb N$ with the discrete topology.\nIf doesnt exist an infinite collection of disjoint basic open sets we have that exist an infinite collection of non-disjoint open sets. If we take infinite countable points each one belonging to a different but non-disjoint open set and cause the space is $T_1$ we have that for every open set at most a finite number of points ($F$ in the expression $\\color{red}{(1)}$) not belong to them cause the statement on $\\color{red}{(1)}$, so the subspace is homeomorphic to $\\Bbb N$-cofinite.\n\nQuestion: \n\nI feel my proof correct but not enough clear, maybe you can point how to clear it or if it lacks something?\nThe expression on $\\color{red}{(1)}$ is enough understandable?\n\nThank you in advance.\n\nEDITION: the expression $\\color{red}{(1)}$ is very hard to write it correctly to me by now, so Im going to describe in words: you can extend the definition of $T_1$-space not only to some $x,y\\in X$ if not to $x_1,x_2,x_3,...,x_n\\in X$. If you make all considerations then exist some $U_i$ for every $x_i$ where $x_i\\in U_i$ but the others points does not belong to $U_i$. These collection of $U_i$ I call $\\mathcal{U}$.\nThe intersection of every $U_i$ belongs to $\\tau_1$ because is a finite intersection of open sets, and no one $x_i$ belongs to this intersection, obviously, and these $x_i$ are finite. This is what I wanted express on $\\color{red}{(1)}$, sorry for the inconvenience :S\n",
    "proof": "To address the questions you asked (sorry to be so late with this, but I've only had very limited time the last couple of days):\nFirst, your expression $\\color{red}{(1)}$ can be stated as:\nGiven a finite set $F\\subseteq X$, for each $x \\in F$, there is an open neighborhood $U_x$ of $x$ such that for all $y \\in F, y \\ne x \\implies y \\notin U_x$. \nYou can define $\\mathcal U_F = \\{U_x\\ |\\ x \\in F\\}$. I put the $F$ subscript on to remind you that $\\mathcal U$ is defined from knowing $F$ first.\nIn point 2, you seem to be saying that in the case where $X$ does not contain an infinite collection of pairwise-disjoint empty sets, we can choose a countably infinite collection of open sets $\\mathcal A$ and for each $A \\in \\mathcal A$, a point $x_A \\in A$, such that if $A \\ne B$, then $x_A \\ne x_B$. However, like Jake1234, I don't see how $\\color{red}{(1)}$ shows that $\\{x_A\\ |\\ A \\in \\mathcal A\\}$ has a cofinite subspace topology.\n",
    "tags": [
      "general-topology",
      "proof-verification",
      "proof-writing"
    ],
    "score": 6,
    "answer_score": 1,
    "is_accepted": true,
    "question_id": 1582039,
    "answer_id": 1585068
  },
  {
    "theorem": "Show that the set of polynomials with rational coefficients is countable.",
    "context": "Problem: Show that the set of polynomials with rational coefficients is countable. \nIdea: We know that the set of rational numbers is denumerable. This implies that the set of rational numbers is countable. We also know that the degree that each polynomial can be is a natural number (I think, and I'm not sure how to word this.) Therefore, I think we can reason through this somehow, by showing that the plane $Q X N$ Is denumerable. I'm just not sure how to do this. Any ideas.\nNote: this is for my introduction to proofs class study quide. So, I would prefer not to go to over the top. \n",
    "proof": "Let us begin by stating that the set of rationals is countable, as Z2 is countable and every rational number can be expressed as an ordered pair of integers.\nThen we can enumerate the polynomials as follows:\n1.Start with n=1\n2.For every m on [1,n] list the next order-m polynomial\n3.Increase n by 1 and return to step 2.\n\nFor a proper enumeration of each order of polynomials (which must exist as, Q being countable, Qn is countable for all finite n and every order-n rational-coefficient polynomial can be expressed as an ordered n-tuple), every polynomial of rational coefficients will eventually appear exactly once.\n",
    "tags": [
      "polynomials",
      "proof-verification",
      "proof-writing",
      "rational-numbers"
    ],
    "score": 6,
    "answer_score": 2,
    "is_accepted": false,
    "question_id": 1574436,
    "answer_id": 1574460
  },
  {
    "theorem": "Function Surjectivity Proof",
    "context": "I have this question:\n\nProve that a function $f:X\\rightarrow Y$ is surjective iff for any\n  finite set $Z$ and any function $g:Z\\rightarrow Y$ there exists a\n  function $h:Z\\rightarrow X$ such that $g$ is their composition: $f\n \\circ h = g$. Assume $X,Y$ are also finite.\n\nCan someone please show me how to complete this proof? I'm struggling with proving the forward direction, and think my work on the backwards direction (below) may not be rigorous enough.\nIf $f:X \\rightarrow Y$ is surjective, then for any $g:Z \\rightarrow Y$ there exists $h:Z \\rightarrow X$ s.t. $f \\circ h = g$. We can show that this holds: we know there is some $y' \\in Y$ s.t. $g(z') = y'$, where $z' \\in Z$. We also know that for the same $z'$, there is some $x' \\in X$ s.t. $h(z') = x'$, as we may define $h$ as such. So we can define $f:X\\rightarrow Y$ as mapping each $x'$ to $y'$, i.e. $f(x')=y'$,so $\\forall y \\in Y, \\exists z \\in Z s.t. f(h(z)) = y$. \n",
    "proof": "It turns out that there is no need for $X,Y$ to be finite.\nFor the forward direction, suppose that $f:X\\to Y$ is surjective, that $Z$ is finite, and that $g:Z\\to Y.$ We must define $h:Z\\to X$ such that $f\\circ h=g$. In particular, then, we need $f(h(z))=g(z)$ for each $z\\in Z.$ Since $g:Z\\to Y$ and since $f:X\\to Y$ is surjective, then for each $z\\in Z$ there is some $x\\in X$ such that $f(x)=g(z).$ That is, the set $R_z:=\\{x\\in X:f(x)=g(z)\\}$ is non-empty for all $z\\in Z$. Since $Z$ is finite, then we may choose $x_z\\in R_z$ for all $z\\in Z,$ and define $h(z)=x_z,$ which will readily have the desired property, as you can (and should) prove.\nHint for the backward direction: Assume that for any finite set $Z$ and any function $g:Z\\to Y$ there is a function $h:Z\\to X$ such that $f\\circ h=g.$ In particular, let $Z$ be some one-element set. The hypothesis should allow you to conclude directly that every element of $Y$ is in the range of $f$.\n",
    "tags": [
      "elementary-set-theory",
      "functions",
      "proof-writing",
      "proof-verification"
    ],
    "score": 6,
    "answer_score": 5,
    "is_accepted": true,
    "question_id": 528688,
    "answer_id": 528698
  },
  {
    "theorem": "Induction proof for the lengths of well-formed formulas (wffs)",
    "context": "Use induction to show that there are no wffs of length 2, 3, or 6, but that any other positive length is possible.\nThe wffs in question are those associated with sentential/propositional logic.  So, sentence symbols would have length 1, where a sentence symbol represents some statement that evaluates to true or false, i.e. \"The dog ran across the room\" could be represented by the capital letter D.  Also, the only eligible binary connective are (^, v, ->, <->).\nI can see how there couldn't be wffs of length 2, because each wff must be surrounded by parentheses and contain at least one sentence symbol:\n    (D) has length 3\n\nI am not sure how to complete this proof using induction though.\n",
    "proof": "I assume that a wff is either\n\na sentence symbol of length 1\nof the form $(\\neg \\alpha)$ of length $L+3$ where $\\alpha$ is a wff of length $L$\nof the form $(\\alpha\\circ\\beta)$ of length $L_1+L_2+3$, where $\\alpha,\\beta$ are wff of lengths $L_a,L_2$ and $\\circ\\in\\{\\land\\lor,\\to,\\leftrightarrow\\}$\n\nThen use structiral induction to show\nProposition. If $\\alpha$ is a wff then its length $L(\\alpha)$ is in $\\mathbb N\\setminus\\{2,3,6\\}$\nProof: We assume that $L(\\alpha)\\in\\mathbb N$ is already known, so it suffices to show $L(\\alpha)\\notin\\{2,3,6\\}$.\n\nIf $\\alpha$ is a sentence symbol then $L(\\alpha)=1\\in\\mathbb N\\setminus\\{2,3,6\\}$\nIf $\\alpha$ is of the form $(\\neg\\beta)$, then $L(\\alpha)=L(\\beta)+3$. If we assume that $L(\\alpha)\\in\\{2,3,6\\}$ we conclude $L(\\beta)\\in \\{-1,0,3\\}$, contradicting the induction hypothesis $L(\\beta)\\in\\mathbb N\\setminus\\{2,3,6\\}$. Therefore $L(\\alpha)\\in\\mathbb N\\setminus\\{2,3,6\\}$ also in this case\nIf $\\alpha$ is of the form $(\\beta\\circ\\gamma)$, we have $L(\\alpha)=L(\\beta)+L(\\gamma)+3$, especially $L(\\alpha)\\ge 5$. We need only exclude $L(\\alpha)=6$, which would require that of of the sub-lengths is $1$ and the other os $2$, but that is not possible.\n\nProposition. If $n\\in\\mathbb N\\setminus\\{2,3,6\\}$, then there exists a wff $\\alpha$ with $L(\\alpha)=n$.\nProof: $A$, $(\\neg A)$, $(A\\land A)$ are examples of lengths $1,4,5$.\n$((A\\land A)\\land A)$ is of length $9$\nSince negating increases the length by $3$, we can obtain any length $n=4+3k$ and any length $n=5+3k$ and any lenbgth $n=9+3k$ with $k\\ge 0$.\n",
    "tags": [
      "logic",
      "proof-writing",
      "induction",
      "propositional-calculus"
    ],
    "score": 6,
    "answer_score": 5,
    "is_accepted": true,
    "question_id": 485484,
    "answer_id": 485519
  },
  {
    "theorem": "Real analysis: Dyadic squares and rigorous proof writing",
    "context": "I have been working my way through an exercise from Pugh's \"Real Mathematical Analysis\" which asks me to prove that two dyadic squares in $\\mathbb{R}^2$ of same size either intersect along a common edge, have a common vertex, are the same or are disjoint. It seems intuitively obvious to me (and I have made some progress in the proof as well), but I struggle with rigorously writing it down. Here's how far I got.\nIntuition/Explanation\nDyadic cubes are fairly easy to explain: a dyadic cube in $\\Bbb R^m$ is the cartesian product of $m$ intervals of same length, with the catch that the interval can be written as $[\\frac{a}{2^k}, \\frac{a+1}{2^k}]$, with $a\\in\\mathbb{Z}$ and $k\\in\\mathbb{N}$ including $k = 0$. So for example, $[\\frac{a}{2^k}, \\frac{a+1}{2^k}]\\times[\\frac{b}{2^k}, \\frac{b+1}{2^k}]$ would be an example of a planar dyadic cube (dyadic square) as the length of both intervals is the same, namely $\\frac{1}{2^k}$.\nMy idea has been that if there was a common point $x$ in any 2 different dyadic squares which is neither on a common edge nor vertex, then I could show that one of the squares would have their edge along a \"coordinate line\" which is not a dyadic number. To show what I mean, I attached the image below:\n\nSuppose the green square was an ordinary dyadic square. My assumption is that if there was a square such as the red one intersecting the green one, then there's no way coordinate line A (I don't know the right term I could use to describe the line) has a natural number as its value (again I don't know how to say this in mathematical terms, as that would imply that there was a integer between two adjacent integers.\nMy question is: How could I put this into words?\nMy progress so far\nSuppose there are 2 dyadic squares A, B of same size such that they are not the same, do not intersect along a common line or vertex. Let $M = \\{x\\in \\mathbb{R}|x\\in A \\land x\\in B\\}$.\nSince they intersect, they can intersect only in one of 2 possible ways (both drawn out below):\n\nLet $x$ be any point in $M$. Consider the closest square side lying above $x$. This side is between two dyadic rationals, namely $\\frac{a}{2^k}$ and $\\frac{a+1}{2^k}$. This side lying between these two dyadic rationals would also be (by hypothesis) a dyadic rational (described in picture as $\\frac{c}{2^k}$, implying that $\\frac{a}{2^k} < \\frac{c}{2^k} < \\frac{a+1}{2^k}$, from which $a < c < a+1$ would follow, which is not possible for any $c\\in \\mathbb{Z}$\nIt is clear to me that there are some large gaps in this proof:\n\nHow could I show that the only way the two squares could intersect is in the ways I've drawn out below?\n\nHow could I show that it follows that there is a line lying above any point in the set M?\n\nOverall, how do I make this proof more \"mathematical\"?\n\n\nLastly, I ask for some advice - I often catch myself having an idea such as this one but then greatly struggle with putting the proof together. How do I get better at this / practice this skill in a more targeted fashion?\n",
    "proof": "It is notoriously difficult to translate/materialize visual understanding into rigorous mathematical reasoning. Through millions of years of evolution (or by God's design), we human beings have acquired amazing visual intelligence that far surpasses our capability of logical reasoning.\n\nBack to this particular problem.\nThe trick here is to analyze dimension by dimension. Or, analyze the simpler case, dyadic intervals first and fully. Even though you can \"see\" the case of dyadic squares clearly does not mean it should be easy to prove the case of 2-dimension directly, as you have experienced. Always, always, always try to investigate a simpler case first. (Did I say always three times? Yes, I did.)\nSuppose we have two dyadic intervals of same size, $[\\frac{a}{2^k}, \\frac{a+1}{2^k}]$ and $[\\frac{b}{2^k}, \\frac{b+1}{2^k}]$. These two intervals\n\nare the same if $a=b$,\nshare an endpoint and no more if $|a-b|=1$,\ndisjoint otherwise. (Do we need a proof here? Here it is anyway. We have $|a-b|>1$. WLOG, suppose $a<b$. Then $a+1<b$. $\\frac{a+1}{2^k}<\\frac b{2^k}$, which means the two intervals are disjoint.)\n\nNow let us check two dyadic squares of same size, i.e., $I_1\\times I_2$ and $J_1\\times J_2$, where $I_1, I_2, J_1, J_2$ are dyadic intervals of same size. Thanks to our classification above of two dyadic intervals of same size, we can split the cases of two dyadic squares of same size into 4 disjoint cases below. Note that\n$$(I_1\\times I_2)\\cap (J_1\\times J_2)=(I_1\\cap J_1)\\times(I_2\\cap J_2).$$\n\nIf $I_1$ and $J_1$ are disjoint or $I_2$ and $J_2$ are disjoint, the two squares are disjoint.\nIf $I_1=J_1$ and $I_2=J_2$, the two squares are the same.\nIf $I_1$ and $J_1$ share only an endpoint $x$ and $I_2$ and $J_2$ share only an endpoint $y$, the two squares shares only a vertex, $(x,y)$.\nThe remaining cases are\n\nwhen $I_1=J_1$ and $I_2$ and $J_2$ share only an endpoint $y$. Then the two squares shares a common edge, $I_1\\times y$.\nwhen $I_2=J_2$ and $I_1$ and $J_1$ share only an endpoint $x$. Then the two squares shares a common edge, $x\\times I_2$.\n\n\n\nWe are done proving.\nYou can see that the description of $1$-dimensional cases make it very easy to handle $2$-dimensional cases and, in fact, all higher-dimensional dyadic cubes.\n\n\nI often catch myself having an idea such as this one but then greatly struggle with putting the proof together.\n\nIt is a common situation. It happens to people (including me of course) numerous times and constantly.\n\nHow do I get better at this / practice this skill in a more targeted fashion?\n\nThis is a huge topic. Here is a very brief reply.\nYou are doing very well. Keep practice. Learn techniques, terminologies, conventions, etc. from examples where people put clear and rigorous proof when it appears difficult for you.\n",
    "tags": [
      "real-analysis",
      "geometry",
      "solution-verification",
      "proof-writing"
    ],
    "score": 6,
    "answer_score": 4,
    "is_accepted": true,
    "question_id": 4601778,
    "answer_id": 4601888
  },
  {
    "theorem": "Number of cute permutations",
    "context": "\nA permutation of the numbers $1,2,3,\\dots,n$ is called cute if there is exactly one number that is greater than its position. For example, $1,4,3,2$ is a cute permutation (when $n=4$) because only the number $4$ is greater than its position. How many cute permutations there for a fixed $n$?\n\nThe problem is from a local math contest. Here is my attempt in solving the problem:\nI've noticed that if $p_1,p_2,\\dots,p_n$ is a cute permutation where $p_k>k$ then for all $i\\neq k$, we have $p_i\\leq i$. But I don't think this is helpful in finding the number of cute permutations.\nI also tried for small values of $n$ by listing all the possible permutations.\n\nFor $n=2$, it's easy to see that there is only one such permutation.\n\nFor $n=3$, I found $4$ permutations.\n\nFor $n=4$, I found $10$ permutations.\n\n\nFrom here it seems to me that the the number of cute permutations is $\\binom 22+\\binom 32+\\dots+\\binom n2$. But I couldn't find a way to show that.\nSo, how to solve the problem? And what happens if we call a permutation less cute if there are exactly two numbers that are greater than its position? Can we solve in general?\n",
    "proof": "Here is how we form a cute permutation: select $m$ numbers; $x_{1}<x_{2}<...<x_{m}$. Put $x_{m}$ in $x_{1}$'s original location then for all $i\\in[1,2,...,m-1]$ put $x_{i}$ in $x_{i+1}$'s original location. The number of cute permutations is given by the following expression:\n$$\n\\sum_{m=2}^{n}{\\binom{n}{m}}=2^{n}-\\left(n+1\\right)\n$$\nThe idea is that out of $n$ numbers, some will change position ($m$ of them) while the rest will remain at their original position. Out of all the numbers that change position only one move to the left while the others move to the right.\nBy the way for $n=4$ there are $11$ cute permutations instead of $10$\n",
    "tags": [
      "combinatorics",
      "proof-writing",
      "contest-math"
    ],
    "score": 6,
    "answer_score": 3,
    "is_accepted": false,
    "question_id": 4223712,
    "answer_id": 4223729
  },
  {
    "theorem": "Show that the sequence defined as $x_{n+1}=\\frac{1}{1+x_n}$ converge",
    "context": "Show that the sequence $(x_n)$ defined as $x_{n+1}=\\frac{1}{1+x_n},x_0=0$ converge. My attempt is to show that $(x_n)$ is Cauchy so i would like to have a feedback on my proof,please.\nFirst of all we show by induction that $2>x_n\\ge0$. The previous inequality is true for $n=0$ so we can suppose that it works up to a certain $n$. We want to proof right now that it holds fot $n+1$ and so show that $2>x_{n+1}\\ge0$:\n$2>x_{n+1}\\ge0 \\iff 2>\\frac{1}{1+x_n}\\ge0$.\nIt is trivial that $x_{n+1}\\ge0$ by induction hypothesis (because $x_n\\ge0$). Moreover, as $2>x_n\\ge0$, we have that\n$x_{n+1}=\\frac{1}{1+x_n}\\le1<2$. So we can conclude that $2>x_n\\ge0 \\ \\forall n\\ge0$\nThen, $\\forall p\\ge2$ we have:\n$|x_p-x_{p-1}|=\\Big|\\frac{x_{p-2}-x_{p-1}}{(1+x_{p-1})(1+x_{p-2})}\\Big|\\le 1 \\cdot|x_{p-2}-x_{p-1}|=...=1^{p-1}|x_0-x_1|=1^{p-1}$\nWe show now that $(x_n)$ is Cauchy. So, by définition of Cauchy sequence, we have to show that $\\forall \\epsilon>0 \\ \\exists N \\ \\forall m,n>N$: $|x_m-x_n|<\\epsilon$. But,\n$|x_m-x_n|\\le|x_m-x_{m-1}|+|x_{m-1}+x_{m-2}|+...+|x_{n+1}-x_n|\\le(m-n-1)$\nSo $(x_n)$ is Cauchy\nI am not reall sure if this proof holds, because i think that in case where $m=n-1$ it doesn't work,or...?\n",
    "proof": "Let $x_{1}>0$ and $\\displaystyle x_{n+1}=\\frac{1}{1+x_{n}}$. Notice that $x_{n}>0$ (by induction) and that :\n$$\n|x_{n+2}-x_{n+1}|=\\left|\\frac{1}{1+x_{n+1}}-\\frac{1}{1+x_{n}}\\right|=\\frac{|x_{n}-x_{n+1}|}{(1+x_{n+1})(1+x_{n})}\\leq\\frac{4}{9}|x_{n}-x_{n+1}|\n$$\nHence, $x_{n}$ is a contraction and hence it is Cauchy, and so converges by completeness of $(\\mathbb{R},|.|)$. Let $L$ be its limit. We have that $\\displaystyle L=\\frac{1}{1+L}$ that is :\n\\begin{align*}\n0&=L^{2}+L-1\\\\&=(L+0.5)^{2}-0.25-1\\\\&=\\left(L-\\frac{-1+\\sqrt{5}}{2}\\right)\\left(L-\\frac{-1-\\sqrt{5}}{2}\\right)\n\\end{align*}\nWe obtain that either $\\displaystyle L=\\frac{-1+\\sqrt{5}}{2}$, or $\\displaystyle L=\\frac{-1-\\sqrt{5}}{2}$. However, since $L\\geq0$ and since $x_{n}>0$, we conclude that :\n$$\n\\lim_{n\\to\\infty} x_{n}=\\frac{-1+\\sqrt{5}}{2}\n$$\nwhere $\\displaystyle\\varphi:=\\frac{-1+\\sqrt{5}}{2}$ is the golden ratio.\n\nAs $\\text{Theo Bendit}$ mentioned, What's wrong with your proof is that it  fails to make $\\left|x_{m}-x_{n}\\right|$ small.\n",
    "tags": [
      "sequences-and-series",
      "limits",
      "convergence-divergence",
      "proof-writing"
    ],
    "score": 6,
    "answer_score": 4,
    "is_accepted": true,
    "question_id": 4024548,
    "answer_id": 4024554
  },
  {
    "theorem": "Is my proof of $|a| \\leq b \\iff -b \\leq a \\leq b$ correct?",
    "context": "Background\nHello, I'm teaching myself proofs, and am unsure whether or not my proof of $\\forall a,b \\in \\mathbb{R}(|a| \\leq b \\iff -b \\leq a \\leq b)$ is correct.  Your feedback is greatly appreciated.\nProof\nProof.  $(\\rightarrow)$ Suppose $a,b \\in \\mathbb{R}$ and $ |a| \\leq b$.  We consider both cases.\nCase 1. $a \\geq 0$.  Then $|a| = a \\leq b$ by definition of absolute value.  Since $a \\geq 0$, then $-a \\leq 0$ by multiplying the inequality by $-1$.  Similarly, multiplying $a \\leq b$ by $-1$ gives us $-a \\geq -b$, or equivalently $-b \\leq -a$.  Since $-a \\leq 0$ and $a \\geq 0$, or equivalently $0 \\leq a$, then $-a \\leq a$ by transitivity.  We now have the following inequality,\n$$\n-b \\leq -a \\leq a \\leq b\n$$\nWhich means $-b \\leq a \\leq b$ by transitivity.\nCase 2. $a < 0$.  Then, $|a| = -a \\leq b$ by definition of absolute value.  Multiplying both sides of the inequality by $-1$, we get $a \\geq -b$, or equivalently, $-b \\leq a$.  Since $-b \\leq a$ and $a < 0$, then $-b < 0$.  Also, since $-b < 0$, multiplying by $-1$ means $b > 0$, or equivalently $0 < b$.  But since $a < 0$, then $-a > 0$.  By transitivity, $a < 0 < -a$, means $a < -a$.  Note that $-a \\leq b$.  So we have,\n$$\n-b \\leq a < -a \\leq b\n$$\n$\\color{blue}{\\text{I am unsure about this part.  How do I introduce equality?}}$ Therefore, $-b \\leq a < -a \\leq b$.  Since $a < 0$ and $-a > 0$, the only time $a = -a$ is when $a = -a = 0$.  So,\n$$\n-b \\leq 0 \\leq b\n$$\nOr, $-b \\leq a \\leq b$ by substitution.\nSince we've exhausted all cases, if $|a| \\leq b$ then $-b \\leq a \\leq b$ for all $a,b \\in \\mathbb{R}$.\n$(\\leftarrow)$ Suppose $a,b \\in \\mathbb{R}$ and $-b \\leq a \\leq b$.  Then, $a \\geq -b$ and $a \\leq b$.  We must show $|a| \\leq b$.  We consider two cases.\nCase 1. $a < 0$.  Multiplying $a \\geq -b$ by $-1$, we get $-a \\leq b$.  By definition of absolute value, $-a = |a| \\leq b$.\nCase 2. $a \\geq 0$.  Since $a \\leq b$, then $a = |a| \\leq b$ by definition of absolute value.\nTherefore, $|a| \\leq b$ when $-b \\leq a \\leq b$ for all $a,b \\in \\mathbb{R}$.  $\\qquad \\Box$\nQuestion\nI'm really self-concious about my ability to do this sort of stuff, so I hope I haven't butchered this.  The blue highlighted part is where I'm most uncertain.  I've seen some books (e.g. Rosen's Discrete Math book), use the following definition for absolute value:\n$$\n|a| = a \\text{ when } a \\geq 0 \\text{, and } -a \\text{ when } a \\leq 0.\n$$\nIn other words, he uses the $\\geq$ and $\\leq$ relations in both cases.  Is this okay? I've always seen the definition as $a < 0$ for one case, and $a \\geq 0$ for the other.  Does this matter?  If it does, how do you introduce the equality like I had to for Case 2 in the $(\\rightarrow)$ proof?  Also, when using transitivity, and you have something like\n$$\n-b \\leq a < -a \\leq b\n$$\nI'm guessing you can't conclude $-b \\leq a \\leq b$.  Since we haven't established that $a = -a$, so we can't say more than what we're given, which is that $a < -a$, so we could only say $-b \\leq a < b$.  Is my reasoning correct here?\n",
    "proof": "The two definitions of absolute value are equivalent. Some would argue that the one that you’re using is better style, because the cases are disjoint, but the one that you found in Rosen is also acceptable, since the two parts of the definition agree when both apply, i.e., when $a=0$.\nThere is no need to introduce equality after you arrive at\n$$-b\\le a<-a\\le b\\;:\\tag{1}$$\nthat immediately implies that $a<b$, and if $a<b$, then certainly $a\\le b$, so from $(1)$ you can immediately conclude that $-b\\le a\\le b$.\nI would probably have organized the proof of $(\\leftarrow)$ a little differently, doing Case 2 first, since it’s truly trivial: if $a\\ge 0$, then $|a|=a$, and we’re assuming that $-b\\le a\\le b$, so $-b\\le|a|\\le b$. That, however, is a matter of taste.\n",
    "tags": [
      "proof-writing",
      "proof-explanation",
      "solution-verification"
    ],
    "score": 6,
    "answer_score": 4,
    "is_accepted": true,
    "question_id": 3736979,
    "answer_id": 3736999
  },
  {
    "theorem": "I&#39;ve worked out the reasoning, but how do I write the proof?",
    "context": "This started our with a pretty trivial problem that went:\n\nFill in the blanks with whole numbers to make mathematically true statements.  Do not use the same number twice within a statement.$$\\frac{*}4+\\frac{1}*=\\frac{*}{20}$$\n\nNow solutions were pretty easy, so I decided to change the problem and asked myself what solutions could be made when I must use a number twice. Solutions were easy for this format $$\\frac{a}4+\\frac{1}a=\\frac{b}{20}$$ where $a\\ne b$.  But are there any solutions for the following format?\n$$\\frac{b}4+\\frac{1}a=\\frac{a}{20}$$ \nTo determine if there were any, I firstly rearranged the equation into a quadratic form, i.e.$$0=a^2-5ba+20$$ which yields solutions if $$a=\\frac{5b\\pm\\sqrt{25b^2-80}}2$$Now this can only satisfy the condition of \"whole numbers\" if $\\sqrt{25b^2-80}$ is a whole number.  (Even then there is more that needs to be satisfied, so this is a minimal condition.) At this point, I didn't know how to prove this formally, so I decided to use excel to determine ${25b^2-80}$ for different values of b, and then use the vlookup function to find the nearest square, $n^2$, below that value.  I then subtracted these two values because I figured that I was looking for any instances where $$\\delta=25b^2-80-n^2\\equiv0$$  Now I didn't find any, however, I found an unexpected pattern for the difference,  $\\delta$ given b=2, 3, ...\nThe value of $$\\delta = (4, 1, 31, 16, 36, 56, 76, 9, 19, 29, 39, ...)$$  That is, for $b>8$, $$\\delta =10(b-8)+9$$ \nI therefore have two questions. Why did this pattern emerge for $\\delta$?  And how do you formally write this reasoning, which does show that no value of \"$a$\" exists that is a whole number?    \n",
    "proof": "You have several choices already for the proof, so I'll focus on the pattern of values of $\\delta.$\nI think this is easier to reason about if we write the expression under the radical as $(5b)^2 - 80.$\nIn order for the radical to be a whole number, then,\nwe would need the difference between two squares to be $80,$\nwhere one of the squares is a square of a multiple of $5.$\nFor small values of $b$ there can be one or more squares strictly between \n$(5b)^2 - 80$ and $(5b)^2,$ \nso $\\delta$ ends up being the difference between $(5b)^2$ and $(5b - n)^2$ \nwhere $n \\geq 2.$ But \n$$ (5b)^2 - (5b - 1)^2 = (5b)^2 - ((5b)^2 - 2(5b) + 1) = 10b - 1, $$\nso if $b \\geq 9$ then the difference between $(5b)^2$ and the next smaller square is at least $89,$ which is greater than $80.$\nHence there are no squares at all in the numbers from $(5b)^2 - 80$ to $(5b)^2$\n(other than $(5b)^2$ itself), so $\\delta$ is just the difference between\n$(5b)^2 - 80$ and the next smaller square, which is $(5b - 1)^2$:\n\\begin{align}\n \\delta &= ((5b)^2 - 80) - (5b - 1)^2 \\\\\n&= (10b - 1) - 80 \\\\\n&= 10(b - 8) - 1\\\\\n&= 10(b - 9) + 9. \n\\end{align}\n(Note that this is slightly different from the formula written in the question.)\nBy the way, since we found while doing this that there are no squares at all in the numbers from $(5b)^2 - 80$ to $(5b)^2 - 1$ when $b \\geq 9,$\na corollary is that $(5b)^2 - 80$ is not a square, so after checking each case where $b < 9$ individually, you have shown that $(5b)^2 - 80$ cannot be a square.\n",
    "tags": [
      "proof-writing"
    ],
    "score": 6,
    "answer_score": 0,
    "is_accepted": true,
    "question_id": 3164219,
    "answer_id": 3164360
  },
  {
    "theorem": "Issue with Matrix Multiplication using the Formal Definition",
    "context": "I am writing a formal proof to show that if $B$ is the matrix obtained by interchanging the rows of a $2\\times2$ matrix $A$, then $\\det(B)=-\\det(A)$.  My reasoning and proof are coming along nicely but I hit a bump in the road that highlighted to me a gap in my knowledge - that is, I guess I do not completely understand the definition of matrix multiplication.  Note I went the rigorous route here only because I wanted to prove to myself I fully understood matrix multiplication... and I don't.  My proof thus far is:\n\nLet $E$ be the elementary matrix obtained by performing a type 1 elementary row operation on $I_2$.  By Theorem 3.1 (Friedberg), $B=EA$.  Note\n$$\\det(A) =\\det\\begin{pmatrix}\n    a & b \\\\\n    c & d\n    \\end{pmatrix}=ad-bc$$ By the definition of matrix multiplication,\n\\begin{align}\n& B_{ij}=(EA)_{ij} \\\\[10pt]\n= {} & \\sum_{k=1}^2 E_{ik}A_{kj} \\text{ for } 1\\le i\\le2\\text{, }1\\le j\\le2 \\\\[10pt]\n= {} & E_{i1}A_{1j}+E_{i2}A_{2j}\\text{ for } 1\\le i\\le2\\text{, }1\\le j\\le2 \\\\[10pt]\n\\vdots\\,\\,\\, \\\\[10pt]\n= {} & \\begin{pmatrix}\n    c & d \\\\\n    a & b\n    \\end{pmatrix}_{ij}\n\\end{align}\nIf $B=EA=\\begin{pmatrix}\n    c & d \\\\\n    a & b \\\\\n    \\end{pmatrix}$, then by the definition of a determinant of a $2\\times2$ matrix,\n\\begin{align}\n\\det(B) & =\\det(EA)=bc-ad \\\\[10pt]\n& =-(ad-bc) \\\\[10pt]\n& =-\\det(A)\n\\end{align}\n\nMy issue is, how do I formally express the steps where I put my \"$\\cdots$\"?  That is, the column and row vector multiplication and their sum? Maybe I'm wrong, but I feel most sources don't fully explain all the steps of matrix multiplication and just resort to hand-waving.  The way I think about it - the column and row vectors I will be multiplying in my proof are actually just $2\\times1$ and $1\\times2$ matrices, respectively.  I know they result in a $2\\times2$ matrix, but how?  And why?\n",
    "proof": "If I understood correctly, the matrix $E = \\begin{bmatrix}E_{11} & E_{12} \\\\ E_{21} & E_{22}\\end{bmatrix}$ is given by\n$$E = \\begin{bmatrix}0 & 1 \\\\ 1 & 0\\end{bmatrix}$$\nso for $1\\le i,j\\le 2$ we have$$(EA)_{ij} = \\sum_{k=1}^2E_{ik}A_{kj} =  E_{i1}A_{1j}+E_{i2}A_{2j}$$\nIf $i= 1$ then $$(EA)_{1j} = E_{11}A_{1j}+E_{12}A_{2j} = 0 \\cdot A_{1j}+1\\cdot A_{2j} = A_{2j}$$\nIf $i= 2$ then $$(EA)_{2j} = E_{21}A_{1j}+E_{22}A_{2j} = 1 \\cdot A_{1j}+0\\cdot A_{2j} = A_{1j}$$\nSo $$(EA)_{ij} = \\begin{bmatrix}A_{21} & A_{22} \\\\ A_{11} & A_{12}\\end{bmatrix}_{ij} = \\begin{bmatrix}c & d \\\\ a & b\\end{bmatrix}_{ij}$$\n",
    "tags": [
      "linear-algebra",
      "matrices",
      "proof-writing",
      "vectors"
    ],
    "score": 6,
    "answer_score": 2,
    "is_accepted": true,
    "question_id": 2853727,
    "answer_id": 2853762
  },
  {
    "theorem": "Does there exist a formal language capable of expressing any mathematical proof without resorting to drawings or natural language sentences?",
    "context": "Practically everytime I read a theorem proof in a book, paper, slides, website, pdf, anywhere, the proof is written using natural language (e.g. sentences in English) and sometimes drawings (e.g. 2D drawings if it's a proof about Geometry, Graph Theory, etc.). Even if the author does her/his best at writing a very technical and formal proof, full of formal symbols, at one point or another they can't help but resort to natural language and/or drawings to get some point across. My question is: does there exist a formal language capable of expressing any mathematical proof without resorting to drawings or natural language sentences?\nUPDATE: As @Noah Schweber suggested in the comments to his answer, I should make clear that a very important underlying reason for my interest in formal proofs has to do with the issue of confidence in natural language proofs. That is, if we are not using any formal language to prove most theorems, then how can we be so sure that we have proved anything at all? Because, what if we made some mistake at some point during our natural language reasoning? Since we don't have a full symbolic formal proof, how can we truthfully check that our proof is indeed correct? I think this is a very valid concern considering that natural languages are incredibly prone to ambiguities, which I guess most would agree is an undesirable property when we want to prove mathematical statements with crystal clear certainty.\n",
    "proof": "\nat one point or another they can't help it but resort to natural language and/or drawings to get some point across. My question is: does there exist a formal language capable of expressing any mathematical proof without resorting to drawings or natural language sentences?\n\nI think you're misunderstanding the goal of the text: it's to explain the result to the reader. Proofs are presented using natural language and occasionally pictures in order to be better understood by the reader.\nFor what it's worth, I don't mean to imply that the desire for a fully formal proof is bad. You may be interested in$^*$ mathematical logic in general (and in particular the syntax of first-order logic and set theory); more specifically, I think both formal (= machine-verifiable) proof and automated theorem proving will be of interest to you. But it's important to keep in mind the fact that mathematical texts (tend to) seek to explain mathematics to the reader, and so - even if one has a purely formal proof in hand - need to make concessions to how humans actually learn.\n\nAs a historical note, you may be interested in the book Principia Mathematica by Russell and Whitehead. More modern sources you might be interested in reading, on the issue of certainty in mathematics (as you've clarified in your comments below), include the following:\n\nThis math.stackexchange question.\nThese slides and this expository paper by John Harrison.\nVarious existing public \"libraries\" of fully-formalized proofs, such as this one. \n\nAnd there are many others, found quickly by googling the relevant terms. It's a big and important problem!\n\n$^*$I don't mean to imply that texts on mathematical logic will be written in the way you want - indeed, like other mathematical texts they're trying to explain something to the reader, so use natural language - but the subject itself has a lot to say about formal proofs as mathematical objects, and the formal languages developed and studied in mathematical logic are  capable of faithfully expressing mathematical theorems and proofs.\n",
    "tags": [
      "proof-writing",
      "formal-languages"
    ],
    "score": 6,
    "answer_score": 4,
    "is_accepted": true,
    "question_id": 2773040,
    "answer_id": 2773057
  },
  {
    "theorem": "Books or authors who write about how to write mathematics properly",
    "context": "I've always been interested how to write maths properly. Depending of the knowledge of the reader, you can't solve every detail of a proof of a theorem. You have to show the most important facts, unless your proof turns out tedious and languish. I'm writing a little paper about this to my students and I would like to know some authors who discuss about this, I need some references.\n",
    "proof": "Jean Pierre Serre is in some sense a reference on good article writing. I would  watch the awesome lecture of him \"How to write mathematics badly\". Gives very good insights that you could pass to your students.\nI would also suggest Mathematical Writing, of Donald Knuth\n",
    "tags": [
      "reference-request",
      "soft-question",
      "proof-writing",
      "book-recommendation"
    ],
    "score": 6,
    "answer_score": 2,
    "is_accepted": false,
    "question_id": 2083738,
    "answer_id": 2085051
  },
  {
    "theorem": "Is it good to use mean value theorem in $\\epsilon-\\delta$ continuity proofs?",
    "context": "I wanted to prove $f(x) = \\cos(x)$ is continuous using $\\epsilon-\\delta$ proof\nCouple of posts on MSE appealed to MVT to resolve this problem.\nNamely:\n\n$\\exists c \\in [x,x_o]$ s.t. $|\\cos(x)-\\cos(x_o)| = |\\sin(c)||x-x_o|$\n\nTada!\nProblem here is that we are appealing to the fact $\\sin(x)$ is the derivative of $\\cos(x)$\n...which necessarily implies that $\\cos(x)$ is continuous.\nIs it \"good\" to use MVT in proving a function is continuous?\n",
    "proof": "I think you answered your own question: You're assuming what you want to prove$\\dots$\n",
    "tags": [
      "calculus",
      "real-analysis",
      "soft-question",
      "continuity",
      "proof-writing"
    ],
    "score": 6,
    "answer_score": 3,
    "is_accepted": true,
    "question_id": 1685054,
    "answer_id": 1685058
  },
  {
    "theorem": "Don&#39;t understand theorem $\\exists z\\in \\mathbb{R}\\forall x\\in \\mathbb{R}^+\\left[\\exists y\\in \\mathbb{R}(y-x=y/x)\\leftrightarrow x\\neq z\\right]$.",
    "context": "I am reading a book on proof-writing techniques. One of the main ideas is that whenever you want to prove a statement starting with existential quantifier, you must choose a particular instance of an object, such that what follows is true. It is called \"existential instantiation\".\nThere is a theorem in the exercises that I am given to prove.\n$\\textbf{Theorem:}$ $\\exists z \\in \\mathbb{R} \\forall x \\in \\mathbb{R}^+ \\left[ \\exists y \\in \\mathbb{R} (y - x = y / x) \\leftrightarrow x \\neq z \\right] $.\n$\\textbf{Proof:}$ At this point I am already confused. Do I get it right that in this case I must use existential instantiation and proceed to choose a particular value of $z$, let's say $z_0 \\in \\mathbb{R}$ such that $\\forall x \\in \\mathbb{R}^+ \\left[ \\exists y \\in \\mathbb{R} (y - x = y / x) \\leftrightarrow x \\neq z_0 \\right]$?\nThen I move on and choose an arbitrary $x \\in \\mathbb{R}^+$ (\"universal instantiation\").\nSo now I have to prove $\\exists y \\in \\mathbb{R} (y - x = y / x) \\leftrightarrow x \\neq z_0$.\n($\\rightarrow$) Assume $\\exists y \\in \\mathbb{R} (y - x = y / x)$. Then I must prove that $x \\neq z_0$. Again, using existential instantiation, I choose a particular $y$, say $y_0 \\in \\mathbb{R}$ such that $ (y_0 - x = y_0 / x)$. At this point it gets really interesting. How do I show that $ (y_0 - x = y_0 / x) \\rightarrow x \\neq z_0$ if the expression on the left does not even have $z_0$?!\n($\\leftarrow$) I need to prove that $x \\neq z_0 \\rightarrow \\exists y \\in \\mathbb{R} (y - x = y / x)$. I must show that there does exist a $y$ that works. Consider $y = (x^2 + x) / (x - 1)$. This particular $y$ is going to work. But again, I don't see how that follows from the fact that $x \\neq z_0$!\nSo my idea so far is that this exercise is given to let us understand some point about existential instantiations, and that is exactly the point I don't seem to get. And this tricky $z$ is ruining everything, we could completely throw it away and nothing would have changed.\n$\\textbf{So my question is:}$ how exactly do we prove this theorem and what am I missing to complete the proof?\n",
    "proof": "To add a bit to the comments: existential instantiation is generally something that you use when you already have an existentially quantified sentence (say, in the premises). Here, what you need is to prove an existentially quantified sentence. In order to do this, the best way is to exhibit the particular $z$ in question. This is easier to see in practice, so I'll prove an easier theorem just to show you what I mean. \nSuppose I want to prove, from the field axioms, that $\\forall x \\forall z \\exists y (x = y + z)$ (i.e. that subtraction is possible). So what do I do? I need to work with arbitrary numbers, which I'll be able to (in this case) universally generalize, and with a specific number that I want to existentially quantify over. Let's go over this in steps.\n(1) So let $x$ and $z$ be arbitrary numbers. \n(2) By the field axioms, we know that there is a number, called $-z$, such that $z + (-z) = 0$. \n(3) So choose $y$ such that $y = -z + x$ (which we know to exist, since addition is well defined according to these axioms);\n(4) Now consider $z + y$, which, again, we know to be well defined.\n(5) By using simple identity laws, plus associativity, $z + y = z + ((-z) + x)  = (z + (-z)) + x = 0 + x = x$.\n(6) Since $x$ and $z$ were arbitrary, for every $x$ and $z$, there is a desired $y$ that proves the theorem.\nNotice that the crucial step in the above proof is (3), where I deliberately chose a $y$ that I knew would give the desired result. That's generally the trick when your goal is to prove existentially quantified sentences: you don't want to deal with arbitrary objects, but with specifically chosen ones. In your particular case, you have to carefully select a $z$ that will give you the desired result. Obviously, you won't always be able to discover which object is the necessary one; in this case, the only resource we have left to prove an existentially quantified statement is to argue by contradiction (i.e. suppose there is no such $z$; then contradiction ensues).\n",
    "tags": [
      "logic",
      "proof-verification",
      "proof-writing"
    ],
    "score": 6,
    "answer_score": 2,
    "is_accepted": false,
    "question_id": 921422,
    "answer_id": 921450
  },
  {
    "theorem": "What is an instance of a mousetrap proof?",
    "context": "A part of the first chapter of the book The spirit and the uses of the mathematical sciences  talks about the beauty of mathematics. The author quotes from a lecture of Hasse and introduces the notion of a mousetrap proof. I feel that the author gives a lucid exposition on what talked about, but regretfully without an example of a proof of such type. \nSeeking after is at least one example of a proof of such type. \nIf answering this question necessitates more information, please feel free to state that.\nThe original paragraph is in the following formulation, the chapter having which is entitled The characteristic features of mathematical thought and written by PROF. DR. RER. NAT. J. WEISSINGER:\nCriteria of beauty at the second level, according to Hasse, are purposefulness and elegance. Purposefulness means that at every point of a proof we should know where we stand and should have the goal in view. The opposite of this is the so-called mousetrap proof, in which we are nudged forward conclusion by conclusion until suddenly the door snaps shut. We feel ambushed, extraordinarily stupid, and irritated by the esoteric ingenuity of the author, yet when we try to gnaw at the bars of the logical conclusions, we are compelled to admit that the proof is solid and without flaws.\nBeauteous, is not it? Here the boldfaced words are those quotation-marked in the original text.\n",
    "proof": "The following is an excerpts from here (an inaugural lecture of some sort at Heidelberg university):\n\"Auf der zweiten Stufe der Schönheitskriterien stehen nach Hasse die Forderungen nach Eleganz und nach Zielstrebigkeit. Die letzte bedeutet, daß man an jeder Stelle eines Beweises weiß, wo man steht, und das Ziel vor Augen sieht. Das Gegenteil eines solchen Beweises ist der sogenannte ,,Mausefallenbeweis''. Man tastet sich Schluß für Schluß vorwärts und mit einem Mal fällt die Klappe zu: Man kommt sich überrumpelt und außerordentlich dumm vor; aber so viel man auch an den Gitterstäben der logischen Schlüsse zu nagen sucht, man ist gefangen, der Beweis ist lückenlos richtig.\"\nEuclid's proof of the Pythagorean theorem is not a \"mousetrap proof\" in this philosophical sense. When it is called by that name then because the movements of the quadrangles in Euclid's proof look like what's happening when a mousetrap fires.\n",
    "tags": [
      "proof-writing",
      "terminology"
    ],
    "score": 6,
    "answer_score": 1,
    "is_accepted": true,
    "question_id": 861126,
    "answer_id": 861146
  },
  {
    "theorem": "Product of a family of spaces of countable tightness",
    "context": "I recently learned the concept of cardinal functions and some of the definitions and theorems are not clear to me. How can we prove this theorem?\nFinite family of compact spaces of countable tightness \nhas countable tightness.\nEvey compact space has countable tightness?\n",
    "proof": "A corollary of Exercise 3.12.8 (d) of Engelking's “General topology” claims: if $f:X\\to Y$ is a closed continuous map of topological spaces, $Y$ is regular, and for each $x\\in X$ we have $t(f(x),Y)\\le\\kappa$ and $t(x,f^{-1}(X))\\le\\kappa,$ then $t(x,X)\\le\\kappa$.\nNot every compact space has a countable tightness, because $t(X)=w(X)$ for each dyadic compact $X$, by Exercise 3.12.12 (h) of the same book.  \n",
    "tags": [
      "general-topology",
      "proof-writing",
      "cardinals"
    ],
    "score": 6,
    "answer_score": 4,
    "is_accepted": true,
    "question_id": 361081,
    "answer_id": 361087
  },
  {
    "theorem": "The author of my book simplifies his solutions to an extent that I am uncomfortable with, so are my solutions to homework over doing it?",
    "context": "This question can be summarized as: How explicit does one need to be when writing proofs? To what extent can one implicity write a proof safely?\nThe first chapter of our text in elementary discrete math is very brief in the solutions. As an example from the text:\n\nIf\n$C = \\{1, 3\\}$ and $A = \\{1, 2, 3, 4\\}$\nby inspection, every element of $C$ is an element of $A$. Therefore, $C$ is a subset of $A$ and we write $C \\subseteq A$\n\nSo when I get to homework that asks: Show, as in the previous example, that $A \\subseteq B$.\n\n$A = \\{1, 2\\}$, $B = \\{3, 2, 1\\}$\n\nI solved this myself but, first, here is the text's solution:\n\nLet $x \\in A$. Then $x = 1$ or $x = 2$. In either case, $x \\in B$. Therefore $A \\subseteq B$\n\nGiven that sets are the first section of the first chapter, and in fact this is all covered in the first dozen pages, we obviously have not talked about proofs, or what flavor of statements take the form of expressions like $x = 1$ or $x = 2$. So I am unable myself at this point to critique the author on his proofs at such an early stage.\nMy question, is the author simplifying this process dramatically and therefore showing a thorough proof is undesirable at this stage, and in practice unexpected? Or has the author shown a completely valid proof given that the question itself is very much simple (perhaps I should be as explicit as the author, or the question asked)?\nConsider my eccentric proof to the same problem:\n$\\forall x(x \\in B \\rightarrow (0 < x \\leq 3 \\land x \\in \\mathbb{Z}^+))$\nAssume $x \\in A$, then $x \\in \\mathbb{Z}^+$ and $0 < x \\leq 2 < 3$\nTherefore $\\forall x(x \\in A \\rightarrow x \\in B)$\nTherefore $A \\subseteq B$\nIs this over doing things, as the author's solutions suggest? I really need to ask my teacher tomorrow to know in terms of the class assignments, but I would really like to know if the community agrees or disagrees and to what extent when it comes to being abrupt in the proof writing.\nOften in basic algebra classes, for example, the author will simplify evaluating or solving larger expressions, at many times skipping two or more steps that could have been written explicitly, but it is assumed that one could follow logically without seeing those steps. So another way of asking my question is am I myself skipping such steps, despite my attempts to be explicit, and is it, synonymously with many basic algebra solutions, a waste of time to be overly explicit?\n",
    "proof": "These are very straightforward solutions. When showing one set is a subset of another, all you need to show is that $x\\in A$ implies $x\\in B$. So in your exercise, $x\\in A$ implies $x=1$ or $x=2$. The fact that $1$ and $2$ are both in $B$ implies that regardless of what $x$ is, it necessarily must be in $B$.\nLater in the textbook, you may see much more implicit \"proofs\" where the author gives you a sketch of what to do while leaving the tedious details to you, the student.\nWhen it comes to homework, you should certainly have your specific teacher/professor critique your work. What may be blatantly obvious to you may not be so obvious to another person, or your professor could want you to fully justify every detail for the mental exercise.\n",
    "tags": [
      "proof-writing"
    ],
    "score": 6,
    "answer_score": 4,
    "is_accepted": true,
    "question_id": 278325,
    "answer_id": 278329
  },
  {
    "theorem": "How do I show that two sets are equal.",
    "context": "This is an ever so slightly modified version of a question from my book. My teacher went over this with me, but I would like an explanation that I can keep coming back to until I have this method figured out precisely.\n$A = \\{1, 2, 3, 4\\}$\n$B = \\{ n | n \\in \\mathbb{Z}^+, n^2 < 17 \\}$ where $\\mathbb{Z}^+$ is the positive integers.\nShow that $A = B$\n\nI understand that to do this I must show for every $n$:\n$n \\in B \\rightarrow n \\in A$\nand\n$n \\in A \\rightarrow n \\in B$\n\nHow to do that is still something I am not entirely sure how to do correctly, so I will give it my best and someone can show me errors or how to complete it.\n\nIf, $n \\in B$ then $n^2 < 17$\n$n < \\sqrt{25}$, therefore $n < 5$\nSince $n$ is a positive integer, $0 < n$\ntherefore, for every $n \\in B, 0 < n < 5$, therefore $n \\in A$\nFor every element $n$ such that $n \\in A$, $n$ is one of 1, 2, 3, or 4\n(I am probably taking too long to do this, but I am unsure how thorough it needs to be, so I will stop here to not waste time. What I would like to know is how can I write this quicker but still make valid statements and show that A = B more easily? I am also making statements that seem unnecessary or unconventional and perhaps even invalid from a logical perspective. I appreciate any help you can offer, the more the merrier!)\n\nTo finish the proof with help from amWhy:\n$4$ is the largest element of $A$, and $4 < \\sqrt{17}$\nAssuming $n \\in A$, $0 < n \\leq 4$, $n^2 \\leq 16 < 17$, therefore, $\\forall n(n \\in A \\rightarrow n \\in B)$\nTherefore $A = B$, hurray!\n",
    "proof": "Your proof strategy is fine:\nTo show that sets $A = B$, one usually wants to show that $A\\subseteq B \\text{ and}\\; B \\subseteq A$, which means, equivalently $$ (n \\in A \\rightarrow n \\in B \\;\\text{ and}\\;\\; n \\in A \\rightarrow n \\in B)$$\nFirst part:\n\nIf, $n \\in B$ then $n^2 < 17$, and $n < \\sqrt{25}$, therefore $n < 5$\n\nReplace this last statement above with:\n$n^2 \\lt 17$ which implies $n \\lt \\sqrt{17}$, therefore, $n \\leq 4$.\n\nSince $n$ is a positive integer, $0 < n,\\;$ therefore, for every $\\;n \\in B, 0 < n < 5$.\nFor every element $n$ such that $n \\in A$, $n$ is [one] of $1, 2, 3, \\,\\text{or}\\; 4$\n\n\nCan you proceed with the other direction? $n \\in A \\rightarrow n \\in B\\;$?\nSo for (II), We assume $n \\in A$, $n\\in \\{1, 2, 3, 4\\}$ and show that it follows that $n \\in B$. It suffices to check that the largest element $n \\in A$, $n = 4$,  is such that $4^2 \\lt 17$, then the square of the rest of the values in A must also be less than $17$, since $\\forall n \\in A, n^2 \\leq 4^2 = 16 \\lt 17.$ Hence, $\\forall n \\in A, n \\in B$.\nPart I and Part II show that $A = B$\n",
    "tags": [
      "elementary-set-theory",
      "proof-writing"
    ],
    "score": 6,
    "answer_score": 4,
    "is_accepted": true,
    "question_id": 275603,
    "answer_id": 275608
  },
  {
    "theorem": "Induction without integers (aka Structural Induction)",
    "context": "While composing the following question, I had an \"ah-ha\" moment. I still want to post the question along with my answer to show what I have learned. Any comments or additional answers will be greatly appreciated.\nRecently I encountered a theorem in An Introcution to Mathematical Logic and Type Theory: To Truth through Proof by Peter B. Andrews where the author states that it can be used in inductive proofs without using integers. The theorem is stated as\n\n1000 Principle of Induction on the Construction of a Wff. Let $\\mathscr{R}$ be a property of formulas, and let $\\mathscr{R}(\\mathbf{A})$ mean that $\\mathbf{A}$ has property $\\mathscr{R}$. Suppose\n(1) $\\mathscr{R}(\\mathbf{q})$ for each propositional variable $\\mathbf{q}$.\n(2) Whenever $\\mathscr{R}(\\mathbf{A})$, then $\\mathscr{R}(\\mathord{\\sim} \\mathbf{A})$.\n(3) Whenever $\\mathscr{R}(\\mathbf{A})$ and $\\mathscr{R}(\\mathbf{B})$, then $\\mathscr{R}([\\mathbf{A} \\lor \\mathbf{B}])$.\nThen every wff has property $\\mathscr{R}$.\n\nI am quite familiar with mathematical induction on integers. Mine typically take the following form:\n\nBlah, blah, blah. Therefore, the statement holds for $n=1$.\nNow assume that the statement is true for some integer $n$. Now consider the statement for $n+1$. Yadda, yadda, yadda (eventually transforming the statement for $n+1$ into a statement involving $n$). Therefore, the statement holds for $n+1$.\n\nI'm also comfortable with using strong induction:\n\nBlah, blah, blah. Therefore the statement holds for $n=1$.\nNow assume that the statement is true for all integers $k$ such that $1 \\le k \\le n$ for some integer $n$. Now consider the statement for $n+1$. Yadda, yadda, yadda (eventually transforming the statement for $n+1$ into a statement involving integers $k$ between $1$ and $n$). Therefore, the statement holds for $n+1$.\n\nOne example where I would use this is in graph theory to prove a statement about trees. I proceed by induction on the number of vertices in a tree. This maps the object of interest (a tree) to the integers.\nWhen I tried to apply the theorem about wffs, I got stuck how to proceed. How do I apply this theorem to a proof?\n",
    "proof": "$\\def\\p{{\\mathscr R}}$If you are uncomfortable with the idea of doing induction directly on the structure, or if you want to reduce the question to one of induction on the integers, you can proceed by induction on the size of the WFF, for some suitable definition of \"size\". One typical definition is that the size of the WFF is the number of logical operators it contains.\nThen to prove that $\\p(W)$ holds for every WFF $W$, you proceed as follows:\n\nShow that $\\p(W)$ holds for each $W$ of the smallest possible size. (1, if you are counting atoms, or 0, if you are counting operators.)\nShow that if $\\p(W)$ holds for all $W$ of size less than $n$, then it must hold for all $W$ of size $n$, as follows: Let $W$ be a WFF of size $n$.  Then it must have the form $\\sim A$ for some wff $A$ of size $n-1$, or the form $A\\oplus B$ for some operator $\\oplus$ and some wffs $A$ and $B$ each of size at most $n-1$. Fill in the inductive argument for the two or more cases.\n\nFormulated in this way, the induction is just a regular strong induction on integers, where instead of proving the statement \"$\\p(W)$ holds for all wffs $W$\", you reformulate it as \"For all numbers $n$, $\\p(W)$ holds for each wff of size $n$\".\nBut this sort of transformation should not really be necessary.  The induction principle can be stated more generally.  Suppose $S$ is any set, and $\\prec$ is a well-founded order on $S$; this means that every subset of $S$ is either empty, or contains a $\\prec$-minimal element, which is an element $m\\in S$ such that no other element $m'\\in S$ has $m'\\prec m$.\nA typical example would be that wffs can be ordered by an ordering that says that $a\\prec b$ whenever $a$ is a subformula of $b$.  For example, $(x\\land  y)$ is a subformula of $\\lnot(x\\land y)\\lor z$, so $(x\\land  y)\\prec \\lnot(x\\land y)\\lor z$.\nThe ordering $\\prec$ need not be total, which means that it is possible that none of $a\\prec b, a=b, $ and $b\\prec a$ need be true. For the $\\prec$ of the previous paragraph, we have neither $a\\land b \\prec a\\lor b$ nor $a\\lor b \\prec a\\land b$. That is quite all right.\nThen you can use well-founded induction as follows:\n\nLet $F$ be the set of wffs for which $\\p$ is false. Suppose $F$ is nonempty. Then by the well-foundedness of $\\prec$, the set $F$ has a $\\prec$-minimal element $m$.\nShow that $m$ cannot have any subformulas, as follows:  Using some properties of $\\p$, show that $\\p$ must fail for one of the subformulas of $m$.  (This will depend on the details of $\\p$ itself.)  But then this subformula of $m$ would be an element of $F$, which would contradict the hypothesis that $m$ was $\\prec$-minimal in $F$.\nSo $m$ has no subformulas. Rule out this case using some property of $\\p$.\n\nThis rules out the possibility that $F$ actually contains a $\\prec$-minimal element $m$, and the only remaining possibility is that $F$ is empty, and so $\\p$ holds for every wff.\nOrdinary induction is a special case of well-founded induction which uses the well-founded relation $\\lt$ on the natural numbers. The well-foundedness of $\\lt$ is equivalent to the so-called well-ordering principle of the natural numbers, which states that every subset of $\\Bbb N$ either contains a $\\lt$-minimal element, or is empty.\n",
    "tags": [
      "logic",
      "induction",
      "proof-writing"
    ],
    "score": 6,
    "answer_score": 3,
    "is_accepted": true,
    "question_id": 178932,
    "answer_id": 178963
  },
  {
    "theorem": "Use combinatorial proof to show that $\\sum\\limits_{k=m}^{n}\\binom{k}{m}=\\binom{n+1}{m+1}$.",
    "context": "I am going through a self-teaching journey in mathematics. Right now I am reading Book of Proof, by Richard Hammack, and in the Chapter on Counting, I came across the following exercise:\nUse combinatorial proof to show that $\\sum\\limits_{k=m}^{n}\\binom{k}{m}=\\binom{n+1}{m+1}$.\nMy proof was the following:\nSuppose $m\\leq n$ and let $S=\\{0,1,2,\\ldots,m,\\dots,n\\}$. Notice that $|S| = n+1$ and that the right hand side of the equation, by definition, is the number of subsets of order $m+1$ of $S$.\nNow, we will count the numbers of subsets of order $m+1$ of $S$ in a different way: for every element $k$ of $S$ such that $m\\leq k\\leq n$ we count the number of subsets of order $m$ of $S$ such that, for every subset we construct, all elements are less than $k$. By construction, there are always $k$ numbers less than $k$ in $S$, therefore, there are $\\binom{k}{m}$ such subsets.\nFor each of these subsets, it's union with $\\{k\\}$ is a different subset of order $m+1$ of $S$. Since $k$ could be any number in $[m,n]$, we have that $\\sum\\limits_{k=m}^{n}\\binom{k}{m}$ counts all subsets of $S$ with order $m+1$.\nSince the right-hand and left-hand sides are solutions to the same counting problem, we conclude they are equal.  $\\blacksquare$\nI desperately need some feedback on my proof-writing skills. It feels like my argument is solid, but at the same time, my writing might be somewhat convoluted. I don't know if it is clear enough, not straightforward enough, or if I explained too much. How detailed should I be?\nFeel free to be very critical. I wanna be able to write proofs acceptably.\n",
    "proof": "I think that your proof is well written.  Initially, I thought that your proof had a flaw in it, but then, I re-considered.\nYou are in effect partitioning the $~\\binom{n+1}{m+1}~$ subsets into groups, where group $k$ has all such subsets whose largest element is $(k)$.  This means that you have partitioned the subsets into non-intersecting groups whose union does equal all of the pertinent subsets.\nThe only possible criticism that I would offer is that at first, it was not immediately clear that your approach was valid.  I had to do some thinking to realize that you were validly partitioning the $~\\binom{n+1}{m+1}~$ subsets.  So, your proof could (perhaps) be improved by making it crystal clear that your partitioning is valid (i.e. that the partitioning represents mutually exclusive groups whose union does comprise all possible subsets).\nPersonally, in such a situation, I (sometimes) assume that my target audience is high school students.\n",
    "tags": [
      "combinatorics",
      "discrete-mathematics",
      "solution-verification",
      "proof-writing",
      "combinatorial-proofs"
    ],
    "score": 6,
    "answer_score": 1,
    "is_accepted": true,
    "question_id": 4476940,
    "answer_id": 4476961
  },
  {
    "theorem": "Having some trouble with proving for some sets $M,N$ that $M \\cap N = M$ if and only if $M \\subseteq N$",
    "context": "Proposition: $\\textit{Let M and N be sets. Prove that $M \\cap N = M$ iff $M \\subseteq N$}\\\\$\n\nMy Proof:\nGoing from the definition of set intersection and subsets, the above statement can be rewritten as follows:\n$$\\{a: (a \\in M \\wedge a \\in N) \\equiv a \\in M \\} \\equiv \\{a: a \\in M \\Rightarrow a\\in N\\}.$$\nNow let $p = a \\in M$ and $q = a \\in N$. This allows us to further rewrite the above statement in predicate logic:\n$$((p \\wedge q) \\equiv p) \\equiv (p \\Rightarrow q).$$\nThis is a tautology, as shown by the truth table below.\n\nTherefore the intersection of the sets M and N equal N if and only if M $\\subseteq$ N.\n\nIs my proof even correct? My math professor doesn't find it sound to convert this statement into predicate logic and go from there, he just breaks apart the if and only if into two implications, and then goes by the definition of subsets.  Which approach is better? Am I not employing the correct definition of set equivalence in terms of logic?\n",
    "proof": "\nHaving some trouble with proving for some sets $M,N$ that $M \\cap N = M$ if and only if $M \\subseteq N$\n\nTo be clear, the proof is required for an arbitrary pair of sets $M,N,$ which is to say that it's for all sets $M,N$ rather than some sets $M,N.$\n\nProposition: $$M \\cap N = M\\quad\\text{iff}\\quad M \\subseteq N.$$\nMy Proof: the above statement can be rewritten as follows:\n$$\\{a: (a \\in M \\wedge a \\in N) \\equiv a \\in M \\} \\equiv \\{a: a \\in M \\Rightarrow a\\in N\\}.$$\n\nYou seem to be conflating equivalence, logical equivalence, and equality.\nAnd, just to be clear: while $M \\cap N$ is a set, $M \\subseteq N$ is not.\nNow, consider the set $\\{a: a \\in M \\Rightarrow a\\in N\\}.$ It is the empty set if, in a given context, $M \\not\\subseteq N;$ otherwise, it is the universal set.\n\nNow let $p = a \\in M$ and $q = a \\in N$. This allows us to further rewrite the above statement in predicate logic:\n$$((p \\wedge q) \\equiv p) \\equiv (p \\Rightarrow q).$$\n\nThis is more coherent, but better if you use change all the metalogical symbols to logical symbols, and you do need to quantify the predicates ($M\\subseteq N$ means $∀a\\;(p{\\implies}q)$): $$∀a\\Big(\\big(a{\\in}M \\wedge a{\\in}N\\big) \\leftrightarrow a{\\in}M\\Big) \\leftrightarrow ∀b\\Big(b{\\in}M \\to b{\\in}N\\Big).$$\nThis correctly rewrites the given proposition, but unfortunately it cannot be derived using a truth table.\nIncidentally, $$∀a\\Big(\\big(a{\\in}M \\wedge a{\\in}N\\big) \\leftrightarrow a{\\in}M\\Big) \\leftrightarrow ∀a\\Big(a{\\in}M \\to a{\\in}N\\Big)\\\\\\not\\equiv\\\\∀a\\;\\left[\\Big(\\big(a{\\in}M \\wedge a{\\in}N\\big) \\leftrightarrow a{\\in}M\\Big) \\leftrightarrow \\Big(a{\\in}M \\to a{\\in}N\\Big)\\right].$$\nUsing formal logic to prove the given proposition is not advantageous.\n",
    "tags": [
      "elementary-set-theory",
      "solution-verification",
      "proof-writing",
      "predicate-logic",
      "alternative-proof"
    ],
    "score": 6,
    "answer_score": 3,
    "is_accepted": true,
    "question_id": 4399003,
    "answer_id": 4399247
  },
  {
    "theorem": "How to give formal proof in logic?",
    "context": "I am taking a course on mathematical logic and am struggling to give formal proofs of theorems or claims. Currently doing first order logic. Here, an example:\n\nClaim: $\\models (\\exists x)\\big(A(x)\\Rightarrow (\\forall y)(A(y))\\big)$\n\nwhere $A$ is an unary predicate. I am asked to prove this. This is how I would reason: \n\nBy the rule of propositional logic $P\\Rightarrow Q$ is equivallent to $\\neg P\\vee Q$. So we wish to show that $$\\models(\\exists x)\\big(\\neg A(x)\\vee (\\forall y)(A(y))\\big)$$\n  So, assume we have an arbitrary interpretation of language $\\mathcal{L}=\\{A\\}$, that is a nonempty set $M$ together with unary predicate $A$.. By Tarski's definition of truth, there is some $m\\in M$ for which is $\\neg A(m)$ or for all $n\\in M$ $A(n)$ holds. But this is clearly true (logically valid).  Because whenever there is some $m\\in M$ for which $\\neg A(m)$, then the formula is satisfied. If not, then for all $n\\in M$ for which $A(n)$, which makes it also satisfied.\n\nNow, I am wondering whether this can be taken as a formal proof of the claim that the formula is logically valid in any interpretation of the language $\\mathcal{L}=\\{A\\}$. It seems really trivial, but I am afraid I am using too much of set theory. \nAnother reasoning could be: In fact, when $A$ is an unary predicate, that means $A$ is a subset of $M$. Now distinguish two cases: 1. $A=M$, then $(\\forall y)((y\\in M) \\Leftrightarrow (y\\in A))$ in other words $(\\forall y)(A(y))$. If $A\\subsetneq M$ then it is clear there is some $m\\in M$ that is not in $A$, in this case the part $(\\exists x)(\\neg A(x))$ is true. Now, this uses the definition of equality between sets and what is a subset.\nAnother idea: Using a contradiction. Assume this was not logically valid, so there exists some interpretation $\\mathcal{M}$ and value assignment $e$ for which $\\mathcal{M}\\models \\neg(\\exists x)\\big(A(x)\\Rightarrow (\\forall y)(A(y))\\big)$, that is $\\mathcal{M}\\models(\\forall x)(A(x)\\wedge (\\exists y)(\\neg A(y))$, thus for all $m$ $\\mathcal{M}\\models A(m)\\wedge (\\exists y)(\\neg A(y))$, by definition again there is some $n\\in M$ for which $\\mathcal{M}\\models A(m)\\wedge \\neg A(n)$, this gives $\\mathcal{M}\\models A(n)$ (the $n$ must be one of all of the $m$'s) but also $\\mathcal{M}\\models \\neg A(n)$. Which is contradiction by the law of excluded middle, so the statement before was logically valid.\nOr just by looking: It is clear that either for all $x$ $A(x)$ holds, or there is some $x$ for which $A(x)$ doesn't hold...\nAll this seems like a handwaving...\nSo, what tools/methods/kind of reasoning should I use at most when proving such claims and theorems in logic/model theory? I would be grateful for opinions and/or more examples (sources).\n",
    "proof": "Your proof is correct. You used the mathematical definitions of all the terms involved, like \"valid\", \"interpretation\", and \"true in an interpretation\" and came up with a rigorous argument that the statement was in fact valid according to those definitions.\nPeople often are under the mistaken impression that just cause we're studying logic, we need to stop being mathematicians and turn into C++ compilers. (Mathematical) logic is a branch of mathematics like any other, where we argue informally$^*$ but rigorously about mathematical constructions. It's just that in the case of logic, our arguments are often about formal proofs, which are themselves mathematical constructions. However, in a certain sense, the whole point of semantics and model theory is to eschew formal proofs and to instead think about sentences in their 'plain meaning' within mathematical structures they might talk about.\nSemantics do require stronger philosophical precepts than syntactical proofs (as you allude to, there is some set theory involved). You could also construct a formal proof of your statement in some proof calculus for first order logic, and such a construction would only involve finitary ideas. If we do this we would say that $\\vdash (\\exists x)\\big(\\neg A(x)\\vee (\\forall y)(A(y))\\big)$ (with a $\\vdash$ meaning \"provable (in some given deductive system)\" rather than a $\\models$ meaning \"valid\"). \nRemarkably, the standard proof systems for first order logic are complete and sound, i.e. they can prove a sentence if and only if it is semantically valid, i.e. $\\vdash \\phi\\iff \\models\\phi$. While formal proofs are less of a philosophical burden, as a practical matter it is generally harder to construct formal proofs than to make semantic arguments. But formal proofs are very important philosophically, especially if you're worried about foundational matters. They are also interesting objects of study in their own right. (And the notion that “true in all models” and “effectively provable in a formal system” are equivalent is particular to first order logic, anyway.)\n$^*$Here we might have a clash of terminology between the way you’re using the term \"formal\" vs the way logicians typically use the term. To a logician, a formal proof of a logical sentence is a mathematical object constructed according to some formal mathematical rules for proof construction. A rigorous natural language argument that a certain mathematical statement is true is an informal proof, regardless of how water-tight and well-explained the reasoning is. (Although you will still hear logicians occasionally use 'formal' to mean 'careful / detailed / rigorous'). Formal proofs are meant as models of well-reasoned arguments, and of course the more careful / detailed / rigorous - ly an informal proof is phrased, the easier it is to imagine it being translated into a formal proof in some suitable system.\n",
    "tags": [
      "proof-verification",
      "logic",
      "proof-writing",
      "first-order-logic",
      "model-theory"
    ],
    "score": 6,
    "answer_score": 3,
    "is_accepted": true,
    "question_id": 3053892,
    "answer_id": 3054285
  },
  {
    "theorem": "Common roots of recursive defined polynomial",
    "context": "I have a series of polynomials $P_j(x)$ given by the recursive formula\n$$P_{j+1}=\\frac{e_j}{c_j}xP_{j}-\\frac{f_j}{c_j}P_{j-1}\n$$\nwith $P_{-1} \\equiv 0$, $P_0 \\equiv 1$, where\n$$c_j = (j+1)(j+2\\kappa+1),\\\\\ne_j = (2j+2\\kappa+1)(j+\\kappa+1),\\\\\nf_j = (j+\\kappa)(j+\\kappa+1),\\\\\nj=0, \\dots, N-1.$$\nFor $\\kappa=\\dfrac{1}{2}$ numerical results indicate that the roots of the polynomial $P_i$ are a subset of the roots of $P_{2i+1}$. E.g. The most simple case:\n$$P_1(x)=\\frac{3}{2}x, \\qquad x_1=0\\\\\nP_2(x)=\\frac{5}{2}x^2-\\frac{5}{8}, \\qquad x_{1,2}=\\pm\\frac{1}{2},\\\\\nP_3(x)=\\frac{35}{8}x^3-\\frac{35}{16}x \\qquad x_1=0, \\ x_{2,3}=\\pm-\\frac{1}{\\sqrt{2}}.$$\nHow can this be proved or disproved?\n",
    "proof": "Define $Q_n(x) = \\dfrac{(2n)!!}{(2n - 1)!!} P_{n - 1}(x)$ for $n \\geqslant 0$, where $(-1)!! = 0!! = 1$, then $Q_0(x) = 0$, $Q_1(x) = 2$, and for $n \\geqslant 0$,$$\n(n + 1)(n + 2) P_{n + 1}(x) = (2n + 2)\\left( n + \\frac{3}{2} \\right) xP_n(x) - \\left( n + \\frac{1}{2} \\right)\\left( n + \\frac{3}{2} \\right) P_{n - 1}(x)\\\\ \\Longrightarrow Q_{n + 2}(x) = 2xQ_{n + 1}(x) - Q_n(x).$$\nTo prove that $P_n(x) \\mid P_{2n + 1}(x)$ for $n \\geqslant -1$, it is equivalent to prove that $Q_n(x) \\mid Q_{2n}(x)$ for $n \\geqslant 0$. Now it suffices to restrict the domains of all $Q_n$'s to $\\mathbb{R} \\setminus (-1, 1)$. Solving the recurrence relation,$$\nQ_n(x) = \\frac{1}{\\sqrt{x^2 - 1}} ((x + \\sqrt{x^2 - 1})^n - (x - \\sqrt{x^2 - 1})^n), \\quad \\forall n \\geqslant 0\n$$\nthen for any $n \\geqslant 0$,$$\n\\frac{Q_{2n}(x)}{Q_n(x)} = (x + \\sqrt{x^2 - 1})^n + (x - \\sqrt{x^2 - 1})^n \\in \\mathbb{R}[x] \\Longrightarrow Q_n(x) \\mid Q_{2n}(x).\n$$\nTherefore, $P_n(x) \\mid P_{2n + 1}(x)$ for any $n \\geqslant -1$, which implies the set of all roots of $P_n$ (multiplicity counted) is a subset of those of $P_{2n + 1}$.\n",
    "tags": [
      "polynomials",
      "proof-writing",
      "roots",
      "recursive-algorithms",
      "common-root"
    ],
    "score": 6,
    "answer_score": 3,
    "is_accepted": false,
    "question_id": 2862104,
    "answer_id": 2870195
  },
  {
    "theorem": "A combinatorial proof of Wilson&#39;s theorem [Proof Verification]",
    "context": "I am trying to prove the non-trivial assertion of Wilson's theorem,\n\nIf $p$ is a prime, $p$ divides $(p-1)! - (p-1)$.\n\nI'm going to consider the set of $p$-cycles on $\\mathbb{S}(\\mathbb{Z}_p)$, which I'll denote $C_p$. Now, $C_p$ has size $(p-1)!$ for the following reason: it suffices to choose the images of $1,,...,p$ without repeating values or having fixed points, therefore we have $p-1$ options for the image of $1$, $p-2$ for the image of $2$, and so on until we get to the image of $p$, which is at this point already determined. Now, let's observe that if $a \\in \\mathbb{Z}_p^{\\times}$, the translation given by summing $a$,\n$$\n\\tau_a : \\mathbb{Z}_p \\rightarrow \\mathbb{Z}_p\n\\\\ x \\ \\mapsto a + x \n$$\nis an element of $C_p$ with inverse $\\tau_{-a}$.\nWe can then define the set $X_p = C_p - \\{\\tau_a : a \\in \\mathbb{Z}_p^{\\times}\\}$ with $|X| = (p-1)! - (p-1)$, and the following equivalence relation on $X_p$:\n$$\nf \\sim g \\iff f = \\tau_{-a}\\cdot g\\cdot \\tau_a, \\ \\text{ for some } a \\in \\mathbb{Z}_p\n$$\nIf we can conclude that each equivalence class on $X_p/\\sim$ has size $p$, this will imply that $p \\ | \\ |X_p| = (p-1)! - (p-1)$ as we claim. It is clear that if $f \\in \\mathbb{S}(\\mathbb{Z}_p)$, \n$$\n[f] \\subseteq \\{\\tau_{-a}\\cdot f\\cdot \\tau_a : a \\in \\mathbb{Z}_p\\}\n$$\nTo finish, it will be sufficient to see the other inclusion, that is, that for any $a$ in $\\mathbb{Z}_p$ the element $\\tau_{-a}\\cdot f\\cdot \\tau_a$ is not a translation and therefore is in $X_p$ (the fact that $\\tau_{-a}\\cdot f\\cdot \\tau_a \\sim f$ is immediate). Let's suppose that, on the contrary, there exists $b \\in \\mathbb{Z}_p^{\\times}$ such that\n$$\n\\tau_{-a}\\cdot f\\cdot \\tau_a \\equiv \\tau_b\n$$\nand therefore\n$$\nf \\equiv \\tau_a\\cdot \\tau_b \\cdot \\tau_{-a} \\equiv \\tau_b\n$$\nsince translation commute. We've reached then a contradiction, since $f$ cannot be a translation, so in effect each class is as previously described and has size $p$, which concludes the proof.\nIs this correct? I'd also appreciate any suggestions to simplify the argument. Thanks in advance!\nEdit: I've noticed that it is also necessary to see that the set $\\{\\tau_{-a}\\cdot f\\cdot \\tau_a : a \\in \\mathbb{Z}_p\\}$ has in fact $p$ distinct elements. Any ideas?\n",
    "proof": "This looks correct, and the set $\\{\\tau_{-a}\\cdot f\\cdot\\tau_{a}:a\\in\\Bbb{Z}/(p)\\}$ does have $p$ distinct elements, since if $\\tau_{-a}\\cdot f\\cdot\\tau_{a} = \\tau_{-b}\\cdot f\\cdot\\tau_{b}$ then $f = \\tau_{b-a}\\cdot f\\cdot\\tau_{a-b}$ implying $a-b=0$ and thus $a=b$.\nThis last bit is because when looking at only $p$ letters the only permutations on $p$ letters that  $p$ cycle commutes with are its powers.  And the powers of $\\tau_{a-b}$ are all the translations, and $f$ is not a translation.  \nTo see the claim: conjugation by $\\sigma$ takes a cycle $(abc...)$ to $(\\sigma(a)\\sigma(b)\\sigma(c)...)$.  You can see that there are only $p$ ways this can be the original cycle, so only $p$ possible $\\sigma$s do the trick, and we already know the $p$ powers of the original cycle are among them, so nothing else can work.\n",
    "tags": [
      "combinatorics",
      "elementary-number-theory",
      "proof-verification",
      "proof-writing"
    ],
    "score": 6,
    "answer_score": 3,
    "is_accepted": true,
    "question_id": 2743936,
    "answer_id": 2743961
  },
  {
    "theorem": "If a ≡ b (mod n) and m|n, then a ≡ b (mod m)",
    "context": "Prove the following, for positive integers $m$ and $n$.\nIf $a \\equiv b \\pmod{n}$ and $m\\mid n$, then $a \\equiv b \\pmod{m}$.\nThis seems to me to be simple transitivity with the Fundamental theorem of arithmetic. \n$a \\equiv b \\pmod{n}$ and $m\\mid n$ means that $n \\mid a - b$ so there is an integer k such that $kn = a - b$. Since $m|n$ there is an $l$ such that $lm = n$. Thus by substitution $klm = a - b$. Which means that $m \\mid a - b$, this proves that $a \\equiv b \\pmod{m}$. \nMy question is firstly is this proof sound? And also if there is any more detail or a more concise way of writing this proof. I am new to proof writing in this format.\nAny tips would be greatly appreciated. Thank you.\n",
    "proof": "This is a perfect proof. This is sufficiently concise.\n",
    "tags": [
      "elementary-number-theory",
      "proof-verification",
      "proof-writing",
      "modular-arithmetic"
    ],
    "score": 6,
    "answer_score": 3,
    "is_accepted": true,
    "question_id": 2642195,
    "answer_id": 2642699
  },
  {
    "theorem": "Proof convexity of mean absolute error",
    "context": "I am given the cost function: mean absolute error\n$$ \\frac{1}{N} \\sum_{n=1}^{N} \\big|y_n - f(x) \\big|$$\nIf $f(x)$ is a linear regression: $f(x) = x^Tw$, where $w$ are the parameters of the regression function and $x$ the input, how can I prove that the mean absolute error is convex as a function of the parameters $w$ ?\nconvexity is defined here: http://mathworld.wolfram.com/ConvexFunction.html\n",
    "proof": "I too am in the process of working it out, but here is what I have:\nWe only need to verify that $g(w)=\\lvert{y_n-f(x_n)}\\rvert$ is convex, since we know that the sum of convex functions is also a convex function. \nThe condition for convexity is the following: for $0\\leq\\lambda\\leq1$ a function g is convex iff \n$g(\\lambda w_1 + (1-\\lambda)w_2) \\leq \\lambda g(w_1)+(1-\\lambda)g(w_2)$\nSo we have for our function $g(w)=\\lvert{y_n-x_n^{T}w}\\rvert$, \n$\\lvert y_n-x_n^{T}(\\lambda w_1 + (1-\\lambda)w_2)\\rvert \\leq \\lambda \\lvert{y_n-x_n^{T}w_1}\\rvert + (1-\\lambda)\\lvert{y_n-x_n^{T}w_2}\\rvert$\nSince $0\\leq\\lambda\\leq1$, I can put the $\\lambda$ and $(1-\\lambda)$ inside the absolute value, since these are positive and therefore won't make a difference to the equation.\n$\\lvert y_n-x_n^{T}(\\lambda w_1 + (1-\\lambda)w_2)\\rvert \\leq  \\lvert{\\lambda y_n -\\lambda x_n^{T}w_1}\\rvert + \\lvert{(1-\\lambda)y_n- (1-\\lambda)x_n^{T}w_2}\\rvert$\nThis looks familiar, since I can see that if I take the right hand side of my inequality and denote the insides of the absolute values by $a$ and $b$ \n$a = {\\lambda y_n -\\lambda x_n^{T}w_1}$ \n$b = {(1-\\lambda)y_n- (1-\\lambda)x_n^{T}w_2}$\nAnd the inside of the absolute value on the left side of my inquality is:\n$a+b = y_n-x_n^{T}(\\lambda w_1 + (1-\\lambda)w_2)$\nThen I see that my inequality is actually \n$\\lvert a+b \\rvert \\leq \\lvert a \\rvert + \\lvert b \\rvert $, which is true. Therefore $g(w)$ is convex and $MAE(w)$ is convex.\n",
    "tags": [
      "proof-writing"
    ],
    "score": 6,
    "answer_score": 3,
    "is_accepted": true,
    "question_id": 2579595,
    "answer_id": 2580888
  },
  {
    "theorem": "Why 1/x is elementary function?",
    "context": "This is so obvious, that $\\frac{1}{x}$ is elementary function. But how this can be proven? I've been searching for information, and have found a whole list of elementary functions, and $\\frac{1}{x}$ is one of them. It is like an axiom, which is always true. There are many ways to prove that different compound functions are elementary, but $\\frac{1}{x}$ is always considered to be elementary.\nWe were give a list of properties, based on which we should prove that $\\frac{1}{x}$ is also elementary. I'm quite confused, because I have no idea how to start proving. Would appreciate any kind of help.\n\nBase Cases.\n\nIdentity function, $id(x) = x$ is in EF.\nAny constant function is in EF.\nThe sine function $sin(x)$ is in EF\n\nConstructor Cases. If $f,g \\in EF$, then so are\n\n$f+g$, $fg$, $2^g$\nThe inverse function $f^{-1}$;\nThe composition $f \\circ g$.\n\n\nOriginal: Given properties\n",
    "proof": "Recall that $\\cot x=\\tan(\\pi/2-x)$, so\n$$\n\\frac{1}{x}=\\cot\\arctan x\n$$\nMore precisely, for $x>0$ you have\n$$\n\\arctan x+\\arctan\\frac{1}{x}=\\frac{\\pi}{2}\n$$\nso\n$$\n\\frac{1}{x}=\\tan\\left(\\frac{\\pi}{2}-\\arctan x\\right)=\\cot\\arctan x\n$$\nand, for $x<0$,\n$$\n\\arctan x+\\arctan\\frac{1}{x}=-\\frac{\\pi}{2}\n$$\nso\n$$\n\\frac{1}{x}=\\tan\\left(-\\frac{\\pi}{2}-\\arctan x\\right)=\n-\\cot(-\\arctan x)=\\cot\\arctan x\n$$\nThe tangent and the cotangent are elementary, because so are the sine and the cosine.\nThe exponential function is elementary, because $e^x=2^{x/\\!\\log 2}$. Therefore also the natural logarithm is elementary. Thus\n$$\n\\frac{1}{\\cos^2x}=\\exp(-\\log(\\cos^2x))\n$$\nis elementary and\n$$\n\\tan x=\\sin x\\cos x\\frac{1}{\\cos^2x}\n$$\nSimilarly for the cotangent.\n",
    "tags": [
      "functions",
      "discrete-mathematics",
      "logic",
      "proof-writing"
    ],
    "score": 6,
    "answer_score": 2,
    "is_accepted": true,
    "question_id": 2495089,
    "answer_id": 2495149
  },
  {
    "theorem": "Prove ${{n+m+1}\\brace m}=\\sum_{k=0}^mk{{n+k}\\brace k}$ via double-counting",
    "context": "\nI want to show by a double-counting argument that$${{n+m+1}\\brace m}=\\sum_{k=0}^mk{{n+k}\\brace k}$$for $m,n\\in\\Bbb N$ (where $0\\in\\Bbb N$).\n\nNote that ${{n}\\brace k}$ is a Stirling number of the second kind (i.e. the number of partitions of $[n]:=\\{1,2,\\ldots,n\\}$ into $k$ blocks). I am just starting to do double-counting proofs, so I just want to make sure I am doing this right. I would like some feedback on my proof: whether it is correct or not, what are some improvements I could make, etc. Here is my proof:\nWe know that ${{n+m+1}\\brace m}$ is the number of ways to partition $[n+m+1]$ into $m$ blocks. Now we focus on the $n+k+1$ element of $[n+m+1]$ for some $0\\leq k\\leq m$. Every element past $n+k+1$ we put in its own block in the partition, which accounts for $(n+m+1)-(n+k+1)=m-k$ of the blocks. We partition the first $n+k$ elements into the remaining $k$ blocks in ${{n}\\brace k}$ ways. Then we stick the element $n+k+1$ into one of these $k$ blocks (which can be done in $k$ ways). Hence, there are $k{{n+k}\\brace k}$ ways to partition the set with $m$ blocks with our choice of $k$. We sum over all possibilities of $k$ to account for all choices of the $n+k+1$ element to obtain ${{n+m+1}\\brace m}=\\sum_{k=0}^mk{{n+k}\\brace k}$ total partitions of $[n+m+1]$ into $m$ blocks.\nThanks in advance for any feedback. If you have another way to prove this I'd love to see that as well.\n",
    "proof": "For the double counting argument I  would put it like this. Suppose we\npartition $n+m+1$ labeled  elements into $m$ sets. Now  let $n+q+1$ be\nthe minimum value so that all elements labeled from $n+q+2$ to $n+m+1$\nreside in singleton sets. This gives $m-q$ singletons. Here we clearly\nhave $q\\ge 1$  (when $q=0$ we get $m$ singletons  which leaves nothing\nto cover the elements  up to $n+1$) as well as $q\\le  m$ ($q=m$ is the\ntop case where $n+m+1$ is not a singleton).  Note also that $n+q+1$ is\nin a  partition that contains  at least  one additional element  or it\nwould not have  been minimal. Therefore the partition  of the elements\nfrom the  lower end (the  chain of singletons is  the upper end)  is a\npartition  of $n+q+1$  into  $q$  elements with  $n+q+1$  not being  a\nsingleton.  This means  we obtain an ordinary partition  of $n+q$ into\n$q$ elements  when we  remove $n+q+1$ from  its partition.   There are\n${n+q\\brace q}$  of these and  we have $q$ possibilities  for $n+q+1,$\ngiving the formula\n$${n+m+1\\brace m} = \\sum_{q=1}^m q  {n+q\\brace q}.$$\nFor an algebraic proof, use induction starting at $m=1$ where we find\n$${n+2\\brace 1} = 1 \\times {n+1\\brace 1}$$\nwhich holds  by inspection. Supposing it  holds for $m$ we  get in the\ninduction step\n$${n+m+1\\brace m} + (m+1){n+m+1\\brace m+1}\n= \\sum_{q=1}^{m+1} q  {n+q\\brace q}.$$\nThe left is just the basic Stirling number recurrence and we obtain\n$${n+m+2\\brace m+1}\n= \\sum_{q=1}^{m+1} q  {n+q\\brace q}$$\nas desired.\n",
    "tags": [
      "combinatorics",
      "proof-verification",
      "proof-writing"
    ],
    "score": 6,
    "answer_score": 3,
    "is_accepted": true,
    "question_id": 2466394,
    "answer_id": 2466926
  },
  {
    "theorem": "Is this proof that *on average it takes $e$ straws to break the camel&#39;s back* sufficient and clear?",
    "context": "Problem: There is a camel with a tolerance of weight 1. We stack straws on his back one by one, and the weight of each straw is a number randomly sampled from a uniform distribution between 0 and 1. Prove that, in expectation, it takes $e$ straws to break the camel's back.\n\nProof:\nCall $P(k, n)$ the probability of surpassing $k$ weight with exactly $n$ straws (before the $n$th straw the total weight was below $k$, and after the $n$th straw the total weight was above $k$). By definition, given that the first straw weighed $x$, the probability of surpassing $k$ weight with the remaining $n-1$ straws is $P(k-x, n-1)$. So if you consider all possible first straw weights $0$ to $k$ (assume $n\\geq2$ so that we only want to include the case where the first straw weight is less than $k$), that leads us to\n$$P(k, n) = \\int_0^k P(k-x, n-1) \\mathrm{d}x \\quad if n\\ge2 \\quad (1)$$\n$$P(k, 1) = 1-k \\quad if n=1 \\quad (2)$$\nThe case of $n=1$ is because we are sampling uniformly from $0$ to $1$ and to surpass $k$ with $1$ straw the sample needs to come from that portion beyond $k$, which is between $k$ and $1$, thus $1-k$.\nBy definition of expected value, our final answer will be the  infinite weighted sum of probabilities, with the weights being the number of straws:\n$$\\sum_{m=1}^\\infty mP(1, m) \\quad (3)$$\nSo now let's prove the value of $P(k, n)$ using induction.\nInduction step: Assume\n$$P(k, n) = \\frac{k^{(n-1)}(n-k)}{n!} \\quad (4)$$\nand use that assumption to prove that\n$$P(k, n+1) = \\frac{k^n(n+1-k)}{(n+1)!} \\quad (5)$$\nBase case: Prove that the (4) holds for $n=1$.\nInduction Step:\n$$P(k, n+1) = \\int_0^k P(k-x, n) \\mathrm{d}x \\quad by (1)$$\n$$= \\int_0^k \\frac{(k-x)^{(n-1)}(n-(k-x))}{n!} \\mathrm{d}x \\quad by (4)$$\n$$= \\frac{1}{n!}\\Big[n \\int_0^k (k-x)^{(n-1)} \\mathrm{d}x - \\int_0^k (k-x)^n \\mathrm{d}x\\Big]$$\n$$= \\frac{1}{n!}\\Big[\\frac{n(-(k-x)^n)}{n}\\Big|_{x=0}^k - \\frac{-(k-x)^{n+1}}{n+1}\\Big|_{x=0}^k\\Big]$$\n$$= \\frac{1}{n!}\\Big[k^n - \\frac{k^{n+1}}{n+1}\\Big]$$\n$$= \\frac{k^n(n+1-k)}{(n+1)!}$$\nBase Case:\n$$P(k, 1) = 1 - k$$\n$$= \\frac{k^{1-1}(1-k)}{1!}$$\nThus we have proved\n$$P(k, n) = \\frac{k^{(n-1)}(n-k)}{n!} \\quad (4)$$\nand substituting in $k = 1$ we get\n$$P(1, n) = \\frac{(n-1)}{n!}$$\nFinally, substituting into (3) we get\n$$\\sum_{m=1}^\\infty mP(1, m) = \\sum_{m=1}^{\\infty} \\frac{m(m-1)}{m!} = e$$\n$\\square$\n",
    "proof": "We give an alternate derivation using indicator random variables. Let $X_k=1$ if the sum of the first $k$ weights is $\\le 1$, and let $X_k=0$ otherwise. We want $E(1+\\sum X_k)$, which is $1+\\sum E(X_k)$. \nThe probability that the sum of the first $k$ straws is $\\le 1$ is the proportion of the unit hypercube of dimension $k$ occupied by the part with $x_1+x_2+\\cdots +x_k\\le 1$. This proportion is $\\frac{1}{k!}$. Thus $\\sum E(X_k)=e-1$, and therefore the required expectation is $e$.\nRemark: Congratulations on pushing successfully through a difficult calculation. There is overlap between our calculations. I cheated by quoting a standard volume result, while a significant part of your work in effect was an evaluation of the multiple integral that gives us the volume of the region where $x_1+\\cdots +x_k\\le 1$, with the $x_i$ non-negative.\n",
    "tags": [
      "proof-verification",
      "proof-writing"
    ],
    "score": 6,
    "answer_score": 3,
    "is_accepted": true,
    "question_id": 1486466,
    "answer_id": 1486532
  },
  {
    "theorem": "How would you interpret the following statement involving &quot;a.e.&quot;?",
    "context": "Here is an edited fragment from an exercise:\n\nLet $(X, \\mathcal A, \\mu)$ be a measure space, $(f_n)$ be a sequence of such and such functions. If $f(x)= \\lim f_n(x)$ exists for almost every $x\\in X$ then $f$ has such and such properties (in particular $f$ is measurable).\n\nI'm confused about the role of $f$. Is it already given in the statement? E.g. if in the proof, one would set $f$ to zero on some convenient set of measure zero, it wouldn't be OK? (A proof which I encountered does exactly that.)\nThe question probably has little to do with measure theory but that a general \"if something shows up in the assumption, must it be fixed from there on?\" but I've added the context just in case.\n",
    "proof": "If the measure space is not complete, then following statement is, in general, not correct.\n\nLet $(f_n)_{n \\in \\mathbb{N}}$ be a sequence of measurable functions. If $f(x) = \\lim_{n \\to \\infty} f_n(x)$ exists for almost every $x \\in X$, then $f$ is measurable.\n\nJust pick a non-measurable set $N$ such that there exists $M \\supseteq N$ measurable with $\\mu(M)=0$ (such a set $N$ exists if the measure space is not complete). Then $f_n(x) := 0$ converges for almost every $x \\in X$ to $f(x) := 1_N(x)$, but $f$ is not measurable.\nHowever, the following statement holds true:\n\nLet $(f_n)_{n \\in \\mathbb{N}}$ be a sequence of measurable functions. If $f(x) = \\lim_{n \\to \\infty} f_n(x)$ exists for almost every $x \\in X$, then $f$ has a measurable modification, i.e. there exists a measurable function $\\tilde{f}$ such that the set $\\{x; \\tilde{f}(x) \\neq f(x)\\}$ is contained in a $\\mu$-null set.\n\nNote that the function $\\tilde{f}$ satisfies in particular $$\\tilde{f}(x) = \\lim_{n \\to \\infty} f_n(x)$$ for $\\mu$-almost all $x \\in X$.\n",
    "tags": [
      "analysis",
      "measure-theory",
      "proof-writing",
      "fake-proofs",
      "almost-everywhere"
    ],
    "score": 6,
    "answer_score": 4,
    "is_accepted": true,
    "question_id": 1434943,
    "answer_id": 1436195
  },
  {
    "theorem": "Define $S\\equiv\\{ x\\in \\mathbb{Q}\\mid x^2&lt;2\\}$. Show that $\\sup S=\\sqrt{2} $.",
    "context": "\nDefine $S\\equiv\\{ x\\in \\mathbb{Q}\\mid x^2<2\\}$. Show that $\\sup S=\\sqrt{2} $.\n\n\nFor this question, I think that I would use the completeness axiom. As $3$ is greater than $2$, so $S$ has a upper bound. To yield a contradiction, I think I need to find there exists an constant $c$ which is a least upper bound such that $b^2>2$, then there exists a number $r$ such that $b-r$ also in upper bound of $S$ which gives a contradiction since $b$ is the least upper bound of $S$. Then we can end the proof.\n\nDoes the idea right? If not, can anyone give a suggestion or a hit to write a better proof? Thanks.\n",
    "proof": "Hint: Here is an elementary argument. \nIf you take any $x \\in S$ and a rational number $r < 1$ such that $$0 < r < (2 - x^2)/(2x  + 1)$$\nshow that $x + r \\in S$. This shows that $S$ has no maximum element. \nConsider $T = \\{y \\in \\mathbb Q; y^2 > 2\\}$ and similarly show that there is no minimum element in $T$. \nConclusion: If $a = \\sup S$ then $a^2 = 2$, $a > 0$. Use a simple contradiction. \n",
    "tags": [
      "real-analysis",
      "proof-writing"
    ],
    "score": 6,
    "answer_score": 2,
    "is_accepted": false,
    "question_id": 1325659,
    "answer_id": 1325673
  },
  {
    "theorem": "equivalence classes of ∼ are left cosets of H in G - my attempt",
    "context": "\nLet $H$ be a subgroup of G, and define a relation $∼$ on G by the\n  rules that $x∼y$ mean $x^{-1}y\\in H $. Show that $∼$ is an equivalence\n  relation and its equivalence classes are the left cosets of $H$.\n\nMy attempt\nWe know that a relation $R$ on a set $X$ is a set of ordered pairs of members of $X$ which satisfies the condition of the given relation.\nTo prove that the relation is an equivalence relation, we need to check if the relation $∼$ have all the properties\n\n$∼$ is reflexive if $x∼x$ for all $x\\in G$\n$∼$ is symmetric if $x∼y \\Rightarrow y∼x$ for all $x,y\\in G$\n$∼$ is transitive if $x∼y$ & $y∼z$ then $x∼z$ for all $x,y,z\\in G$\n\nSo, we can see that the relation is reflexive since\n$$x^{-1}x=1 \\forall x\\in H$$\nIt's also symmetric\n$$x^{-1}y {,} \\forall x,y\\in H$$\nand since the identity element exist we know that an element $j$ exist such that\n$$x^{-1}yj=1 <=> j=y^{-1}x \\in H$$\nTherefore its symmetric. The relation is also transitive:\n$$x∼y .AND. y∼z. THEN.x∼z \\forall x,y,z \\in H$$\n$$(x^{-1}y)(y^{-1}z)=x^{-1}z \\in H$$\nSince the relation have all the properties listed above, the relation is an equialance relation. Now we are going to show that the equivalence classes of this relation is the left cosets. Since the distinct left cosests form a partition of $G$, its equal to the equivalence classes because the equivalence classes are the parts of the partition of $G$, which means the equivalence classes are also forming the partition.\nThe equivalence classes are the set\n$$[x]=\\{y \\in G | y∼x\\}$$\nand by the reflexivitive we get\n$$[x]=\\{y \\in G | x∼y\\}=\\{y\\in G|x^{-1}y\\in H\\}$$\n$x^{-1}y\\in H$ gives us that \n$$y=x(x^{-1}y)=y\\in xH$$\nThis means that \n$$y=xh$$\nfor some $h \\in H$ and this gives us\n$$h=x^{-1}y$$\nwhich is the relation $x∼y$. Therefore\n$$[x]=xH$$\n",
    "proof": "You start wrong. The property you have to prove are\n\n$\\sim$ is reflexive, that is, $x\\sim x$ for all $x\\in G$,\n$\\sim$ is symmetric, that is, for all $x,y\\in G$, $x\\sim y$ implies $y\\sim x$,\n$\\sim$ is transitive, that is, for all $x,y,z\\in G$, $x\\sim y$ and $y\\sim z$ implies $x\\sim z$.\n\nNote that the relation is on $G$, not on $H$.\nWith this correction, your proof is good, apart from the symmetry.\n\nSuppose $x\\sim y$; then $x^{-1}y\\in H$ and therefore $(x^{-1}y)^{-1}\\in H$. Since $(x^{-1}y)^{-1}=y^{-1}(x^{-1})^{-1}=y^{-1}x\\in H$, we conclude that $y\\sim x$.\n\nThe proof that $[x]=xH$ is good, but not clearly written down.\nWe have $[x]=\\{y\\in G:x\\sim y\\}$ by definition.\n\nSuppose $y\\in[x]$. Then $x^{-1}y\\in H$, so $y=x(x^{-1}y)\\in xH$. Therefore $[x]\\subseteq xH$.\nSuppose $y\\in xH$. Then $y=xh$ for some $h\\in H$, so $x^{-1}y=h\\in H$.  Therefore $xH\\subseteq[x]$.\n\n",
    "tags": [
      "group-theory",
      "discrete-mathematics",
      "proof-writing",
      "equivalence-relations",
      "normal-subgroups"
    ],
    "score": 6,
    "answer_score": 3,
    "is_accepted": true,
    "question_id": 805975,
    "answer_id": 806003
  },
  {
    "theorem": "Help to understand the proof of $ \\lim \\limits_{x\\to 0^+} f \\left(\\frac{1}{x}\\right)=\\lim \\limits_{x\\to \\infty}f(x)$",
    "context": "The following is an answer to the proof of \n$$ \\lim \\limits_{x\\to 0^+}f\\left( \\frac{1}{x} \\right)=\\lim \\limits_{x\\to \\infty}f(x)$$\n\nIf $l=\\lim \\limits_{x\\to \\infty}f(x)$, then for every $\\epsilon>0$ there is some $N$ such that $|f(x)-l|<\\epsilon$ for $x>N$, and we can clearly assume that $N>0$. Now if $0<x<\\frac{1}{N}$, then $\\frac{1}{x} > N$, so $|f\\left(\\frac{1}{x}\\right)-l|<\\epsilon$. Thus, $\\lim \\limits_{x\\to 0^+}f\\left( \\frac{1}{x} \\right)=l$.\n\nI don't get how if $\\frac{1}{x} > N$, we can conclude that $|f\\left(\\frac{1}{x}\\right)-l|<\\epsilon$.\n",
    "proof": "Let $L=\\lim \\limits_{y\\to \\infty}f(y)$. Then by definition, we know that:\n\nFor every $\\epsilon^*>0$, there is some $N>0$ such that $|f(y)-L|<\\epsilon^*$ for all $y>N$.\n\nWe want to show that $\\lim \\limits_{x\\to 0^+}f\\left( \\frac{1}{x} \\right)=L$. That is, we want to show that:\n\nFor every $\\epsilon>0$, there is some $\\delta>0$ such that if $0<x<\\delta$, then $|f(1/x)-L|<\\epsilon$.\n\n\nWith that in mind, choose any $\\epsilon>0$. Now let $\\epsilon^*=\\epsilon$. Using the first definition, let $\\delta=\\dfrac{1}{N}$, where $N$ corresponds to our selection of $\\epsilon^*=\\epsilon$. \nNow suppose that $0<x<\\delta=\\dfrac{1}{N}$. Then we know that $\\dfrac{1}{x}>N$. Now let $y=\\dfrac{1}{x}$. Since $y=\\dfrac{1}{x}>N$, it follows by the first definition that $\\left|f\\left(\\dfrac{1}{x}\\right)-L\\right| < \\epsilon^*=\\epsilon$, as desired.\n",
    "tags": [
      "algebra-precalculus",
      "proof-writing"
    ],
    "score": 6,
    "answer_score": 3,
    "is_accepted": true,
    "question_id": 455942,
    "answer_id": 455965
  },
  {
    "theorem": "$S=\\{(X,Y) \\in \\mathcal{P}(A) \\times \\mathcal{P}(A) \\mid \\forall x \\in X \\exists y \\in Y(xRy)\\}.$ If R is symmetric, must S be symmetric?",
    "context": "I'm working on an exercise from How To Prove It by Velleman, and I'm having a hard time.\n\nSuppose $R$ is a relation on $A$ and define a relation S on $\\mathcal{P}(A)$ as follows: $$S=\\{(X,Y) \\in \\mathcal{P}(A) \\times \\mathcal{P}(A) \\mid \\forall x \\in X \\exists y \\in Y(xRy)\\}.$$ (b) If $R$ is symmetric, must S be symmetric?\n(Parts (a) and (c) ask the same for reflexivity and transitivity, but I'm only having trouble with part (b) at the minute.)\n\nI've tried a few examples and everything I've tried appears to be symmetric, but perhaps I'm not considering the right examples.\nI've started by trying to show that if $(X,Y) \\in S$ then $(Y,X) \\in S$. So suppose $(X,Y) \\in S$. We need to show that $(Y,X) \\in S$ which means $\\forall y \\in Y \\exists x\\in X(yRx)$. So assume $y \\in Y$.\nI'm having trouble finding some $x \\in X$ such that $yRx$. I know if I can find any $x \\in X$, I can immediately infer $xRy'$ for some $y' \\in Y$ due to $(X,Y) \\in S$, and from there can infer $y'Rx$ since $R$ is symmetric. The only problem I see with this is that without showing that $y'=y$, I can't infer $(Y,X) \\in S$ because $y'$ is a only a specific element of $Y$. I can't think of any obvious ways of doing this.\nI also tried considering cases when $X= \\varnothing$, and $X \\neq \\varnothing$. It seems to me that $X=\\varnothing$ makes no sense at all for $S$, because we'll never have any elements of $X$ to consider finding a corresponding value of $y$ such that $xRy$. At least with $X \\neq \\varnothing$ I would have some $x \\in X$ to work with. I think this method suffers from the last method in the same way as before, as we can only consider a specific element $y' \\in Y$ where $xRy'$ from the statement $(X,Y) \\in S$.\nAll of this leads me to believe there is some counterexample I am missing. I feel completely lost, any help with this would be greatly appreciated!\n",
    "proof": "Your intuition was good; this need not be true. Here's a sample from a large class of counterexamples: Let $A=\\{1,2,3,4\\}$ and $R = \\{ (1,3), (2,3), ~~ (3,1), (3,2) \\}$.\nNow $R$ is symmetric, but if $X=\\{1,2\\}$ and $Y=\\{3,4\\}$, then $(X,Y)\\in S$ but $(Y,X)\\notin S$.\n",
    "tags": [
      "elementary-set-theory",
      "proof-writing",
      "relations"
    ],
    "score": 6,
    "answer_score": 2,
    "is_accepted": true,
    "question_id": 374815,
    "answer_id": 374825
  },
  {
    "theorem": "Proving {$b_n$}$_{n=1}^\\infty$ converges given {$a_n$}$_{n=1}^\\infty$ and {$a_n b_n$}$_{n=1}^\\infty$",
    "context": "Suppose {$a_n$}$_{n=1}^\\infty$ and  {$b_n$}$_{n=1}^\\infty$ are sequences such that {$a_n$}$_{n=1}^\\infty$ coverges to A$\\neq$0 and {$a_n b_n$}$_{n=1}^\\infty$ converges.  Prove that {$b_n$}$_{n=1}^\\infty$ converges.\nWhat I have so far:\n$b_n = {a_n b_n \\over a_n}$ $\\to$ $C \\over A$, $A\\neq0$\n|$b_n - {C \\over A}$| = |${a_n b_n \\over a_n} - {C \\over A}$| = |${Aa_nb_n - Ca_n \\over Aa_n}$| $ \\leq $ |${1 \\over Aa_n}||Aa_nb_n - Ca_n$|=|${1 \\over Aa_n}||a_n(Ab_n - C)$|\n$\\leq |{1 \\over Aa_n}||a_n||(Ab_n - C)$|.  Note: since $a_n$ converges, there is M>0 such that |$a_n| \\leq$M for all n $ \\in\\Bbb N$.\nThus, |${1 \\over Aa_n}||a_n||(Ab_n - C)$| = |${1 \\over M}||M||(Ab_n - C)$|.  And this is where I get lost.  Any thoughts? Or am I completely wrong to begin with?\n",
    "proof": "Mh what is about something like:\n$a_n b_n$ converges now let's call the limit $C$, as $A\\neq 0$ we can write \n$C=A\\cdot B$ with $B=\\frac{C}{A}$ \n$$\n\\begin{align*}\n |a_n b_n - A \\cdot B| &= |a_n b_n - b_n A +b_n A - A\\cdot B|\\\\\n&=|b_n(a_n-A) + A(b_n-B)| \\\\\n\\end{align*} $$\nBecause $a_n$ converges the first part converges to zero, as we know the lhs converges the rhs has to converges to, so \n$$|A(b_n-B)|=|A| |b_n-B| $$ \nmust converge to zero, as we know $|A|\\neq 0$ we know \n$$ |b_n-B|$$ converges to $0$.  And so $b_n$ converges to $B$\nAs robjohn pointed out we get the triangle inequality \n$$|A(b_n-B)|\\leq |a_n b_n - AB| + |b_n(a_n-A)| $$ As we know $a_nb_n$ converges with $a_n$ not converging to $0$. If $b_n$ is not bounded $a_n b_n$ can't converge, as we know $a_nb_n$ is convergent, we get $b_n$ is bounded.\n",
    "tags": [
      "real-analysis",
      "proof-writing"
    ],
    "score": 6,
    "answer_score": 1,
    "is_accepted": true,
    "question_id": 308158,
    "answer_id": 308171
  },
  {
    "theorem": "Show $V_{F_{\\mu}}(-\\infty,x]=|\\mu|((-\\infty,x])$ for all $x\\in\\mathbb{R}$ ($\\mu$ a finite signed measure on $(\\mathbb{R},\\mathscr{B}(\\mathbb{R}))$).",
    "context": "My Question\nI want to prove the following:\n\nClaim$\\quad$ Let $\\mu$ be a finite signed measure on $(\\mathbb{R},\\mathscr{B}(\\mathbb{R}))$. Show that $V_{F_{\\mu}}(-\\infty,x]=|\\mu|((-\\infty,x])$ for all $x\\in\\mathbb{R}$.\n\nMy Attempt\nHere is my attempt so far:\n\nProof$\\quad$ Let $\\mu$ be a finite signed measure on $(\\mathbb{R},\\mathscr{B}(\\mathbb{R}))$. Let $x\\in\\mathbb{R}$. Let $\\mathscr{S}$ be the collection of finite sequences $\\{t_i\\}_{i=0}^n$ such that\n$$\n-\\infty < t_0 < t_1 < \\dots < t_n \\leq x.\n$$\nWe first prove that $V_{F_{\\mu}}(-\\infty,x]\\leq|\\mu|((-\\infty,x])$. Let $\\{t_i\\}_{i=1}^n\\in\\mathscr{S}$. Then\n\\begin{align*}\n\\sum_{i=1}^n|\\mu((t_{i-1},t_i])| &\\leq \\sum_{i=1}^n|\\mu|((t_{i-1},t_i])\\\\\n&= |\\mu|\\left(\\bigcup_{i=1}^n(t_{i-1},t_i]\\right)\\\\\n&\\leq |\\mu|((-\\infty,x]).\n\\end{align*}\nSo $|\\mu|((-\\infty,x])$ is an upper bound of the set $\\left\\{\\sum_{i=1}^n|\\mu((t_{i-1},t_i])|:\\{t_i\\}_{i=0}^n\\in\\mathscr{S}\\right\\}$. But\n\\begin{align*}\nV_{F_{\\mu}}(-\\infty,x] &= \\sup\\left\\{\\sum_{i=1}^n|F_{\\mu}(t_i)-F_{\\mu}(t_{i-1})|:\\{t_i\\}_{i=1}^n\\in\\mathscr{S}\\right\\} \\\\\n&= \\sup\\left\\{\\sum_{i=1}^n|\\mu((-\\infty,t_i])-\\mu((-\\infty,t_{i-1}])|:\\{t_i\\}_{i=0}^n\\in\\mathscr{S}\\right\\}\\\\\n&= \\sup\\left\\{\\sum_{i=1}^n|\\mu((t_{i-1},t_i])|:\\{t_i\\}_{i=0}^n\\in\\mathscr{S}\\right\\}.\n\\end{align*}\nThus, $V_{F_{\\mu}}(-\\infty,x]\\leq|\\mu|((-\\infty,x])$.\n\nWhere I Got Stuck:\nI couldn't prove the other direction. Here is my attempt, but I don't think it is correct, because $V_{F_{\\mu}}$ is not necessarily a measure.\n\nNext we show that $V_{F_{\\mu}}(-\\infty,x]\\geq|\\mu|((-\\infty,x])$. Consider $\\{t_0\\}\\in\\mathscr{S}$ where $t_0=x$. Then\n$$\n|\\mu((-\\infty,x])| = |F_{\\mu}(t_0)| \\leq V_{F_{\\mu}}(-\\infty,x].\n$$\nBut $|\\mu|$ is the smallest of those positive measure $\\nu$ that satisfy $|\\mu((-\\infty,x])|\\leq\\nu((-\\infty,x])$. Thus $|\\mu|((-\\infty,x])\\leq V_{F_{\\mu}}(-\\infty,x]$. Hence we have proved that $V_{F_{\\mu}}(-\\infty,x]=|\\mu|((-\\infty,x])$.\n\nCould someone please help me out with the proof? Thanks a lot in advance!\n",
    "proof": "I'm happy with your proof $V_F(-\\infty,x]\\le|\\mu|((-\\infty,x])$. In the converse, note $|\\mu|((-\\infty,x])=\\lim_{n\\to\\infty}|\\mu|((-n,x])$ by the upper continuity of measure; we also have assumed this is a finite quantity. Fix $\\epsilon>0$; I may find $n$, $|\\mu|((-n,x])>|\\mu|((-\\infty,x])-\\epsilon$. But clearly $V_F(-\\infty,x]\\ge|\\mu|((-n,x])$ from considering the straightforward finite partition $t_0=-n<x=t_1$. We see $V_F(-\\infty,x]>|\\mu|((-\\infty,x])-\\epsilon$, and we see it for any $\\epsilon$. Hence $V_F\\ge|\\mu|$ on this interval.\nCombining that with your previous inequality, you have a total equality.\n",
    "tags": [
      "real-analysis",
      "analysis",
      "measure-theory",
      "proof-writing",
      "bounded-variation"
    ],
    "score": 6,
    "answer_score": 2,
    "is_accepted": true,
    "question_id": 4953330,
    "answer_id": 4954001
  },
  {
    "theorem": "Proving a Probability Limit is Non Zero",
    "context": "I am reading these lecture notes: on page 1, it mentions (indirectly) that :\n\nDefine the Score Function as the first derivative of the likelihood\nIf the Expected Value of the Score Function is not equal to 0, then the resulting MLE estimator will not be Consistent.\n\nI am trying to understand why this is true.\nI understand that this is the definition of Consistency in Statistics: An estimator $\\hat{\\theta}$ is consistent if when the sample size $n$ grows to infinity, the empirical estimator $\\hat{\\theta}_n$ subtracted from the theoretical estimator $\\theta$ has a 0 probability of being greater than some small number $\\epsilon$:\n$$\\lim_{{n \\to \\infty}} P(|\\hat{\\theta}_n - \\theta| > \\epsilon) = 0$$\nI also understand how to obtain the Score Function for a given Likelihood Function:\n\nA random variable $X$.\n\nA probability density function (pdf) $f(x; \\theta)$.\n\nThe likelihood of $f(x; \\theta)$ given data $x_1, x_2, ..., x_n$ is\n\n\n$$L(\\theta; x) = \\prod_{i=1}^{n} f(x_i; \\theta)$$\n\nThe log-likelihood is\n\n$$\\log L(\\theta; x) = \\sum_{i=1}^{n} \\log f(x_i; \\theta)$$\n\nThe derivative of the log-likelihood is\n\n$$\\frac{d}{d\\theta} \\log L(\\theta; x) = \\frac{d}{d\\theta} \\sum_{i=1}^{n} \\log f(x_i; \\theta)$$\n\nThe MLE estimator of $\\theta$ is the only solution $\\hat{\\theta}$\nof the equation\n$$\\frac{d}{d\\theta} \\log L(\\theta; x) = 0,$$\nprovided this equation has a unique solution.\n\nThe expected value of the derivative of the log-likelihood is zero, i.e.,\n\n\n$$E\\left[\\frac{d}{d\\theta} \\log L(\\theta; x)\\right] = 0$$\nMy Question: But mathematically, how can we show that if $E\\left[\\frac{d}{d\\theta} \\log L(\\theta; x)\\right] \\neq 0$ , then $$\\lim_{{n \\to \\infty}} P(|\\hat{\\theta}_n - \\theta| > \\epsilon) \\neq 0$$ ???\nCan someone please show me how this can be proven?\nThanks!\n",
    "proof": "Hi: Note that this is not an answer ( I needed the space ) but here is a proof that the maximum likelihood estimator is consistent\nUnfortunately, that does not prove that, if the estimator is not the MLE, then it is NOT consistent. So, this is close to a proof but it's not quite there.\nhttps://www2.econ.iastate.edu/classes/econ671/hallam/documents/Asymptotic_Dist.pdf\nI'm not clear on how to prove that the proof is an if and only if.\n#====================================================================\nADDENDUM: Later on I found this. It is a thread showing how the MLE convergence proof breaks down when you don't use the MLE in the various steps. So, with a lot of effort, you can probably use this to prove the other direction. I don't have time to go through the details and show it but Alecos' threads tend to be VERY CLEAR.\nhttps://stats.stackexchange.com/questions/127527/in-max-likelihood-the-expected-score-is-zero-for-the-true-values-is-it-also-tr\n#===================================================================\n11-01-2023: SECOND ADDENDUM TO ANSWER. THIS ONE REALLY IS AN ANSWER.\n#======================================================================\nstats_noob: I don't have time to write it all out but, re-reading the actual question you posed, it eventually hit me that it's not as hard as I suspected.\nSTEP 1) Take asymptotic_dist.pdf and read the first 3 sections  extremely carefully atleast upto and including equation 17).\nSTEP 2) Notice that 17) uses the fact that the expected value of the score ( which is the first derivative of the log likelihood ) = 0. !!!!\nSTEP 3) But, in the statement of the question, you state the expected value of the score does not equal zero. Therefore, the proof used in the \"Consistency of the MLE\" section, namely section 3, breaks down and we are done. QED.\nIf you don't follow above, let me know. But I think that answer is sufficient. To summarize: Statement in question causes plim of $\\beta_{0}$ in second order taylor expansion to not be zero.\nThis ruins the consistency proof so that the aforementioned MLE described in the question statement cannot be consistent.\n-\n",
    "tags": [
      "calculus",
      "probability",
      "limits",
      "statistics",
      "proof-writing"
    ],
    "score": 6,
    "answer_score": 1,
    "is_accepted": false,
    "question_id": 4792134,
    "answer_id": 4796299
  },
  {
    "theorem": "Proof verification: $\\lim_{x\\to 2} \\frac{\\sqrt{x^2 + 5} - 3}{x - 2} = \\frac23$",
    "context": "The question is as follows:\n\nProve that $\\displaystyle\\lim_{x\\to 2} \\dfrac{\\sqrt{x^2 + 5} - 3}{x - 2} = \\dfrac23$.\n\nMy proof is:\n\nFix $\\varepsilon > 0$.\n\n\nNote that $$\\begin{array}{rcl}\\left|\\dfrac{\\sqrt{x^2 + 5} - 3}{x - 2} - \\dfrac23\\right| &=& \\left|\\dfrac{x + 2}{\\sqrt{x^2 + 5} + 3} - \\dfrac23\\right|\\\\&=& \\dfrac13\\left|\\dfrac{3x - 2\\sqrt{x^2 + 5}}{3 + \\sqrt{x^2 + 5}}\\right|\\\\&=& \\dfrac13 \\left|\\dfrac{5x^2 - 20}{\\left(3 + \\sqrt{x^2 + 5}\\right)\\left(3x + 2\\sqrt{x^2 + 5}\\right)}\\right|\\\\&=& \\dfrac{5|x - 2|\\cdot|x + 2|}{3\\left|3 + \\sqrt{x^2 + 5}\\right|\\cdot \\left|3x + 2\\sqrt{x^2 + 5}\\right|}\\end{array}$$\n\n\nPick $\\delta = \\min\\left\\{1, \\dfrac{21\\varepsilon}5\\right\\}$. Suppose that $0 < |x - 2| < \\delta < 1$. Then $1 < x < 3$. Therefore, $|x + 2| < 5$, $\\left|3 + \\sqrt{x^2 + 5}\\right| > 3 + \\sqrt 6 > 5$, and $\\left|3x + 2 \\sqrt{x^2 + 5}\\right| > 3 + 2\\sqrt 6 > 7$.\n\n\nTherefore,\n$$\\left|\\dfrac{\\sqrt{x^2 + 5} - 3}{x - 2} - \\dfrac23\\right| < \\dfrac{5 \\cdot 5}{3 \\cdot 5 \\cdot 7} |x - 2| < \\dfrac5{21} \\delta < \\varepsilon$$\n\n\nHence, $\\displaystyle\\lim_{x\\to 2} \\dfrac{\\sqrt{x^2 + 5} - 3}{x - 2} =\\dfrac23$.\n\nIs my proof correct?\n",
    "proof": "Yes, you have the main idea correct.\nThough, there seems to be too much formalism without explaining in words, which may obscure your proof.\nOne key step you make is to rewrite the function as\n$$\n\\frac{\\sqrt{x^2+5}-3}{x-2}=\\frac{x^2+5-9}{(x-2)(\\sqrt{x^2+5}+3)} \n=\\frac{x+2}{\\sqrt{x^2+5}+3}\\;.\n$$\nAt this point, you can simply apply one of the limit laws and continuity of the functions to conclude that\n$$\n\\lim_{x\\to 2}\\frac{x+2}{\\sqrt{x^2+5}+3}=\\frac{4}{6}=\\frac23\\;.\n$$\nStudents usually confuse \"formal/rigorous proof\" with proofs written in terms of $\\epsilon$-$\\delta$.\nIf one does want to proceed with an $\\epsilon$-$\\delta$ proof, then one needs to estimate the quantity\n$$\n\\left|\\frac{x+2}{\\sqrt{x^2+5}+3}-\\frac23\\right|\\;\\tag{1}\n$$\nInstead of handling this on an ad hoc way, one can follow the idea of proving the quotient law for limits to estimate (1) as\n$$\n\\begin{align}\n\\left|\\frac{x+2}{\\sqrt{x^2+5}+3}-\\frac46\\right| \n=& \\frac{|6f(x)-4g(x)|}{|6g(x)|} \\\\\n=& \\frac{|6f(x)-6\\cdot 4+6\\cdot 4- 4g(x)|}{|6g(x)|}\\\\\n\\le&\\frac{6|f(x)-4|+4|g(x)-6|}{|6g(x)|}\\tag{2}\n\\end{align}\n$$\nwhere $f(x)=x+2$ and $g(x)=\\sqrt{x^2+5}+3$. It is then clear that one should use the following facts to get a desired $\\delta$:\n\n$f$ and $g$ are continuous at $x=2$;\n$g$ is bounded away from $0$ near $x=2$.\n\nTo spell out the formalism, one can take $\\delta=\\min(\\delta_1,\\delta_2,\\delta_3)$ such that\n\\begin{align}\n|f(x)-4|<\\epsilon\\qquad\\textrm{whenever } |x-2|<\\delta_1\\\\\n|g(x)-6|<\\epsilon\\qquad\\textrm{whenever } |x-2|<\\delta_2\\\\\n|g(x)|\\ge |g(x)-6+6|\\ge 6-|g(x)-6|\\ge 1\\qquad\\textrm{whenever } |x-2|<\\delta_3\\tag{3}\n\\end{align}\nCombining with (2) one has\n$$\n\\left|\\frac{x+2}{\\sqrt{x^2+5}+3}-\\frac46\\right| \\le\n\\frac{10\\epsilon}{6}\\;.\n$$\n",
    "tags": [
      "calculus",
      "analysis",
      "proof-writing",
      "solution-verification"
    ],
    "score": 6,
    "answer_score": 2,
    "is_accepted": true,
    "question_id": 4308750,
    "answer_id": 4459187
  },
  {
    "theorem": "How to rigorously prove that $|A|\\neq|A\\cup B|$, when $B\\not\\subseteq A$ and $A$ is finite.",
    "context": "By definition of cardinality, $|X|=|Y|$ if there exists a bijective function $f:X\\to Y$.\nI would like to rigorously prove that if $A$ is a finite set, then $|A|\\neq |A\\cup B|$ when $B\\not\\subseteq A$. However, this seems less trivial than anticipated.\nI must point out, the reason this is less trivial than anticipated is that I am working in a set theory in which I have to prove this before I define the natural numbers. In other words, I cannot simply claim that since $A$ is finite, there exists a bijection between it and some set $\\{0,\\dots,n\\}$, or claim that $A=\\{a_1,\\dots,a_n\\}$ (as is done in the answer to this question) where $n\\in\\mathbb{N_0}$, because the natural numbers are not defined yet. So, by finite, what I mean is that at the very least, we know that $|A|<|Z|$, where the countably infinite set $Z$ is defined by\n$$\\exists Z[\\emptyset,\\{\\emptyset\\}\\in Z\\wedge\\forall x(x\\in Z \\iff x\\subseteq Z\\wedge |x|\\neq |Z|)]$$\nIt seems natural to start with a proof by contradiction. So, on the contrary, let's suppose $|A|=|A\\cup B|$. Thus, there must exist some bijective function $g:A\\to A\\cup B$. This is where I immediately find myself stuck. To you and I, this is a trivial impossibility, as the range of $g$ is bigger than the domain. Hence, there must exist some $b\\in A\\cup B$ s.t. $\\nexists a\\in A(g(a)=b).$\nHowever, this does not seem rigorous to me. It seems I have just jumped to the conclusion and claimed that there is a contradiction in plain language because I am biased by knowing that it was true in the first place. I would like to know how to formulate this proof more concretely.\n\nHow do I prove this with as few logical leaps as possible, without reference to the natural numbers, and hopefully pertaining as closely as I can to first-order logic alone?\n\n\nEdit: As pointed out in the comments, I should specify what axioms can be assumed.\nAxioms: We can assume many of the axioms of ZF, such as Extensionality, Regularity, Specification, Pairing, Union, and Powerset.\nSets: We can also assume that the empty set, $\\emptyset$, exists, that the previously mentioned $Z$ exists, and that $A\\in Z$ (hence why it must be the case that $|A|<|Z|$ since $A\\in Z \\implies A\\subseteq Z\\wedge |A|\\neq|Z|$). Importantly, we can also assume that all sets are hereditary sets.\nCardinality: On top of the definition for cardinality equality, we can assume that $|X|\\leq|Y|$ if there exists an injective function from $X$ into $Y$. Let's say that $X$ is finite if it has cardinality equal to the cardinality of some subset of $Z$, but cardinality not equal to $|Z|$.\n",
    "proof": "Suppose there is some bijection $g : A \\cup B \\to A$.\nTake $b \\in B$ such that $b \\notin A$. Consider the sequence $s : Z \\to A \\cup B$ defined by $s_0 = b$ and $s_{succ(n)} = g(s_n)$.\nClaim: $s: Z \\to A \\cup B$ is injective.\nProof: We must show for all $n \\in Z$, for all $m \\in Z$, $s_n = s_m$ implies $n = m$. We proceed by induction on $n$.\nCase 1: $n = 0$. we must show that for all $m \\in Z$, $s_m = b$ implies $m = 0$. We proceed by induction on $m$.\nCase 1a: $m = 0$. Immediate.\nCase 1b: $m = succ(k)$. Then $b = s_{succ(k)} = g(s_k)$. Now $g(s_k) \\in A$, but $b \\notin A$. Contradiction.\nCase 2: $n = succ(k)$ and for all $m$, if $s_m = s_k$ then $m = k$. We must show for all $m \\in Z$, if $s_m = g(s_k)$ then $m = succ(k)$. We proceed by induction on $m$.\nCase 2a: $m = 0$. Then $b = g(s_k)$. Again, this cannot happen.\nCase 2b: $m = succ(j)$. Then $g(s_j) = g(s_k)$. Then $s_j = s_k$. Then $j = k$. Then $m = succ(k)$.\nThus, we see that $|Z| \\leq |A \\cup B| = |A|$.\nBut you have already stated by assumption that $|A| < |Z|$. Contradiction.\nNote: you might need Cantor-Schroder-Bernstein for the last bit. But the proof of that is fairly elementary.\nEdit: my answer was meant to work with OP's original definition of $Z$ as $\\{\\emptyset, \\{\\emptyset\\}, \\{\\{\\emptyset\\}\\}, ..., \\}$. However, OP has since attempted to redefine $Z$.\nThe problem, as @Troposphere has pointed out, is that OP's attempted definition of $Z$ may not necessarily uniquely define $Z$.\nI am not certain that this is actually the case, since @Troposphere's argument appears to rely on claim that $\\beth_\\omega$ is a regular cardinal. So I will further update this post when I have a good idea whether this is true.\nEdit 2: If there is a strongly inaccessible cardinal $\\lambda$, then $V_\\lambda$ satisfies the condition for $Z$. I do not know if any uncountable set satisfying the condition for $Z$ must be an inaccessible cardinal, but I have shown that any such cardinal must be regular (which precludes @Troposphere's attempt).\nSo this definition of $Z$ doesn't work (unless ZF proves there are no strongly inaccessible cardinals, which seems unlikely).\nWhy does $|Z|$ have to be regular? Let $\\alpha = |Z|$.\nNote that for all $a, b \\in Z$, we have $\\{a, b\\} \\subseteq Z$ and $|\\{a, b\\}| \\leq 2 < |Z|$ (since $\\emptyset, \\{\\emptyset\\}, \\{\\{\\emptyset\\}\\} \\in Z$). Therefore, $\\{a, b\\} \\in Z$.\nThus, if $a, b \\in Z$, we have $\\{a\\} = \\{a, a\\} \\in Z$ and $\\{a, b\\} \\in Z$. Therefore, $(a, b) = \\{\\{a\\}, \\{a, b\\}\\} \\in Z$.\nConsider the fact that for all ordinals $\\beta < \\alpha$, we have $\\beta \\in Z$. This can be proved by ordinal induction, since if $\\beta < \\alpha$ and for all $\\gamma \\in \\beta$, $\\gamma \\in Z$, then we have $|\\beta| < Z$ and $\\beta \\subseteq Z$; hence, $\\beta \\in Z$.\nNow suppose $\\beta < \\alpha$ is an ordinal. Consider the fact that a function $f : \\beta \\to Z$ is a set of cardinality $|\\beta| < \\alpha$, and consider that each element of $f$ is a pair $(\\gamma, z)$, where $z \\in Z$ and $\\gamma \\in \\beta \\in Z$. Therefore, each $f : \\beta \\to Z$ is an element of $Z$. So $Z^\\beta \\subseteq Z$ for all $\\beta < \\alpha$.\nConsider some $\\beta < \\alpha$ and some ordinals $C_\\gamma < \\alpha$ for $\\gamma < \\beta$. Let $\\lambda = \\sup\\limits_{\\gamma \\in \\beta} C_\\gamma < \\alpha$. I claim that $\\lambda < \\alpha$.\nFor consider functions $\\lambda \\to Z$. Let us define $f : (\\lambda \\to Z) \\to Z$ as follows:\nFor each $g : \\lambda \\to Z$, for each $\\gamma \\in \\beta$, we define $f_g(\\gamma) = g|_{C_\\gamma} : C_\\gamma \\to Z$. Since we know that $(C_\\gamma \\to Z) \\subseteq Z$, we see that $f_g(\\gamma) \\in Z$ for all $\\gamma$. Hence, $f_g : \\beta \\to Z$, so $f_g \\in Z$ for all $g$.\nAnd we can recover $g$ from $f_g$, since $\\bigcup\\limits_{\\gamma \\in \\beta} C_\\gamma = \\lambda$ and we can recover any function from its restriction to a covering. Therefore, $f$ is injective. Then we see that $\\lambda < 2^\\lambda \\leq |Z^\\lambda| \\leq |Z| = \\alpha$. So $\\lambda < \\alpha$, as required.\nTherefore, $\\alpha$ must be a regular cardinal.\nI do not see how to extend this to a proof that $|Z|$ must be a strong limit cardinal. But I suspect that is the case.\n",
    "tags": [
      "proof-writing",
      "set-theory",
      "cardinals"
    ],
    "score": 6,
    "answer_score": 2,
    "is_accepted": false,
    "question_id": 4228501,
    "answer_id": 4228532
  },
  {
    "theorem": "Circular reasoning in proving $\\lim_{x\\to a}(\\sin x) = \\sin a$",
    "context": "I just started learning about epsilon-delta limit proofs, and I want to know how to prove using the epsilon-delta definition of a limit that $\\lim_{x\\to a}(\\sin x) = \\sin a$\nI tried and failed, so I looked it up online and found the trick is to use the identity $\\sin x < x$. I cannot find any proofs that do not use this identity.\nI had never seen this identity before, so I searched for its proof and found this proof that uses the mean value theorem. Again, I haven't yet learnt the mean value theorem, but according to the website, it requires a continuous (and differentiable) function $f$.\nBut the concept of continuity is defined using the epsilon-delta limit definition! In fact, the fact that $\\sin(x)$ is continuous is exactly the statement that I'm trying to prove above: $\\lim_{x\\to a}(\\sin x) = \\sin a$\nThis is clearly circular reasoning. My question is how does one escape it? Either there must be a way to prove $\\lim_{x\\to a}(\\sin x) = \\sin a$ without the identity $\\sin x < x$, or we need to prove $\\sin x < x$ without the fact that sin is continuous. Or I suppose there could be a 3rd option? I can't find any answers on how to do it, which I find most strange...\n",
    "proof": "A very analytic approach is to start from integrals and define $\\log, \\exp, \\sin$ and show that these are smooth, and therefore continuous, on their domains.\nFirst we define the natural logarithm by\n$$\n\\ln x := \\int_1^x \\frac{dt}{t}\n$$\nIt's easy to show the logarithm laws using this definition and integration rules, and that $\\ln$ is differentiable.\nThen we define the exponential function as its inverse,\n$$\n\\exp := \\ln^{-1}\n$$\nBy the inverse function theorem, $\\exp$ is differentiable and thus continuous.\nThe Maclaurin/Laurent series of $\\exp$ has infinite radius of convergence so $\\exp$ can be extended from $\\mathbb{R}$ to a smooth function on all of $\\mathbb{C}.$ We can therefore define the function $\\sin$ by\n$$\n\\sin x := \\frac{\\exp(ix)-\\exp(-ix)}{2i}\n$$\nwhich will also be smooth and thus continuous.\n",
    "tags": [
      "trigonometry",
      "proof-writing",
      "alternative-proof",
      "epsilon-delta",
      "fake-proofs"
    ],
    "score": 6,
    "answer_score": 1,
    "is_accepted": true,
    "question_id": 3795202,
    "answer_id": 3795284
  },
  {
    "theorem": "Fubini&#39;s theorem for integrable functions.",
    "context": "I have gone through the proof of Fubini's theorem for non-negative measurable functions from the book An Introduction to Measure and Integration by Inder K Rana. The satement of the theorem is as follows $:$\nTheorem $1$ $:$ Let $(X \\times Y, \\mathcal A \\otimes \\mathcal B, \\mu \\times \\nu)$ be the product measure space induced by the $\\sigma$-finite measure spaces $(X,\\mathcal A, \\mu)$ and $(Y,\\mathcal B, \\nu).$ Then for any non-negative $\\mathcal A \\otimes \\mathcal B$- measurable function $f,$ the following staements hold $:$\n$($i$)$ For any $x_0 \\in X,y_0 \\in Y$ the maps $x \\longmapsto f(x,y_0)$ and $y \\longmapsto f(x_0,y)$ are $\\mathcal A$-measurable and $\\mathcal B$-measurable respectively.\n$($ii$)$ The map $x \\longmapsto \\displaystyle {\\int_{Y}} f(x,y)\\ d\\nu(y)$ is $\\mathcal A$-measurable and the map $y \\longmapsto \\displaystyle {\\int_{X}} f(x,y)\\ d\\mu(x)$ is $\\mathcal B$-measurable.\n$($iii$)$ $\\displaystyle {\\int_{X}} \\left ( \\displaystyle {\\int_{Y}} f(x,y)\\ d\\nu(y) \\right ) d\\mu(x) = \\displaystyle {\\int_{Y}} \\left ( \\displaystyle {\\int_{X}} f(x,y)\\ d\\mu(x) \\right ) d\\nu(y) = \\displaystyle {\\int_{X \\times Y}} f(x,y)\\ d(\\mu \\times \\nu) (x,y).$\nThe general version of the above theorem states as follows $:$\nTheorem $2$ $:$ Let $(X \\times Y, \\mathcal A \\otimes \\mathcal B, \\mu \\times \\nu)$ be the product measure space induced by the $\\sigma$-finite measure spaces $(X,\\mathcal A, \\mu)$ and $(Y,\\mathcal B, \\nu).$ Then for any $f \\in L_1 (\\mu \\times \\nu),$ the following staements hold $:$\n$($i$)$ The maps $x \\longmapsto f(x,y)$ and $y \\longmapsto f(x,y)$ are $\\mu$-integrable a.e. $y(\\nu)$ and $\\nu$-integrable a.e. $x(\\mu)$ respectively.\n$($ii$)$ The map $x \\longmapsto \\displaystyle {\\int_{Y}} f(x,y)\\ d\\nu(y)$ is $\\mu$-integrable a.e. $x(\\mu)$ and the map $y \\longmapsto \\displaystyle {\\int_{X}} f(x,y)\\ d\\mu(x)$ is $\\nu$-integrable a.e. $y(\\nu).$\n$($iii$)$ $\\displaystyle {\\int_{X}} \\left ( \\displaystyle {\\int_{Y}} f(x,y)\\ d\\nu(y) \\right ) d\\mu(x) = \\displaystyle {\\int_{Y}} \\left ( \\displaystyle {\\int_{X}} f(x,y)\\ d\\mu(x) \\right ) d\\nu(y) = \\displaystyle {\\int_{X \\times Y}} f(x,y)\\ d(\\mu \\times \\nu) (x,y).$\nI tried to prove the above theorem with the help of Theorem $1.$ Here's what I did $:$\nMy attempt $:$ Let $f^+$ and $f^-$ be the positive and the negative part of the function $f$ respectively. Since $f \\in L_1(\\mu \\times \\nu),$ $f^+$ and $f^-$ are both non-negative $\\mathcal A \\otimes \\mathcal B$-measurable functions. Applying Theorem $1$ $($iii$)$ to $f^+$ and $f^{-}$ we have\n\\begin{align*}\\displaystyle {\\int_{X}} \\left ( \\displaystyle {\\int_{Y}} f^+(x,y)\\ d\\nu(y) \\right ) d\\mu(x) = \\displaystyle {\\int_{Y}} \\left ( \\displaystyle {\\int_{X}} f^+(x,y)\\ d\\mu(x) \\right ) d\\nu(y) & = \\displaystyle {\\int_{X \\times Y}} f^+(x,y)\\ d(\\mu \\times \\nu) (x,y) \\\\ & \\leq \\displaystyle {\\int_{X \\times Y}} |f(x,y)|\\ d(\\mu \\times \\nu) < +\\infty. \\end{align*}\n\\begin{align*}\\displaystyle {\\int_{X}} \\left ( \\displaystyle {\\int_{Y}} f^-(x,y)\\ d\\nu(y) \\right ) d\\mu(x) = \\displaystyle {\\int_{Y}} \\left ( \\displaystyle {\\int_{X}} f^-(x,y)\\ d\\mu(x) \\right ) d\\nu(y) & = \\displaystyle {\\int_{X \\times Y}} f^-(x,y)\\ d(\\mu \\times \\nu) (x,y) \\\\ & \\leq \\displaystyle {\\int_{X \\times Y}} |f(x,y)|\\ d(\\mu \\times \\nu) < +\\infty. \\end{align*}\nThis shows that the map $x \\longmapsto \\displaystyle {\\int_Y} f^+(x,y)\\ d\\nu(y)$ is $\\mu$-integrable, the map $y \\longmapsto \\displaystyle {\\int_X} f^+(x,y)\\ d\\mu(x)$ is $\\nu$-integrable, the map  $x \\longmapsto \\displaystyle {\\int_Y} f^-(x,y)\\ d\\nu(y)$ is $\\mu$-integrable and the map $y \\longmapsto \\displaystyle {\\int_X} f^-(x,y)\\ d\\mu(x)$ is $\\nu$-integrable.\nSo the map $y \\longmapsto f^+(x,y)$ is $\\nu$-integrable a.e. $x(\\mu)$ and the map $y \\longmapsto f^-(x,y)$ is $\\nu$-integrable a.e. $x(\\mu).$ Hence $y \\longmapsto f(x,y)$ is $\\nu$-integrable a.e. $x(\\mu).$\nSimilarly, the map $x \\longmapsto f^+(x,y)$ is $\\mu$-integrable a.e. $y(\\nu)$ and the map $x \\longmapsto f^-(x,y)$ is $\\mu$-integrable a.e. $y(\\nu).$ Hence $x \\longmapsto f(x,y)$ is $\\mu$-integrable a.e. $y(\\nu).$ This proves $($i$).$\nSince $f \\in L_1(\\mu \\times \\nu)$ it follows that \\begin{align*} \\int_{X \\times Y} f(x,y)\\ d(\\mu \\times \\nu) (x,y) & = \\int_{X \\times Y} f^+(x,y)\\ d(\\mu \\times \\nu) (x,y) - \\int_{X \\times Y} f^-(x,y)\\ d(\\mu \\times \\nu) (x,y) \\\\ & = \\int_X \\left ( \\int_{Y} f^+(x,y)\\ d{\\nu(y)} \\right ) d{\\mu}(x) - \\int_X \\left ( \\int_{Y} f^-(x,y)\\ d{\\nu(y)} \\right ) d{\\mu}(x) \\end{align*}\nNow how do I proceed? Any help will be highly appreciated.\nThanks in advance.\n",
    "proof": "The assertion of Fubini's theorem for any integrable function what has been made in the book An Introduction to Measure and Integration by Inder K Rana is not correct. It should be the following $:$\n\nTheorem (Fubini) $:$ Let $(X, \\mathcal A, \\mu)$ and $(Y,\\mathcal B, \\nu)$ be two complete $\\sigma$-finite measure spaces. Let $(X \\times Y,\\mathcal A \\otimes \\mathcal B,\\mu \\times \\nu)$ be the product measure space induced by $(X,\\mathcal A, \\mu)$ and $(Y,\\mathcal B, \\nu).$ Let $f \\in L_1(\\mu \\times \\nu).$ Then there exist $g \\in L_1(\\mu)$ and $h \\in L_1(\\nu)$ such that $$\\int_{X \\times Y} f\\ d(\\mu \\times \\nu) = \\int_X g\\ d\\mu = \\int_Y h\\ d\\nu.$$\n\nLet us begin the proof from the last equality what I obtained i.e. \\begin{align*} \\int_{X \\times Y} f(x,y)\\ d(\\mu \\times \\nu) (x,y) & = \\int_X \\left ( \\int_{Y} f^+(x,y)\\ d{\\nu(y)} \\right ) d{\\mu}(x) - \\int_X \\left ( \\int_{Y} f^-(x,y)\\ d{\\nu(y)} \\right ) d{\\mu}(x)\\ \\ \\ \\ {\\label \\equation (1)}\\end{align*}\nLet \\begin{align*} E : & = \\left \\{x \\in X\\ \\bigg |\\ \\int_Y f^+(x,y)\\ d\\nu(y) < +\\infty \\right \\} \\\\  F : & = \\left \\{x \\in X\\ \\bigg |\\ \\int_Y f^-(x,y)\\ d\\nu(y) < +\\infty \\right \\} \\end{align*} Since the maps $x \\longmapsto \\displaystyle {\\int_Y} f^+(x,y)\\ d\\nu(y)$ and $x \\longmapsto \\displaystyle {\\int_Y} f^-(x,y)\\ d\\nu(y)$ are both $\\mu$-integrable it follows that $\\mu (E^c) = \\mu(F^c) = 0.$ Define a function $g^+ : X \\longrightarrow \\Bbb R$ defined by $$g^+(x) = \\left ( \\displaystyle {\\int_Y} f^+(x,y)\\ d\\nu(y) \\right ) \\chi_E (x),\\ x \\in X$$ and a function $g^- : X \\longrightarrow \\Bbb R$ defined by $$g^-(x) = \\left ( \\displaystyle {\\int_Y} f^-(x,y)\\ d\\nu(y) \\right ) \\chi_F (x),\\ x \\in X$$ Then clearly $g^+(x),g^-(x) < +\\infty,\\ $ for all $x \\in X.$ Moreover \\begin{align*} g^+(x) & = \\displaystyle {\\int_Y} f^+(x,y)\\ d\\nu(y) ,\\ \\text{for a.e.}\\ x(\\mu) \\\\ g^-(x) & = \\displaystyle {\\int_Y} f^-(x,y)\\ d\\nu(y) ,\\ \\text{for a.e.}\\ x(\\mu) \\end{align*} Let $g : = g^+ - g^-.$  Since the maps $x \\longmapsto \\displaystyle {\\int_Y} f^+(x,y)\\ d\\nu(y)$ and $x \\longmapsto \\displaystyle {\\int_Y} f^-(x,y)\\ d\\nu(y)$ are both $\\mu$-integrable and $(X,\\mathcal A,\\mu)$ is a complete measure space it follows that $g^+,g^-,g \\in L_1(\\mu)$ and we have the following equality \\begin{align*}  \\int_X g^+\\ d\\mu  & = \\int_X \\left (\\int_Y f^+(x,y)\\ d\\nu(y) \\right ) d\\mu(x) \\\\ \\int_X g^-\\ d\\mu  & = \\int_X \\left (\\int_Y f^-(x,y)\\ d\\nu(y) \\right ) d\\mu(x) \\\\ \\int_X g\\ d\\mu  & = \\int_X g^+\\ d\\mu - \\int_X g^-\\ d\\mu \\end{align*} From the above three equalities it follows that $$\\int_X \\left (\\int_Y f^+(x,y)\\ d\\nu(y) \\right ) d\\mu(x) - \\int_X \\left (\\int_Y f^-(x,y)\\ d\\nu(y) \\right ) d\\mu(x) = \\int_X g\\ d\\mu.$$\nNow from $(1)$ it follows that $$\\int_{X \\times Y} f\\ d(\\mu \\times \\nu) = \\int_X g\\ d\\mu.$$\nSimilarly by observing that \\begin{align*} \\int_{X \\times Y} f(x,y)\\ d(\\mu \\times \\nu) (x,y) & = \\int_Y \\left ( \\int_{X} f^+(x,y)\\ d{\\mu(x)} \\right ) d{\\nu}(y) - \\int_Y \\left ( \\int_{X} f^-(x,y)\\ d{\\mu(x)} \\right ) d{\\nu}(y) \\end{align*} and by exploiting the completeness of the measure space $(Y,\\mathcal B,\\nu)$ we can find out $h \\in L_1(\\nu)$ such that $$\\int_{X \\times Y} f\\ d(\\mu \\times \\nu) = \\int_Y h\\ d\\nu.$$\nThis completes the proof.\nQED\n",
    "tags": [
      "integration",
      "measure-theory",
      "proof-writing",
      "product-space",
      "fubini-tonelli-theorems"
    ],
    "score": 6,
    "answer_score": 1,
    "is_accepted": true,
    "question_id": 3773134,
    "answer_id": 3774223
  },
  {
    "theorem": "Inequality assurance",
    "context": "Suppose we have $$0<a_{1},a_{2},b_{1},b_{2},c_{1},c_{2},d_{1},d_{2}<1$$ and we know that $$\\frac{a_{1}}{b_{1}} > \\frac{c_{1}}{d_{1}},$$\n$$\\frac{a_{2}}{b_{2}} > \\frac{c_{2}}{d_{2}}$$\nWhat else we need to prove in order to be sure that:\n$$\\frac{a_{1}+a_{2}}{b_{1}+b_{2}} > \\frac{c_{1}+c_{2}}{d_{1}+d_{2}}?$$\nThis is just an example. The number of variables might be more albeit obeying the same conditions as mentioned. \nIn general, I want to prove that:\n$$\\frac{\\sum_{i=1}^{n}a_{i}}{\\sum_{i=1}^{n}b_{i}} > \\frac{\\sum_{i=1}^{n}c_{i}}{\\sum_{i=1}^{n}d_{i}}?$$\nwhere $$\\frac{a_{i}}{b_{i}} > \\frac{c_{i}}{d_{i}},$$\n$$\\forall 1 \\le i \\le n, \\;\\;\\; n \\in \\mathbb{N}^{+}$$\n\nEDIT1 (bounty will be awarded for this part)\nAlternatively, we can state the problem as follows: \nThe (least) necessary conditions we need in order to guarantee that:\n$$\\dfrac{\\sum_{i=1}^{n} \\alpha_{i} x_{i}}{\\sum_{i=1}^{n}x_{i}}\n>\n\\dfrac{\\sum_{i=1}^{n} \\beta_{i} y_{i}}{\\sum_{i=1}^{n}y_{i}},$$\nwhere we know that:\n$$\\alpha_{i} > \\beta_{i} > 1,$$\n$$\\bigg( \\sum_{i=1}^{n}x_{i} + \\sum_{i=1}^{n}y_{i} \\bigg) \\in (0,1).$$\n\nEDIT2 (as for the answer to which the bounty is awarded)\nAlthough the condition suggested by @quasi is a very strong one, I awarded him/er the bounty since his/er answer was the cleanest. My problem remains unsolved, though!\n",
    "proof": "If the variables $a_1,b_1,c_1,d_1,a_2,b_2,c_2,d_2$ are scaled by a positive factor, the values of the fractions being compared will still be the same, hence the condition\n$$0 < a_1,b_1,c_1,d_1,a_2,b_2,c_2,d_2 < 1$$\ncan be replaced by the simpler condition\n$$a_1,b_1,c_1,d_1,a_2,b_2,c_2,d_2 > 0$$\nTo see that the conditions\n$$a_1,b_1,c_1,d_1,a_2,b_2,c_2,d_2 > 0$$\n$$\\frac{a_{1}}{b_{1}} > \\frac{c_{1}}{d_{1}},$$\n$$\\frac{a_{2}}{b_{2}} > \\frac{c_{2}}{d_{2}}$$\nare not sufficient to force\n$$\\frac{a_{1}+a_{2}}{b_{1}+b_{2}} > \\frac{c_{1}+c_{2}}{d_{1}+d_{2}}$$\nlet $a_1,b_1,c_1,d_1,a_2,b_2,c_2,d_2$ be given by\n$$\na_1 = 1,\\;\\; b_1 = 2,\\;\\; c_1 = 3,\\;\\;d_1 = 7,\\;\\; \na_2 = 1,\\;\\; b_2 = 5,\\;\\; c_2 = 1,\\;\\;d_2 = 6\n$$\nThen\n$$\\frac{a_1}{b_1} = \\frac{1}{2} > \\frac{3}{7} = \\frac{c_1}{d_1}$$ \n$$\\frac{a_2}{b_2} = \\frac{1}{5} > \\frac{1}{6} = \\frac{c_2}{d_2}$$\n$$\\text{but}$$\n$$\n\\frac{a_1 + a_2}{b_1 + b_2} \n= \\frac{1 + 1}{2 + 5} \n= \\frac{2}{7}\n< \\frac{4}{13}\n= \\frac{3 + 1}{7 + 6}\n= \\frac{c_1 + c_2}{d_1 + d_2} \n$$\n\nHowever, for positive real numbers $x_1,...,x_n$ and $y_1,...,y_n$, we always have\n$$\n\\min\\left\\{\\frac{x_i}{y_i}\\right\\}\n\\le \\frac{\\sum_{i=1}^{n}x_{i}}{\\sum_{i=1}^{n}y_{i}}\n\\le \\max \\left\\{\\frac{x_i}{y_i}\\right\\}\n$$\nTo justify the above claim, let\n$$m = \\min\\left\\{\\frac{x_i}{y_i}\\right\\}$$\n$$M = \\max\\left\\{\\frac{x_i}{y_i}\\right\\}$$\nThen\n$$m \n\\;\\;\\le\\;\\; \n\\frac{x_i}{y_i}\n\\;\\;\\le\\;\\;\nM\\;\\;\\text{ for all }i$$\n$$\n\\implies\\;\nmy_i \n\\;\\;\\le\\;\\; \nx_i \n\\;\\;\\le\\;\\; \nMy_i\n\\;\\;\\text{ for all }i\n\\qquad\\;\\;\\;\\;\n$$\n$$\n\\implies\\; m\\sum_{i=1}^{n}y_i \n \\;\\;\\le\\;\\;\n\\sum_{i=1}^{n}x_i \n \\;\\;\\le\\;\\;\nM\\sum_{i=1}^{n}y_i\n\\qquad\\qquad\\qquad\n$$\n$$\n\\implies\\; m  \\;\\;\\le\\;\\;  \\frac{\\sum_{i=1}^{n}x_{i}}{\\sum_{i=1}^{n}y_{i}}  \\;\\;\\le\\;\\;  M\n\\qquad\\qquad\\qquad\n$$\n$$\n\\implies\\; \n\\min\\left\\{\\frac{x_i}{y_i}\\right\\}\n\\;\\;\\le\\;\\;\n\\frac{\\sum_{i=1}^{n}x_{i}}{\\sum_{i=1}^{n}y_{i}}\n\\;\\;\\le\\;\\;\n\\max \\left\\{\\frac{x_i}{y_i}\\right\\}\n\\qquad\\qquad\\qquad\n$$\nas claimed.\n\nThus, for the question at hand, if \n$a_1,...,a_n$ and $b_1,...,b_n$ \nare positive real numbers, the condition\n$$\\min\\left\\{\\frac{a_i}{b_i}\\right\\} > \\max\\left\\{\\frac{c_i}{d_i}\\right\\}$$\nwould be sufficient$\\,-\\,$even without the other conditions, to force \n$$\\frac{\\sum_{i=1}^{n}a_{i}}{\\sum_{i=1}^{n}b_{i}} > \\frac{\\sum_{i=1}^{n}c_{i}}{\\sum_{i=1}^{n}d_{i}}$$\nsince in that case,\n$$\n\\frac{\\sum_{i=1}^{n}a_{i}}{\\sum_{i=1}^{n}b_{i}}\n\\ge\n\\min\\left\\{\\frac{a_i}{b_i}\\right\\} > \\max\\left\\{\\frac{c_i}{d_i}\\right\\}\n\\ge\n\\frac{\\sum_{i=1}^{n}c_{i}}{\\sum_{i=1}^{n}d_{i}}\n$$\n",
    "tags": [
      "inequality",
      "proof-verification",
      "summation",
      "proof-writing"
    ],
    "score": 6,
    "answer_score": 2,
    "is_accepted": false,
    "question_id": 2184527,
    "answer_id": 2184553
  },
  {
    "theorem": "Show that Lipschitz $\\|\\nabla f(x) - \\nabla f(y)\\| \\leq L\\|x - y\\|$ is implied by $f(y) \\leq f(x) + \\nabla f(x)^T(y-x) + \\dfrac{L}{2}\\|y-x\\|^2$",
    "context": "Pg 12 - 14 http://www.seas.ucla.edu/~vandenbe/236C/lectures/gradient.pdf\nDef: A $C^1$ convex function $f$ is Lipschitz smooth if $\\exists L > 0$ s.t. $\\forall x, y\\in \\mathbb{R}^n$\n    \\begin{equation}\n\t\t\\|\\nabla f(x) - \\nabla f(y)\\| \\leq L\\|x - y\\|\n\t\\end{equation}\n\nClaim: A $C^1$ convex function $f$ that satisfies $$f(y) \\leq f(x) + \\nabla f(x)^T(y-x) + \\dfrac{L}{2}\\|y-x\\|^2$$ is Lipschitz Smooth\n\n(Note: 1. the reverse implication is referred to as the \"quadratic upper bound property\" 2. One poster suggested to use fenchel duality to show this Lipschitz Smoothness, Strong Convexity and the Hessian)\nProof attempt:\nIt seems that the direct approach is through re-arrange and combine, which gives: \n$$0 \\leq (\\nabla f(x)-\\nabla f(y))^T(y-x) +  L\\|y-x\\|^2$$\n$$(\\nabla f(y)-\\nabla f(x))^T(y-x) \\leq    L\\|y-x\\|^2$$\nUsing CS-inequality on the above $(\\nabla f(y)-\\nabla f(x))^T(y-x) \\leq    L\\|y-x\\|^2$ gives:\n$$(\\nabla f(y)-\\nabla f(x))^T(y-x) \\leq    \\|\\nabla f(y)-\\nabla f(x)\\|\\|y-x\\|$$\nNow I have:\n\n$$(\\nabla f(y)-\\nabla f(x))^T(y-x) \\leq    L\\|y-x\\|^2$$\n$$(\\nabla f(y)-\\nabla f(x))^T(y-x) \\leq    \\|\\nabla f(y)-\\nabla\n    f(x)\\|\\|y-x\\|$$\n\nHow do I conclude $\\|\\nabla f(x) - \\nabla f(y)\\| \\leq L\\|x - y\\|$?\n\n",
    "proof": "Co-coercivity of the gradient (Lecture 1, slides 15-16 in Vandenberghe's 236c notes) tells us that\n$$\n\\frac{1}{L} \\| \\nabla f(y) - \\nabla f(x) \\|^2 \\leq \n(\\nabla f(y) - \\nabla f(x))^T (y - x)\n$$\nfor all $x,y$.\nNow combine this with your equation 1) to conclude that\n$$ \n\\| \\nabla f(y) - \\nabla f(x) \\| \\leq L \\| y - x \\|\n$$\nfor all $x,y$.\n",
    "tags": [
      "multivariable-calculus",
      "inequality",
      "proof-writing",
      "convex-analysis",
      "convex-optimization"
    ],
    "score": 6,
    "answer_score": 2,
    "is_accepted": true,
    "question_id": 2020979,
    "answer_id": 2021718
  },
  {
    "theorem": "Complete Graphs as Unions of Paths",
    "context": "Show that for $n \\geq 2$ the complete graph $K_n$ is the union of paths of distinct lengths. \nI have been stuck on this problem for the past couple of days now and would really like to see a solution/proof.\nWhat I have tried so far is the following:\nWe know that the size of the set of edges $E(K_n)$ is $|E(k_n)| = {n \\choose 2} = \\frac{n!}{2!(n-2)!} = \\frac{n(n-1)}{2} = \\displaystyle \\sum_{i=1}^{n-1} i$. \nFrom here, I considered $K_{n+1}$ and the respective size of the edge set which came out to $\\frac{n(n+1)}{2}$. If I understand correctly, then we need to somehow choose a partition of $K_n$, so maybe separating the vertex set into two different sets might help, but I am not even sure if this is the right way to go about it.\nMany thanks in advance for your time. Any help is greatly appreciated. \n",
    "proof": "If $n=2k+1$, the graph is the disjoint union of $k$ Hamiltonian-cycles and from there you know what to do.\nIf $n=2k$, use the previous construction for $n-1$ with the additional constraint that exactly two paths start at each vertex but one, and then extend each path with an extra edge.\n",
    "tags": [
      "graph-theory",
      "proof-writing",
      "induction"
    ],
    "score": 6,
    "answer_score": 2,
    "is_accepted": true,
    "question_id": 1660244,
    "answer_id": 2244073
  },
  {
    "theorem": "Proof by counter example of optimal solution for Coin Changing problem (no nickels)",
    "context": "I'm a tutoring a student whose working on the classical coin changing problem. For those who are unfamiliar with problem or the greedy algorithm used for it. The goal is find the fewest number coins required to give $x$ change using quarters ($25¢$), dimes ($10¢$), nickels ($5¢$), and pennies ($1¢$). The greedy algorithm basically says pick the largest coin available. I know that the greedy approach is optimal as long as you have all the coins available for example: Find change for $16¢$.\nOptimal solution: $1$ dime, $1$ nickel and $1$ penny $(10 + 5 + 1)$. Three total coins.\nHowever, if you no longer have nickels available to choose. The greedy algorithm does not hold for every case. For example: find change for $40¢$. The greedy algorithm says to pick $1$ quarter, $1$ dime, and $5$ pennies $(25 + 10 + 1 + 1 + 1 + 1 + 1)$. Seven coins total. A more optimal solution is to pick $4$ dimes instead $(10 + 10 + 10 + 10)$. Four coins total.\nHe's suppose to prove that what is the most number of pennies an optimal solution can have and what is the most number of dimes that an optimal solution can have? \nWe both agree that without nickels available you must use pennies as long as $x < 9$ (assuming $x$ is change left). If $x \\geq 10$ then it's better to pick a dime instead.\nAnd in the case of dimes, the most dimes you can have is $4$ as long as $x < 50$. When $x \\geq 50$ a more optimal solution is to use at least two quarters.\nThe problem I'm having is coming up with a solid proof for this. I can explain it in words but am unable to come up with a mathematical reasoning. I'm thinking proof by contradiction, but I don't know how to write one for this scenario. Any help would be appreciated. \n",
    "proof": "If you want a general solution, check the papers \"Canonical Coin Systems for Change-Making Problem\", by Xuan Cai (arXiv:0809.0400v2), \"Combinatorics of the Change-Making Problem\", by A. Niewiarowska and M. Adamaszek (arXiv:0801.01.20v2), and \"What This Country Needs is an 18c Piece\", by Jeffrey Shallit. \n",
    "tags": [
      "algorithms",
      "proof-writing",
      "induction"
    ],
    "score": 6,
    "answer_score": 2,
    "is_accepted": false,
    "question_id": 972861,
    "answer_id": 972975
  },
  {
    "theorem": "Proofs as games?",
    "context": "A long time ago (but I can't remember when), I was introduced to the (pedagogical) concept of writing a proof as giving a winning strategy for a game.  Basically, given a statement $\\forall x\\exists y \\forall z\\exists w P(x,y,z,w)$ (or similar), there is a corresponding game, where one player is $\\forall$ (sometimes pronounced $\\forall$belard or $\\forall$dversary), the other player is $\\exists$ (sometimes pronounced $\\exists$loise, which is apparently hilarious if you know French).\nAnyway, the objective of $\\forall$ is to make $P$ false; the objective of $\\exists$ is to make $P$ true.  They take turns playing (turn order defined by quantifier order) and the statement is true if and only if $\\exists$ has a winning strategy.\nOf course you'd have to prove this makes sense, but the above can be made a theorem and proven.  However, I'm teaching a course on proof, and can't for the life of me find a citation for any of this for my students to read.  Not the applications of this into model theory, etc., but just the explanation of this as a metaphor.\nIs this familiar to anyone?  And can you remember where you saw it before?  I'm trying to find something tangible I can give to my students.\n",
    "proof": "Wikipedia has an article about this although it isn't very good, but it does have many other references you could consult. \nThe idea is crucial to understanding why the strategies of so many games are PSPACE-complete. This is because the central PSPACE-complete problem, analogous to SAT for NP-complete problems, is the satisfiability problem for quantified boolean formulas, and asserting that $P_1$ wins some game can be understood as claiming that there exists a $P_1$ move such that for all $P_2$ moves there exists a $P_1$ move… etc.\n",
    "tags": [
      "reference-request",
      "proof-writing",
      "education"
    ],
    "score": 6,
    "answer_score": 1,
    "is_accepted": false,
    "question_id": 501848,
    "answer_id": 501878
  },
  {
    "theorem": "Flaw in induction proof that the Fibonacci sequence is bounded by $(5/3)^n$",
    "context": "\nThe Fibonacci sequence is defined by $a_1 = 1, a_2 = 1$ and for all $n \\ge 2, a_{n+1} = a_n + a_{n-1}$. Thus the sequence begins\n$$1,1,2,3,5,8,13,21,...$$\nProve that for all $n \\ge 1, a_n < (5/3)^n$\n\nHere is what I tried. But I am not sure what is wrong with it.\nBase case, $n = 1$:\n$$a_1 = 1 > \\frac{5}{3}$$\nInductive step:\nAssume that $a_n < (\\frac{5}{3})^n$ for all $1 \\le k \\le n$ as the inductive hypothesis.\n$$a_{n+1} = a_n + a_{n-1}$$\n$$a_{n+1} < \\left(\\frac{5}{3}\\right)^n + \\left(\\frac{5}{3}\\right)^{n-1}$$\n$$a_{n+1} < \\left(\\frac{5}{3}\\right)^{n-1} \\cdot \\left(\\frac{5}{3}+1\\right)$$\n$$a_{n+1} < \\left(\\frac{5}{3}\\right)^{n-1} \\cdot \\left(\\frac{8}{3}\\right)$$\n$$\\left(\\frac{8}{3}\\right) < \\left(\\frac{5}{2}\\right)^2 = \\frac{25}{9}$$\n$$a_{n+1} < \\left(\\frac{5}{3}\\right)^{n-1} \\cdot \\left(\\frac{5}{3}\\right)^2$$\n$$a_{n+1} < \\left(\\frac{5}{3}\\right)^{n+1}$$\n",
    "proof": "A couple typos, and suggestion to consider; but substantively, there is very little \"wrong\" with your proof. You did the hardest part, and the proof is fine (with typos corrected!)\n\nAs pointed out in the comments, you want the direction of the inequality for the base case $a_1$ reversed. (Typo?). \nWith this particular proof, I'd suggest also considering the second\nbase case: establishing: $$a_2 = 1 \\lt \\left(\\frac 53\\right)^2.$$\nWhat I suspect is another typo, you typed $\\left(\\frac 52\\right)^2$\nbut want $\\left(\\frac 53\\right)^2$ in the third from the last line.\nFinally, and more importantly, [here's the suggestion I am asking you\nto consider]: don't be afraid to use more words in a proof: Explain\nwhat you are doing. You need to make clear the relationships between\nyour lines of reasoning: Line (i) $\\iff$ Line (j)? Or does one given\ninequality imply the subsequent inequality? If so, indicate so, in\nwords or in symbols: e.g., using: \"$\\implies.$\" \n\n",
    "tags": [
      "inequality",
      "proof-verification",
      "proof-writing",
      "induction",
      "fibonacci-numbers"
    ],
    "score": 6,
    "answer_score": 2,
    "is_accepted": false,
    "question_id": 432625,
    "answer_id": 432649
  },
  {
    "theorem": "Prove that if $\\lim_{x\\to \\infty} f&#39;(x) = 0$, then $\\lim_{x\\to \\infty} f(x+1)-f(x) = 0$",
    "context": "I'm working on this proof and I think I have a sketch but I'm not sure it's rigorous enough.\nSuppose $f:\\Bbb R \\to \\Bbb R$ is differentiable and that$$\\lim_{x\\to \\infty} f'(x) = 0$$ Prove that $$\\lim_{x\\to \\infty} f(x+1)-f(x) = 0$$\nProof:\nWe want to prove that for any $\\epsilon > 0$, there is an $x_0$ such that $|f(x+1)-f(x)|< \\epsilon$ for all $x \\ge x_0$. By the Mean Value Theorem, there is some $c \\in (x, x+1)$ such  that $f(x+1) - f(x) = f'(c)$. By the convergence of $f'(x)$, there is some $y_0$ such that $|f'(x)| < \\epsilon$ for all $x \\ge y_0$. Thus $$x \\ge y_0 \\implies c>y_0 \\implies |f'(c)| = |f(x+1) - f(x)| < \\epsilon$$ and $x_0 = y_0$. $\\square$\nThanks!\n",
    "proof": "Yes your proof is well rigorous, well done!\nLet me just propose a more concise proof that does not require bringing up any $\\epsilon$: by the mean value theorem there is a function $\\xi: \\mathbb R\\to \\mathbb R$ such that\n$$f(x+1)-f(x) = f(\\xi(x))$$\nand $x<\\xi(x)< x+1$. Then\n$$\\lim_{x\\to\\infty} f(x+1)-f(x) = \\lim_{x\\to\\infty} f(\\xi(x)) = 0.$$\n",
    "tags": [
      "limits",
      "solution-verification",
      "proof-writing"
    ],
    "score": 6,
    "answer_score": 1,
    "is_accepted": false,
    "question_id": 4728979,
    "answer_id": 4728993
  },
  {
    "theorem": "Orthocenter of a triangle collinear with two points in the circumcircle.",
    "context": "\n\nThis difficult elementary geometry problem was proposed by @Nyafh54 and receiving no answer was deleted twice despite several upvotes. We republish it here mainly for the information of the O.P. who showed be interested in the subject, and those who gave their opinion in favor of the problem.\nThe statement is true even for shapes of the triangle such that, for example, the segment $DOE$ is not contained inside the triangle. We have this, in particular, for the triangle of vertices $A=(3,4),B=(0,0), C=(8,0)$ in the attached figure.\nHeading\nHINT.-$(1)$ Vertices $A;B;C$ given, we know how to determine the circumcenter $O$, the orthocencer $H$ and the circumcircle.\n$(2)$ $P$ and $S$ points are determined by the bisector of $BC$ side and $D$ and $E$ points by the bisector of segment $AS$ so we have got three points $D,E,S$.\n$(3)$ The statement can lead people to the construction of the circumcircle of the triangle $\\triangle{DES}$ then by intersection with the first circumcircle to determine the $X$ point and this is the difficulty of the problem for beginners.\n$(4)$ This difficulty can be avoided by intersecting the line $PH$ with the first circumcircle so we get a point $X$. We don’t know yet that $X$ belongs to the circumcircle of new triangle $\\triangle {DES}$ but we can show this proving that the quadrilateral $DESX$ is cyclic. For this, already having the coordinates $(x_i,y_i)$ of the points $D,E,S,X$ we can verify the Ptolemy's theorem: $$\\overline{DE}\\cdot\\overline{XS}+\\overline{DX}\\cdot\\overline{ES}=\\overline{DS}\\cdot\\overline{XE}$$ Another way is using the equation of the circumcircle of $\\triangle {DES}$\n$$\\det\\begin{vmatrix} x^2+y^2&x&y&1\\\\x_1^2+y_1^2&x_1&y_1&1\\\\ x_2^2+y_2^2&x_2&y_2&1\\\\ x_3^2+y_3^2&x_3&y_3&1\\end{vmatrix}=0$$ and verify this equality  putting the fourth point $X$ instead of the generic $(x.y)$.\n",
    "proof": "\nLet M be the intersection of $AS$ and $PX\n$. Clearly $P,O,S$ lies on the same line, which is a diameter of circle $O$, so we have $\\angle PXS=90^\\circ$. Now since $AS$ bisects $\\angle BAC$ and $DE$ perpendicularly bisects $AS$, quadrilateral $AESD$ is a rhombus and $DS=ES$, so we know $MS$ lies on a diameter line (symmetry line) of the blue circle. Furthermore since $\\angle MXS=90^\\circ$ we know point $M$ lies on the blue circle.\nEven further, since $\\angle MED = \\angle MSD = \\angle MAD$, and also since $AM\\perp ED$, we know $EM\\perp AD$ and $M$ is the orthocenter of $\\triangle AED$.\n\nForget about the whole $PX$ line first. Denote the other intersection of $AD$ and the blue circle as $T$. Denote the other intersection of $AE$ and the blue circle as $U$.\nConstruct $H$ as the orthocenter of $\\triangle ABC$. Since the details are pretty long, here's a sketch of the essential steps:\n(1) The intersection $CH$ and $ST$, denoted $R$, lies on circle $O$. The intersection of $BH$ and $SU$, denoted $V$, also lies on circle $O$.\n(2) $H$ lies one the line $TU$.\n(3) $XH$ bisects $\\angle TXU$ from angle bisector theorem on ${TX\\over UX}={TH\\over UH}$.\n(4) Therefore $X,H,M$ lies on the same line, which is the original $PX$ line.\nHere's the details:\n(1) First notice that $D,E,T,U$ are cocyclic on the blue circle, $\\triangle ATU$ is similar to $\\triangle AED$ which is isosceles. So $T,U$ are symmetric over the diameter $M,S$. Next, since $EM$ is parallel to $CH$ (both are perpendicular to $AB$), we know $$\\angle AEM= \\angle ACR$$ Also since $M,E,U,S$ are cocyclic on the blue circle, $\\angle AEM=\\angle ASU$. Applying the symmetry of $T,U$ over $MS$, we have $\\angle ASU=\\angle AST=\\angle ASR$. So overall we have $$\\angle AEM=\\angle ASR$$ This means $\\angle ACR = \\angle ASR$ and therefore $R$ lies on the circumcircle of $\\triangle ACS$, which is circle $O$. $V$ lies on circle $O$ goes by the exact same logic.\n(2)Since $S$ is the midpoint of $BC$, $VU$ bisects $\\angle CVH$. Also since $\\angle ASR=\\angle ASV$ we have arc $AR$ and $AV$ are the same arc, so $CU$ bisects $\\angle VCH$. Therefore $U$ is the incenter of $\\triangle VCH$ and $HU$ bisects $\\angle VHC$. By similar argument $T$ is the incenter of $\\triangle RBH$ and $HT$ bisects $\\angle RHB$. So $HT$ and $HU$ lies on the same line which is $TU$.\n(3)\n\nNotice that arcs $RX$ and $TX$ corresponds to the same angle $\\angle RSX$ in two circles, while arcs $VX$ and $UX$ also corresponds to the same angle $\\angle VSX$ in the same two circles, so $\\triangle RXV$ and $\\triangle TXU$ are similar. From this similarity we obtain $${XT\\over XU}={XR\\over XV}$$\nIn addition, since $\\angle RXV=\\angle TXU$ which means $\\angle RXT=\\angle VXU$ and the above ${XT\\over XU}={XR\\over XV}$, we have triangles $\\triangle RXT$ and $\\triangle VXU$ are similar too. From this similarity we obtain $${XR\\over XV}={RT\\over UV}$$\nCombining the two emphasized results we get $${XT\\over XU}={RT\\over UV}$$\n\nNow go back to this diagram, since $\\triangle RBH$ and $\\triangle VCH$ are similar for obvious reason, and $T,U$ are the corresponding incenters of the two triangles, we get $\\triangle RTH$ and $\\triangle VUH$ are also similar and $${RT\\over UV}={TH\\over UH}$$\nCombining all emphasized results above, we get ${XT\\over XU}={TH\\over UH}$ which completes our angle bisector theorem condition for step (3). Indeed $XH$ bisects $\\angle TXU$.\n(4) $XH$ bisects $\\angle TXU$ means $XH$ bisects $\\angle DXE$ as well, because arcs $DT$ and $EU$ are the same angle. This means the extension of $XH$ lies on $M$, which is the midpoint of arc $DE$. This implies $X,H,M$ are colinear, which completes our whole proof.\n",
    "tags": [
      "geometry",
      "proof-writing",
      "euclidean-geometry",
      "triangles",
      "circles"
    ],
    "score": 6,
    "answer_score": 1,
    "is_accepted": false,
    "question_id": 4573810,
    "answer_id": 4574773
  },
  {
    "theorem": "Proving that $\\{a\\in\\mathbb{Q}|a&gt;0$ and $a^2&lt;2\\}$ has no least upper bound in $\\mathbb{Q}$",
    "context": "I am going through a proof found in: http://www.math.columbia.edu/~harris/2000/2016Dedcuts.pdf\nIn it he finds a smaller upper bound to the supposed least upper bound: proof\nHowever in Step 1, I don't understand how the proof goes from $4\\frac{1}{n}-\\frac{1}{n^2}$ to it being less than\n$4\\frac{1}{n}-\\frac{1}{n}$, it reasons that $\\frac{1}{n^2}≤\\frac{1}{n}$ but shouldn't that mean subtracting the larger number makes the result smaller than if i were to subtract the smaller number>?\ni.e. $x<y$ then $z-x>z-y$ right? But here its saying that $z-y>z-x$\nIf it is a typo, can the proof still be salvaged?\n",
    "proof": "write $r = \\frac{p}{q}$ in lowest terms, meaning $p,q$  are positive integers  and $\\gcd(p,q)=1.$  Note $p^2 - 2 q^2 > 0$  and name $k = p^2 - 2 q^2 > 0$\nMake the new rational number\n$$  s = \\frac{3p+4q}{2p+ 3q} $$\nWe calculate $$ (3p+4q)^2 - 2 (2p+3q)^2 = p^2 - 2 q^2 = k   $$\nbut $s$  has a larger denominator.\nAs to the relative size of the fractions,\n$$  \\frac{p^2}{q^2} = \\frac{k+ 2q^2}{q^2}  =  2  + \\frac{k}{q^2} $$\nBut then\n$$  \\frac{(3p+4q)^2}{(2p+3q)^2} = \\frac{k+ 2(2p+3q)^2}{(2p+3q)^2}  =  2  + \\frac{k}{(2p+3q)^2} $$\nis smaller  because $2p+3q > q$\n",
    "tags": [
      "real-analysis",
      "inequality",
      "proof-writing",
      "solution-verification",
      "proof-explanation"
    ],
    "score": 6,
    "answer_score": 1,
    "is_accepted": false,
    "question_id": 4367868,
    "answer_id": 4367900
  },
  {
    "theorem": "Find the minimum of $\\frac{a^3+2020}{b}+\\frac{b^3+2020}{a}$",
    "context": "Problem says:\n\nLet $a,b>0$ and $2(a^2+b^2)-(a+b)=2ab$\nFind the minimum of\n$$\\frac{a^3+2020}{b}+\\frac{b^3+2020}{a}$$\n\n\nThe things I have done:\n$$\\begin{align}\\frac{2ab+(a+b)}{2}≥2ab \\end{align}$$\n$$\\begin{align} &\\implies 2ab+(a+b)≥4ab \\\\\n&\\implies a+b≥2ab \\\\\n&\\implies \\frac {ab}{a+b}≤\\frac 12\\end{align}$$\n\n$$\\begin{align}&2(a^2+b^2)-(a+b)=2ab, ~ab>0\\end{align}$$\n$$\\begin{align}&\\implies 2\\left((a+b)^2-2ab\\right)-(a+b)-2ab=0\\\\\n&\\implies 2(a+b)^2-4ab-(a+b)-2ab=0 \\\\\n&\\implies 2(a+b)^2-(a+b)-6ab=0 \\\\\n&\\implies \\frac 13(a+b)-\\frac{ab}{a+b}-\\frac 16=0\\\\\n&\\implies \\frac{ab}{a+b}=\\frac 13(a+b)-\\frac 16≤\\frac 12\\\\\n&\\implies a+b≤2\\\\\n&\\implies 2≥a+b≥2ab \\\\\n&\\implies 0<ab≤1 \\end{align}$$\n\n$$\\begin{cases}\\frac {a^3}{b}+\\frac{b^3}{a} ≥2ab \\\\ 2020 \\left(\\frac 1a+\\frac 1b \\right)≥\\frac{4040}{\\sqrt{ab}}\\end{cases}$$\n$$\\begin{align}\\implies &\\frac{a^3+2020}{b}+\\frac{b^3+2020}{a}\\\\\n&≥2ab+\\frac{4040}{\\sqrt{ab}} \\end{align}$$\n\nLet $n:=ab$,  then we have\n$$f(n)=2n^2+\\frac{4040}{n},~ 0<n≤1$$\n$$\\begin{align}f(n)-4042&=2n^2+\\frac{4040}{n}-4042\\\\\n&=\\frac{2n^3-4042n+4040}{n}\\\\\n&=\\frac{2(n-1)(n^2+n-2020)}{n}\\\\\n&≥0 \\end{align}$$\n$$\\begin{align} &\\implies f(n)-4042≥0 \\\\\n&\\implies f(n)≥4042,~ 0<n≤1\\end{align}$$\n\n$$\\begin{align}\\frac{a^3+2020}{b}+\\frac{b^3+2020}{a}\\\\ ≥2ab+\\frac{4040}{\\sqrt{ab}}≥4042\\end{align}$$\n$$\\min\\left\\{\\frac{a^3+2020}{b}+\\frac{b^3+2020}{a},~{\\large{\\mid}} 2(a^2+b^2)-(a+b)=2ab ∧ ~a>0∧b>0\\right\\}=4042 ~ \\text{at}~ ab=1$$\n\n$$\\begin{cases}ab=1 \\\\ a+b=2 \\end{cases} \\implies a=b=1$$\n$$\\min\\left\\{\\frac{a^3+2020}{b}+\\frac{b^3+2020}{a},~{\\large{\\mid}} 2(a^2+b^2)-(a+b)=2ab ∧ ~a>0∧b>0\\right\\}=4042 ~ \\text{at}~ a=b=1.$$\n\n\nCyclicity/symmetry  argument (?)\n\nLet,\n$$f(a,b)=\\frac{a^3+2020}{b}+\\frac{b^3+2020}{a}$$\nThen suppose that, $f(a,b)$ gets its minimum value at the point $a=m$. Substitution $a\\longmapsto b$ shows that, $f(a,b)$ also gets its minimum value at the point $b=m$. This means, we have $a=b.$\nI see that $$2(a^2+b^2)-(a+b)=2ab$$ is also simmetric/cyclic.\nWe get,\n$$\\begin{align}2(a^2+b^2)-(a+b)=2ab \\end{align}$$\n$$\\begin{align}&\\implies 4a^2-2a-2a^2=0 \\\\\n&\\implies a=b=1\\end{align}$$\n$$\\begin{align}\\min \\left\\{f(a,b) \\mid 2(a^2+b^2)-(a+b)=2ab ∧ ~a>0∧b>0\\right\\}=4042.\\end{align}$$\n\nQuestion:\n\nWhat are the points that are not rigorously correct (I refer to both proofs) in the things I do?\n\nPlease, don't post the correct solution.\nThank you for reviewing.\n",
    "proof": "Your cyclic argument is wrong. It should be\n\nIf $f(a, b)$ attains its minimum at $(m, n)$, then it also attains its minimum at $(n, m)$\n\n\nYour first argument looks correct. No issues that I can spot thus far.\n\nHere's a slightly more straightforward way of showing that the condition implies $ab\\leq1$ (Your approach, though correct, seems meandering/unclear what it's aiming towards.)\nLet $ S = a+b$, $P = ab$. (For symmetric/cyclic equations, these are good to define.)\nWe have $ S^2 \\geq 4P$ since $a, b > 0$.\nThe condition states that $2(S^2 - 2P)- S = 2P \\Rightarrow 2S^2 - S = 6P$.\nHence $ 1.5 S^2 \\geq 6P = 2S^2 - S  \\Rightarrow 2S - S^2  \\geq 0 \\Rightarrow  2 \\geq S \\geq 0 $.\nSince $ S^2 \\geq 4P$, thus $\\Rightarrow 1 \\geq P$.\n",
    "tags": [
      "algebra-precalculus",
      "inequality",
      "proof-writing",
      "contest-math",
      "maxima-minima"
    ],
    "score": 6,
    "answer_score": 1,
    "is_accepted": true,
    "question_id": 4137964,
    "answer_id": 4138321
  },
  {
    "theorem": "Seeking a proof equivalent to the story &quot;She said that I would stop her if she told me why she is leaving, so I decided to stop her anyway!&quot;",
    "context": "I am looking for an interesting mathematical proof which is equivalent to the story:\n\n\"She said that I would stop her if she told me why she is leaving, so I decided to stop her anyway!\"\n\nMathematically, I guess, this could be phrased, for a language $\\mathcal L$ in propositional logic, as:\n$$\\Big(\\exists A\\in Prop(\\mathcal L) \\,\\,\\text{s.t.}\\,\\,\\vdash A\\,\\,\\&\\,\\, A\\implies B\\Big)\\implies B. $$\nI am no logician and probably am wrong regarding to the last part, so please correct me, as long as my informal description is clear.\nEdit: Clearly, there are many situations where you know\n\\begin{align*}\n\\vdash& A\\\\\n\\vdash& A\\implies B\\vee C\\\\\n\\vdash& (B\\implies D)\\&(C\\implies D)\\\\\n&\\quad\\Downarrow\\\\\n\\vdash& D\\end{align*} \nFor example, have a function with two fixed points, each of which is rational, and have a convergent recursive sequence defined by this function. You know it converges to a fixed point but you do not know to which, however you do know that the limit is rational.\n$\\textbf{Edit:}$\nIn my example, it looks like $A$ could be \"She was leaving to save her brother from Dracula\" and $B$ is \"I stopped her from leaving\".\n",
    "proof": "The original phrase\n\nShe said that I would stop her if she told me why she is leaving, so I decided to stop her anyway!\"  $~(0)$\n\nThe phrase $(0)$ reads as $(1)$, which is logically equivalent to the statement $B$. In $(1)$, I have symbolized \"I stopped her\" as $B$, and \"she told me why she is leaving\" as $W$.\n\\begin{equation}\\tag{1}\n(W\\rightarrow B)~\\&~B\n\\end{equation}\nCareful reading of $(0)$ shows that the girl did not explain why she was leaving, so it's not necessary for the other person to stop her.\nYour statement\n\\begin{equation}\\tag{2}\n\\Big(\\exists A\\in Prop(\\mathcal L) \\,\\,\\text{s.t.}\\,\\,\\vdash A\\,\\,\\&\\,\\, A\\implies B\\Big)\\implies B\n\\end{equation}\nThere are a few nuances with (2). To best state explicitly that the right hand side of $(2)$ is a sentence in propositional logic, you should bind both $A$ and $B$, as opposed to just $A$.\nSecondly, you have placed the quantification and meta level proof symbol ($\\vdash$), in the antecedent of your implication using brackets. This means that $(2)$ is equivilent to $(3)$, which is clearly not what you intended to say.\n\\begin{equation}\\tag{3}\n\\big(\\exists A\\in Prop(\\mathcal L) \\,\\,\\text{s.t.}\\,\\,\\nvdash A\\,\\,\\&\\,\\, A\\implies B\\Big)\\lor B\n\\end{equation}\nA better symbolization of $(3)$ is $(4)$.\n\\begin{equation}\\tag{4}\n\\exists A,B \\in Prop(\\mathcal L) \\,\\,\\text{s.t.}\\,\\,\\vdash (A\\,\\,\\&\\,\\, A\\implies B)\\implies B\n\\end{equation}\nYour first edit\nYour edit is a lot clearer, and the symbolization for this statement is good. It has a different logical form to $(0)$ however, and appears to be unrelated.\n",
    "tags": [
      "logic",
      "proof-writing",
      "propositional-calculus"
    ],
    "score": 6,
    "answer_score": 1,
    "is_accepted": true,
    "question_id": 3483151,
    "answer_id": 3487748
  },
  {
    "theorem": "New to proofs and would like your shared experience",
    "context": "An integer $a$ is \"happy\" if it can be written on the form $a=6k+3$ for some integer k. Let $y$ denote an integer. Prove that $y$ is \"happy\" if and only if $y^2$ is \"happy\".\nMy proof:\nAssume that $y$ is a happy integer, that is $y=6k+3$. Then we have $y^2=(6k+3)^2=36k^2+36k+6+3=$ $\\color{blue}{\\textrm{6($6k^2+6k+1$)}}$$+3$ which is the same as $\\color{blue}{\\textrm{6$m$}}$$+3$ for some integer $m$. Thus we have that if $y$ is happy then so is $y^2$.\nNext we show that there exist no integers that yield a happy square other than those integers who can be expressed on the form $6k+3$.\n$1. (6k+1)^2=36 k^2 + 12 k + 1 \\neq6m+3$\n$2. (6k+2)^2=36 k^2 + 24 k + 4\\neq6m+3$\n$3. (6k+4)^2=36 k^2 + 48 k + 16 \\neq6m+3$\n$4. (6k+5)^2=36 k^2 + 60 k + 25 \\neq6m+3$\nWe find that the only way of achieving a happy integer by squaring is by squaring an already happy integer. Since squaring a number is a reversable operation, if we take the square root of a happy number and it is still an integer, then it can only have mapped onto a happy number $6k+3$. Hence we have shown that iff a square is happy then so is its square root (if it is still an integer). ■\nMy questions: \nFirst of all if it is correct? Furthermore, I feel like the proof have unnecessary steps somehow and would very much like to shorten it, how can one do that?. Also, I have just started to read about how to prove/present the idea of a proof by oneself - was there anything you wished you knew regarding proofs during these early stages that I am going through that might be useful to know before taking on more advance excersices?\n",
    "proof": "How about using prime factorization, as follows. (Just noted this is basically what is suggested in lulu's comment.)\n\n$\\Rightarrow$\nIf $$y = 6k+3 = 3(2k+1),$$ for some $k \\in \\Bbb Z$, then $y$ must be of the form \n$$ y = 3\\prod_i p_i^{j_i},$$\nwith $p_i$ odd primes.\nSo \n$$y^2 = 3\\cdot\\underbrace{\\left(3 \\prod_i p_i^{2j_i}\\right)}_{\\small\\mbox{odd}},$$\nand thus also $y^2$ is of the form\n$$y^2 = 3(2h+1),$$\nfor some $h \\in \\Bbb Z$\n\n$\\Leftarrow$\nConversely, if $z$ is the square of some integer $y$ and\n$$z = 3(2k+1),$$\nthen it must be\n$$z = 3\\left(3\\prod_i p_i^{2j_i}\\right),$$\nwith $p_i$ odd primes, because, the product is odd.\nThus \n$$y = 3 \\underbrace{\\prod_i p_i^{j_i}}_{\\small{\\mbox{odd}}}.$$\nTherefore it must be\n$$y= 3(2h +1),$$\nfor some $h\\in\\Bbb Z$.\n",
    "tags": [
      "proof-verification",
      "proof-writing",
      "alternative-proof",
      "proof-theory"
    ],
    "score": 6,
    "answer_score": 1,
    "is_accepted": true,
    "question_id": 3321389,
    "answer_id": 3321438
  },
  {
    "theorem": "Non calculus proof of $SS_1 = T^2$",
    "context": "The equation of a pair of tangents from $(x_1,y_1)$ to the circle $x^2+y^2+2gx+2fy+c=0$ is given by $T^2= SS_1$\nwhere:\n$S= x^2+y^2+2gx+2fy+c \\\\ S_1= x_1^2+y_1^2+2gx_1+2fy_1+c \\\\ T = xx_1+yy_1+g(x+x_1)+f(y+y_1)+c $\nCan someone provide me the non calculus proof for this? \n",
    "proof": "Note: I am using the standard notation for $S_1,S_{11}$\nLemma 1: The tangent from point $(x_1,y_1)$ on a conic $S$ is given by $$S_1=0$$\nProof: For a conic $S\\equiv Ax^2+By^2+2Hxy+2Gx+2Fy+C$\nWe note that the equation $$A(x-x_1)(x-x_2)+B(y-y_1)(y-y_2)+H(x-x_1)(y-y_2)+H(x-x_2)(y-y_1)=\\\\Ax^2+By^2+2Hxy+2Gx+2Fy+C$$\nIs in fact first degree and represents a line passing through 2 points $(x_1,y_1),(x_2,y_2)$ if they lie on S.\nFor double-contact (touching) we put $x_2=x_1$ and get\n$$\nAxx_1+Byy_1+H(xy_1+x_1y)+G(x+x_1)+F(y+y_1)+C=0 \\equiv S_1=0\n$$\nLemma 2: The chord of contact of point $(x_2,y_2)$ or the line passing through both tangent points is given by $$S_2=0$$\nProof:\nWe know both tangents pass through $(x_2,y_2)$ and we need to find $(x_1,y_1)$\nNote: Here $(x_1,y_1)$ represents both the points\nSince $(x_2,y_2)$ satisfies the equation we have\n$$\nS_{12}=0\\\\\nAx_1x_2+By_1y_2+H(x_1y_2+x_2y_1)+G(x_1+x_2)+F(y_1+y_2)+C=0\n$$\nSo the equation\n$$\nS_2=0\\\\\nAxx_2+Byy_2+H(xy_2+x_2y)+G(x+x_2)+F(y+y_2)+C=0\n$$\npasses through both tangent points hence it is the chord of contact\nNote: 1,2 are used only for ease of writing\nLemma 3: Every conic passing through the intersection of a conic $S$ and two straight lines $u,v$ belong to the family $$S=\\lambda (u\\cdot v)$$ where $\\lambda$ is any real number.\nProof: The equation is satisfied by the 4 intersection points. Since we need 5 points to specify a conic we introduce the $\\lambda$ coefficient to give us 5 degrees of freedom.\nCor 1: If $u,v$ are coincident and the intersecting points are $P,Q$ the new conic passes through $P,Q$ twice (The solution to the intersection has equal roots) i.e the new conic touches $S$ at $P,Q$\nThe pair of tangents can be written in the above form with $u,v=T$ which is the chord of contact of point $(x_1,y_1)$.\nSo we get $$S=\\lambda T^2 \\equiv S=\\lambda S_1^2$$\nSince the 5th point is $(x_1,y_1)$ we substitute it in place of $x$ to find $\\lambda$\n$$S_{11}=\\lambda S_{11}^2$$\nSo finally the required equation is\n$$SS_{11}=T^2$$\n",
    "tags": [
      "algebra-precalculus",
      "proof-writing",
      "analytic-geometry"
    ],
    "score": 6,
    "answer_score": 1,
    "is_accepted": false,
    "question_id": 2452839,
    "answer_id": 4945544
  },
  {
    "theorem": "Finding the relation between identity and identical functions",
    "context": "Consider the function $f(x,a,b,c,d) = (((x \\oplus a) + b) \\oplus c) + d$ defined on $\\mathbb{Z}_ {2^n}$, where $\\oplus$ denotes bitwise exclusive-OR (XOR) and $+$ denotes addition modulo $2^n$.\nLet's start with two definitions:\n\n$f(\\cdot,a,b,c,d)$ is an identity function on $\\mathbb{Z}_{2^n}$ if $\\forall x \\in \\mathbb{Z}_{2^n}, f(x,a,b,c,d) = x$.\n$f(\\cdot,a',b',c',d')$ is identical to $f(\\cdot,a,b,c,d)$ if $\\forall x \\in \\mathbb{Z}_{2^n}, f(x,a',b',c',d') = f(x,a,b,c,d)$.\n\nIn answer to this question, it has been shown that the number of identity functions is equal to $n \\times 2^{n+2}$.\nFurthemore, when looking in practice for several $n$, it seems that $n \\times 2^{n+2}$ is also the number of identical functions for a given $(a,b,c,d) \\in (\\mathbb{Z}_{2^n})^4$.\nIn fact, identity functions are the specific case where $(a,b,c,d) \\in (\\mathbb{Z}_{2^n})^4$ is such that $f(x,a,b,c,d) = x$.\nIs there a way to prove that the number of identity functions is equal to the number of identical functions? In other terms, how can we extend the demonstration for all $(a,b,c,d) \\in (\\mathbb{Z}_{2^n})^4$? \n",
    "proof": "I think this conjecture is false, actually—the number of \"identical\" functions seems to depend on the particular tuple.\n$\\newcommand{\\blank}{\\underline{\\hspace{0.7em}}}$\nFor example, when $n=3$, it seems like the tuple $\\langle 0,0,0,0\\rangle$ (which yields the identity function) belongs to a group of 96 = $4n\\cdot 2^n$ tuples that all yield the same function, consistent with the result you've already proven.\nBut for the tuple $\\langle 0, 1, 1, 0\\rangle$, it seems like there are only 32 such tuples. If this is true, I suspect it will be because the collection of tuples that yield the identity have more symmetries available than the tuples that yield $\\langle 0, 1, 1, 0\\rangle$. This is a counterexample which establishes a negative result, showing that not all cosets of equivalent tuples have the same size $4n\\cdot 2^n$.\nHere is a partial explanation. Fix a particular bitstring length $n$. To avoid writing too many parentheses, assume that all operators are left-associative so that $a\\boxplus b\\boxplus c \\boxplus d$ means $((a\\boxplus b)\\boxplus c)\\boxplus d$.\nThe number $h \\equiv 2^{n-1}$ has several special properties in this setting:\n\nThe function $(\\blank\\oplus h)$ behaves identically to the translation $(\\blank + h)$. Adding $h$ is equivalent to xoring by $h$.\nViewed as an xor, it flips the highest bit of each number. Viewed as a translation (consider the numbers $0\\ldots (2^n-1)$ arranged in a circle for modular arithmetic), it rotates each number by a half-turn. \nBecause of this half-turn, adding/xoring with $h$ causes numbers higher than $h$ to become less than $h$, and vice-versa. \nAnd in fact, out of all possible xor functions $(\\blank \\oplus a)$ and all possible translation functions $(\\blank + b)$, the only xor that is equivalent to some translation is $(\\blank\\oplus h)$ and the only translation that is equivalent to some xor is $(\\blank + h)$.\nAdding/xoring $h$ twice does nothing: $\\blank + h + h = \\blank \\oplus h \\oplus h = \\blank + h \\oplus h = \\blank \\oplus h + h = \\blank$.\n\nNow suppose we have a function\n$$\\blank \\oplus a + b$$\nWe know that operating by $h$ twice does nothing, so this function is equivalent to another one with the same form but with different parameters $a^\\prime$ and $b^\\prime$:\n$$\n\\begin{align*}\n\\blank \\oplus a + b &= \\blank \\oplus a \\oplus h + h + b\\\\\n&= \\blank \\oplus (a\\oplus h) + (h+b)\\\\\n&= \\blank \\oplus a^\\prime + b^\\prime\n\\end{align*}\n$$\nTake a more complex example:\n$$\\blank \\oplus a + b \\oplus c + d$$\nUsing the same trick as before, we can find other tuples $\\langle a^\\prime, b^\\prime, c^\\prime, d^\\prime\\rangle$ which yield an equivalent function: We can insert a pair of $h$s between $a$ and $b$, or between $b$ and $c$, or between $c$ and $d$, or any combination of these— all of them yield different numbers.\nBecause we can insert a pair of $h$s in three different locations, and each combination of insertions yields a different tuple, we know that there are $2^3=8$ combinations.\nAs a consequence of this trick, we therefore know that for each tuple $\\langle a, b, c, d\\rangle$, there are at least eight tuples (itself included) which yield the same function.  \nMoreover, because of property (3), we can always apply an appropriate combination to find a representative tuple where $a,c < h$ (for example).\n\nAs for the identity, we know that $\\blank \\oplus 0 + 0 \\oplus 0$ is an identity function. How many other such tuples also yield the identity function?\n\nIf we consider tuples with no xor component $(a=c=0)$, there are $2^n$ tuples $\\langle 0, b, 0, -b\\rangle$ equivalent to the identity.\nIf we consider tuples with no translation component $(b=d=0)$, there are $2^n$ tuples $\\langle a, 0, \\overline{a}, 0\\rangle$ equivalent to the identity.\nIf we apply our multiplicity trick to the previous tuples, we multiply the number of results approximately eightfold, because each of the previous examples is independent of each of the others. \n\nThis is not an exhaustive careful counting of the possibilities (and in fact, we've counted $\\langle 0,0,0,0\\rangle$ more than once) but it makes clear one apparent advantage of the identity function. In particular, we can make both its translation and xor components independently zero, which means that we can obtain many different independent prototypes. \nIn contrast, other tuples like $\\langle 0, 1, 1, 0\\rangle$ do not have as many independent prototypes, because each position is more tightly coupled to the others; there is no way to \"zero out\" the interaction.\n\nThe tuples I find in particular are:\n\nFOR THE IDENTITY\n(0, 0, 0, 0)\n(0, 0, 4, 4)\n(0, 1, 0, 7)\n(0, 1, 4, 3)\n(0, 2, 0, 6)\n(0, 2, 4, 2)\n(0, 3, 0, 5)\n(0, 3, 4, 1)\n(0, 4, 0, 4)\n(0, 4, 4, 0)\n(0, 5, 0, 3)\n(0, 5, 4, 7)\n(0, 6, 0, 2)\n(0, 6, 4, 6)\n(0, 7, 0, 1)\n(0, 7, 4, 5)\n(1, 0, 1, 0)\n(1, 0, 5, 4)\n(1, 2, 1, 6)\n(1, 2, 5, 2)\n(1, 4, 1, 4)\n(1, 4, 5, 0)\n(1, 6, 1, 2)\n(1, 6, 5, 6)\n(2, 0, 2, 0)\n(2, 0, 6, 4)\n(2, 2, 2, 2)\n(2, 2, 6, 6)\n(2, 4, 2, 4)\n(2, 4, 6, 0)\n(2, 6, 2, 6)\n(2, 6, 6, 2)\n(3, 0, 3, 0)\n(3, 0, 7, 4)\n(3, 1, 3, 1)\n(3, 1, 7, 5)\n(3, 2, 3, 2)\n(3, 2, 7, 6)\n(3, 3, 3, 3)\n(3, 3, 7, 7)\n(3, 4, 3, 4)\n(3, 4, 7, 0)\n(3, 5, 3, 5)\n(3, 5, 7, 1)\n(3, 6, 3, 6)\n(3, 6, 7, 2)\n(3, 7, 3, 7)\n(3, 7, 7, 3)\n(4, 0, 0, 4)\n(4, 0, 4, 0)\n(4, 1, 0, 3)\n(4, 1, 4, 7)\n(4, 2, 0, 2)\n(4, 2, 4, 6)\n(4, 3, 0, 1)\n(4, 3, 4, 5)\n(4, 4, 0, 0)\n(4, 4, 4, 4)\n(4, 5, 0, 7)\n(4, 5, 4, 3)\n(4, 6, 0, 6)\n(4, 6, 4, 2)\n(4, 7, 0, 5)\n(4, 7, 4, 1)\n(5, 0, 1, 4)\n(5, 0, 5, 0)\n(5, 2, 1, 2)\n(5, 2, 5, 6)\n(5, 4, 1, 0)\n(5, 4, 5, 4)\n(5, 6, 1, 6)\n(5, 6, 5, 2)\n(6, 0, 2, 4)\n(6, 0, 6, 0)\n(6, 2, 2, 6)\n(6, 2, 6, 2)\n(6, 4, 2, 0)\n(6, 4, 6, 4)\n(6, 6, 2, 2)\n(6, 6, 6, 6)\n(7, 0, 3, 4)\n(7, 0, 7, 0)\n(7, 1, 3, 5)\n(7, 1, 7, 1)\n(7, 2, 3, 6)\n(7, 2, 7, 2)\n(7, 3, 3, 7)\n(7, 3, 7, 3)\n(7, 4, 3, 0)\n(7, 4, 7, 4)\n(7, 5, 3, 1)\n(7, 5, 7, 5)\n(7, 6, 3, 2)\n(7, 6, 7, 6)\n(7, 7, 3, 3)\n(7, 7, 7, 7)\n\n------------------------\nFOR TRANSLATE 1, XOR 1\n\n(0, 1, 1, 0)\n(0, 1, 5, 4)\n(0, 3, 1, 6)\n(0, 3, 5, 2)\n(0, 5, 1, 4)\n(0, 5, 5, 0)\n(0, 7, 1, 2)\n(0, 7, 5, 6)\n(3, 1, 2, 2)\n(3, 1, 6, 6)\n(3, 3, 2, 4)\n(3, 3, 6, 0)\n(3, 5, 2, 6)\n(3, 5, 6, 2)\n(3, 7, 2, 0)\n(3, 7, 6, 4)\n(4, 1, 1, 4)\n(4, 1, 5, 0)\n(4, 3, 1, 2)\n(4, 3, 5, 6)\n(4, 5, 1, 0)\n(4, 5, 5, 4)\n(4, 7, 1, 6)\n(4, 7, 5, 2)\n(7, 1, 2, 6)\n(7, 1, 6, 2)\n(7, 3, 2, 0)\n(7, 3, 6, 4)\n(7, 5, 2, 2)\n(7, 5, 6, 6)\n(7, 7, 2, 4)\n(7, 7, 6, 0)\n\n",
    "tags": [
      "linear-algebra",
      "finite-groups",
      "proof-writing",
      "modular-arithmetic",
      "binary"
    ],
    "score": 6,
    "answer_score": 1,
    "is_accepted": true,
    "question_id": 2365778,
    "answer_id": 2377026
  },
  {
    "theorem": "Understanding how a proof was developed",
    "context": "Basically, my question is about methods of approaching a given proof. Let me elaborate on that with an example. \nProposition: Suppose $a$ and $b$ $\\in R$ and $a < b$. There is always a rational number, say $x$, in the range $a < x < b$.\nProof: Chose N $\\in N$ sufficiently large that $\\frac{1}{N} < (b-a)$. Chose the least integer $z$ such that $b$N $\\leq z$. $z$ being the least such integer, $(z-1) < b$N. Thus $\\frac{(z-1)}{N} < b$. From then on, proof applies some inequality manipulation to show that this number is also greater than $a$. At this step, we conclude by noting that $\\frac{(z-1)}{N}$ is a rational number. \nNow I picked this example since it is simple but the following things I'll say plagues me in most analysis proofs, since I started studying it. I consider a proof understood if I can fully see how the mathematician might have come up with the proof. Here, for example, it took me a lot of time to make sense why we chose N in such a way. Verifying the deductive steps is easy most of the time but uncovering the intention behind each step takes significant time. Studying rigorous real analysis for the first time, I feel like my approach to understanding proofs makes me waste a lot of time.\nShould I skim faster and skip details at first reading, should I abandon my obsession with justifying each step? I expected studying rigorous mathematics to take more time then my past engineering math efforts but this feels painful at times and makes you feel like giving up. Is there a book that teaches the skills needed, if there is any such skills, to better uncover the intention behind proofs?\n",
    "proof": "When you're reading a proof, you're only seeing the finished product---the only standard is that each step is logically true, not that it is intuitively obvious where each value comes from (although good exposition can help!). What you're missing is that there is a bunch of scratch work going on before a formal proof is written, often by working backwards from the conclusion. Using your example, my scratch work / thought process would be something like:\n\nWant: $a < p/N < b$. Multiply through by $N$ to obtain $a N < p < bN$. \nSince $p$ and $N$ are integers, we need to pick $N$ large enough so that $bN - aN > 1$ in order for such a $p$ to exist (need a big enough \"gap\" to include an integer). Since $b>a$, that means we need $N > \\frac{1}{b-a}$. Equivalently, $\\frac{1}{N} < b-a$. For simplicity, take the smallest such integer for $N$.\nNow that we have $N$, how can we construct $p$? Etc.\nAlternatively, you could envision all the $N$-adic rationals $\\{ p/N ~:~ p \\in \\mathbb{Z} \\}$ and think of increasing $N$ (decreasing the spacing) until one of them must land in between $a$ and $b$. In order for that to happen, the distance between consecutive pairs must be sufficiently small. Etc.\n\nNote that this string of reasoning would not constitute a formal proof---we started out by assuming our conclusion and made observations from there. However, it is instructive for telling us how our proof will need to be constructed, in particular telling us how to find $N$ and $p$ in order to eventually construct a rational number between $a$ and $b$.\n",
    "tags": [
      "real-analysis",
      "analysis",
      "proof-writing"
    ],
    "score": 6,
    "answer_score": 1,
    "is_accepted": false,
    "question_id": 2244122,
    "answer_id": 2244164
  },
  {
    "theorem": "Prove that if $\\mathcal F \\subseteq \\mathcal G$ then $\\bigcap\\mathcal G \\subseteq\\bigcap\\mathcal F$",
    "context": "This is Velleman's exercise 3.3.13. Suppose $\\mathcal F $ and  $\\mathcal G$ are families of sets and $\\mathcal F \\subseteq \\mathcal G$. Prove that $\\bigcap\\mathcal G \\subseteq\\bigcap\\mathcal F$. \nMy approach so far:\nLet $x$ be an arbitrary element of $\\bigcap\\mathcal G$. Then $x$ is an element of every $A\\in\\mathcal G$. To show that $x$ needs to be an element of $\\bigcap\\mathcal F$ it suffices to show that it has to be an element of every $A\\in\\mathcal F$. Suppose $A_0$ is an arbitrary element of $\\mathcal F$. Due to $\\mathcal F \\subseteq \\mathcal G$ any $A\\in \\mathcal F$ is an element of $\\mathcal G$. So $A_0 \\in \\mathcal G$. Since $x$ is an arbitrary element of $\\bigcap\\mathcal G$, it follows that $x \\in A_0$. This shows that every $x \\in\\bigcap\\mathcal G$ will be an element of $\\bigcap\\mathcal F$.\nAny comments, suggestions and improvements relating to my attempt are appreciated. I am a noob so feel free to bash it as good as you can. Thanks in advance.\n",
    "proof": "Looks correct to me. Your proof is very detailed and commendably rigorous—I think that's a good thing, given that the result might, at first sight, seem counterintuitive.\nTo highlight the intuitive content of the result, let me present a less formal but perhaps more illuminating argument. Think of $\\bigcap \\mathcal G$ as the set of all points that satisfy all the restrictions embodied by the family of sets $\\mathcal G$ (viz., such points must be contained in every $G\\in\\mathcal G$). Similarly, $\\bigcap \\mathcal F$ is the set of all points that satisfy all the restrictions embodied in $\\mathcal F$. Since $\\mathcal F\\subseteq\\mathcal G$, $\\mathcal F$ embodies fewer restrictions. Therefore, the condition $\\bigcap \\mathcal F$ is, in fact, looser than $\\bigcap \\mathcal G$, so more points pass it. In other words, if a point clears all the requirements embodied by $\\bigcap \\mathcal G$, it must a fortiori clear the less stringent requirements embodied by $\\bigcap \\mathcal F$. This implies that $\\bigcap \\mathcal G\\subseteq \\bigcap \\mathcal F$.\n",
    "tags": [
      "proof-writing",
      "first-order-logic"
    ],
    "score": 6,
    "answer_score": 1,
    "is_accepted": true,
    "question_id": 1382589,
    "answer_id": 1382598
  },
  {
    "theorem": "Proving that the absolute value of x is greater then or equal to $0$",
    "context": "My Question reads:\nProve for all $x\\in\\mathbb{R}$, $|x|\\geq\\ 0$. \nThis is for a set theory class where we know that $\\mathbb{R}$ is the set of Dedekind cuts.\nFor each $x\\in\\mathbb{R}$, we define\n$|x|$ = max{$x, −x$}\n = $x ∪ (−x)$\nFor any $x\\in\\mathbb{R}$, we define\n$−x$ = {$r\\in\\mathbb{Q} | (∃s > r) − s\\notin\\ x$}.\nWe define the binary relation $<_\\mathbb{R}$ on R by\n$x <_\\mathbb{R} y$ iff $x\\subset\\ y$.\nI am not too sure where to start for this. Should we consider subsets instead? Is this saying that $0$ is a subset of $|x|$? \nA Dedekind Cut is a subset $x\\subset\\mathbb{Q}$ such that:\n$\\emptyset\\neq\\ x\\neq\\mathbb{Q}$\n$x$ is downwards closed, i.e. if $q\\in\\ x$ and $r<q$, then $r\\in\\ x$\n$x$ has no largest element.\nAlso, $0$ = {$r\\in\\mathbb{Q} | r < 0$}\n",
    "proof": "By definition, we are to show that either $0\\subseteq x$ or $0\\subseteq -x$. \nSuppose that $0\\subseteq x$ does not hold. This means there exists some $s\\in\\mathbb{Q}$ such that $s\\notin x$ but $s\\in 0$ (i.e. $s<0$).\n[Remark: This depends on your definition of Dedekind cut. $s\\in 0$ may stand for $s\\le 0$ if the other convention is used.]\nIn this case, for any $r\\in 0$, i.e. $r<0$ [or $r\\le 0$], we have $-s>r$. By definition of $-x$, we see that $r\\in -x$, since $-(-s)=s\\notin x$. Hence $0\\subseteq -x$.\nTherefore, $|x|\\ge 0$.\n",
    "tags": [
      "discrete-mathematics",
      "proof-writing",
      "real-numbers"
    ],
    "score": 6,
    "answer_score": 0,
    "is_accepted": true,
    "question_id": 2466818,
    "answer_id": 2466851
  },
  {
    "theorem": "Is there a geometrical method to prove $x&lt;\\frac{\\sin x +\\tan x}{2}$?",
    "context": "Suppose $x \\in (0,\\frac{\\pi}{2})$\nand we want to prove $$x<\\frac{\\sin x +\\tan x}{2}$$I tried to prove it by taking $f(x)=\\sin x+ \\tan x -2x$ and show $f(x) >0 ,when\\space  x \\in (0,\\frac{\\pi}{2})$ take f'$$f'=\\cos x +1+\\tan ^2 x-2\\\\=\\tan^2 x-(1-\\cos x)\\\\=\\tan ^2 x-2sin^2(\\frac x2)$$ I get stuck here ,because the last line need to be proved $\\tan ^2 x>2sin^2(\\frac x2) ,when\\space  x \\in (0,\\frac{\\pi}{2})$\n$\\bf{Question}:$ Is there a geometrical method to prove the first inequality ? (or other idea)\nThanks in advance.  \n$\\bf{Remark}: $I can see the function is increasing $when\\space  x \\in (0,\\frac{\\pi}{2})$ like below :https://www.desmos.com/calculator/www2psnhmu\n",
    "proof": "\nWe have $\\tan x\\gt x$  for $x\\in(0,\\frac\\pi2)$ the result follows,  Using AM-GM-HM inequalites we have ,\n$$\\color{blue}{\\frac{\\sin x+ \\tan x}{2} \\ge \\sqrt{\\sin x\\tan x} \\ge \\frac{2}{\\frac{1}{\\sin x }+\\frac{1}{\\tan x} } = 2\\tan \\frac x2 \\gt x}$$\n\nIndeed, $$ \\frac{1}{\\sin 2u }+\\frac{1}{\\tan 2u}= \\frac{1}{\\sin 2u }+ \\frac{\\cos 2u }{\\sin 2u } =\\frac{2\\cos^2 u}{2\\cos u\\sin u} = \\frac{1}{\\tan u}$$\n",
    "tags": [
      "calculus",
      "geometry",
      "trigonometry",
      "proof-writing",
      "alternative-proof"
    ],
    "score": 5,
    "answer_score": 10,
    "is_accepted": false,
    "question_id": 2391978,
    "answer_id": 2504218
  },
  {
    "theorem": "How do I begin proving this binomial coefficient identity: ${n\\choose 0} - {n\\choose 1} + {n\\choose 2} - {n\\choose 3} + \\dots = 0$",
    "context": "This is a homework question.\nI'm asked to prove the identity:\n$${n\\choose 0} - {n\\choose 1} + {n\\choose 2} - {n\\choose 3} + \\dots = 0$$\n(The sum ends with ${n\\choose n} = 1$, with the sign of the last term depending on the parity of n.)\nI recognize that the sequence:\n $${n\\choose 0}, {n\\choose 1}, {n\\choose 2}, {n\\choose 3}$$\ncorresponds to the binomial coefficients. That is, if I choose $n = 5$, I get the sequence $1, 5, 10, 10, 5, 1$.\nWorking this out (or just looking at Pascal's triangle), it's obvious that this theorem is true. It looks like the triangle / the binomial coefficients are \"symmetric\", and so if you add one and subtract one and keep going, it's evident they will cancel out to be zero.\nBut how do I prove this? Is there a set way? Are there multiple methods for proving this? How should I get started, or what are some names of proving methods I should look into to begin?\n",
    "proof": "Expand $(1-1)^n$ using the binomial theorem.                           \n",
    "tags": [
      "combinatorics",
      "summation",
      "proof-writing",
      "binomial-coefficients"
    ],
    "score": 5,
    "answer_score": 17,
    "is_accepted": true,
    "question_id": 765106,
    "answer_id": 765116
  },
  {
    "theorem": "Is the interior of the closure of a set equal to the interior of that set?",
    "context": "I tried to prove that with the set being  subset of a space X with metric d,\n\" the interior of the closure of a set equal to the interior of that set\".\nI proved that the interior of,namely, $A$ is included in the interior of the closure of $A$. But I could not prove the reverse, in special because I think that there can be points that are limit points of A and is contained in the interior of the closure, am I wrong?\nI am doing this to prove that the closure is equal to the union of the interior points of the closure with the set of all limit points of the set.\nIs the aforementioned statement true?\nThank you.\n",
    "proof": "The claim isn't true. \nThe set of rational numbers in the unit interval $[0,1]$ has empty interior, but its closure is the whole interval, so the interior of its closure is the open interval $(0,1)$.\n",
    "tags": [
      "general-topology",
      "proof-verification",
      "algebraic-topology",
      "proof-writing",
      "proof-explanation"
    ],
    "score": 5,
    "answer_score": 26,
    "is_accepted": true,
    "question_id": 1914098,
    "answer_id": 1914100
  },
  {
    "theorem": "Prove that $(a+1)(a+2)...(a+b)$ is divisible by $b!$",
    "context": "The problem is following, prove that:\n$$(a+1)(a+2)...(a+b)\\text{ is divisible by } b!\\text{ for every positive integer a,b}$$\n\nI've tried solving this problem using mathematical induction, but I don't think that i did it correctly. Here's what i've done\n$1.\\ b=1\\ (Basis)$\n$$b!| (a+1)(a+2)...(a+b)$$\n$$1 | a+1 \\text{, which is true}$$\n$2.\\ b=k\\ (Induction\\ Hypothesis)$\n$$k! | (a+1)(a+2)...(a+k)\\text{, we assume it's true}$$\n$$k!*n = (a+1)(a+2)...(a+k)\\text{, n is some positive integer}$$\n$3.\\ b=k+1\\ (Inductive\\ Step)$\nIn order to prove I should get:\n$$(k+1)!*m = (a+1)(a+2)...(a+k)(a+k+1)\\text{, where m is some positive integer}$$\n$$(a+1)(a+2)...(a+k)(a+k+1) = k!*n*(a+k+1)$$\n$$(a+1)(a+2)...(a+k)(a+k+1) = (k+1)!*n + k!*n*a$$\n$$(a+1)(a+2)...(a+k)(a+k+1) - a(a+1)(a+2)...(a+k) = (k+1)!*n$$\n$$(a+1)(a+2)...(a+k)(k+1) = (k+1)!*n$$\nAnd as you can see I'm returning at the beginning.\nAlso I've tried to use the fact that in b consecutive numbers, there must be at least one that divides b, but since i eliminate that number (because I couldn't be sure that the quotient is divisible with (b-1) or (b-2) or ... or 2. And after this i can't continue with (b-1) using this method, because it's not necessary for the other (b-1) to be consecutive.\n",
    "proof": "$$\n\\frac{(a+b)(a+b-1)(a+b-2)\\cdots(a+1)}{b(b-1)(b-2)\\cdots3\\cdot2\\cdot1}=\\binom{a+b}{b}\n$$\nSince binomial coefficients are listed in Pascal's Triangle, they are integers. \n",
    "tags": [
      "proof-writing",
      "induction",
      "divisibility",
      "factorial"
    ],
    "score": 5,
    "answer_score": 8,
    "is_accepted": true,
    "question_id": 387070,
    "answer_id": 387084
  },
  {
    "theorem": "Show that for all real numbers $a$ and $b$, $\\,\\, ab \\le (1/2)(a^2+b^2)$",
    "context": "so as in the title, I have the following theorem to prove. \nTheorem\nShow that for all $a$, $b\\in \\mathbb R$, that the following inequality holds,\n$\\begin{equation} ab \\leq \\frac{1}{2}(a^2 + b^2) \\end{equation}$.\nMy attempt\nSo if $a=b=0$, then the inequality is trivially $0 \\leq 0$ and we are ok.\nIf $a > 0$ and $b < 0$, then the product on the right is less than zero, while the one on the right is greater than $0$, so the inequality holds.\nThe problem that I am having is that if the numbers are the same sign, we have that both products will be positive and now we have to the show the inequality is true in this case. \nI tried making the following re-arrangement as a starting point $\\begin{equation} \\\\ 2ab \\leq (a^2 + b^2) \\end{equation}$\nBut I am not really sure where to go from here. I've been staring at it for a while and I just can't seem to get anything out of it. I noticed that it looks rather similar to the law of cosines for theta equal to 90 degrees and $c = 0$, though that would be wierd indeed. Anyway that hasn't been talked about in the book yet so I don't think it matters. \nWe have been introduced to the binomial theorem, difference of powers and geometric sum theorems, but I couldn't find any enlightenment from these.\nI don't really want an answer if you can help it, but a hint in the right direction would be appreciated if you could offer one. Thanks.\n",
    "proof": "Hint: $\\forall a,b \\in \\mathbb R, \\; (a-b)^2\\ge 0$\n",
    "tags": [
      "real-analysis",
      "inequality",
      "proof-writing"
    ],
    "score": 5,
    "answer_score": 4,
    "is_accepted": true,
    "question_id": 943994,
    "answer_id": 943998
  },
  {
    "theorem": "I was asked to prove the Principle of Cauchy Induction",
    "context": "Question: Carefully prove the Principle of Cauchy Induction: Suppose $P(n)$ is a property that a\nnatural number $n$ may or may not have. Suppose that\n\n$P(2)$ holds,\nFor every $n ≥ 2$, if $P(n)$ holds, then $P(2n)$ holds, and\nFor every $n ≥ 3$, if $P(n)$ holds, then $P(n − 1)$ holds.\n\nThen $P(n)$ holds for every natural number $n ≥ 2$.\n",
    "proof": "Suppose it is not true. \nA smallest integer $m\\geq2$ must exist with $\\neg P(m)$ and it easy to prove on base of (a), (b), (c) that $m\\geq4$.\nThen also $\\neg P(m+1)$ and one of $m$ and $m+1$ is even and can be written as $2k$ where $k$ is an integer that satisfies $2\\leq k<m$. \nThen also $\\neg P(k)$ but this contradicts the minimality of $m$.\n",
    "tags": [
      "real-analysis",
      "proof-writing",
      "induction",
      "proof-explanation"
    ],
    "score": 5,
    "answer_score": 15,
    "is_accepted": false,
    "question_id": 3318885,
    "answer_id": 3318894
  },
  {
    "theorem": "Prove that the real root of $x^3 + x + 1$ is irrational",
    "context": "Using wolframalpha.com we get that the real root of this polynomial is $-0.68233$\nThe only way that I have found how to prove it is using the Rational Root theorem.\nUsing that theorem the possible rationals roots are 1 and -1, but none of them work.\nThen the real root has to be irrational. \nMy questions are:\n\nIs there other way to prove it without using the Rational Root Theorem?\nIs my proof written well?\n\n",
    "proof": "First Question\nFor another approach without using the Rational Root Theorem is to solve the cubic.\nThe roots of a cubic equation of the form $x^3+px+q=0$ are given by,\n$\\sqrt[3]{-\\dfrac{q}{2}+\\sqrt{\\dfrac{q^2}{4}+\\dfrac{p^3}{27}}}+\\sqrt[3]{-\\dfrac{q}{2}-\\sqrt{\\dfrac{q^2}{4}+\\dfrac{p^3}{27}}}\\tag{1}$$ \\omega\\left(\\sqrt[3]{-\\dfrac{q}{2}+\\sqrt{\\dfrac{q^2}{4}+\\dfrac{p^3}{27}}}\\right)+\\omega^2\\left(\\sqrt[3]{-\\dfrac{q}{2}-\\sqrt{\\dfrac{q^2}{4}+\\dfrac{p^3}{27}}}\\right)\\tag{2}$$\\omega^2\\left(\\sqrt[3]{-\\dfrac{q}{2}+\\sqrt{\\dfrac{q^2}{4}+\\dfrac{p^3}{27}}}\\right)+\\omega\\left(\\sqrt[3]{-\\dfrac{q}{2}-\\sqrt{\\dfrac{q^2}{4}+\\dfrac{p^3}{27}}}\\right)\\tag{3}$\nWhat's left is solving the cubic $x^3+x+1=0$ using the above formulas and finding which among them is real and whether it is rational or irrational. I leave that to you.\nSecond Question\nWhat you have described as your proof is really not a proof. It can be said to be a sketch of the proof. For example, you said that,\n\n...Using that theorem the possible rationals roots are $1$ and $-1$, but none of them work.\n\nIt isn't illogical to ask,\n\nHow exactly does the theorem lets you conclude that the only possible rational roots are $1$ and $-1$?\n\nTo answer this you have to prove your conclusion. To be precise, what your argument lacks is elaboration. Otherwise, your argument is fine.\nIn short, when you are writing proofs, imagine that you are writing it for someone who wants to have a clear picture of your argument from your proof. Your job will be to make it as clear and as rigorous as possible.\nFor example, you may have written the proof as follows,\n\nA real root always  exists since the degree of the equation is odd.\nThe only way that I have found how to prove it is using the Rational Root theorem. Which tells us that...(insert a brief statement of the Rational Root Theorem here).\nUsing this theorem the possible rationals roots are $1$ and $-1$, but none of them work. Since $f(x)=x^3+x+1\\implies f(1)=3\\ne 0$ and $f(-1)=1\\ne 0$.\nHence the real root has to be irrational.\n\n",
    "tags": [
      "elementary-number-theory",
      "proof-verification",
      "proof-writing"
    ],
    "score": 5,
    "answer_score": 3,
    "is_accepted": true,
    "question_id": 1087242,
    "answer_id": 1087249
  },
  {
    "theorem": "Proving $n+3 \\mid 3n^3-11n+48$",
    "context": "I'm really stuck while I'm trying to prove this statement:\n$\\forall n \\in \\mathbb{N},\\quad (n+3) \\mid  (3n^3-11n+48)$.\nI couldn't even how to start.\n",
    "proof": "Assuming that you’re actually trying to prove that $n+3$ divides $3n^3-11n+48$, you can always simply do a polynomial long division:\n$$\\begin{array}{rrr|rr}\n&&&&&3n^2&-&9n&+&16\\\\ \\hline\nn&+&3&3n^3&&&-&11n&+&48\\\\\n&&&3n^3&+&9n^2\\\\ \\hline\n&&&&&-9n^2&-&11n\\\\\n&&&&&-9n^2&-&27n\\\\ \\hline\n&&&&&&&16n&+&48\\\\\n&&&&&&&16n&+&48\\\\ \\hline\n\\end{array}$$\nThere’s no remainder, so\n$$3n^3-11n+48=(n+3)(2n^2-9n+16)\\;.$$\n",
    "tags": [
      "elementary-number-theory",
      "polynomials",
      "proof-writing"
    ],
    "score": 5,
    "answer_score": 8,
    "is_accepted": true,
    "question_id": 342193,
    "answer_id": 342218
  },
  {
    "theorem": "Prove that the integers $x$, $x+6$, $x+12$, $x+18$, $x+24$ can only be prime if $x$ is $5$.",
    "context": "Prove that the integers $x$, $x+6$, $x+12$, $x+18$, $x+24$ can only be prime if $x$ is $5$.\nI am very new to proofs and not completely sure of how to approach this one. I tried several different values for $x$ other than $5$ and came up with values that are not prime. However, I can't see how I could generalize this question to prove that  it works if $x$ is $5$.\nHelp would be appreciated.\nThanks :)\n",
    "proof": "Hint: Show that one of the numbers is a multiple of $5$. One way to do that: Write $x=5k+r$.\n",
    "tags": [
      "elementary-number-theory",
      "proof-writing",
      "divisibility"
    ],
    "score": 5,
    "answer_score": 12,
    "is_accepted": false,
    "question_id": 1274180,
    "answer_id": 1274186
  },
  {
    "theorem": "Prove that $\\left(\\frac{3+\\sqrt{17}}{2}\\right)^n + \\left(\\frac{3-\\sqrt{17}}{2}\\right)^n$ is always odd for any natural $n$.",
    "context": "\nProve that $$\\left(\\frac{3+\\sqrt{17}}{2}\\right)^n + \\left(\\frac{3-\\sqrt{17}}{2}\\right)^n$$\nis always odd for any natural $n$.\n\nI attempted to write the binomial expansion and sum it so the root numbers cancel out, and wanted to factorise it but didn't know how. I also attempted to use induction but was not sure how to proceed.\n",
    "proof": "Note that it satisfies the following recursive formula:\n$$a_{n+2}=3a_{n+1}+2a_n\\tag{$\\star$}$$\nwhere $a_n=\\left(\\frac{3+\\sqrt{17}}2\\right)^n+\\left(\\frac{3-\\sqrt{17}}2\\right)^n$.\nThus, if $a_{n+1}$ is odd, then $a_{n+2}$ is odd.\n\n$(\\star)$ comes from noting that\n$$a^2=3a+2\\implies a=\\frac{3\\pm\\sqrt{17}}2$$\nAnd applying theories of linear recursives.\nThis technique is famous, take the Fibonacci sequence for example:\n$$a_{n+2}=a_{n+1}+a_n\\implies a^2=a+1$$\nThis quadratic has two solutions $a=\\phi,-\\phi^{-1}$.  Thus, the Fibonacci sequence has the following formula:\n$$a_n=\\frac{\\phi^n-(-\\phi)^{-n}}{\\sqrt5}$$\nwhere $\\phi$ is the golden ratio.\n",
    "tags": [
      "algebra-precalculus",
      "proof-writing",
      "recurrence-relations",
      "quadratics"
    ],
    "score": 5,
    "answer_score": 11,
    "is_accepted": true,
    "question_id": 2156316,
    "answer_id": 2156325
  },
  {
    "theorem": "doubt regarding a step proof of Cauchy-Schwarz inequality. Is it valid?",
    "context": "I'm quite new to math proofs,I can't understand why the part where we set $\\alpha=\\|\\boldsymbol{v}\\|^2$ e $\\beta=-\\boldsymbol{u} \\cdot \\boldsymbol{v}$ works\n, why are proofs like those possible and valid? By setting alpha and beta to these values aren't we proving the theorem just for the case where $\\alpha=\\|\\boldsymbol{v}\\|^2$ e $\\beta=-\\boldsymbol{u} \\cdot \\boldsymbol{v}$, the proof isn't generalizing enough or am I missing something?\nTheorem : If $(V, \\cdot)$ is an euclidean vector space (real), then $\\forall \\boldsymbol{u}, \\boldsymbol{v} \\in V$, we have:\n\n$|\\boldsymbol{u} \\cdot \\boldsymbol{v}| \\leq\\|\\boldsymbol{u}\\|\\|\\boldsymbol{v}\\|$,  Cauchy-Schwarz inequality.\n\nProof. Let us first prove the Cauchy-Schwarz inequality. It is clear that the inequality is verified if at least one of the two vectors is null. We therefore assume that they are both nonzero. Let us consider $\\boldsymbol{w}=\\alpha \\boldsymbol{u}+\\beta \\boldsymbol{v}, \\operatorname{with} \\alpha, \\beta \\in \\mathbb{R}$,\n$$\n\\boldsymbol{w} \\cdot \\boldsymbol{w}=(\\alpha \\boldsymbol{u}+\\beta \\boldsymbol{v}) \\cdot(\\alpha \\boldsymbol{u}+\\beta \\boldsymbol{v})=\\alpha^2\\|\\boldsymbol{u}\\|^2+\\beta^2\\|\\boldsymbol{v}\\|^2+2 \\alpha \\beta \\boldsymbol{u} \\cdot \\boldsymbol{v} \\geq 0 .\n$$\n*** then taking $\\alpha=\\|\\boldsymbol{v}\\|^2$ e $\\beta=-\\boldsymbol{u} \\cdot \\boldsymbol{v}$, we get\n$$\n\\|v\\|^4\\|\\boldsymbol{u}\\|^2+(\\boldsymbol{u} \\cdot \\boldsymbol{v})^2\\|v\\|^2-2\\|v\\|^2(\\boldsymbol{u} \\cdot \\boldsymbol{v})^2=\\|\\boldsymbol{v}\\|^2\\left(\\|\\boldsymbol{v}\\|^2\\|\\boldsymbol{u}\\|^2-(\\boldsymbol{u} \\cdot \\boldsymbol{v})^2\\right) \\geq 0 .\n$$\nSince $v \\neq 0$, we can divide by $\\|v\\|^2$, and get the inequality\n$$\n\\|\\boldsymbol{v}\\|^2\\|\\boldsymbol{u}\\|^2 \\geq(\\boldsymbol{u} \\cdot \\boldsymbol{v})^2, \\Longrightarrow|\\boldsymbol{u} \\cdot \\boldsymbol{v}| \\leq\\|\\boldsymbol{u}\\|\\|\\boldsymbol{v}\\| .\n$$\n",
    "proof": "In the proof provided, we see that the author have shown the truthness of\n$$\\alpha^2\\|\\boldsymbol{u}\\|^2+\\beta^2\\|\\boldsymbol{v}\\|^2+2 \\alpha \\beta \\boldsymbol{u} \\cdot \\boldsymbol{v} \\geq 0\n$$ for all $\\boldsymbol{u},\\boldsymbol{v}\\in V$ and for all $\\alpha,\\beta\\in\\mathbb{R}$.\nIn the logical form, we can write\n$$\\left(\\forall\\boldsymbol{u},\\boldsymbol{v}\\in V\\right)\\left(\\forall\\alpha,\\beta\\in\\mathbb{R}\\right)\\left(\\alpha^2\\|\\boldsymbol{u}\\|^2+\\beta^2\\|\\boldsymbol{v}\\|^2+2 \\alpha \\beta \\boldsymbol{u} \\cdot \\boldsymbol{v} \\geq 0\\right).$$\nWe know that if $\\forall xP(x)$ is true, then $P(x_0)$ is true, because the truthness of $\\forall xP(x)$ means that $P(x)$ is true no matter what value is assigned to $x$. Therefore, we can choose any value, say $x_0$, for $x$ and conclude that $P(x_0)$ is true. Logicians call this rule of inference universal instantiation.\nThus, if\n$$\\left(\\forall\\boldsymbol{u},\\boldsymbol{v}\\in V\\right)\\left(\\forall\\alpha,\\beta\\in\\mathbb{R}\\right)\\left(\\alpha^2\\|\\boldsymbol{u}\\|^2+\\beta^2\\|\\boldsymbol{v}\\|^2+2 \\alpha \\beta \\boldsymbol{u} \\cdot \\boldsymbol{v} \\geq 0\\right)$$\nis true, then we can choose $\\alpha_0=\\|\\boldsymbol{v}\\|^2\\in\\mathbb{R}$ and $\\beta_0=-\\boldsymbol{u} \\cdot \\boldsymbol{v}\\in\\mathbb{R}$ to conclude that\n$$\\left(\\forall\\boldsymbol{u},\\boldsymbol{v}\\in V\\right)\\left(\\alpha_0^2\\|\\boldsymbol{u}\\|^2+\\beta_0^2\\|\\boldsymbol{v}\\|^2+2 \\alpha_0 \\beta_0 \\boldsymbol{u} \\cdot \\boldsymbol{v} \\geq 0\\right)$$ is also true.\n",
    "tags": [
      "linear-algebra",
      "proof-writing",
      "proof-explanation"
    ],
    "score": 5,
    "answer_score": 10,
    "is_accepted": true,
    "question_id": 4816015,
    "answer_id": 4816064
  },
  {
    "theorem": "Show that $a^2 + b^2 + c^2 \\geqslant ab + bc + ca$ for all positive integers a, b, c",
    "context": "How would you show that the following inequality holds? Could you please write your reasoning by solving this problem too?\n$a^2 + b^2 + c^2 \\ge ab + bc + ca$  for all positive integers a, b, c\nI have tried:\n$a^2 + b^2 + c^2 - ab - ab + ab \\ge bc + ca $\n$(a^2 -2ab + b^2) + c^2 + ab \\ge bc + ca $\n$(a-b)^2 + c^2 + ab - bc - ca \\ge 0 $\n$(a-b)^2 + c(c - a) - b (c - a) \\ge 0 $\n$(a-b)^2 + (c - a)(c - b) \\ge 0 $\nWell, $(a-b)^2$ is positive. But how do we know if $(c-a)(c-b)$ is positive as well?\nPS: The problem is from the book \"How to think like a mathematician\" by Kevin Housten.\n",
    "proof": "Hint: Consider $(a-b)^2+(b-c)^2+(c-a)^2$. \nRemark: Using the above hint, you can show that the desired inequality holds for all real numbers $a$, $b$, and $c$.\n",
    "tags": [
      "algebra-precalculus",
      "proof-writing"
    ],
    "score": 5,
    "answer_score": 10,
    "is_accepted": true,
    "question_id": 982891,
    "answer_id": 982909
  },
  {
    "theorem": "How do I show that the set of odd natural numbers is closed under the operation * defined by a*b=a+b+ab?",
    "context": "I really need help with this question. I am required to  show that the set of odd natural numbers is closed under the operation * defined by a*b=a+b+ab, and I'm not quite sure how. Any work/help is greatly appreciated.\n",
    "proof": "Let $a = 2n + 1$ and let $b = 2m + 1$ where $n, m \\geq 0$.\nWe want to show that the set of natural odd numbers are closed under the defined operation $*$. \nSo: $$a*b = a + b + ab$$\n$$= (2n + 1) + (2m + 1) + (2n + 1)(2m + 1)$$\n$$= (2n + 2m + 2) + (4nm + 2n + 2m + 1)$$\n$$= (4n + 4m + 4mn + 2) + 1$$\nThus $*$ is closed under the defined operation.\n",
    "tags": [
      "proof-writing",
      "natural-numbers",
      "binary-operations"
    ],
    "score": 5,
    "answer_score": 5,
    "is_accepted": true,
    "question_id": 725627,
    "answer_id": 725652
  },
  {
    "theorem": "What are the finite order elements of $\\mathbb{Q}/\\mathbb{Z}$?",
    "context": "I need to find what are the at the group $\\mathbb{Q}/\\mathbb{Z}$.\nI think that any element at this group has a finite order, but I don't know how to prove it... I'd like to get help with the proof writing...  \nIf I'm wrong, I'd like to to know it too...\nBTW:\n$\\mathbb{Z}=(\\mathbb{Z},+)$\n$\\mathbb{Q}=(\\mathbb{Q},+)$\nThank you!\n",
    "proof": "Yes, you're correct that this is a group in which every element has finite order. Since you've asked about proof writing, I'll write out essentially a full proof.\n\nChoose some $\\alpha \\in \\Bbb{Q} / \\Bbb{Z}$. By definition of the quotient group, there is a rational number $r$ for which\n$$\\alpha = r + \\mathbb{Z}$$\nNow by the definition of $\\mathbb{Q}$, there are integers $a$ and $b \\ne 0$ (we take $b > 0$) without any loss of generality) for which\n$$r = \\frac a b \\implies \\alpha = \\frac a b + \\mathbb{Z}$$\nTherefore,\n$$b\\alpha = b\\left(\\frac a b + \\Bbb Z\\right) = b \\frac a b + \\Bbb{Z} = a + \\Bbb{Z}$$\nBut since $a$ is an integer, $a + \\Bbb{Z} = \\Bbb{Z}$ is the identity in $\\Bbb{Q} / \\Bbb{Z}$, so $\\alpha$ has finite order (in fact, the order is at most $b$) as desired.\n",
    "tags": [
      "group-theory",
      "proof-writing",
      "divisible-groups"
    ],
    "score": 5,
    "answer_score": 11,
    "is_accepted": true,
    "question_id": 612702,
    "answer_id": 612704
  },
  {
    "theorem": "Does this proof contain circular logic?",
    "context": "I am trying to prove that $e^x$ is a solution to $f'(x)=f(x)$, and there is a point at which I am concerned that there might be circular logic.\nHere's what I've got so far:\n$$f'(x)=f(x)$$\n$$\\frac{f'(x)}{f(x)}=1$$\n$$\\int\\frac{f'(x)}{f(x)}dx=x+c_0$$\nLetting $y=f(x)$ gives\n$$\\int\\frac{dy}{y}=x+c_0$$\nWhich gives $$\\ln|y|=x+c_0$$\n$$f(x)=c_1e^x$$\nQED\nThe bit I'm concerned about is $\\int\\frac{dy}{y}=\\ln|y|$.\nIs there any proof of $$\\int\\frac{dx}{x}=\\ln|x|$$ \nwhich doesn't use $\\frac{d}{dx}e^x=e^x$?\nEdit: If it wasn't clear, I am asking if there is a way that one can prove that $$\\int\\frac{dx}{x}=\\ln|x|$$\nwithout relying on the fact that $\\frac{d}{dx}e^x=e^x$?\n",
    "proof": "Here's a way to remove the circularity.\nDefine $g(x) = \\int_1^x dt/t$. Then\n$$\ng(xy) = \\int_1^{xy}\\frac{dt}{t} = \\int_1^x\\frac{dt}{t} + \\int_x^{xy}\\frac{dt}{t} = \\int_1^x\\frac{dt}{t} + \\int_1^y\\frac{du}{u} = g(x)+g(y)\n$$\nwhere $t = xu$ is used for the substitution. This property is unique to logarithm functions, and its base will be the number $e$ such that $g(e) = 1$. So $g(x) = \\log_e(x)$, and $g^{-1}(x) = e^x$.\n",
    "tags": [
      "proof-verification",
      "logic",
      "proof-writing"
    ],
    "score": 5,
    "answer_score": 5,
    "is_accepted": true,
    "question_id": 2973913,
    "answer_id": 2974009
  },
  {
    "theorem": "How to prove a sequence does not converge?",
    "context": "I want to show that the sequence $$a_n=\\frac{1}{n}+(-1)^n$$ does not converge to a limit.\nI know that if a sequence $\\left( a_n\\right)_{n \\in \\mathbb{N}}$ converges to a limit L, then $\\forall\\epsilon >0\\;\\;\\exists N\\in\\Bbb N \\;\\text{such that}\\;\\forall n\\geq N, \\;\\;|a_n-L|\\leq\\epsilon$, but I am not sure how to use this to prove a sequence that does not converge to a limit.\nPlease give me some suggestions on where can I get started and how to write out the proof, thanks to anyone who can help.\n",
    "proof": "If a sequence $\\{ a_{n} \\}_{n = 1}^{\\infty}$ converges to a limit $L$, then every subsequence must converge (can you prove this part on your own?), and the subsequences must all converge to the same limit (can you prove this part on your own?).\nBut it is easy to prove that the subsequence $\\{ a_{2n} \\}_{n = 1}^{\\infty}$ of even indices converges to $1$ (can you do this on your own?) while the subsequence $\\{a_{2n - 1} \\}_{n = 1}^{\\infty}$ of odd indices converges to $-1$ (can you do this part on your own?).\nBecause of this, the sequence does not converge since it has two subsequences that converge to different limits.\n",
    "tags": [
      "real-analysis",
      "sequences-and-series",
      "proof-writing"
    ],
    "score": 5,
    "answer_score": 4,
    "is_accepted": true,
    "question_id": 981064,
    "answer_id": 981081
  },
  {
    "theorem": "Proof by Induction for Exponential Term",
    "context": "Prove by mathematical induction that $3^n\\geq n2^n \\text{ } \\forall n\\in \\mathbb{N}$ the set of natural numbers.  So after showing that $S(1)$ is true, we assume $S(k)$ is true and try to prove that $S(k+1)$ is true.  I make it this far, and get stuck with further justification:\n$$\\begin{align}\n3^k &\\geq k2^k \\\\\n3\\times 3^k &\\geq 2\\times k2^k \\text{ since } 3>2 \\\\\n3^{k+1} & \\geq k2^{k+1}\n\\end{align}$$\nBut now I have to somehow get that $k+1$ term in front of $2^{k+1}$, but I'm not sure how to do it in a way that's justified.  Perhaps I've taken a wrong starting approach.  How would you do this?\n",
    "proof": "Assume that $3^k \\ge k\\cdot 2^k$ for some $k$. So\n$3^{k+1}=3\\cdot 3^k \\ge 3\\cdot k\\cdot 2^k$.\nfor $k\\ge 2$, you have $3\\cdot k = 2\\cdot k+k \\ge 2\\cdot k+2 = 2(k+1)$, so\n$3^{k+1} \\ge 2\\cdot (k+1)\\cdot 2^k = (k+1)\\cdot2^{k+1}$\nbut, initially, you must show that S(1) and S(2) is true, because the induction step holds only for $k\\ge2$.\n",
    "tags": [
      "inequality",
      "proof-writing",
      "induction",
      "exponential-function",
      "integers"
    ],
    "score": 5,
    "answer_score": 6,
    "is_accepted": true,
    "question_id": 2644123,
    "answer_id": 2644142
  },
  {
    "theorem": "Prove a convex and concave function can have at most 2 solutions",
    "context": "Let $ a,b \\in \\mathbb{R}$\nLet $f(x) : [a,b] \\rightarrow \\mathbb{R} $ is a continuous function in $ [a ,b] $, differentiable and strictly convex in $ (a, b) $\nand $g(x) : [a,b] \\rightarrow \\mathbb{R} $ is a continuous function in $ [a ,b] $, differentiable and strictly concave in $ (a, b) $\nHow can I prove the intersection of $ f $ and $ g $ can have a maximum of two roots $f(x)-g(x)=0 $ ?\n",
    "proof": "If $f(x)$ is strictly convex and $g(x)$ is strictly concave, then $f(x) - g(x)$ is also strictly convex (the negative of a concave function is convex).\nA strictly convex function has its derivative monotonically increasing and so can have at most two zeroes.\nThis comes about because a differentiable function with three zeroes on a closed interval $[a,b]$ must have at least two extrema on $(a,b)$ (a consequence of Rolle's Theorem, I think) but having two extrema violates monotonicity of the derivative.\n",
    "tags": [
      "derivatives",
      "proof-writing",
      "roots"
    ],
    "score": 5,
    "answer_score": 4,
    "is_accepted": true,
    "question_id": 2112108,
    "answer_id": 2112128
  },
  {
    "theorem": "Sum of cubes proof",
    "context": "Prove that for any natural number n the following equality holds: \n$$ (1+2+ \\ldots + n)^2 = 1^3 + 2^3 + \\ldots + n^3 $$\nI think it has something to do with induction?\n",
    "proof": "By induction:\nfor $n=1$ it works. Then, suppose it works for a $n$. Then,\n$$(1+...+n+(n+1))^2=\\underbrace{(1+...+n)^2}_{=1^3+...+n^3\\ by\\ hyp.}+2(1+...+n)(n+1)+(n+1)^2$$$$=1^3+...+n^3+(n+1)\\big(2\\underbrace{(1+...+n)}_{=\\frac{n(n+1)}{2}}+(n+1)\\big)$$$$=1^3+...+n^3+(n+1)\\big(n(n+1)+(n+1)\\big)$$$$=1^3+...+n^3+(n+1)(n+1)(n+1)$$$$=1^3+...+n^3+(n+1)^3.$$\nQ.E.D.\n",
    "tags": [
      "discrete-mathematics",
      "proof-writing"
    ],
    "score": 5,
    "answer_score": 7,
    "is_accepted": false,
    "question_id": 973242,
    "answer_id": 973265
  },
  {
    "theorem": "Minimum of $x^2 + 3x - 1$ on $[0,1]$ and $[-2,2]$",
    "context": "Consider the problem of finding the absolute minimum of the function $f : [0,1] \\rightarrow \\mathbb{R}$ that satisfies $f(x)=x^2 + 3x - 1$ everywhere.\nSuppose we suspect, by graphical methods, that $f$ attains its absolute minimum at the point $0 \\in [0,1]$. Thus its absolute minimum is believed to equal $f(0)=-1$.\nHow would you prove, in a rigorous and logically explicit manner, that $f$ really does achieve its absolute minimum at the point $0$ in its domain? Please, make your premises explicit and identify all the major theorems you use.\nConsider also the function $g : [-2,2] \\rightarrow \\mathbb{R}$ with the same defining equation. Suppose you believe that $g$ attains its absolute minimum at $-3/2$ (which is a local minimum), thus its absolute minimum is $g(-3/2)=-13/4$. The same question applies: how would you confirm your suspicions?\n",
    "proof": "For any $x\\in [0,1]$, $f(x)=x^2+3x-1\\geq 0+0-1=-1$.\nFor any $x\\in [-2,2]$, $g(x)=x^2+3x-1=(x+\\frac 32)^2-\\frac{13}{4}\\geq 0-\\frac{13}{4}=-\\frac{13}{4}$.\n",
    "tags": [
      "calculus",
      "real-analysis",
      "optimization",
      "proof-writing"
    ],
    "score": 5,
    "answer_score": 9,
    "is_accepted": false,
    "question_id": 344632,
    "answer_id": 344635
  },
  {
    "theorem": "Proof by going the reverse way",
    "context": "Proving a mathematical statement usually involves coming to the required result from a known/given result(s). Am I allowed to do this the other way round i.e. coming to a known result from the required statement? Here is a simple example:\nProof of AM-GM inequality for two numbers:\n$ (\\sqrt x - \\sqrt y)^2 ≥ 0 $\n$\\implies x + y - 2\\sqrt{xy} ≥ 0 $\n$\\implies \\dfrac{x + y}2 ≥ \\sqrt{xy} $\nMy presentation:\nIf\n$ \\dfrac{x + y}2 ≥ \\sqrt xy $\n$\\iff x + y ≥ 2\\sqrt{xy} $\n$\\iff x + y - 2\\sqrt{xy} ≥ 0$\n$ \\iff (\\sqrt x - \\sqrt y)^2 ≥ 0 $\n, which is true.\n$\\therefore (\\sqrt x - \\sqrt y)^2 ≥ 0 \\implies \\dfrac{x + y}2 ≥ \\sqrt xy$\nIs the second method considered a valid method of proof?\nI know the second proof is just the reverse of the former but in some cases, it seems easier to go from the required statement to the given.\n",
    "proof": "Your presentation is good, in fact almost perfect, however you should simply omit the initial word \"If\" which serves no purpose. The point of your presentation is that you are starting from the statement to be proved, then applying laws of algebra to find a sequence of equivalent statements, and then observing that the final statement of this sequence is true, hence all of them are true.\n",
    "tags": [
      "logic",
      "proof-writing"
    ],
    "score": 5,
    "answer_score": 7,
    "is_accepted": true,
    "question_id": 4125587,
    "answer_id": 4125597
  },
  {
    "theorem": "I don&#39;t understand how implication works in logic",
    "context": "The statement $(p \\wedge q ) \\Rightarrow (p \\Rightarrow q)$ is a tautology, i.e., always true.\nNow, imagine I am writing a proof and I want to show that $p \\Rightarrow q$. I show that statement $p$ is always true, then I show that statement $q$ is always true, then I can end my proof and claim $p \\Rightarrow q.$\nThis doesn't make sense to me. If I prove that $\\sqrt2$ is irrational and then I prove that $7$ is odd, I don't understand how \"$\\sqrt2$ is irrational\" $\\Rightarrow$ \"$7$ is odd\".\nCan someone help this disconnect I am having? Perhaps, you can provide information about what these logical statements actually mean and how they are used.\n",
    "proof": "Note that logical implication (aka ‘material conditional’) does not translate directly into our intuitive understanding of implication. We often think of implication as synonymous with causality, whereas the material conditional exists irrespective of causality. Mathematics isn’t a science, and its truths are almost always absolute (relative to the chosen axioms), which makes mathematical truth rather than causation its object of study. You can try and shift your thinking in this direction. Try and think of statements like $a\\implies b$ as “if $a$ is true, then $b$ also happens to be true” rather than “$a$ causes $b$”.\nNote that the main instrument of mathematics is proof, and as used in proofs, the material implication’s purpose is not to formalise some causal connection between mathematical facts, but rather to verify that one of the statements is indeed a fact. Implications of the sort you mention, where the consequent is tautologically true, are of little interest to mathematicians, even though they are correct owing to the truth-functional definition of the material conditional.\nTo help you come to terms with this definition, remember that the only situation where $a\\implies b$ fails to be true is when $a$ is true and $b$ is false. When doing proofs and saying that $b$ follows from $a$, what we really mean is that whenever $a$ is true, $b$ cannot possibly be false (hence is also true). The material conditional guarantees that. And it is that guarantee that is its sole purpose, and what makes making logical deductions in proofs at all possible.\n",
    "tags": [
      "logic",
      "proof-writing",
      "propositional-calculus"
    ],
    "score": 5,
    "answer_score": 11,
    "is_accepted": true,
    "question_id": 4068732,
    "answer_id": 4068748
  },
  {
    "theorem": "prove there is no smallest positive rational number",
    "context": "How would I prove there is no smallest positive rational number?\nwhat is the best method to prove this statement?\n",
    "proof": "Assume that $q$ is the smallest positive rational number. But then clearly $\\frac{q}{2}$ is also rational and \n$$0 < \\frac{q}{2} < q.$$\n",
    "tags": [
      "discrete-mathematics",
      "proof-writing"
    ],
    "score": 5,
    "answer_score": 6,
    "is_accepted": false,
    "question_id": 1171640,
    "answer_id": 1171646
  },
  {
    "theorem": "Proving an infinite number of primes of the form 6n+1",
    "context": "The proofs given on other sites weren't that clear and used different methods that I have yet to learn.\n\nProve that there are an infinite number of primes of the form 6n+1.\nThe hint that was given was:\nLet p = p1, p2, ..., pk + 1, where p1 = 2, p2 = 3,...pk are the first k primes.  Show that p is prime.\n(p1 means p sub 1, p2 means p sub 2, and pk is p sub k.  Wasn't sure how to write it on this.)\n\nCan someone explain this hint on how they came about of p1 = 2, p2=3, etc, and prove this please?  \nAlso, how would the proof change if the form changed?  (\"Prove that there are infinite number of primes of the form....\")\n\n\n\n",
    "proof": "Hint is clearly false. Let me answer a question that was not asked as it used the hint that is provided. I realize that this does not help OP but hopefully it will help the OP in some other problem.\nShow that there are infinitely many primes of the form $6n-1$\nSuppose not. Let there be only finitely many primes of the form $6n-1$, say $p_1$, $p_2$ $\\cdots$, $p_k$. Let\n$$\nP = 6 p_1 p_2 p_3 \\cdots p_k - 1\n$$\nNow every prime is either of the form $6n-1$ or $6n+1$ and product of any two numbers of the form $6n+1$ is also of the form $6n+1$. So the question is\nWhat are the prime dividers of $P$?\nThey all can't be of the form $6n+1$ since $P$ is of the form $6n -1$. So it must have at least one prime factor of the form $6n-1$. Clearly $p$ is not divisible by any of the primes $p_1$, $p_2$, $\\cdots$ $p_k$. So there has to be a prime of the form $6n-1$ which is different from these primes. Hence, there has to be infinitely many primes of the form $6n-1$.\nI fully realize that this does not answer OP's question but the method and the hint is similar. Hope this helps\n",
    "tags": [
      "elementary-number-theory",
      "proof-writing"
    ],
    "score": 5,
    "answer_score": 9,
    "is_accepted": true,
    "question_id": 671820,
    "answer_id": 671854
  },
  {
    "theorem": "Why do irrationality proofs of $\\sqrt x$ not apply when $x$ is a perfect square?",
    "context": "When trying to prove that a particular root (say $\\sqrt{2}$ or $\\sqrt{10}$) cannot be rational, I always see a particular indirect proof that goes something like this:\nSuppose $\\sqrt{x}$ were rational; then, there would be two integers $a$ and $b$ such that $a/b$ was $\\sqrt{x}$. We can also assume that $a$ and $b$ have no common factors, because we can simplify $a/b$ as much as we want before we begin.\nThen, $(a/b)^2 = x$, or $a^2/b^2=x$, or $a^2 = x * b^2$. But, if $a * a$ is a multiple of $x$ then $a$ must also be a multiple of $x$, so we can rewrite $a$ as $a = cx$ and substitute:\n$(c * x)^2 = x * b^2$, or $x^2 * c^2 = x * b^2$. Divide by $x$ and we get that $b^2 = x * c^2$.\nBut we've now shown that both $a$ and $b$ have a factor of x, contradicting our original assumption; this means that $\\sqrt{x}$ cannot be rational.\nI see why this proof works, what I don't see is why you can't plug in a number with an actual rational root, like 16, and not form the same proof that $\\sqrt{16}$ cannot be rational.\n",
    "proof": "The proof will work for any integer $\\rm\\,x\\,$ (or rational) that is not a perfect square, since it will have a prime factor occurring to an odd power, which is precisely what's needed for the proof to succeed. From $\\rm\\:a^2 = x b^2\\:$ we deduce, by unique prime factorization, that the power of every prime $\\rm\\:p\\:$ dividing $\\rm\\:x\\:$ must be even, being the difference of two even integers (the power of $\\rm\\:p\\:$ in $\\rm\\:a^2\\:$ minus the power in $\\rm\\:b^2.\\,$\nSimilarly for $\\,\\rm n$'th roots: from $\\rm \\,a^n = x b^n\\,$ we deduce that all primes occur to powers divisible by $\\rm\\,n,\\,$ so $\\,\\rm x\\,$ is an $\\rm\\,n$'th power.\nGenerally it is not true that $\\rm\\:x\\:|\\:a^2\\:\\Rightarrow\\:x\\:|\\:a\\ $ (e.g. let $\\rm\\:x = a^2 > 1)$. In fact this property is true iff $\\rm\\:x\\:$ is squarefree, which is why the proof works for $\\rm\\:x\\:$ prime (or a product of distinct primes) which, having at least one prime to the power one, certainly does have a prime occurring to an odd power. The general case reduces to this case by pulling the square part of $\\rm\\:x\\:$ out of $\\rm\\:\\sqrt{x},\\:$ i.e. if $\\rm\\:x = n^2 y,\\:$ with $\\rm\\:y\\:$ squarefree then we have $\\rm\\:\\sqrt{x} = \\sqrt{n^2 y} = n\\sqrt{y}\\in \\mathbb Q\\iff\\sqrt{y}\\in\\mathbb Q.$ Therefore your method of proof will work if you first reduce this way to the case where $\\rm\\:x\\:$ is squarefree.\n",
    "tags": [
      "elementary-number-theory",
      "proof-writing",
      "irrational-numbers",
      "rationality-testing"
    ],
    "score": 5,
    "answer_score": 6,
    "is_accepted": true,
    "question_id": 162119,
    "answer_id": 162122
  },
  {
    "theorem": "On Mathematical Induction",
    "context": "Let's suppose we are asked to prove $1+2+\\ldots+n=\\frac{n(n+1)}{2}$, for a natural number $n$. Is the use of mathematical induction inevitable in this situation? For instance, what makes the following proof not mathematically sound?\n$S=1+\\dots+n$\n$S=n+\\dots+1$\nAdding both sides of the two equations gives $2S=\\overbrace{(n+1)+\\dots+(n+1)}^{n \\text{ times}}$, and dividing through by $2$ yields the result.\n",
    "proof": "OK, you proved that $$1 + 2 + \\dots + n = \\frac{n(n+1)}{2}$$ without explicitly using induction. \nBut your proof relies, often implicitly, on a lot other results in arithmetic. For instance, that addition is commutative. And how do you prove commutativity? Trust me, you need induction (see here for instance). Maybe you can find a proof of commutativity that do not use induction explicitly, but it necessarily uses other lemmas that rely on induction. \nSo, also your proof relies (indirectly) on induction. \nThe \"necessity\" of induction in arithmetic to prove non-trivial properties of natural numbers has been formalized for the first time by Peano.\nIf you are interested to know what you can prove in arithmetic without never using induction (not even implicitly), see Robinson arithmetic.\n",
    "tags": [
      "algebra-precalculus",
      "discrete-mathematics",
      "logic",
      "proof-writing",
      "induction"
    ],
    "score": 5,
    "answer_score": 8,
    "is_accepted": true,
    "question_id": 3583357,
    "answer_id": 3583380
  },
  {
    "theorem": "Direct telescopic proof for sum of $1^2+2^2+...+n^2$",
    "context": "I was teaching $$\\sum_{k=1}^{n}k=\\frac{n(n+1)}{2}\\\\\\sum_{k=1}^{n}k^2=\\frac{n(n+1)(2n+1)}{6}$$ for the first I did direct telescopic proof like below \n$$\\sum_{k=1}^{n}k=\\sum_{k=1}^{n}k(\\frac{k+1}{2}-\\frac{k-1}{2})=\\sum_{k=1}^{n}(\\frac{k(k+1)}{2}-\\frac{k(k-1}{2}))=\\\\\\sum_{k=1}^{n}(f(k)-f(k-1))=\\frac{n(n+1)}{2}-0$$\nfor the second I did a classic proof $$\\begin{align}\n\\sum_{k=1}^n k^2\n& = \\frac{1}{3}(n+1)^3 - \\frac{1}{2}n(n+1) - \\frac{1}{3}(n+1) \\\\\n& = \\frac{1}{6}(n+1) \\left[ 2(n+1)^2 - 3n - 2\\right] \\\\\n& = \\frac{1}{6}(n+1)(2n^2 +n) \\\\\n& = \\frac{1}{6}n(n+1)(2n+1)\n\\end{align}$$ some of the student asked for direct telescopic proof for the case...\nI can't find this kind of proof. can anybody help me to find, or write this kind proving.  \\\nI tried to rewrite $1=\\frac{k+1}{2}-\\frac{k-1}{2}$and I have \n$$\\sum_{k=1}^{n}k^2(\\frac{k+1}{2}-\\frac{k-1}{2})=$$ I can't go further more.\nI promised to my students to try to find a direct proof Idea. Thanks for any help.\n",
    "proof": "Use binomial coefficients. Write $k^2=2\\binom{k}{2}+\\binom{k}{1}$ so $\\sum_{k=1}^nk^2=\\sum_{k=1}^n(a_k-a_{k-1})$ with $$a_n=2\\binom{n+1}{3}+\\binom{n+1}{2}=\\frac13 n(n^2-1)+\\frac12 n(n+1)=\\frac16 n(n+1)(2n+1).$$\n",
    "tags": [
      "sequences-and-series",
      "proof-writing",
      "alternative-proof"
    ],
    "score": 5,
    "answer_score": 6,
    "is_accepted": true,
    "question_id": 3369847,
    "answer_id": 3369852
  },
  {
    "theorem": "Considering sets in proofs",
    "context": "In order to show that $\\forall x\\gt0, \\ \\exists n_0\\in \\mathbb N: \\ n_0(n_0+1)\\le x\\lt (n_0+1)(n_0+2)$, the proof in my textbook considers the set $A=\\{n\\in \\mathbb N :n(n+1)\\le x\\}$ and then goes to show that $A$ has a maximal element $n_0$, which completes the proof.  \nNow, I want to know what has led to this kind of argument, because I have begun to see it very frequently and even some of my classmates often use this method. But to me it doesn't seem to be very useful and I certainly wouldn't be able to tell if it's a good way to approach a certain proof.  \nI understand that such motivations usually can't be explained easily; in that case, I'm asking for other problems you know of that use a similar approach.  \n",
    "proof": "Everyone has their own way of proving things, and that's OK. The statement you made can be shown to be true in different ways, and what counts is that you prove it, not how you prove it. The way you learn how to prove it is that you prove many many other statements as well, and then you get used to it. Repetitio mater studiorum est.\nHowever, if you want a path that leads to the particular proof, in this case, my thoughts would proceed as follows:\n\nI look at the statement. OK, it's saying that I can squeeze any positive real $x$ between two numbers. OK, let's imagine the $x$ as some point on the positive real line.\nHmm, the two numbers, $n_0(n_0+1)$ and $(n_0+1)(n_0+2)$ are both integers.\nNot only are they integers, they are two integers from a monotonically increasing sequence of integers, $a_n = n(n+1)$.\nSo... this sequence of integers, it's really a series of points on the real line. Let's imagine them as crosses. (yes, really, I do that. Don't judge) The order I draw them in is left to right.\nSo what I now have is the statement that there exist two crosses so that the previous cross is to the left of $x$, and the next one is to the right. Well... sure they do! I just need to find the last cross on the left of $x$, and the next one must be on the right of it (otherwise, the previous one wasn't the last one!).\nOK, what I just said in point 5 can be said formally as \"I need to find the maximum value of $n$ such that $a_n < x$. This can be rewritten formally as finding a maximum element from some set.\n\nOnce this train of thought concludes, I go down to really writing down the proof, and the proof \"starts\" with introducing the set. Sure, the proof does, but the thought process that lead me to the proof started long before.\n",
    "tags": [
      "proof-writing"
    ],
    "score": 5,
    "answer_score": 11,
    "is_accepted": true,
    "question_id": 2986960,
    "answer_id": 2986971
  },
  {
    "theorem": "Showing the standard topology on $\\mathbb{R}$ has a countable basis",
    "context": "I would like to show that the standard topology on $\\mathbb{R}$ has a countable basis.\nAs far as I understand, the standard topology on $\\mathbb{R}$ is generated by open intervals. So I can construct one basis as $\\{(i,i+1),(i+1/2,i+3/2)\\}$ for $i\\in\\mathbb{Z}$ which is clearly countable.\nEDITED: Thanks to Mees de Vries for pointing it out that my example is not a base. So is there any better way to construct a base?\nIs it okay to construct one basis to show the existence of a basis? Or is there way to show in general without constructing a specific example?\nCould somebody please give some light on this? Thanks.\n",
    "proof": "Your set isn't a basis. If you intersect $(1,2)$ and $(3/2,5/2)$ (using $i=1$) you get $(3/2,2)$ which does not contain any of your \"basis\" elements. \nIf you use your set as a sub-basis, it still isn't enough because you can't generate open intervals shorter than 1/2 in length.\nThe standard approach to finding a countable basis for $\\mathbb{R}$ (or $\\mathbb{R}^n$) is to use the rational numbers.\nSince $\\mathbb{Q}$ is countable so is $\\mathbb{Q}^2$ (countable cartesian product countable is still countable). Consider open intervals with rational endpoints: $$B = \\{ (r,s) \\;|\\; r,s \\in \\mathbb{Q} \\}$$\nThen $B$ has the same cardinality as $\\mathbb{Q}^2$ (i.e. it's countable). \nThis forms a basis for $\\mathbb{R}$'s standard topology. To see this notice that these sets are open intervals (thus open in $\\mathbb{R}$'s topology). So every open set generated by this basis is a standard open set. Conversely, if $O$ is open in $\\mathbb{R}$ and $x \\in O$ then there is an open interval $(a,b) \\subseteq O$ such that $x \\in (a,b)$. Thus $a<x<b$. Pick any rational number between $a$ and $x$ (say $r$) and any rational number between $x$ and $b$ (say $s$). Then $a<r<x<s<b$ and so $x \\in (r,s) \\subseteq (a,b) \\subseteq O$. So $O$ is open relative to our new base $B$. Thus $B$ generates the standard topology.\n",
    "tags": [
      "general-topology",
      "proof-writing",
      "alternative-proof"
    ],
    "score": 5,
    "answer_score": 8,
    "is_accepted": true,
    "question_id": 1935684,
    "answer_id": 1935695
  },
  {
    "theorem": "Show that the closure of $A$ is the intersection of all closed sets containing $A$, topology proof needed",
    "context": "I want to show that given $(X, \\mathcal{T})$, we define $\\overline A = \\{x \\in X| \\forall U \\in \\mathcal{T}, x \\in U \\implies U \\cap A \\neq \\varnothing\\}$ (definition of closure from Munkres), then\n\nShow that $\\overline A = \\bigcap\\{C \\subseteq X | C \\text{ is closed }, A\n\\subseteq C\\}$\n\nI find this really hard to tackle because some unnaturalness in that $\\overline A$ is specified with respect to open sets, but then it is alternatively defined as intersection of closed sets..how to juggle between open and closed?\nSeveral other posts also doesn't help...\n\nThe closure of A is the smalled closed set containing A is proved in terms of accumulation points and limit points which I do not define\nProving that the closure of a subset is the intersection of the closed subsets containing it is defined wrt of metric spaces\n\nI am stuck on both inclusions and need some help\nAttempt:\n$(\\overline A \\subseteq \\bigcap\\{C \\subseteq X | C \\text{ is closed }, A\n\\subseteq C\\})$\n\n\nLet $x \\in \\overline A$, then $\\forall U \\in \\tau, x \\in U \\implies A\n   \\cap U \\neq \\varnothing$. We want to show that $x \\in \\bigcap\\{C\n   \\subseteq X | C \\text{ is closed }, A \\subseteq C\\}$\nSo we know that $x$ is contained in some $U' \\in \\mathcal{T}$ that\nhas non-empty intersection with $A$, $x$ not necessarily in $A$. \nLet $C_1$ be a closed set containing $A$, then $U' \\cap C_1 \\neq\n   \\varnothing$.  Let $C_2$ be a closed set containing $A$, then $U'\n   \\cap C_2 \\neq \\varnothing$. Assuming $C_1 \\subseteq C_2$, then   $U'\n   \\cap C_1 \\cap C_2 \\neq \\varnothing$. \nContinue this way, $U' \\cap \\bigcap\\limits_{\\alpha \\in I} C_\\alpha\n   \\neq \\varnothing$, where $\\bigcap\\limits_{\\alpha \\in I} C_\\alpha$ is\nthe intersection of all closed sets containing $A$ \nWe know already that $x \\in U'$, but from above how can we see that $x\n   \\in \\bigcap\\limits_{\\alpha \\in I} C_\\alpha$? From figure below, it\nseems that $x$ will not be in $\\bigcap\\limits_{\\alpha \\in I}\n   C_\\alpha$\n\n\n$( \\bigcap\\{C \\subseteq X | C \\text{ is closed }, A\n\\subseteq C\\} \\subseteq \\overline A)$\n\nLet $x \\in  \\bigcap\\{C \\subseteq X | C \\text{ is closed }, A\n   \\subseteq C\\}$, we want to show that $x \\in \\overline A$. It suffices\nto show that $\\forall U \\in \\mathcal{T}, x \\in U \\implies U \\in A\n   \\neq \\varnothing$.\nSince $x \\in \\bigcap \\{C\\}$, then there exists some closed set $C'\n   \\subseteq X$ such that $x \\in C'$. Let $U \\in \\mathcal{T}$ containing\n$x$, then we will show that $U \\cap A \\neq \\varnothing$\nWe know that $x \\in C' \\cap U$, then $x \\in \\bigcap{C} \\cap U$. At this point however I still don't know whether $U \\cap A \\neq \\varnothing$. Couldn't we have a case in figure below where $x \\in \\bigcap \\{C\\}$ and $x \\in U$, but $U \\cap A = \\varnothing$?\n\n\n",
    "proof": "You’re getting bogged down in the details of the definitions and thereby making it much harder than it really is.\nFor the first inclusion, start, as you did, with an arbitrary $x\\in\\operatorname{cl}A$. Let $C$ be any closed set such that $A\\subseteq C$. Suppose that $x\\notin C$: then $x\\in X\\setminus C$, and $X\\setminus C$ is open, so $(X\\setminus C)\\cap A\\ne\\varnothing$. But on the other hand we know that $A\\subseteq C$, so $A\\cap(X\\setminus C)=\\varnothing$. This contradiction shows that $x\\in C$, and since $C$ was an arbitrary closed set containing $A$, we conclude that\n$$\\operatorname{cl}A\\subseteq\\bigcap\\{C\\subseteq X:A\\subseteq C\\text{ and }C\\text{ is closed}\\}\\;.$$\nFor the opposite inclusion just observe that $\\operatorname{cl}A$ is one of the closed sets containing $A$, so if $x\\in\\bigcap\\{C\\subseteq X:A\\subseteq C\\text{ and }C\\text{ is closed}\\}$, then automatically $x\\in\\operatorname{cl}A$. It follows that\n$$\\bigcap\\{C\\subseteq X:A\\subseteq C\\text{ and }C\\text{ is closed}\\}\\subseteq\\operatorname{cl}A$$\nand hence that\n$$\\bigcap\\{C\\subseteq X:A\\subseteq C\\text{ and }C\\text{ is closed}\\}=\\operatorname{cl}A\\;.$$\n",
    "tags": [
      "real-analysis",
      "general-topology",
      "proof-verification",
      "proof-writing",
      "proof-explanation"
    ],
    "score": 5,
    "answer_score": 14,
    "is_accepted": true,
    "question_id": 1819839,
    "answer_id": 1819982
  },
  {
    "theorem": "Discrete Mathematics: $x\\leq y+\\epsilon \\implies x\\leq y$",
    "context": "\nLet $x$ and $y$ be real numbers. Prove that if $x\\leq y + \\epsilon$ for every positive real number $\\epsilon$, then $x\\leq y$.\n\nI would like a hint as to how to prove this. Thank you. Pictorial proof would be nice too.\n\nSo this is how I word-smithed the answer given by the author of my accepted answer:\nWe will proceed by showing the contrapositve. We are given that\n$$x\\leq y+\\epsilon \\implies x\\leq y,$$\nso the contrapositive is resolved as\n$$x > y \\implies x>y+\\epsilon,$$\nor by substituting $x-y = \\omega$ simply\n$$\\omega > 0 \\implies \\omega > \\epsilon,$$\nbut if $\\omega >0$, then $\\epsilon = \\frac{\\omega}{2} > 0$; however, $\\omega \\leq \\frac{\\omega}{2}$ is false. Therefore, we can assert that for all $\\epsilon > 0$\n$$\\omega\\leq\\epsilon \\implies \\omega \\leq 0;$$\nquod erat demonstrandum.\nHere is my pictorial representation:\n\n",
    "proof": "The claim is equivalent to showing that if $\\omega\\leq \\epsilon$ for each $\\epsilon >0$, then $\\omega\\leq 0$.\nBut, if $\\omega>0$, then $\\epsilon=\\frac \\omega 2 >0$ and $\\omega \\leq \\frac\\omega 2$ does not hold. Having proven the contrapositive, we can assert that $$(\\forall\\epsilon >0\\;:\\;\\omega\\leq\\epsilon )\\implies \\omega \\leq 0$$\nNow let $\\omega =x-y$.\nPictorially If for any $\\color{green}{\\epsilon >0}$ we choose, $\\omega$ is to the left of  $\\color{green}{\\epsilon}$, it must be the case $\\omega$ is to the left of the green bar, that is, on the red side $\\color{red}{\\omega <0}$ (strictly negative numbers) or that it is on the breaking point, that is $\\color{orange}{\\omega=0}$.\n\nADD The contrapositive of the assertion is $$\\omega >0\\implies (\\exists \\epsilon >0:\\omega\\not\\leq \\epsilon)$$ or, which is the same, $$\\omega >0\\implies (\\exists \\epsilon >0:\\omega> \\epsilon)$$\nWe proved the contrapositive with $\\epsilon =\\omega /2$.\n",
    "tags": [
      "calculus",
      "proof-writing"
    ],
    "score": 5,
    "answer_score": 11,
    "is_accepted": true,
    "question_id": 438477,
    "answer_id": 438481
  },
  {
    "theorem": "Proof of $n^2 \\leq 2^n$.",
    "context": "I am trying to prove that $n^2 \\leq 2^n$ for all natural $n$ with $n \\ne 3$.\nMy steps are: \n\ninduction base case: $n=0:$ $0² \\leq 2⁰$ which is okay.   \ninductive step: $n \\rightarrow n+1:$ $(n+1)²\\leq2^{n+1}$ $$(n+1)^2 = n^2 + 2n + 1 = ...help...\\leq 2^{n+1}$$\n\nI know the bernoulli inequality but don't know where to use it, if I even need to. I have problems when it comes to proving things which are based on orders.. \n",
    "proof": "First since one must have $n\\neq 3$, the induction base must be $n=4$.\nFor the induction step: Suppose $n^2\\le 2^n$. \nThen,\n$$(n+1)^2=n^2+2n+1\\le 2^n+2n+1\\le 2^n+2^n=2^{n+1}$$\nbecause $2n+1\\le 2^n$ for $n\\ge 3$ (why is this true?).\nIf you had started with inductive base $0,1$ or $2$, then you would have ran into problems because $2n+1\\le 2^n$ doesn't hold for $n=2$\nProof of $2n+1\\le 2^n$ for $n\\ge 3$.\nInduction base: For n=3, $$2\\cdot 3+1=7\\le 8=2^3$$\nInduction step: Assume that for $n\\ge 3$, $2n+1\\le 2^n$. Then\n$$2(n+1)+1=2n+1+2\\le 2^n+2\\le 2^n+2^n=2^{n+1}$$\nand so we are done\n",
    "tags": [
      "inequality",
      "proof-writing",
      "induction",
      "problem-solving"
    ],
    "score": 5,
    "answer_score": 9,
    "is_accepted": true,
    "question_id": 263825,
    "answer_id": 263828
  },
  {
    "theorem": "Proof by induction:$\\frac{3}{5}\\cdot\\frac{7}{9}\\cdot\\frac{11}{13}\\cdots\\frac{4n-1}{4n+1}&lt;\\sqrt{\\frac{3}{4n+3}}$",
    "context": "In the very beginning I'm going to refer to similar posts with provided answers:\nInduction Inequality Proof with Product Operator $\\prod_{k=1}^{n} \\frac{(2k-1)}{2k} \\leq \\frac{1}{\\sqrt{3k+1}}$ (answered by Özgür Cem Birler)\nProve that $\\prod\\limits_{i=1}^n \\frac{2i-1}{2i} \\leq \\frac{1}{\\sqrt{3n+1}}$ for all $n \\in \\Bbb Z_+$ \nI examined the solutions and tried to apply the methods used there to make sure whether I understand it or not. I'm concerned about the step of induction.\nA task from an earlier exam at my university:\n\nProve by induction:\n$$\\frac{3}{5}\\cdot\\frac{7}{9}\\cdot\\frac{11}{13}\\cdots\\frac{4n-1}{4n+1}<\\sqrt{\\frac{3}{4n+3}}$$\n\nAttempt:\nrewritten:\n$$\\prod_{i=1}^n\\frac{4i-1}{4i+1}<\\sqrt{\\frac{3}{4n+3}}$$\n$(1)$ base case: $\\tau(1)$\n$$\\frac{3}{5}=\\sqrt{\\frac{4}{7}}<\\sqrt{\\frac{3}{7}}$$\n$(2)$ assumption: \nLet:$$\\frac{3}{5}\\cdot\\frac{7}{9}\\cdot\\frac{11}{13}\\cdots\\frac{4n-1}{4n+1}<\\sqrt{\\frac{3}{4n+3}}$$\nhold for some $n\\in\\mathbb N$\n$(3)$ step: $\\tau(n+1)$\n$$\\frac{4n+3}{4n+5}\\cdot\\prod_{i=1}^n\\frac{4i-1}{4i+1}<\\frac{4n+3}{4n+5}\\cdot\\sqrt{\\frac{3}{4n+3}}=\\frac{\\sqrt{3(4n+3)}}{4n+5}<\\sqrt{\\frac{3}{4n+7}}$$\n$$\\frac{12n+9}{16n^2+40n+25}<\\frac{3}{4n+7}\\iff\\frac{48n^2+120n+63-48n^2-120n-75}{\\underbrace{(4n+5)^2(4n+7)}_{>0}}<0\\iff-\\frac{12}{(4n+5)^2(4n+7)}<0$$\nIs this combined legitimate?\n",
    "proof": "I concur with everyone else that it's basically right. Everyone has their own style, but the following is how I would probably write up the \"meat and potatoes\" of the proof:\n\nLet's start with some preliminary observations. Note that\n\\begin{align}\n\\frac{4n+3}{4n+5}\\cdot\\sqrt{\\frac{3}{4n+3}}\n&=\\frac{4n+3}{4n+5}\\cdot\\frac{\\sqrt{3}}\n{\\sqrt{4n+3}}\\\\[1em]\n&=\\frac{\\sqrt{4n+3}\\cdot\\sqrt{3}}{4n+5}\\\\[1em]\n&= \\sqrt{\\frac{(4n+3)(3)}{(4n+5)^2}}\\\\[1em]\n&= \\sqrt{\\frac{12n+9}{16n^2+40n+25}}.\n\\end{align}\nMore concisely, we have\n$$\n\\frac{4n+3}{4n+5}\\cdot\\sqrt{\\frac{3}{4n+3}}=\\sqrt{\\frac{12n+9}{16n^2+40n+25}}.\\tag{1}\n$$\nAs another observation, note that\n\\begin{align}\n\\frac{12x+9}{16x^2+40x+25} < \\frac{3}{4x+7}\n&\\iff \\frac{12x+9}{16x^2+40x+25} - \\frac{3}{4x+7} < 0\\\\[1em]\n&\\iff \\frac{(48x^2+120x+63)-(48x^2+120x+75)}{(4x+5)^2)(4x+7)}<0\\\\[1em]\n&\\iff \\frac{-12}{(4x+5)^2(4x+7)}<0\\\\[1em]\n&\\iff \\frac{12}{(4x+5)^2(4x+7)}>0\\\\[1em]\n&\\iff x\\in\\Bigl(-\\frac{7}{4},-\\frac{5}{4}\\Bigr)\\cup\\Bigl(-\\frac{5}{4},\\infty\\Bigr).\n\\end{align}\nMore specifically, note that, for any natural number $n$, it follows from above that\n$$\n\\frac{12n+9}{16n^2+40n+25} < \\frac{3}{4n+7}.\\tag{2}\n$$\n\nNow your proof can flow very naturally:\n\\begin{align}\n\\prod_{i=1}^{k+1}\\frac{4i-1}{4i+1}\n&=\\prod_{i=1}^k\\frac{4i-1}{4i+1}\\cdot\\frac{4(k+1)-1}{4(k+1)+1}\\\\[1em]\n&<\\frac{4k+3}{4k+5}\\cdot\\sqrt{\\frac{3}{4k+3}}\\\\[1em]\n&=\\sqrt{\\frac{12k+9}{16k^2+40k+25}} & \\text{(by $(1)$)}\\\\[1em]\n&<\\sqrt{\\frac{3}{4k+7}} & (\\text{$\\sqrt{x}$ strictly increases and by $(2)$})\\\\[1em]\n&= \\sqrt{\\frac{3}{4(k+1)+3}}. & \\text{(desired conclusion)}\n\\end{align}\nMaybe that's overdrawn and slightly verbose, but that's how I would write it up if I were doing it for a class.\n",
    "tags": [
      "elementary-number-theory",
      "proof-writing",
      "induction",
      "proof-explanation"
    ],
    "score": 5,
    "answer_score": 4,
    "is_accepted": true,
    "question_id": 3537475,
    "answer_id": 3537509
  },
  {
    "theorem": "Show that $g(a) = g(b) = 0,\\ \\int_a^b f(x)g(x)dx=0 $ implies $f(x)=0$",
    "context": "\nSuppose $f$ is continuous on $[a, b]$, if for every continuous function $g$ on $[a, b]$ with $g(a) = g(b) = 0, \\int_{a}^{b}f(x)g(x) dx = 0$,  Show $f(x) = 0, \\forall x \\in [a, b]$, \n\nI want to prove by contradiction, and then find a continuous $g$ such that $g(a) = g(b) = 0$ but $\\int_{a}^{b}f(x)g(x) dx \\neq 0$\nProof: Suppose by contradiction that $f(x) > 0 $ for some $x_0 \\in [a, b]$. Since $f$ is continuous, $\\exists \\delta$ such that $f(x) > 0, \\forall x \\in [x_0 - \\delta, x_0 + \\delta]$. Take $g(x) = \\begin{cases}\n0 & \\text{ if } x \\in (a, x_0 - \\delta) \\cup (x_0+\\delta, b) \\\\ \n-(x - x_0 - \\delta)(x - x_0 + \\delta) & \\text{ if } x \\in (x_0 - \\delta, x_0 + \\delta) \n\\end{cases}$\nNow I want to show that since $g$ is continuous on $[a, b]$, then it is integrable. Thus $\\int_{a}^{b} g(x) dx = sup L(g, p)$. However, since the lower sum is $> 0$, it follows that the supremum is also $> 0$. Therefore $\\int_{a}^{b} > 0$. A contradiction. However, I do not know how to show that $L(g, p) > 0$\n",
    "proof": "Let $g(x)=f(x)(x-a)(b-x)$\n Then $$\\int_a^b f(x)^2 (x-a)(b-x) dx =0 $$\n Note that $$f(x)^2 (x-a)(b-x)\\geq 0,\\ (x-a)(b-x) > 0\\ (x\\in\n (a,b))$$\nIf for some $x\\in [a,b]$, $f(x)\\neq 0$ then since $f$ is continuous, there exists a closed\n  set $x\\in [s,t]$ : $$ f(x)^2\\geq c > 0\\ {\\rm on}\\ [s,t] \\subseteq [a,b]$$\nHence $$ \\int_a^b f(x)^2 (x-a)(b-x) dx \\geq \\int_s^t c (x-a)(b-x) dx\n> 0 $$ So contradiction.\n",
    "tags": [
      "real-analysis",
      "proof-verification",
      "proof-writing"
    ],
    "score": 5,
    "answer_score": 10,
    "is_accepted": true,
    "question_id": 1221795,
    "answer_id": 1229581
  },
  {
    "theorem": "Is &quot;$n$ is an integer and $\\frac{n}{n+1}$ is an integer&quot; true or false? ",
    "context": "I am working through a suggested exercise\n\"If $n$ is an integer, $\\frac{n}{n+1}$ is not an integer\" - I can prove this is false, and I can prove the converse is false, and I can prove the contrapositive is false.\nNow the question asks to show the negation and prove it true or false. Since the original is false by counter example $n=0$, I'm assuming the negation should prove true.\nThe negation of $p \\implies q$ is $p \\wedge ¬q$, so the negation in this case should be\n\"$n$ is an integer and $\\frac{n}{n+1}$ is an integer\" right?\nThis can be true ($n=0$) or false ($n=1$). So is the statement true or not? I assume not, so then the original is false AND the negation is false? getting very confused\n",
    "proof": "The statement\n\nIf $n$ is an integer, then $\\frac{n}{n+1}$ is not an integer.\n\nis strictly speaking neither true nor false before $n$ is given a concrete value. When you say you proved it to be false, I think what you really proved false is\n\nFor all $n$, if $n$ is an integer, then $\\frac{n}{n+1}$ is not an integer.\n\nIf you want a negation of this that you can prove true, you will have to negate the entire unfolded claim, giving:\n\nThere exists an $n$ such that $n$ is an integer and $\\frac{n}{n+1}$ is an integer.\n\n\nAlternatively you can choose to work with statements containing free variables, but then you have to group them into three classes:\nA. Those that are always true.\nB. Those that are sometimes true and sometimes false.\nC. Those that are always false.\nThen both of\n\n\nIf $n$ is an integer, then $\\frac{n}{n+1}$ is not an integer.\n\n$n$ is an integer and $\\frac{n}{n+1}$ is an integer.\n\n\n\n(which are negations of each other) belong in group B. When you proved that the first one \"is false\", what you really proved was just that it is not in group A.\n",
    "tags": [
      "discrete-mathematics",
      "proof-writing"
    ],
    "score": 5,
    "answer_score": 11,
    "is_accepted": true,
    "question_id": 193173,
    "answer_id": 193177
  },
  {
    "theorem": "Prove that $\\left|\\frac{x}{1+x^2}\\right| \\leq \\frac{1}{2}$",
    "context": "Prove that $$\\left|\\frac{x}{1+x^2}\\right| \\leq \\frac{1}{2}$$ for any number $x$.\nMy attempt:\n$$\\left|\\frac{x}{1+x^2}\\right| \\leq \\frac{1}{2} \\\\ \\iff \\frac{x}{1+x^2} \\geq -\\frac{1}{2} \\land \\frac{x}{1+x^2} \\leq \\frac{1}{2}$$ $$\\iff (x+1)^2 \\geq 0 \\land (x-1)^2 \\geq 0$$ the last two inequalities are obviously true, which concludes my proof attempt.\nNot sure if this is a correct way to prove the inequality, also it's clearly not very elegant.\nCould someone please verify my solution, and maybe suggest a more elegant or efficient approach?\n",
    "proof": "Your solution looks fine, as an alternative, by a single inequality, we have\n$$\\left|\\frac{x}{1+x^2}\\right| \\leq \\frac{1}{2} \\iff \\left|1+x^2\\right|\\ge 2|x| \n \\iff x^4-2x^2+1\\ge 0\\iff (x^2-1)^2 \\ge 0$$\nor also\n$$\\left|\\frac{x}{1+x^2}\\right| \\leq \\frac{1}{2} \\iff 1+x^2\\ge 2|x| \n \\iff x^2-2|x|^2+1\\ge 0\\iff (|x|-1)^2 \\ge 0$$\n\nAnother way by AM-GM\n$$\\frac{1+x^2}{2}\\ge \\sqrt{x^2}=|x|$$\n\nAnother way, by $x=\\tan \\theta$ we have\n$$\\left|\\frac{x}{1+x^2}\\right|= \\left|\\frac{\\tan \\theta}{1+\\tan^2 \\theta}\\right|=\\frac12|\\sin 2\\theta|\\le \\frac12$$\n\nAnother way, by rearrangement\n$$\\left|\\frac{1+x^2}{x}\\right|=\\frac{1+x^2}{|x|}=\\frac1{|x|}\\cdot 1+|x|\\cdot 1\\ge \\frac1{|x|}\\cdot |x|+1\\cdot 1= 2$$\n\nAnother one\n$$\\left|\\frac{x}{1+x^2}\\right|\\le \\frac12 \\iff \\frac{2|x|}{(|x|-1)^2+2|x|}\\le 1$$\n",
    "tags": [
      "analysis",
      "inequality",
      "solution-verification",
      "proof-writing"
    ],
    "score": 5,
    "answer_score": 12,
    "is_accepted": true,
    "question_id": 4530875,
    "answer_id": 4530888
  },
  {
    "theorem": "Prove that if $A$ is a positive definite matrix, then $A$ is non-singular.",
    "context": "First, going through what it means to be positive definite and non-singular:\nPositive definite implies\n\n$\\det(A) > 0$\nAll eigenvalues of $A$ are positive, and so $0$ is not an eigenvalue of $A$\n\nNonsingular implies\n\n$\\det(A) \\neq 0$\nAll eigenvalues of A are nonzero\nThe product of eigenvalues of $A$ $= \\det(A)$\n\nIt seems as though these two characterizations go hand in hand, though I assume negative eigenvalues could form a non-singular matrix but not a positive definite matrix. Can this be proven directly, or do I need to figure out how to prove by contradiction?\nThanks!\n",
    "proof": "Hint :\n$$\\det(A) > 0 \\Longrightarrow \\det(A) \\neq 0$$\n",
    "tags": [
      "linear-algebra",
      "matrices",
      "proof-writing",
      "positive-definite"
    ],
    "score": 5,
    "answer_score": 1,
    "is_accepted": true,
    "question_id": 3826080,
    "answer_id": 3826082
  },
  {
    "theorem": "Show that $\\mathbb{R}$ is Hausdorff.",
    "context": "\nShow that $\\mathbb{R}$ with standard topology is Hausdorff.\n\nFor any $x,y\\in\\mathbb{R}$ it is possible to define $\\mathscr{U}_x=(x-\\epsilon,x+\\epsilon)$ for $\\epsilon>0$ and $\\mathscr{U}_y=(y-\\delta,y+\\delta)$ so that $\\mathscr{U}_x\\cap\\mathscr{U}_y=\\emptyset$.\nQuestion:\nIs this proof right? If not. How should I answer the question? What tools should I use?\nThanks in advance!\n",
    "proof": "The argument is not complete, until you find $\\varepsilon$ and $\\delta$ such that\n$$\n(x-\\varepsilon,x+\\varepsilon)\\cap(y-\\delta,y+\\delta)=\\emptyset\n$$\nSince $x\\ne y$, it is not restrictive to assume $x<y$. In order the intersection above is empty, it's sufficient that\n$$\nx+\\varepsilon<y-\\delta\n$$\nthat is, $\\varepsilon+\\delta<y-x$. Take\n$$\n\\varepsilon=\\delta=\\frac{y-x}{3}\n$$\nand you're done.\n",
    "tags": [
      "real-analysis",
      "general-topology",
      "proof-verification",
      "proof-writing"
    ],
    "score": 5,
    "answer_score": 7,
    "is_accepted": true,
    "question_id": 2962278,
    "answer_id": 2962342
  },
  {
    "theorem": "Proving product of two column stochastic matrices is column stochastic (Proof verification)",
    "context": "For a matrix to be column stochastic we know $\\sum_{i=1}^nA_{ij}=1$ for each column $j\\in\\{1,\\ldots,n\\}$. We have $$\\sum_{j=1}^n (AB)_{ji} = \\sum_{j=1}^n \\sum_{k=1}^n A_{jk}B_{ki} = \\sum_{j=1}^nA_{jk} \\sum_{k=1}^nB_{ki} = 1*1 = 1  $$\n",
    "proof": "It works, but a simpler proof may be\n$$\ne^TAB = e^TB = e^T\n$$\n",
    "tags": [
      "linear-algebra",
      "proof-verification",
      "proof-writing",
      "markov-chains",
      "stochastic-matrices"
    ],
    "score": 5,
    "answer_score": 7,
    "is_accepted": true,
    "question_id": 2774019,
    "answer_id": 2774107
  },
  {
    "theorem": "Proof verification : $n\\cdot \\int_{0}^{1} x^n \\cdot f(x) \\, \\mathrm{d}x\\underset{n\\to+\\infty}{\\longrightarrow}f(1) $",
    "context": "Let $f: [0, 1] \\rightarrow \\mathbb{R}$ a continuous function. Proove that : \n$$n\\cdot \\displaystyle \\int_{0}^{1} x^n \\cdot f(x) \\, \\mathrm{d}x\\underset{n\\to+\\infty}{\\longrightarrow}f(1) $$\nI would like to know if what I've done is correct, because my book's solution isn't the same at all and seems more complicated.\nUsing Riemann sum we know that : \n$\\displaystyle \\lim_{n \\rightarrow \\infty}\\int_{0}^{1} x^n \\cdot f(x) \\mathrm{d}x = \\lim_{n \\rightarrow \\infty}\\frac{1}{n}\\cdot\\sum_{i = 0}^{n} g(\\frac{i}{n})$ where $g(x) = x^n \\cdot f(x)$.\nHence : $\\displaystyle \\lim_{n \\rightarrow \\infty} n\\cdot\\int_{0}^{1} f(x) \\cdot x^n \\mathrm{d}x = \\lim_{n \\rightarrow \\infty}\\sum_{i = 0}^{n}g(\\frac{i}{n})$\nYet note that for all $x \\in [0, 1[$ $\\displaystyle \\lim_{n \\rightarrow \\infty} x^n\\cdot f(x) = 0$ and for $x = 1$ we have $g(1) = f(1)$.\nThus : $\\displaystyle \\lim_{n \\rightarrow \\infty} n\\cdot\\int_{0}^{1} f(x) \\cdot x^n \\mathrm{d}x = f(1)$\n",
    "proof": "\"Using Riemann sum we know that :\" Why do we know that, exactly? Riemann sums allow us to say that for every fixed $n_0$, $$\\int_0^1 x^{n_0} f(x)dx = \\lim_{n\\to\\infty}\\frac{1}{n}\\sum_{i=0}^n g\\left(\\frac{i}{n}\\right)$$ with $g(x) = x^{n_0}f(x)$. This is not quite what you wrote... Note the absence of limits on the left hand side.\nIn particular, you are confusing the $n$ (which I wrote $n_0$ to avoid this ambiguity) on the left and the $n$ which goes to infinity in the Riemann sum. Correcting this, what you want to consider is actually\n$$\n\\lim_{n_0\\to\\infty}\\int_0^1 x^{n_0} f(x)dx = \\lim_{n_0\\to\\infty}\\lim_{n\\to\\infty}\\frac{1}{n}\\sum_{i=0}^n g_{n_0}\\left(\\frac{i}{n}\\right)\n$$\n(writing explicitly the dependence of $g$ on $n_0$). And now, essentially what you want to do in your approach is to swap the two limits in the RHS, writing $$\\lim_{n_0\\to\\infty}\\lim_{n\\to\\infty} = \\lim_{n\\to\\infty}\\lim_{n_0\\to\\infty}$$\nBut you cannot do that in general. You need assumptions for this swap to be correct, and that's the crux of the proof.\n",
    "tags": [
      "real-analysis",
      "integration",
      "proof-verification",
      "definite-integrals",
      "proof-writing"
    ],
    "score": 5,
    "answer_score": 5,
    "is_accepted": true,
    "question_id": 2620986,
    "answer_id": 2621015
  },
  {
    "theorem": "Induction in reverse",
    "context": "So the statement that I have to prove is as follows $P(n)=n^3 -n$ is divisible by $3$.\nNow I have to prove this with backwards induction for all negative numbers but I've already done the same thing but with \"straight\" induction for all positive numbers.\nDo I need to start from the case $P(n+1)$ and go from there to $P(1)$ or $P(-1)$?\nLikewise: \n$\\begin{align}\nP(n+1) & = (n+1)^3 - (n+1) \\\\\n& = n^3 + 3\\cdot n^2 + 3 \\cdot n + 1 - n + 1      \\\\\n& = 3n(n+1) + n^3 - n \n\\end{align}$\n$\\text{I.H.: So let's suppose the above statement holds for any}$ $n \\in \\mathbb { Z_0^{-}} $ \nThen I don't know anymore how to continue. \nFor the 2nd part I need to prove that the case of the negative numbers directly follows out of the positive case.\nI don't know how to get this part because I'm not sure of the previous case.\nPlease help me out.\n",
    "proof": "To show that it holds for all negative numbers by backwards induction (your part b)), show that it holds for $n = 0$ (or maybe start at $n=-1$), and then show that anytime it holds for some $n$, it also holds for $n-1$\nFor part d): $n^3-n = n(n^2-1) = n(n+1)(n-1) = (n-1)n(n+1)$. Since these are $3$ consecutive integers, one of them will be divisible by $3$\n",
    "tags": [
      "proof-verification",
      "proof-writing",
      "induction"
    ],
    "score": 5,
    "answer_score": 5,
    "is_accepted": true,
    "question_id": 2427127,
    "answer_id": 2427129
  },
  {
    "theorem": "Steps to prove or disprove if two rings are isomorphic",
    "context": "So i'm struggling on how to prove if two rings are not isomorphic to one another. My professor told me that if a ring is not isomorphic to another, the best way to prove that this is true is to find a preserved property of isomorphisms that is not held.\nSo i considered the following:\n1.Q and R (quotients and rationals)\n2.$~~\\mathbb{Z}/4\\mathbb{Z}\\times \\mathbb{Z}/4\\mathbb{Z}$ and $\\mathbb{Z}/16\\mathbb{Z}$ (Z mod 4 cross Z mod 4 and Z mod 16)\nI cannot seem to think of any of the properties:\ncommunicative, identity, integral domain, and field property that do not hold for rings. My professor told me this it isnt enough to give an example mapping like F: Q -> R and show that it isnt isomorphic.\nHence, how can i show that these two problems above arent isomorphic?\nCan anyone give me some finite steps to prove is something is isomorphic to something or not?\n",
    "proof": "HINT: $Z_{16}$ has an element of additive order $16$. Does $\\Bbb Z_4\\times\\Bbb Z_4$?\nAdded: You don’t want to confine your attention to ‘big’ properties like commutativity or being an integral domain; often the differences are only to be found at a more detailed level.\n",
    "tags": [
      "group-theory",
      "ring-theory",
      "proof-writing",
      "group-isomorphism"
    ],
    "score": 5,
    "answer_score": 9,
    "is_accepted": true,
    "question_id": 1724884,
    "answer_id": 1724888
  },
  {
    "theorem": "$(P\\implies Q) \\implies [(R ∨ P)\\implies (R ∨ Q)]$ is a tautology",
    "context": "I'm currently trying to work on the proof for this tautology.\nBut every time I derive the right side, I end up with a lone $R$ that will never cancel out.\nLike I always end up with\n$$(P\\implies Q) \\implies  ¬P∨Q∨R$$\nAnd the $R$ never goes away at this point.\nWhat can I do to cancel out the $R$?\n",
    "proof": "One way to do this is with the method of analytic tableaux, described in detail in M D’Agostino's Handbook of Tableaux Methods. This approach is somewhat informal but what it produces is often very suggestive of a proof (and is even equivalent to one in many semantics).\nWe start with the negation of $$(P\\to Q)\\to ((R\\vee P)\\to (R\\vee Q))\\tag{1}$$ then apply a series of contradiction-hunting rules to show that the negation is always false. Look:\n.\nThis ends in contradictions. Thus $(1)$ is indeed a tautology.\nI hope that helps $\\ddot\\smile$\n",
    "tags": [
      "logic",
      "proof-writing"
    ],
    "score": 5,
    "answer_score": 1,
    "is_accepted": true,
    "question_id": 661706,
    "answer_id": 662005
  },
  {
    "theorem": "prove $s(x+y)=s(x)s(y)$",
    "context": "I am asked to prove the following:\n\nLet $s(x):=\\sum_{n=0}^{\\infty}\\binom{x}{n}$. Then $s(x+y)=s(x)s(y)$.\n\nI don't know how to start. I am thinking about $\\exp(x)$ function with $\\sum_{n=0}^{\\infty}\\dfrac{x^n}{n!}$. Am I okay with this start? I would then begin like this: \n$$\\sum_{n=0}^{n}\\dfrac{x^n}{n!} \\cdot \\dfrac{y^n}{n!} = .. help = \\frac{1}{n!}(x+y)^n$$\nI am not sure how to continue or whether I am okay. \nThanks for help.\n",
    "proof": "First you must observe that the series converges absolutely.  Then we can use the fact that $\\sum a_{n}b_{n}=AB$ if $\\sum a_{n}=A$ and $\\sum b_{n}=B$ and at least one of the series is absolutely convergent.  With this, we compute:\n\\begin{align*}\ns(x)s(y)\n&=\\sum_{n=0}^{\\infty}\\frac{x^{n}}{n!}\\sum_{m=0}^{\\infty}\\frac{y^{n}}{n!}\\\\\n&=\\sum_{n=0}^{\\infty}\\sum_{j=0}^{n}\\frac{x^{j}y^{n-j}}{j!(n-j)!}\\\\\n&=\\sum_{n=0}^{\\infty}\\frac{1}{n!}\\sum_{j=0}^{n}\\binom{n}{j}x^{j}y^{n-j}\\\\\n&=\\sum_{n=0}^{\\infty}\\frac{(x+y)^{n}}{n!}\\\\\n&=s(x+y).\n\\end{align*}\n",
    "tags": [
      "calculus",
      "proof-writing"
    ],
    "score": 5,
    "answer_score": 3,
    "is_accepted": true,
    "question_id": 278427,
    "answer_id": 278435
  },
  {
    "theorem": "Is this a good enough proof by induction?",
    "context": "I am trying to understand induction. Is the proof good enough to satisfy what was looked for? If not, do you have any feedback for me?\n\nTo show: $\\sum_{i=0}^n i^2 ≤ n^3$ for any $n = 0, 1, 2,\\ldots$\n\nIf $P(n)$ holds then $P(n+1)$ holds as well $\\sum_{i=0}^{n+1} i^2 ≤ (n+1)^3$\n$$\\sum_{i=0}^{n+1} i^2 = \\sum_{i=0}^n i^2 + (n+1)^2 ≤ n^3 + (n+1)^2 ≤ (n+1)^3$$\n$$\\sum_{i=0}^n i^2 + (n+1)^2 ≤ n^3 + 3n^2 + 3n + 1$$\nBy the inductive hypothesis $\\sum_{i=0}^n i^2 ≤ n^3$\n$(n+1)^2 ≤ 3n^2 + 3n + 1$\n$3n+1 ≤ 3n^2$ and\n$(n+1)^2 ≤ 3n^2$\nThus, holds for $n+1$ and proof of induction is complete.\nThanks in advance.\n",
    "proof": "Although the proof seems right, it is not written correctly. Mainly it seems like you assume the claim throughout (even though you don't). Following would be a better way to write it:\nAssume $P(n)$ is true, that is $\\sum_{i=0}^n i^2 \\leq n^3.$ We want to show $\\sum_{i=0}^{n+1} i^2 \\leq (n+1)^3.$\nConsider\n\\begin{align*}\n\\sum_{i=0}^{n+1} i^2 &= \\sum_{i=0}^{n} i^2 + (n+1)^2\\\\\n&\\leq n^3 + (n+1)^2\\hspace{1cm} \\text{(by induction hypothesis)}\\\\\n&= n^3 + n^2+2n+1\\\\\n&\\leq n^3 +3n^2+3n+1\\\\\n&=(n+1)^3. \n\\end{align*}\nHence $P(n+1)$ holds.\n",
    "tags": [
      "proof-writing",
      "induction",
      "solution-verification"
    ],
    "score": 5,
    "answer_score": 7,
    "is_accepted": true,
    "question_id": 3628250,
    "answer_id": 3628286
  },
  {
    "theorem": "Sum of magnitudes of coefficients of polynomial $(x-1)(x-2)(x-3)\\cdots(x-(n-1))$",
    "context": "The title says most of it. I have found that the sum of the coefficients of the polynomial$(x-1)(x-2)(x-3)\\cdots(x-(n-1))$ yields $n!$. \nFor example, the coefficients of the polynomial $(x-1)(x-2)$ sum to $1+3+2=6=3!$ and similarly the coefficients of $(x-1)(x-2)(x-3)$ sum to $1+6+11+6=24=4!$. \nMy question, then is how might I prove this more generally. I have the feeling that induction might be an optimal way to go about this, but am unsure of the specifics. Any help is appreciated!\n",
    "proof": "Hint: Let\n$$f(x) = (x-1)(x-2)(x-3)...(x-(n-1)) \\tag{1}\\label{eq1A}$$\nNote $f(-1) = (-2)(-3)\\cdots(-n) = (-1)^{n-1}(n!)$. Consider what this value is in the expansion of $f(x)$ as a polynomial in $x$, plus how it relates to the sum of the magnitudes of the coefficients.\n",
    "tags": [
      "algebra-precalculus",
      "polynomials",
      "proof-writing"
    ],
    "score": 5,
    "answer_score": 6,
    "is_accepted": true,
    "question_id": 3538650,
    "answer_id": 3538655
  },
  {
    "theorem": "Proving isomorphic groups have the same number of subgroups.",
    "context": "Suppose $G$ and $H$ are groups with identities $e_G$ and $e_H$, respectively, and $\\theta :G\\to H$ is an isomorphism. Prove that if $G$ has exactly $n$ subgroups (where $n$ is some positive integer), then $H$ has exactly $n$ subgroups. \nI've been looking at this problem for my Abstract Algebra course and have been really stuck. I know that isomorphic groups have the same algebraic structure but my professor told me not to use that fact in the proof. I would greatly appreciate any help with this proof! \n",
    "proof": "If $A$ is a subgroup of $G$, then $\\theta(A) $ is a subgroup of $H$. You can prove this directly.\nNext, if $A_1$, $A_2$ are subgroups with $A_1\\neq A_2$, then $\\theta(A_1)\\neq\\theta(A_2)$.\nLastly, if $B$ is a subgroup of $H$, then $A=\\theta^{-1}(B)$ is a subgroup of $G$ with $\\theta(A) =B$.\nThis gives you a bijection between the subgroups of $G$ and the subgroups of $H$, hence in particular they have the same number. \n",
    "tags": [
      "abstract-algebra",
      "group-theory",
      "proof-writing",
      "normal-subgroups",
      "group-isomorphism"
    ],
    "score": 5,
    "answer_score": 4,
    "is_accepted": true,
    "question_id": 3458886,
    "answer_id": 3458894
  },
  {
    "theorem": "Is there a proof that all analytic functions only have one unique Taylor series representation?",
    "context": "I know that a function can admitted multiple series representation (according to Eugene Catalan), but I wonder if there is a proof for the fact that each analytic function has only one unique Taylor series representation. I know that Taylor series are defined by derivatives of increasing order. A function has one and only one unique derivative. So can this fact be employed to prove that each function only has one Taylor series representation?\n",
    "proof": "Well its possible for e.g. $f(x) = \\sum a_n x^n = \\sum b_n (x-1)^n$ simultaneously, but that probably isn't what you meant. Instead lets just consider the behavior at one point, say expanding around $x=0$.\nLet's fix notation-\n\nA \"power series\" (at $x=0$) is any series formally defined by $\\sum_{n=0}^\\infty a_n x^n$. A \"Taylor series\" (at $x=0$) for a smooth (i.e. $C^\\infty$) function $f$ is the power series formally defined by $\\sum_{n=0}^\\infty \\frac{f^{(n)}(0)}{n!} x^n$.\n\nSo any function that is infinitely differentiable (at $x=0$) has a unique Taylor series at 0 [note that the Taylor series may not converge, and if it converges, it may not converge to $f$]. But I think you are trying to ask if any \"analytic function\" (a term I haven't defined yet) is equal at each point to a unique power series, which is the Taylor series.\nYou can first prove the following result, which allows you to define the concept of \"analytic functions\"-\n\nTheorem 1. Any power series $\\sum_{n=0}^\\infty a_n x^n $ that converges at one $x_0$ where $|x_0|=\\rho>0$, converges absolutely and locally uniformly  on the set $|x|<\\rho $, where it defines a $C^\\infty$ function $F(x) := \\sum_{n=0}^\\infty a_n x^n$, and $ a_n = \\frac{F^{(n)}(0) }{n!}$.\nIn particular, the power series is the Taylor series of $F$. An \"analytic function\" (near $x=0$) is defined to be any such function $F$ that can be obtained in this way (i.e. an analytic function is a $C^\\infty$ function locally equal to a convergent power series, its Taylor series.)\n\nSuppose now that we have $\\lim_{N\\to\\infty}\\sum_{n=0}^N a_n x^n = 0$ for $|x|<r$. Then I claim that $a_n = 0$ for all $n$, proving the uniqueness of convergent power series for $f(x) = 0$. This immediately follows from Theorem 1 above, which allows us to talk of the function $F(x) := \\sum_{n=0}^\\infty a_n x^n$. But by hypothesis, $F$ is actually the zero function, so we have $a_n = \\frac{F^{(n)}(0) }{n!} = 0$.\nThis implies the uniqueness of convergent power series (at $0$)  for any analytic function; for if there were two different ones, their difference would be a nonzero convergent power series equal to 0, which doesn't exist.\nI'll sketch the proof of the main result (Theorem 1). We have convergence at $x=x_0$ where $|x_0|=\\rho$. Let $0<r<\\rho$. Then note that we have (from $\\sum_{n=0}^\\infty d_n $ exists implies $ d_n \\to 0$) $$a_n x_0^n \\xrightarrow[n\\to\\infty]{} 0 \\implies |a_n| |x_0|^n = |a_n|\\rho^n \\xrightarrow[n\\to\\infty]{} 0.$$ In particular there exists $M>0$ such that $|a_n| \\rho^n < M$ for all $n$. Therefore for any $x$ such that $|x|\\le r$, by Geometric Series formula, since $\\left(\\frac r{\\rho} \\right)<1$,\n$$  |a_n x|^n \\le   |a_n | r^n =  |a_n | \\rho^n \\left(\\frac r{\\rho} \\right)^n \\le M \\left(\\frac r{\\rho} \\right)^n, \\quad\\sum_{n=0}^\\infty M \\left(\\frac r{\\rho} \\right)^n < \\infty. $$\nSo by the Weierstrass M-test, in fact the series converges absolutely and uniformly (and therefore pointwise) on the closed disk $|x|\\le r$. It therefore defines a function, which we call $F(x)$.\nIf the series can be differentiated term-by-term, then a standard induction argument proves that $a_n = F^{(n)}(0)/n!$. Formally differentiating once, we formally obtain the series $\\sum_{n=1}^\\infty n a_n x^{n-1} = \\sum_{n=0}^\\infty (n+1) a_{n+1} x^n$. Now note that for $|x|\\le r<\\rho$,\n$$ |(n+1) |a_{n+1}| x^{n}| \\le (n+1) |a_{n+1}| r^{n} \\le (n+1) M \\left(\\frac{r}{\\rho}\\right)^n \\le CM \\left(\\sqrt{\\frac{r}{\\rho}}\\right)^{n}, \\\\ \\sum_{n=0}^\\infty CM \\left(\\sqrt{\\frac{r}{\\rho}}\\right)^{n}  < \\infty$$\nsince there exists $C>0$ such that $n+1 < C \\left(\\frac{\\rho}r\\right)^{n/2}$ for all $n$. By Weierstrass M-test, the formal series obtained by term-by-term differentiation converges absolutely and uniformly to some function $G$ on $|x|\\le r$, which implies that $F$ is differentiable with $F'=G$. This argument is repeatable (using instead $n^k < C_k \\left(\\frac{\\rho}r\\right)^{n/2}$), proving by induction that $F$ is $C^\\infty$, and validating the result $ a_n = F^{(n)}(0)/n!$\n.\n",
    "tags": [
      "calculus",
      "sequences-and-series",
      "proof-writing",
      "power-series"
    ],
    "score": 5,
    "answer_score": 5,
    "is_accepted": false,
    "question_id": 3433492,
    "answer_id": 3433667
  },
  {
    "theorem": "In the triangle $\\triangle ABC$, $|AB|^3 = |AC|^3 + |BC|^3$. Prove that $\\angle ACB &gt; 60^\\circ$.",
    "context": "The following was a question in the final of the Flanders Mathematics Olympiad 2018:\n\nIn the triangle $\\triangle ABC$, $|AB|^3 = |AC|^3 + |BC|^3$. Prove that $\\angle ACB > 60^\\circ$.\n\nIn this competition, points are assigned for formulating a rigorous and mathematically sound proof.\nI proved the above by contradiction. Let $\\alpha = \\angle BAC, \\beta = \\angle CBA, \\gamma = \\angle ACB$. Suppose $\\gamma \\le 60^\\circ$:\n$$\\gamma \\le 60^\\circ \\iff \\sin(\\gamma) \\leq \\frac{\\sqrt{3}}{2}$$\nApplying the sine rule:\n$$\\frac{\\sin(\\alpha)}{|BC|} = \\frac{\\sin(\\beta)}{|AC|} = \\frac{\\sin(\\gamma)}{|AB|}$$\n$$\\iff\\frac{|AC|^3}{|AB|^3} + \\frac{|BC|^3}{|AB|^3} = \\frac{\\sin^3(\\alpha) + \\sin^3(\\beta)}{\\sin^3(\\gamma)} = 1$$\n$$\\iff \\sin^3(\\alpha) + \\sin^3(\\beta) = \\sin^3(\\gamma) \\le \\left( \\frac{\\sqrt{3}}{2} \\right)^3$$\n$$\\iff\\sin(\\alpha) \\le \\frac{\\sqrt(3)}{2}, \\, \\sin(\\beta) \\le \\frac{\\sqrt(3)}{2}\\tag{1}$$\nWe also know that:\n$$\\alpha + \\beta = 180^\\circ - \\gamma \\ge 120^\\circ\\tag{2}$$\n$$\\alpha + \\beta < 180^\\circ\\tag{3}$$\nWithout loss of generality, assume $\\alpha \\ge \\beta$. From $(1)$, $(2)$ and $(3)$, it then follows that:\n$$a \\ge 120^\\circ, \\, \\beta \\le 60^\\circ$$\n$$\\implies |BC| > |AB| \\qquad \\unicode{x21af}$$\nIs this answer adequate enough? Can the notation be improved? Are there any alternative approaches to solve this problem?\n",
    "proof": "In the standard notation we need to prove that\n$$\\frac{a^2+b^2-c^2}{2ab}<\\cos60^{\\circ}$$ or\n$$c^2>a^2-ab+b^2$$ or\n$$\\sqrt[3]{(a^3+b^3)^2}>a^2-ab+b^2$$ or\n$$(a+b)^2(a^2-ab+b^2)^2>(a^2-ab+b^2)^3$$ or\n$$ab>0,$$ which is true.\nId est, $$\\measuredangle ACB>60^{\\circ}$$ and we are done!\n",
    "tags": [
      "geometry",
      "proof-verification",
      "proof-writing",
      "contest-math",
      "alternative-proof"
    ],
    "score": 5,
    "answer_score": 6,
    "is_accepted": true,
    "question_id": 3096866,
    "answer_id": 3097099
  },
  {
    "theorem": "Prove ten objects can be divided into two groups that balances each other when placed on the two pans of balance.",
    "context": "There are 10 objects with total weight 20, each of the weight being a positive integer. Given that none of the weights exceed 10, prove ten objects can be divided into two groups that balances each other when placed on the two pans of balance. Hints are appreciated.\nSorry I do not know how to start this problem, so I have not shown any efforts.\n",
    "proof": "This result holds in the general case with $2m$ objects of total weight $4m$, with none of the weights exceeding $2m$. \nIf a list P of positive integers sums to $n$ then P is called a partition of $n$. Let |P| denote the number of elements in P.\nLet S' be the set of partitions P' of $2n$ with |P'| = $n$ (i.e., each partition P' in S' has $n$ elements and P' sums to $2n$) with no element of any P' exceeding $n$.\nWe can transform S' into a set S by subtracting 1 from each element of each P' in S'. S is then the set of all partitions P of $n$ except for the singleton partition [$n$], since we've reduced the sum of each P' by $n$, and each |P| $ \\le n$. Conversely, given S we can recover S' by adding 1 to each existing element in each P and padding each P with extra ones to get the required length.\nSo we can attack the problem stated in the question by looking at the set of all partitions of 10 (except for the partition [10]), rather than the set of partitions of 20 of length 10 with no element exceeding 10.\nGiven a partition P of $n$, divide its elements into two lists A and B of approximately equal sums. Let $a$ be the sum of the elements of A, and $b$ be the sum of the elements of B; $a + b = n$. WLOG, assume $a \\ge b$. We want to minimize the difference $d = a - b$. This can be achieved using a simple \"greedy\" algorithm.\n\nSort P in descending order.\nFor each $p$ in P, append $p$ to whichever of A and B currently has the lowest sum.\nWhen finished, swap A & B, if necessary to ensure $a \\ge b$.\n\nProof that this algorithm does, in fact, minimize $d$ will be left as a exercise for the reader. :)\nNow we just need to add $a$ ones to B and $b$ ones to A to get our balanced weights. But this step is only valid if $|A| \\le b$ and $|B| \\le a$, since we must add a one to each existing element in A and in B.\n$1 \\le b \\le n/2 \\le a \\lt n $\n$n$ is even, so $n/2$ is an integer.\nClearly, for any P, the sum of its elements must be $ \\ge |P|$, since each element in P is at least 1.  \nSo $|B| \\le b \\le a$, thus $|B| \\le a$. It only remains to show that $|A| \\le b$.\nIf $d = 0$ then $a = b = n/2$ and so $|A| \\le n/2 = b$.\nWe now look at the case where $d > 0$.\n$a + b = n$, which is even, so $d = a - b$ is also even. And since it's non-zero in what follows, its minimum value is 2.\nFor any element $t$ in A, if $t < d$ we could swap $t$ over to B to reduce the difference: \n$$\\begin{align}\n\\text{Let }& 0 < t < d\\\\\n\\text{Let }d' & = (a-t) - (b+t)\\\\  \n& = a - b - 2t\\\\  \nd' & = d - 2t\\\\ \n-d & < -t < 0\\\\\n-2d & < -2t < 0\\\\\n-d & < d-2t < d\\\\\n-d & < d' < d\\\\\n|d'| & < d\n\\end{align}$$\nBut A and B have been constructed to minimize $d$, so all elements in A must be $ \\ge d$.\nIf |A| were $\\gt b$, it's minimal value would be $b+1$, and the minimal value of $a$ would thus be \n$$\\begin{align}\n(b + 1)d & = bd + d\\\\\n& = bd + a - b\\\\\n& = b(d-1) + a\\\\\n\\end{align}$$\nBut $d$ is an even integer > 0, so $b(d-1) + a \\ge b + a = n > a$, and we have a contradiction.\nThus $|A| \\le b$.\n\nUpdate\nThat indirect proof that $|A| \\le b$ has been annoying me. :) So here's a direct proof.\nEach element of A is $ \\ge d$, so $a \\ge |A|d$\n$$\\begin{align}\nd & > 1\\\\\nbd & > b\\\\\nbd + d & > b + d = a \\ge |A|d\\\\ \n(b + 1)d & > |A|d\\\\ \nb + 1 & > |A|\\\\\nb & \\ge |A|\\\\\n\\end{align}$$\n\nI found some very efficient algorithms for generating partitions via this answer to the question Algorithm for generating integer partitions. You can see the original Python code for that partitioning algorithm on Jerome Kelleher's site, and his paper discussing the efficiency of various partitioning algorithms on arXiv Generating All Partitions: A Comparison Of Two Encodings. Most partitioning algorithms found on the Net are recursive, so it's nice to see some efficient iterative algorithms. :) According to Jerome's paper, the optimized algorithm listed on his website is the most efficient partitioning algorithm known.\n\nIn closing, here are the results of some Python code I wrote while tackling this problem. Partitions are sorted on d and the partition elements.\n Balanced partitions for 10\n\n 1: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n 0, [1, 1, 1, 1, 1] 5, [1, 1, 1, 1, 1] 5\n[2, 2, 2, 2, 2], [2, 2, 2, 2, 2]\n\n 2: [2, 1, 1, 1, 1, 1, 1, 1, 1]\n 0, [2, 1, 1, 1] 5, [1, 1, 1, 1, 1] 5\n[3, 2, 2, 2, 1], [2, 2, 2, 2, 2]\n\n 3: [2, 2, 1, 1, 1, 1, 1, 1]\n 0, [2, 1, 1, 1] 5, [2, 1, 1, 1] 5\n[3, 2, 2, 2, 1], [3, 2, 2, 2, 1]\n\n 4: [2, 2, 2, 1, 1, 1, 1]\n 0, [2, 2, 1] 5, [2, 1, 1, 1] 5\n[3, 3, 2, 1, 1], [3, 2, 2, 2, 1]\n\n 5: [2, 2, 2, 2, 1, 1]\n 0, [2, 2, 1] 5, [2, 2, 1] 5\n[3, 3, 2, 1, 1], [3, 3, 2, 1, 1]\n\n 6: [3, 1, 1, 1, 1, 1, 1, 1]\n 0, [3, 1, 1] 5, [1, 1, 1, 1, 1] 5\n[4, 2, 2, 1, 1], [2, 2, 2, 2, 2]\n\n 7: [3, 2, 1, 1, 1, 1, 1]\n 0, [3, 1, 1] 5, [2, 1, 1, 1] 5\n[4, 2, 2, 1, 1], [3, 2, 2, 2, 1]\n\n 8: [3, 2, 2, 1, 1, 1]\n 0, [3, 1, 1] 5, [2, 2, 1] 5\n[4, 2, 2, 1, 1], [3, 3, 2, 1, 1]\n\n 9: [3, 2, 2, 2, 1]\n 0, [3, 2] 5, [2, 2, 1] 5\n[4, 3, 1, 1, 1], [3, 3, 2, 1, 1]\n\n10: [3, 3, 1, 1, 1, 1]\n 0, [3, 1, 1] 5, [3, 1, 1] 5\n[4, 2, 2, 1, 1], [4, 2, 2, 1, 1]\n\n11: [3, 3, 2, 1, 1]\n 0, [3, 2] 5, [3, 1, 1] 5\n[4, 3, 1, 1, 1], [4, 2, 2, 1, 1]\n\n12: [3, 3, 2, 2]\n 0, [3, 2] 5, [3, 2] 5\n[4, 3, 1, 1, 1], [4, 3, 1, 1, 1]\n\n13: [4, 1, 1, 1, 1, 1, 1]\n 0, [4, 1] 5, [1, 1, 1, 1, 1] 5\n[5, 2, 1, 1, 1], [2, 2, 2, 2, 2]\n\n14: [4, 2, 1, 1, 1, 1]\n 0, [4, 1] 5, [2, 1, 1, 1] 5\n[5, 2, 1, 1, 1], [3, 2, 2, 2, 1]\n\n15: [4, 2, 2, 1, 1]\n 0, [4, 1] 5, [2, 2, 1] 5\n[5, 2, 1, 1, 1], [3, 3, 2, 1, 1]\n\n16: [4, 3, 1, 1, 1]\n 0, [4, 1] 5, [3, 1, 1] 5\n[5, 2, 1, 1, 1], [4, 2, 2, 1, 1]\n\n17: [4, 3, 2, 1]\n 0, [4, 1] 5, [3, 2] 5\n[5, 2, 1, 1, 1], [4, 3, 1, 1, 1]\n\n18: [4, 4, 1, 1]\n 0, [4, 1] 5, [4, 1] 5\n[5, 2, 1, 1, 1], [5, 2, 1, 1, 1]\n\n19: [5, 1, 1, 1, 1, 1]\n 0, [5] 5, [1, 1, 1, 1, 1] 5\n[6, 1, 1, 1, 1], [2, 2, 2, 2, 2]\n\n20: [5, 2, 1, 1, 1]\n 0, [5] 5, [2, 1, 1, 1] 5\n[6, 1, 1, 1, 1], [3, 2, 2, 2, 1]\n\n21: [5, 2, 2, 1]\n 0, [5] 5, [2, 2, 1] 5\n[6, 1, 1, 1, 1], [3, 3, 2, 1, 1]\n\n22: [5, 3, 1, 1]\n 0, [5] 5, [3, 1, 1] 5\n[6, 1, 1, 1, 1], [4, 2, 2, 1, 1]\n\n23: [5, 3, 2]\n 0, [5] 5, [3, 2] 5\n[6, 1, 1, 1, 1], [4, 3, 1, 1, 1]\n\n24: [5, 4, 1]\n 0, [5] 5, [4, 1] 5\n[6, 1, 1, 1, 1], [5, 2, 1, 1, 1]\n\n25: [5, 5]\n 0, [5] 5, [5] 5\n[6, 1, 1, 1, 1], [6, 1, 1, 1, 1]\n\n26: [2, 2, 2, 2, 2]\n 2, [2, 2, 2] 6, [2, 2] 4\n[3, 3, 3, 1], [3, 3, 1, 1, 1, 1]\n\n27: [3, 3, 3, 1]\n 2, [3, 3] 6, [3, 1] 4\n[4, 4, 1, 1], [4, 2, 1, 1, 1, 1]\n\n28: [4, 2, 2, 2]\n 2, [4, 2] 6, [2, 2] 4\n[5, 3, 1, 1], [3, 3, 1, 1, 1, 1]\n\n29: [4, 3, 3]\n 2, [3, 3] 6, [4] 4\n[4, 4, 1, 1], [5, 1, 1, 1, 1, 1]\n\n30: [4, 4, 2]\n 2, [4, 2] 6, [4] 4\n[5, 3, 1, 1], [5, 1, 1, 1, 1, 1]\n\n31: [6, 1, 1, 1, 1]\n 2, [6] 6, [1, 1, 1, 1] 4\n[7, 1, 1, 1], [2, 2, 2, 2, 1, 1]\n\n32: [6, 2, 1, 1]\n 2, [6] 6, [2, 1, 1] 4\n[7, 1, 1, 1], [3, 2, 2, 1, 1, 1]\n\n33: [6, 2, 2]\n 2, [6] 6, [2, 2] 4\n[7, 1, 1, 1], [3, 3, 1, 1, 1, 1]\n\n34: [6, 3, 1]\n 2, [6] 6, [3, 1] 4\n[7, 1, 1, 1], [4, 2, 1, 1, 1, 1]\n\n35: [6, 4]\n 2, [6] 6, [4] 4\n[7, 1, 1, 1], [5, 1, 1, 1, 1, 1]\n\n36: [7, 1, 1, 1]\n 4, [7] 7, [1, 1, 1] 3\n[8, 1, 1], [2, 2, 2, 1, 1, 1, 1]\n\n37: [7, 2, 1]\n 4, [7] 7, [2, 1] 3\n[8, 1, 1], [3, 2, 1, 1, 1, 1, 1]\n\n38: [7, 3]\n 4, [7] 7, [3] 3\n[8, 1, 1], [4, 1, 1, 1, 1, 1, 1]\n\n39: [8, 1, 1]\n 6, [8] 8, [1, 1] 2\n[9, 1], [2, 2, 1, 1, 1, 1, 1, 1]\n\n40: [8, 2]\n 6, [8] 8, [2] 2\n[9, 1], [3, 1, 1, 1, 1, 1, 1, 1]\n\n41: [9, 1]\n 8, [9] 9, [1] 1\n[10], [2, 1, 1, 1, 1, 1, 1, 1, 1]\n\n",
    "tags": [
      "combinatorics",
      "proof-writing"
    ],
    "score": 5,
    "answer_score": 8,
    "is_accepted": true,
    "question_id": 1281770,
    "answer_id": 1287927
  },
  {
    "theorem": "Prove the third isomorphism theorem",
    "context": "I'm trying to prove the third Isomorphism theorem as stated below\nTheorem. Let $G$ be a group, $K$ and $N$ are normal subgroups of $G$ with $K⊆N$. Then $$(G⁄K)⁄(N⁄K)≅G⁄N.$$\nI look up for some answered on google, but I don't understand any of those. I wonder if any one can show me this proof step by step. I don't want to get it done, I want to understand why this theorem is true.\n",
    "proof": "Hint: Show that $\\phi(gK)=gN$ is a (well-defined) group homomorphism, and that its kernel is $N/K$. Then apply an (earlier) isomorphism theorem.\n",
    "tags": [
      "group-theory",
      "proof-writing",
      "abstract-algebra"
    ],
    "score": 5,
    "answer_score": 5,
    "is_accepted": true,
    "question_id": 366402,
    "answer_id": 366405
  },
  {
    "theorem": "Why is $(a+b)^3=a^3+3a^2b+3ab^2+b^3$ not a mathematical statement?",
    "context": "STATEMENT:\n$$(a+b)^3=a^3+3a^2b+3ab^2+b^3$$\nAs per my knowledge, a sentence is said to be a mathematical statement iff it fulfills both of the following criteria:\n\nThe sentence should be declarative.\n\nIt should have a definite truth value.\n\n\nThe given statement seems to fullfill both of the above criteria - it is indeed declarative in nature and is true for all values of a and b (both real and complex). It should, hence, be a mathematical statement but the textbook says otherwise.\nWhich part am I missing out on?\nSource: Mathematical Reasoning - Writing and Proof (Version 2.1) by Ted Sundstrom, Progress Check 1.1 - Question 8\n",
    "proof": "It is not a mathematical statement because its truth value is unclear. It is not known what $a$ and $b$ are. For example, if $a$ and $b$ are real numbers, then sure, the equation is correct. But if they are elements of a non-commutative structure, then the equation may not hold anymore.\nIn other words, the statement\n\n$\\forall a,b\\in\\mathbb R: (a+b)^3=a^3+3a^2b+3ab^2+b^3$\n\nis true, but the statement\n\n$\\forall a,b\\in\\mathbb R^{2\\times 2}: (a+b)^3=a^3+3a^2b+3ab^2+b^3$\n\nis not true. For example, you could take $$a=\\begin{bmatrix}0&1\\\\0&0\\end{bmatrix}, b=\\begin{bmatrix}0&0\\\\1&0\\end{bmatrix}$$\nfor a simple counterexample, since the left hand side evaluates to $a+b$ while the right hand side evaluates to a zero matrix.\n",
    "tags": [
      "proof-writing"
    ],
    "score": 5,
    "answer_score": 11,
    "is_accepted": true,
    "question_id": 4542130,
    "answer_id": 4542136
  },
  {
    "theorem": "Is there an accepted strategy when tackling proofs involving inequalities?",
    "context": "I am new to mathematical proofs, and I am not quite sure how to tackle problems of that nature. \n\nHere is the problem:\n\nProve that if $b≥-1$, $b≠0$, then $\\frac{4b^2+b+1}{4|b|} ≥ \\sqrt{b+1}$ \nI tried to rearrange the inequality on the right like so,\n $\\frac{4b^2+b+1-4|b|\\sqrt{b+1}}{4|b|} ≥ 0$\n\nbut this didn't really help me much. I am aware that I need to use the fact\nthat $b≥-1$ to proceed further, however, I don't know when it should come into play.\n\nI am also not exactly sure if I always need to start working with the latter part ($\\frac{4b^2+b+1}{4|b|} ≥ \\sqrt{b+1}$) when dealing with proofs of this kind?\nThanks a lot in advance!\n",
    "proof": "In general, there is no method that will solve any inequality. Inequalities need to be tackled case by case, although we can simplify particular forms given prior experience and knowledge. General mathematical intuition and perserverance will go a long way toward unfamiliar forms.\nA specific strategy is to change your variables so the condition is something nice. This usually makes the problem much easier to read and digest. Here it doesn't change too much but it's a good general strategy to keep in mind, so I will demonstrate it anyways.\nLet $b = c-1$. This means if $b \\ge -1$, then $c \\ge 0$. Now we can rewrite the inequality as:\n$$\n\\frac{4(c-1)^2+c}{4|c-1|} \\ge \\sqrt{c}\n$$\nNext, notice that there is a $|c-1|$ in the denominator and a $(c-1)^2$ in the numerator. This is very good because it will be very easy to reduce.\n$$\n\\frac{4(c-1)^2}{4|c-1|} + \\frac{c}{4|c-1|} \\ge \\sqrt{c} \\\\\n|c-1| + \\frac{c}{4|c-1|} \\ge \\sqrt{c}\n$$\nNow multiply out the denominator:\n$$\n4(c-1)^2 + c \\ge 4|c-1|\\sqrt{c} \\\\\n4(c-1)^2 - 4|c-1|\\sqrt{c} + c \\ge 0\n$$\nSetting $0$ on one side is very useful here because we know that $a^2 \\ge 0$ for all real $a$. We can try to see if the left side is a perfect square:\n$$\\left(2|c-1| - \\sqrt{c} \\right)^2 \\ge 0$$\nIt is in fact a perfect square! This means that as long as the expression in parentheses is real, we have a solution. The only way in which this works is if $\\sqrt{c}$ is positive, or in other words, $c \\ge 0$. We remove the $c = 1$ singularity and we are done!\nEDIT: @pidgeon's comment is excellent, because it takes an inequality that is true for an entire domain and applies it directly. You can take that inequality and immediately apply it to a problem if you can find the clever substitutions. This will be much faster than my more brute force approach, but it is the method I use when I don't have a better one.\nAnother really great inequality to always keep on hand is the AM-GM-HM inequality, which simply states that for any set of positive numbers, the arithmetic mean is at least the geometric mean, which is at least the harmonic mean.\n",
    "tags": [
      "inequality",
      "proof-writing",
      "a.m.-g.m.-inequality"
    ],
    "score": 5,
    "answer_score": 6,
    "is_accepted": true,
    "question_id": 3495663,
    "answer_id": 3495694
  },
  {
    "theorem": "Proving $\\sqrt{2}$ is irrational",
    "context": "I had my first lecture the University of Toronto, MAT157 with professor Meinrenken. I am struggling with proof #3 which he briefly walked us through. I do not understand the method of thinking and there are a lot of missing lines. Can someone guide me through how this proof works? I have understood the other two proofs.\nIf it is any help, I am also reading through Calculus 4e, by Michael Spivak. Any input on this proof is helpful. I understand mostly everything until the last three to four lines. Mainly, how do those statements relate to each other? There are some missing steps?\nProof. Let $$S = \\{x\\in\\mathbb R \\mid x=a + \\sqrt{2} b\\}$$ where both $a,b\\in \\mathbb Z$.\nS is closed under multiplication. If $x_1 , x_2 \\in S$ then $x_1 x_2 \\in S$\n$(a_1 + \\sqrt{2} b_1)(a_2 + \\sqrt{2} b_2) = (a_1 a_2 + 2b_1 b_2) + \\sqrt{2} (b_1 a_2 + b_2 a_1)$\nHence, if $x\\in S$, then $x^n= x ... x \\in S$ for all $n\\in\\mathbb N$\nIf $\\sqrt{2} = {p \\over q}$ then if $x\\in S$, then $qx\\in S$.\n$x = a + {p \\over q} b$ and $qx = qa + qb$\nNote $x = \\sqrt{2} -1 \\in S, 0 < x < 1$.\nSo $1 \\over x$ > 1\nChoose $n\\in \\mathbb N$ with $\\left({1 \\over x}\\right)^n> q, $ so $1 > qx^n > 0 $.\nEnd of proof.\nI am struggling specifically with the last three lines of the proof. The rest of the proof is simple for me to understand. \n",
    "proof": "Part of the problem is that you have written the proof in a stream-of-consciousness style; proofs of subclaims are mixed in with the main proof, and variables are used without any explanation of what they stand for.  Let me rewrite the proof so that it is easier to understand.\nProof:  Let $S = \\{x\\in\\mathbb{R} \\mid \\text{for some }a,b\\in\\mathbb{Z}, x = a+\\sqrt{2}b\\}$.\nClaim:  $S$ is closed under multiplication.\nProof of claim:  Suppose $x_1,x_2\\in S$.  Then there are integers $a_1$, $b_1$, $a_2$, $b_2$ such that $x_1 = a_1+\\sqrt{2}b_1$ and $x_2 = a_2+\\sqrt{2}b_2$.  Therefore\n$$\nx_1x_2 = (a_1+\\sqrt{2}b_1)(a_2+\\sqrt{2}b_2)= (a_1a_2+2b_1b_2) + \\sqrt{2}(b_1a_2+b_2a_1)\\in S.\n$$\nThis proves the claim.\nHence if $x \\in S$ then $x^n \\in S$ for every $n \\in \\mathbb{N}$.\nSuppose $\\sqrt{2}$ is rational.  Then there are positive integers $p$ and $q$ such that $\\sqrt{2} = p/q$.\nClaim:  For all $x \\in S$, $qx \\in \\mathbb{Z}$.\nProof of claim:  Suppose $x \\in S$.  Then there are integers $a$ and $b$ such that $x = a+\\sqrt{2}b = a+(p/q)b$.  Therefore $qx = qa+pb \\in \\mathbb{Z}$.  This proves the claim.\nNow let $x = \\sqrt{2}-1 \\in S$.  Note that $0 < x < 1$, so $1/x > 1$.  Therefore $\\lim_{n \\to \\infty} (1/x)^n = \\infty$.  Choose $n \\in \\mathbb{N}$ with $(1/x)^n > q$, so $0<qx^n<1$.  But by the claims above, $x^n \\in S$ and therefore $qx^n \\in \\mathbb{Z}$.  This is a contradiction, because there are no integers between 0 and 1.\n",
    "tags": [
      "real-analysis",
      "calculus",
      "proof-writing",
      "irrational-numbers"
    ],
    "score": 5,
    "answer_score": 10,
    "is_accepted": true,
    "question_id": 3347531,
    "answer_id": 3347568
  },
  {
    "theorem": "Prove that if $a$ and $b$ are real numbers with $0 &lt; a &lt; b$ then $\\frac{1}{b} &lt; \\frac{1}{a}$",
    "context": "\nSuppose a and b are real numbers. Prove that if $0 < a < b$\n  then $\\frac{1}{b} < \\frac{1}{a}$.\n\nMy attempt: \nGiven that $0<𝑎<𝑏$\nWe can write $b$ as $b = an$, where $n>1$\n$$\\tag1\\frac{1}{a} = \\frac{n}{an} = \\frac{n}{b} $$\n$$\\tag2\\frac{n}{b} > \\frac{1}{b} \\implies \\frac{1}{a} > \\frac{1}{b}$$\nIs it correct? \n",
    "proof": "Yes, your solution is correct.\nOf course you could have multiplied both sides of your inequality by $$\\frac {1}{ab}$$ to get the result in one shot.  \n",
    "tags": [
      "inequality",
      "proof-writing",
      "solution-verification"
    ],
    "score": 5,
    "answer_score": 11,
    "is_accepted": true,
    "question_id": 3295820,
    "answer_id": 3295829
  },
  {
    "theorem": "Do you ever use formal logic when working on proofs?",
    "context": "This question has the danger of being subjective but I nevertheless think people might learn from each other's answers.\nMathematics textbooks almost never write their proofs in formal logic, but in \"mathematical natural language\".\nWhat I mean is, that there are two ways to prove a statment: \n\nUsing natural language:   \"take any $\\epsilon$. Now define $\\delta(\\epsilon)$ such that ... Then clearly, for all ...\"\nUsing formal logic with logical deductive rules:\n$\\forall x\\in X\\exists\\delta...$. Applying logic rule $X$ gives $\\exists x ...$.\n\nOf course there is a good reason for using natural language: it is more intuitive and therefore easier to follow and easier to come up with.\nHowever I sometimes find that using formal logical rules can clarify my thinking and help me woth proofs. I find that when I get stuck using natural language, switching to thinking about the problem in formal logic can help, and vice versa.\nSo I'm wondering, do you ever use formal logic while you're working to come up with a proof? Why, why not? What role does it play in your process of coming up with proofs?\nNote: I'm only a relative beginner at proving theorems, since I have much more experience with applied math.\n",
    "proof": "When I'm writing to a proof I use formal way. I do this because I find it easier to remember like this, using that way makes me think about how to write each step and thus makes me remember it better. Another advantage I think it has is that writing formally makes you read formally better which is great thing to know.\nWhen I'm explaining something, giving hints, brainstorm etc. I prefer to use natural way, because I think that it gives you the advantages of understanding \"why it is like it is\" and not only the \"how you get to that\".\n",
    "tags": [
      "proof-writing",
      "problem-solving"
    ],
    "score": 5,
    "answer_score": 4,
    "is_accepted": false,
    "question_id": 2541063,
    "answer_id": 2541080
  },
  {
    "theorem": "Prove by induction that $21 | 4^{n+1} + 5^{2n-1}$",
    "context": "The problem that I have is: Prove by induction that 21 divides 4n+1 + 52n-1\nSo far I have:\nBase Case:\n$n = 1$\n$4^{1+1} + 5^{2-1} = 4^2 + 5^1 = 16 + 5 = 21$\nInductive Step:\nAssume: $4^{k+1} + 5^{2k-1} = 3m$\n$4^{(k+1)+1} + 5^{2(k+1)-1} = 3m$\n$4^{k+2} + 5^{2k+1} = 3m$\nI'm pretty sure I am far off on this one and not sure where to go. \nThanks\n",
    "proof": "You can proceed with the inductive step as follows:\nAssume that $$21 \\mid 4^{k+1}+5^{2k-1}$$which implies \n$$21\\mid 25(4^{k+1}+5^{2k-1})$$ $$\\Longrightarrow21\\mid 25(4^{k+1})+5^{2k+1}$$\nWe can subtract a multiple of $21$ on the right side to obtain\n$$21\\mid 25(4^{k+1})+5^{2k+1}-21(4^{k+1})$$\n$$\\Longrightarrow 21\\mid4(4^{k+1})+5^{2k+1}$$\n$$\\Longrightarrow 21\\mid4^{k+2}+5^{2k+1}$$\n$$\\Longrightarrow 21\\mid4^{(k+1)+1}+5^{2(k+1)-1}$$\nThis completes the proof.\n",
    "tags": [
      "discrete-mathematics",
      "proof-writing"
    ],
    "score": 5,
    "answer_score": 6,
    "is_accepted": true,
    "question_id": 1521838,
    "answer_id": 1521852
  },
  {
    "theorem": "Proving the chain rule by first principles",
    "context": "I'm currently trying to prove:\n$(f(g))'(a)=f'(g(a))*g'(a)$\nI have been given a proof which manipulates:\n$f(a+h)=f(a)+f'(a)h+O(h)$ where $O(h)$ is the error function. However, I would like to have a proof in terms of the standard limit definition of $(1/h)*(f(a+h)-f(a) \\to f'(a)$ as $h \\to 0$\nThanks!\n",
    "proof": "Since $g$ is differentialiable at the point $a$ then it'z continuous and then \n$$\\lim_{x\\to a}g(x)=g(a)$$ \nhence\n$$(f\\circ g)'(a)=\\lim_{x\\to a}\\frac{f(g(x))-f(g(a))}{x-a}=\\lim_{x\\to a}\\frac{f(g(x))-f(g(a))}{g(x)-g(a)}\\frac{g(x)-g(a)}{x-a}\\\\=\\lim_{y\\to g(a)}\\frac{f(y)-f(g(a))}{y-g(a)}\\lim_{x\\to a}\\frac{g(x)-g(a)}{x-a}=f'(g(a))g'(a)$$\n",
    "tags": [
      "analysis",
      "functions",
      "derivatives",
      "proof-writing"
    ],
    "score": 5,
    "answer_score": 11,
    "is_accepted": true,
    "question_id": 757989,
    "answer_id": 757995
  },
  {
    "theorem": "For $n\\ge4$, prove that $1!+2!+\\cdots+n!$ cannot be the square of a positive integer",
    "context": "I'm trying to prove this by induction but seem to be getting nowhere.\n",
    "proof": "Every square is either $0,1$ or $4\\,\\bmod 5$,. For $k\\ge 5$ we have $k!\\equiv 0\\bmod 5$ hence for $n\\ge 4$\n$$1!+2!+\\cdots+n!\\equiv 1!+2!+3!+4!\\equiv 33\\equiv 3\\mod 5$$\nthus it cannot be a square.\n",
    "tags": [
      "number-theory",
      "elementary-number-theory",
      "proof-writing"
    ],
    "score": 5,
    "answer_score": 11,
    "is_accepted": false,
    "question_id": 755255,
    "answer_id": 755269
  },
  {
    "theorem": "Prove $\\frac{1}{2} + \\cos(x) + \\cos(2x) + \\dots+ \\cos(nx) = \\frac{\\sin(n+\\frac{1}{2})x}{2\\sin(\\frac{1}{2}x)}$ for $x \\neq 0, \\pm 2\\pi, \\pm 4\\pi,\\dots$",
    "context": "I know that this can be proven inductively. However, I can't get passed the trig. I am pretty sure trig identities can show that the expression above is true for $n=0$, and that if the expression holds for $n=k$ it holds for $n=k+1$. But alas, I am getting lost in a sea of trig. Hopefully someone can shed some light on this.\n",
    "proof": "Hint:\n$$\\frac{1}{2} + \\sum_{k=1}^n \\cos(kx)\n= \\frac{1}{2}\\sum_{k=-n}^n e^{ikx}$$\n",
    "tags": [
      "sequences-and-series",
      "proof-writing",
      "induction"
    ],
    "score": 5,
    "answer_score": 6,
    "is_accepted": false,
    "question_id": 520719,
    "answer_id": 520736
  },
  {
    "theorem": "Proving that the reciprocal of an irrational is irrational",
    "context": "The question I am working on is:\n\nProve that if x is irrational, then 1/x is irrational.\n\nMy proof differs from the one given in the answer key; but I still feel that mine is valid. Could someone possibly look over my proof to see if it is correct?\nProof by contraposition: If $1/x$ is rational, then x is a rational number.\nAssuming that $x \\ne 0$, then $1/x$ is by definition a rational number; taking the reciprocal of this, x is will be some number, other than zero, that can be written as $x/1$, which is a rational number by definition.\nSince we have proven the contrapositive to be true, then the original statement must be true.\n\nEDIT: I found this solution on the internet. \n\nProof: We prove the contrapositive: If 1=x is rational, then x is rational. So suppose 1=x is rational. Then\n  there exist integers p; q, with q = 0, such that 1 6 =x = p=q. Then x = q=p is clearly rational, unless p = 0.\n  However, the case that p = 0 can't occur, because if p = 0, then 1=x = p=q = 0. But 1=x is never zero. \n\nMy question is, what is the point in mentioning the case that $p=0$. Isn't it safe to assume that, once you reach the point when you take the reciprocal, $p$ can't equal zero?? \n",
    "proof": "Yeah that should do it, even though I would prefer something like for $x\\neq 0$ \n$$ \\frac{1}{x}=\\frac{p}{q}  \\iff x= \\frac{q}{p} $$ \nEdit: Should first mention that $x$ ist rational \n",
    "tags": [
      "discrete-mathematics",
      "proof-writing"
    ],
    "score": 5,
    "answer_score": 5,
    "is_accepted": true,
    "question_id": 303987,
    "answer_id": 303990
  },
  {
    "theorem": "Prove that given a nonnegative integer $n$, there is a unique nonnegative integer $m$ such that $(m-1)^2 ≤ n &lt; m^2$",
    "context": "Prove that given a nonnegative integer $n$, there is a unique nonnegative integer $m$ such that $(m-1)^2 ≤ n < m^2$\nMy first guess is to use an induction proof, so I started with n = k = 0:\n$(m-1)^2 ≤ 0 < m^2 $\nSo clearly, there is a unique $m$ satisfying this proposition, namely $m=1$.\nNow I try to extend it to the inductive step and say that if the proposition is true for any $k$, it must also be true for $k+1$.\n$(m-1)^2 + 1 ≤ k + 1 < m^2 + 1$\nBut now I'm not sure how to proof that. Any ideas?\n",
    "proof": "It's nicer to prove for $$m^2 \\le n < (m+1)^2$$ which is obviously equivalent.\nso take the square root: $$m \\le \\sqrt{n} < m+1$$\nfrom this you can see $m=\\lfloor{\\sqrt{n}}\\rfloor$ is the unique $m$.\n",
    "tags": [
      "proof-writing",
      "induction"
    ],
    "score": 5,
    "answer_score": 3,
    "is_accepted": true,
    "question_id": 283515,
    "answer_id": 283521
  },
  {
    "theorem": "Prove that if $x$ is odd then $x^2 -1$ is divisible by $8$.",
    "context": "\nIf $x$ is odd then prove that $x^2-1$ is divisible by $8$.\n\nI start by writing: $x = 2k+1 $  where $k\\in\\mathbb{N}$.\nThen it follows that:\n$(2k+1)^2 -1 = 4k^2 +4k + 1 -1 $  \nTherefore:\n$$\\frac{4k^2 +4k}{8} = \\frac{k(k+1)}{2}$$ \nAt the end part I can see that for what $k$ is, the number on top is divisible by 2. I was expecting the end result to be a number, not a fraction. Or is it \"divisible\" from the definition $\\text{even} = 2k$ that completes the proof?\n",
    "proof": "An easier way to approach this would be to observe:\nIf x is odd, either x-1 or x+1 is divisible by 4. Then their product is divisible by 8.\n",
    "tags": [
      "algebra-precalculus",
      "elementary-number-theory",
      "proof-writing"
    ],
    "score": 5,
    "answer_score": 9,
    "is_accepted": true,
    "question_id": 242543,
    "answer_id": 242548
  },
  {
    "theorem": "Mathematical induction proof that $\\sum\\limits_{k=2}^{n-1} {k \\choose 2} = {n \\choose 3} $",
    "context": "I have a problem with a mathematical equation. I don’t find the given solution.\nThis is the equation: $\\sum\\limits_{k=2}^{n-1} {k \\choose 2} = {n \\choose 3} $\nI should show with induction that the expression is correct for every $n >= 3$.\nI started with the beginning of the induction. For $n = 3$.\n$\\sum\\limits_{k=2}^{2} {2 \\choose 2}  = 1 = {3 \\choose 3} $\nThat’s correct.\nNow I don’t know how to dissolve that equation. The solution should be:\n$\\sum\\limits_{k=2}^{n-1} {k \\choose 2}  + {n \\choose 2}  = {n \\choose 3}  + {n \\choose 2}  = {n + 1 \\choose 3} $\nWhy ${n \\choose 2}$? It would be awesome, if someone could tell me the steps or give some tips how I could reach that solution.\nEdit:\nWell, at that point I am:\n\nBeginning of the induction: $\\sum\\limits_{k=2}^{2} {2 \\choose 2}  = 1 = {3 \\choose 3}$\nInduction step n + 1: $\\sum\\limits_{k=2}^{n} {k \\choose 2} + {n \\choose 3}$\nThats my assumption: ${n \\choose 3}$.\nChange the first addend with the assumption, so I get ${n \\choose 2} + {n \\choose 3}$\nAdd it into the binomial coefficient formula: ${n! \\choose 2!(n-2)!} + {n! \\choose 3!(n-3)!}$\n\nI have it like @david-mitra. Whats wrong with that way?\nThanks.\n",
    "proof": "We wish to prove:\n$$\r\n\\sum_{k=2}^{n-1} {k\\choose 2}= {n\\choose3},\\quad n\\ge3.\\tag{1}\r\n$$\nIf you want to prove that equation (1) holds for all $n\\ge 3$ using induction:\n1) Show that the equation is true for $n=3$:\n$$\r\n\\sum_{k=2}^{3-1} {k\\choose 2}= {2\\choose2}= {3\\choose3}.\\tag{1}\r\n$$\n2) Assume that the equation is true for $n=m$:\n$$\r\n\\color{maroon}{\\sum_{k=2}^{m-1} {k\\choose 2}}= \\color{maroon}{m\\choose3 }.\r\n$$\n3) Now show that the equation must be true for $n=m+1$:\n$$\\eqalign{\r\n\\sum_{k=2}^{(m+1)-1} {k\\choose 2}\r\n&=\\sum_{k=2}^{m } {k\\choose 2}\\cr\r\n&=\\sum_{k=2}^{m-1 } {k\\choose2} +\\color{darkgreen}{\\sum_{k=m}^m {k\\choose2}}\\cr\r\n&=\\biggl[\\ \\color{maroon}{ \\sum_{k=2}^{m-1 } {k\\choose 2}}\\ \\biggr]+\\color{darkgreen}{m\\choose2}\\cr\r\n&=  \\color{maroon}{ m\\choose 3}  +{m\\choose2}\\cr\r\n \r\n&= {m!\\over (m-3)! 3!}+  {m!\\over (m-2)! 2!}\\cr\r\n&={ m(m-1)(m-2)\\over 3!}   +{ m(m-1) \\over 2!}  \\cr\r\n&=  m(m-1)({m-2\\over 6}+{3\\over 6 })\\cr\r\n \r\n &= { m(m-1)(m+1)\\over 6} \\cr\r\n&={m+1\\choose 3}.\r\n\r\n}\r\n$$\n",
    "tags": [
      "summation",
      "proof-writing",
      "induction",
      "binomial-coefficients"
    ],
    "score": 5,
    "answer_score": 0,
    "is_accepted": true,
    "question_id": 99568,
    "answer_id": 99578
  },
  {
    "theorem": "Difficulty Understanding Rudin&#39;s Example 2.10(b)",
    "context": "I was studying Rudin's Principles of Mathematical Analysis, Chapter 2 Example 2.10(b) and I came across these two examples which I didn't understand and did not get a clear idea of what they meant:\n\nLet $A$ be the set of a real number $x$ such that $x\\in\\mathbb{R}_{(0,1]}$. $\\forall x\\in A$, Let $E_{x}$ be the set of real numbers $y$ such that $y\\in\\mathbb{R}_{(0,x)}$. Then\n$$\n\\bigcup_{x\\in A}E_{x} = E_{1}\\qquad \\text{and}\\qquad \\bigcap_{x\\in A}E_{x} = \\emptyset.\n$$\n\nI am having a hard time understanding them and I would be glad if there is a clear proof for these two results.\n",
    "proof": "Notations: In explicit set-theoretic notations, $E_x$ here means:\n$$\nE_x := \\{y \\in \\Bbb{R} : 0 < y < x\\}\n$$\nThus, in particular, we have that:\n$$\nE_1 := \\{y \\in \\Bbb{R} : 0 < y < 1\\}\n$$\n\nProof of statement: I first prove the first statement. Let $y \\in \\bigcup_{x \\in A} E_x$, so $y \\in E_x$ for some $x \\in A$. In other words, $0 < y < x$ for some $x \\in A$. Now since $x \\in \\Bbb{R}_{(0,1]}$, we have that $x \\leq 1$, so $0 < y < 1 \\implies y \\in E_1$. On the other hand, suppose $y \\in E_1 \\implies 0 < y < 1$. Observe that $y < \\frac{y + 1}{2} < 1$ (can you tell why?), so $y \\in E_\\frac{y+1}{2} \\subseteq \\bigcup_{x \\in A} E_x$.\nFor the second statement, we prove by contradiction. Suppose $\\bigcap_{x \\in A} E_x$ is non-empty, so let $y \\in \\bigcap_{x \\in A} E_x$. Since $y > 0$, we have that $0 < \\frac{y}{2} < y$, so we can't have $y \\in E_\\frac{y}{2}$ (i.e. $y \\notin E_\\frac{y}{2}$). Yet, by definition of intersection, we have that $\\bigcap_{x \\in A} E_x \\subseteq E_\\frac{y}{2}$, a contradiction.\n",
    "tags": [
      "real-analysis",
      "general-topology",
      "elementary-set-theory",
      "proof-writing"
    ],
    "score": 5,
    "answer_score": 3,
    "is_accepted": true,
    "question_id": 3785764,
    "answer_id": 3785783
  },
  {
    "theorem": "Union of nontrivial intervals is a countable sub-union.",
    "context": "\nLet $A=\\bigcup\\limits_{i\\in \\mathcal I} I_i$ where $\\mathcal{I}$ is an indexing set of any size.\nEach $I_i$ is a nontrivial interval, i.e. an interval with at least two points. The intervals are not guaranteed to be open or closed. So any $I_i\\subseteq\\Bbb R$ and has the form of either $(a,b), (a,b], [a,b),$ or $[a,b]$ where $a < b, a,b\\in\\Bbb R$.\nI want to prove that $A=\\bigcup\\limits_{k=1}^\\infty I_{i_k}$ for some countable sub-indexing $\\{i_k\\}_{k=1}^\\infty$.\nsource: exercise in Axler’s Measure, Integration & Real Analysis, section 2D, $\\mathcal N\\underline{o}\\  4$.\n\n\nMy first attempt:\nWe know that any union of disjoint open intervals is a countable union.  Although this isn’t proved and I don’t think it’s stated in Axler’s book, it may be well-known enough to be fair game.  Then, $\\forall I_i$ with end-points $a_i, b_i$ we can form the set $E =\\bigcup\\limits_{i\\in \\mathcal I} \\{a_i,b_i\\}$ which is to say the set of all the endpoints.  $\\forall x_i\\in E$ we take $y_i = \\inf\\{x\\in E: x > x_i\\}$ and then form the intervals $(x_i,y_i)$ which will be disjoint open intervals. (If necessary we can throw away any empty ones.)\nThis kind of seems like it might be heading in the right direction, but what I’m starting to notice is that I am building a collection of open intervals based on a mixture of end-points of the original intervals.  I don’t see how I’m going to continue this path in order to not just get a countable union, but a countable union of the exact same intervals that were in the original set.\n\nEdit:\nThe more I think about this attempt, the more I realize it's doomed in another way.  If $\\{I_i\\}_{i\\in\\mathcal I}$ is the set of all intervals with left and right end-points any ordered pair of irrational numbers, then every interval constructed in the procedure above will be empty.\n\nMy next attempt is to think “Why is this problem in this section? Maybe it has a more measure theoretic solution.” If I take the measure of $A$ it might be infinite.  I could consider taking the measure of $A\\cap [n,n+1]$ and argue that some countable collection of intervals covers this.\nHowever we don’t know if $A$ is closed so we can’t argue from compactness.  And for all we know, even in this bounded interval, this part of $A$ might be composed of an uncountable collection of intervals.\n",
    "proof": "Let $A$ be the union of a collection $\\mathscr{C}$ of non-trivial intervals of $\\mathbb{R},$ and let $B$ be the union of the interiors of the intervals in $\\mathscr{C}.$\nLet $P$ be the set of ordered pairs of rational numbers $\\left\\langle p, q \\right\\rangle$ such that $p < q$ and $(p, q) \\subseteq I$ for some $I \\in \\mathscr{C},$ and choose one such interval $I = J(p, q)$ for each ordered pair $\\left\\langle p, q \\right\\rangle \\in P.$ For every $x \\in B,$ there exists an interval $I = (a,b), (a,b], [a,b),$ or $[a, b]$ in $\\mathscr{C}$ such that $a < x < b.$ Choose rational numbers $p, q$ such that $a < p < x < q < b.$ Then $(p, q) \\subseteq I \\in \\mathscr{C},$ therefore $\\left\\langle p, q \\right\\rangle \\in P,$ and $x \\in (p, q) \\subseteq J(p, q).$ Therefore $B$ is contained in the union of the countable subcollection $\\{J(p, q) : \\left\\langle p, q \\right\\rangle \\in P\\}$ of $\\mathscr{C}.$\nIf $x \\in A \\setminus B,$ then $x$ is a left or right endpoint of a closed or half-closed interval belonging to $\\mathscr{C}.$ Let $L$ be the set of such left endpoints, and $R$ the set of such right endpoints. (There is no assumption that $L$ and $R$ are disjoint.) For all $x \\in L,$ choose an interval $H(x) = [x, h(x)] \\in \\mathscr{C},$ or $H(x) = [x, h(x)) \\in \\mathscr{C}.$ For all $x \\in R,$ choose an interval $K(x) = [k(x), x] \\in \\mathscr{C},$ or $K(x) = (k(x), x] \\in \\mathscr{C}.$ For all $x, x' \\in L,$ if $x \\ne x'$ then $(x, h(x)) \\cap (x', h(x')) = \\varnothing.$ For, if $x < x',$ and $y \\in (x, h(x)) \\cap (x', h(x')),$ then $x' \\in (x, y) \\subseteq H(x),$ but this is a contradiction, because $x'$ is not an interior point of any interval in $\\mathscr{C}.$ The proof is similar if $x > x'.$ Similarly, for all $x, x' \\in R,$ if $x \\ne x'$ then $(k(x), x) \\cap (k(x'), x') = \\varnothing.$ Therefore, distinct rational numbers can be chosen in $(x, h(x))$ for all $x \\in L,$ and in $(k(x), x)$ for all $x \\in R.$ So $L$ and $R$ are both countable.\nIt follows that $A$ is the union of a countable subcollection of $\\mathscr{C},$ because:\n$$\nA \\subseteq \\bigcup\\{J(p, q) : \\left\\langle p, q \\right\\rangle \\in P\\} \\cup \\bigcup\\{H(x) : x \\in L\\} \\cup \\bigcup\\{K(x) : x \\in R\\} \\subseteq A.\n$$\n",
    "tags": [
      "real-analysis",
      "measure-theory",
      "proof-writing"
    ],
    "score": 5,
    "answer_score": 5,
    "is_accepted": true,
    "question_id": 3717486,
    "answer_id": 3718243
  },
  {
    "theorem": "ALL Prime Numbers Within 2 Columns of Number Pyramid - Proof?",
    "context": "I was arranging numbers putting them in different orders when I happened to build a pyramid and noticed that two columns of the pyramid seemed to contain all of the prime numbers. Not just some of them, but all of the first hundred primes at least.\nConsidering how rare I thought that might be, I'm bringing it here for consideration.\nI began with the numbers 5, 6, 7 at top. Then going diagonal to the left for 5, I did 52, then 53, etc as the left side of the triangle. On the right I did 71, then 72, etc going down the right side of the triangle.\nThis means the top row is 5, 6, 7. The next row is 10, 11, 12, 13, 14. The next is 15, 16, 17, 18, 19, 20, 21... and so on.\n\nIf you look at the columns under 5 and 7 they contain ALL of the prime numbers.\nIs there a way to create a proof that all prime numbers do NOT exist in these two columns?\n",
    "proof": "The column under $5$ consists of all numbers of the form $6n-1$, and the column under $7$ consists of all numbers of the form $6n+1$.  All primes $> 3$ are of one of these forms.\n",
    "tags": [
      "proof-writing",
      "prime-numbers"
    ],
    "score": 5,
    "answer_score": 10,
    "is_accepted": true,
    "question_id": 3297805,
    "answer_id": 3297812
  },
  {
    "theorem": "How to prove an implication within an if and only if",
    "context": "Suppose you need to prove that $A\\iff (B\\implies C)$.\nThe two ways to prove this are:\n\n(1a): Suppose $A$ and $B$ are true. Prove that $C$ is true.\n(1b): Suppose $B$ and $C$ are true. Prove that $A$ is true.\n\n(2a): Suppose $A$ and $B$ are true. Prove that $C$ is true.\n(2B): Suppose $A$ is not true and B is true, prove that $C$ cannot be true.\n\nAre these ways correct? I always get confused what you can assume and what you have to prove when there's multiple implications and such in one statement.\n",
    "proof": "Prove both directions:\nForward: assume A, prove ($B \\implies $C). This means \"suppose A and B are true. Prove that C is true\".\nReverse: Assume ($B \\implies $C), prove A. So, you don't  \"Suppose B and C are true. Prove that A is true\". Instead, \"suppose the truth of B implies truth of C, then prove that A is true. $1b$ is incorrect.\nNow, reverse can also be interpreted as suppose A is not true, prove ($B \\implies $C) is not true by the contrapostive. Which means prove $B$ is true and $C$ is false. So you don't \"Suppose A is not true and B is true, prove that C cannot be true.\" Instead, \"Suppose A is not true, prove B is true and prove that C cannot be true.\" $2b$ is incorrect.\n",
    "tags": [
      "proof-writing"
    ],
    "score": 5,
    "answer_score": 5,
    "is_accepted": true,
    "question_id": 2682904,
    "answer_id": 2682911
  },
  {
    "theorem": "how to prove $\\left | \\sqrt[n]{a}- \\sqrt[n]{b}\\right |\\leq \\sqrt[n]{\\left | a-b \\right |}$",
    "context": "$$\\left | \\sqrt[n]{a}- \\sqrt[n]{b}\\right |\\leq \\sqrt[n]{\\left | a-b  \\right |},n\\in \\mathbb{N^{*}},a,b\\geq 0\\\\$$\nI failed to prove it,  so if someone could prove it and explain it to me, it would be great.\n",
    "proof": "Say $x=\\sqrt[n]{a}$ and $y=\\sqrt[n]{b}$. Then we have to prove:\n$$ |x-y|^n \\leq |x^n-y^n|$$ Because of simetry we can assume that $x\\geq y$. Let $z={x\\over y}$. Since $z\\ge 1$ we have to prove:\n$$ (z-1)^n \\leq z^n-1$$\nNow write $t=z-1$, so we have to prove $$t^n\\leq (t+1)^n-1 $$ which is true by binomial theorem:\n$$t^n\\leq t^n + {n\\choose 1}t^{n-1}+{n\\choose 2}t^{n-2}+...t+1-1$$\n",
    "tags": [
      "real-analysis",
      "algebra-precalculus",
      "inequality",
      "proof-writing",
      "absolute-value"
    ],
    "score": 5,
    "answer_score": 7,
    "is_accepted": true,
    "question_id": 2523550,
    "answer_id": 2523582
  },
  {
    "theorem": "$x\\in \\mathbb{R}\\setminus\\{0\\}$ and $x + \\frac{1}{x}$ is an integer. Prove $\\forall n\\in \\mathbb{N}$ $x^n + \\frac{1}{x^n}$ is also an integer.",
    "context": "\nLet $x$ be a non-zero real number such that $x + \\frac{1}{x}$ is an integer. Prove that $\\forall n\\in \\mathbb{N}$ the number $x + \\frac{1}{x}$ is also an integer.\n\nAttempt at solution using induction:\nbase case: $n = 1$ then $x^n + \\frac{1}{x^n} = x + \\frac{1}{x}$ which is an integer\nInductive assumption: Assume that for some $k\\in \\mathbb{N} : x^k + \\frac{1}{x^k}$\nWe have to show that $ x^{k+1} + \\frac{1}{x^{k+1}}$ is an integer.\n$$ x^{k+1} + \\frac{1}{x^{k+1}} = x^k\\cdot x + \\frac{1}{x^k\\cdot x} =  \\frac{(x^k\\cdot x)\\cdot (x^k\\cdot x)+1}{x^k\\cdot x}$$\nI can't find a way to seperate $x$ and $\\frac{1}{x}$ from the term so I can use the inductive assumption.\nEdit: I don't think that this question should count as a duplicate since the linked question is asking to specifically solve another problem and one of the answers of that question utilize the proof from this question in their answer but that question itself is different and not related to this question since that question can be solved without using this proof as other answers of that question don't include it.\n",
    "proof": "$$x^{k+1} + \\dfrac 1 {x^{k+1}} = \\left( x^k + \\dfrac 1 {x^k} \\right) \\left( x + \\dfrac 1 x \\right) - \\left( x^{k-1} + \\dfrac 1 {x^{k-1}} \\right)$$\n",
    "tags": [
      "algebra-precalculus",
      "proof-writing",
      "induction"
    ],
    "score": 5,
    "answer_score": 8,
    "is_accepted": true,
    "question_id": 2506773,
    "answer_id": 2506774
  },
  {
    "theorem": "Proof of $\\sqrt{a} + \\sqrt{b} \\le 2 \\times \\sqrt{a+b}$?",
    "context": "I want to prove that  $\\sqrt{a} + \\sqrt{b} \\le 2 \\times \\sqrt{a+b}$, I had the idea to draw it:\n\nWould it be enough to prove what I want to prove? If not, is there a way to be more precise by still using my method or should I abandon it and use a more \"traditional\" way?  \nThank you.\n",
    "proof": "Note that $a,b$ are both nonnegative.\n$$\\sqrt{a}\\le \\sqrt{a+b}$$\n$$\\sqrt{b}\\le \\sqrt{a+b}$$\nNow add the two.\n",
    "tags": [
      "proof-verification",
      "proof-writing"
    ],
    "score": 5,
    "answer_score": 8,
    "is_accepted": true,
    "question_id": 2162375,
    "answer_id": 2162386
  },
  {
    "theorem": "formal proof from calulus",
    "context": "Given $f:R \\to R$, $f$ is differentiable on $R$ and $\\lim_{x \\to \\infty}(f(x)-f(-x))=0$.\nI need to show that there is $x_0 \\in R$ such that $f'(x_0)=0$\nI am trying to prove it by contradiction .... so i assume there is no $x_0 \\in R$ such that  $f'(x_0)=0$ this means this function is strictly monotonic because the derivative \"respects\" the mean value theorem so it cannot be negative and positive without passing a zero value .... \n(from what i understand the derivative function of a continuous/differentiable functions isn't necessarily continuous but it still respect the mean value theorem i think it's called darboux's theorem )\nnow from $\\lim_{x \\to \\infty}(f(x)-f(-x))=0$ ,we get $\\lim_{x \\to \\infty}f(x)=\\lim_{x \\to \\infty}f(-x)$\nnow the last step is the contradiction its very intuitive and even maybe obvious that we can't get $\\lim_{x \\to \\infty}f(x)=\\lim_{x \\to \\infty}f(-x)$ for strictly monotonic function but i cant think of a formal way to prove it . \n",
    "proof": "Hint: Consider the function $g(x)=f(x)-f(-x)$. Since $g(0)=0$ and $\\lim\\limits_{x\\to\\infty}g(x)=0$, $g$ is either identically $0$ or it has a local maximum or minimum at $x_0\\in(0,\\infty)$. In the same way it was shown for Rolle's Theorem, we have that $g'(x_0)=0$. Thus,\n$$\nf'(x_0)+f'(-x_0)=0\n$$\nIf $f'$ is not identically $0$ and $x_0\\ne0$, apply the Intermediate Value Theorem.\n",
    "tags": [
      "calculus",
      "derivatives",
      "proof-writing"
    ],
    "score": 5,
    "answer_score": 9,
    "is_accepted": true,
    "question_id": 688635,
    "answer_id": 688662
  },
  {
    "theorem": "Still struggling with proofs.",
    "context": "How do you construct rigorous math proofs on your own? Also how do you verify? I am finishing up my first semester of undergraduate analysis and still am struggling with writing proofs. Even though I am getting an A, I just do not feel confident that I can clearly write proofs on my own. \n",
    "proof": "It takes practice to get good at anything. You're just getting started. Visit the Professor during office hours; most faculty like questions from serious students such as yourself. Ask the faculty to recommend good books to help you with proofs.\n",
    "tags": [
      "soft-question",
      "proof-writing"
    ],
    "score": 5,
    "answer_score": 4,
    "is_accepted": false,
    "question_id": 594147,
    "answer_id": 594160
  },
  {
    "theorem": "Proof by contradiction: $ \\emptyset \\subseteq A$",
    "context": "I have to proof by contradiction that: let $ A $ a set and $ \\emptyset $ the empty set, then $ \\emptyset \\subseteq A$;  if  $ \\emptyset \\nsubseteq A$ then  $\\exists x \\in \\emptyset  ( x \\notin A ) $ but for hypothesis \"let $ \\emptyset $ the empty set, then $\\nexists x \\in \\emptyset$\", so I have a contradiction and therefore $ \\emptyset \\subseteq A$ is true! Is it correct? Thank you all in advance\n",
    "proof": "Nitpick (very slight alteration to follow):\n\nLet $ A $ [be] a set and $\\emptyset$ the empty set. Then $ \\emptyset \\subseteq A$.  \n\nProof:\n[Let $A$ be a set and $\\emptyset$ the empty set. Suppose also, for the sake of contradiction, that] $\\; \\emptyset \\nsubseteq A$.\nThen $\\exists x \\in \\emptyset,$ [such that] $( x \\notin A ) $.\nBut by hypothesis, $\\emptyset$ is the empty set, [thus by the definition of the empty set], $\\lnot\\exists x \\in \\emptyset$.\nSo [we] have [reached] a contradiction, and it must therefore follow that] $ \\;\\;\\emptyset \\subseteq A,$ [as desired].\n\n(Note: here $\\lnot\\exists \\equiv \\nexists$)\n",
    "tags": [
      "elementary-set-theory",
      "proof-writing"
    ],
    "score": 5,
    "answer_score": 9,
    "is_accepted": true,
    "question_id": 330390,
    "answer_id": 330405
  },
  {
    "theorem": "Sum of squares are closed under products and squaring.",
    "context": "For any natural number $n\\ge 1$, given pairs $(a_1,b_1),(a_2,b_2),...,(a_n,b_n)$ of integer numbers, there exist integer number $c$ and $d$ such that\n$$\\prod_{i=1}^{n}(a_i^2+b_i^2) = c^2+d^2$$\n\nMy initial approach is\nBase Case: $(a_1^2+b_1^2) = a_1^2+b_1^2$ which is true. (Although it is trivial)\nProve the statement is true when $n=2$: We have $$(a^2+b^2)(c^2+d^2) = (ac-bd)^2+(ad+bc)^2$$\n(Thanks André Nicolas for pointing it out) \nSo if $a,b,c,d$ are integers, $ac,bd,ad,bc$ are all integers and integers are closed under addition and subtraction. Hence $(ac-bd),(ad+bc)$ are integers. \nInductive Hypothesis: $\\prod_{i=1}^{n}(a_i^2+b_i^2) = c^2+d^2$ is true\nInductive Step: $$\\prod_{i=1}^{n+1}(a_i^2+b_i^2) = \\prod_{i=1}^{n}(a_i^2+b_i^2)\\cdot (a_{n+1}^2+b_{n+1}^2) = (c^2+d^2)\\cdot (a_{n+1}^2+b_{n+1}^2)$$ \nWhere $c$ and $d$ are integers. \nBut when we apply $n=2$, we have  $(c^2+d^2)\\cdot (a_{n+1}^2+b_{n+1}^2) = (e^2 + f^2)$ where $e$ and $f$ are integers.\nHence, by the principle of induction, the statement we needed to prove is true.\n",
    "proof": "Let me raise a couple of points:\n\nYou should prove the basis of the induction (the $n=1$ case); even if it is obvious, it should be mentioned.\nThe induction hypothesis that you put forth is just \"If the result holds for $n$, then it holds for $n+1$.\" You are not assuming that the result holds for all values of $n$, just for a single, though unspecified, value. For the inductive step to be valid, the proof must be valid for any particular value of $n$ that you may use. That is: you do not have \"general cases\" and \"specific cases\" of the induction hypothesis, you have a single, fixed, induction hypothesis: that the result holds for a particular, fixed (though unspecified) $n$. You are trying to prove a universal statement (\"For all natural numbers $n$, if the result holds for $n$ then it holds for $n+1$\") by taking an arbitrary $n$. \nIt is possible to do the inductive step by assuming that the result holds for all values of $k$ smaller than $n$; this is sometimes called \"strong\" or \"transfinite induction.\" There are some subtle differences between strong induction as applied to the natural numbers and regular \"one-step-at-a-time\" induction: see for example this post.\nEven so, inductive argument must be such that it must hold for all values of $n$. Your argument does not let you go from $P(1)$ to $P(2)$. \nA  particular \"fake proof\" that I like seems particularly appropriate here. Consider the following statement:\nIn any finite nonempty group of people, if at least one has blue eyes, then they all have blue eyes.\nHere's a \"proof\" by induction: let $n$ be the number of people in the group. We prove the statement by induction. \nBase: $n=1$. Then there is only one person, and if that person has blue eyes, then all persons have blue eyes.\nInduction hypothesis: In any group with $n$ people, if at least one has blue eyes, they all have blue eyes.\nInductive step: Take a group with $n+1$ people, $p_1,p_2,\\ldots,p_n,p_{n+1}$, and assume that one of them has blue eyes. Without loss of generality, say it is $p_1$. So we have the set:\n$$\\{ {\\color{blue}p_1}, p_2,\\ldots,p_n,p_{n+1}\\}.$$\nConsider the set of the first $n$ people: $\\{{\\color{blue}p_1},p_2,\\ldots,p_n\\}$. By the induction hypothesis, since one of them has blue eyes, they all have blue eyes:\n$$\\{ {\\color{blue}p_1}, {\\color{blue}p_2},\\ldots,{\\color{blue}p_n}, p_{n+1}\\}.$$\nNow consider the set of the last $n$ people: \n$$\\{{\\color{blue}p_2},\\ldots, {\\color{blue}p_n}, p_{n+1}\\}.$$\nSince at least one has blue eyes, they all have blue eyes:\n$$\\{{\\color{blue}p_2},\\ldots, {\\color{blue}p_n}, {\\color{blue}p_{n+1}}\\}.$$\nPutting it all together, we have:\n$$\\{ {\\color{blue}p_1}, {\\color{blue}p_2},\\ldots,{\\color{blue}p_n}, {\\color{blue}p_{n+1}}\\}.$$\nHence, all have blue eyes. By induction, the statement is proven.\nWhat is the mistake?\nThe mistake is that the inductive argument does not work when $n=1$. One needs to prove the fact that $P(1)\\implies P(2)$ somehow, and this argument does not work. (In fact, this is the step that doesn't work, period, which is why the statement is false...)\n\nSo, your argument is incomplete, even if reworded appropriately, because (i) you are missing the base; and (ii) the inductive argument does not work for going from $n=1$ to $n=2$.\nIn fact, the base is trivial; the real sticking point is the $n=2$ case.  If you know the $n=2$ case is true, then you can also use it to replace the problem step in your \"induction argument\" as given: delete the step where you claim the conclusion follows because it \"is just a special case of the induction hypothesis\" (which is an invalid argument as given), and replace it with \"since the $n=2$ case is true, then...\"\nThat means that, after adding that the $n=1$ case is immediate and fixing your inductive argument above:\n\nIf you can prove the $n=2$ case directly, then you'll be done.\n\nSo... try to prove the $n=2$ case. If the $n=2$ case works, then you can invoke the $n=2$ case in your inductive step (which would be fine) rather than make the mistake of talking about \"special cases\" of the inductive hypothesis.\n",
    "tags": [
      "elementary-number-theory",
      "proof-writing",
      "diophantine-equations",
      "sums-of-squares"
    ],
    "score": 5,
    "answer_score": 4,
    "is_accepted": true,
    "question_id": 89148,
    "answer_id": 89150
  },
  {
    "theorem": "How to prove that the axis of an ellipse are perpendicular",
    "context": "Some visuals are so obvious that you would think proofs are not needed.\nBut then trying to proof them rigouresly is a whole other kettle of fish.\nI was stumped by the following puzzle I made for myself: (really it is no homework question)\nHow do you proof that the axis of an ellipse are perpendicular?\nYes you can see it, it is obvious but seeing in itself is no proof.\nYes you cannot construct a countermodel, but again that is no proof.\nI am really stumped with this one, it is so obvious, and easy to see, but a proof?\nAs definition of an ellipse I want to use: (reused from Wikipedia)\nAn ellipse is a plane curve surrounding two focal points, such that for all points on the curve, the sum of the two distances to the points is a constant.\nAs definition of the axis i want to use: (all made up by myself, so maybe incorrect, the second one, defining the minor axis,  was a real struggle ;)\nThe first axis of the ellipse is the line containing the longest segment possible between two points on the ellipse.\nThe second axis of the ellipse is the line containing the midpoint of the two focal points and the shortest segment possible between two points on the ellipse,\n",
    "proof": "Let $P$ be any point on the ellipse, $F$ and $F'$ its foci, $O$ the midpoint of $FF'$. Set $OP=r$, $FF'=2c$, $FP+F'P=2a$, $FP=x$, $F'P=2a-x$. From the formula for the length of a median we get:\n$$\nr^2=(a-x)^2+a^2-c^2.\n$$\nFrom that we infer that the minimum length of $r=OP$ occurs for $x=a$. In that case $FP=FP'$, so that $OP\\perp FF'$. The segment formed then by $P$ and its reflection about $O$ is the minor axis (according to your definition) and it is perpendicular to $FF'$.\nOn the other hand, from triangle inequality $FF'\\ge |PF'-PF|$ we get:\n$$\nx\\ge a-c\n$$\nwith equality occurring only when $P$ lies on line $FF'$. Let then $A, A'$ be the intersections of the ellipse with line $FF'$: from the above result it follows that $AA'$ is the longest diameter of the ellipse, and by triangle inequality this is also the longest segment between any two points on the ellipse, hence the major axis (according to your definition). In fact, if $Q$ and $R$ are two points on the ellipse not aligned with $O$, we have:\n$$\nQR<OQ+OR\\le2OA=AA'.\n$$\n",
    "tags": [
      "geometry",
      "proof-writing"
    ],
    "score": 5,
    "answer_score": 9,
    "is_accepted": false,
    "question_id": 4777702,
    "answer_id": 4777786
  },
  {
    "theorem": "Prove that a construction exists: is this a constructive proof or existential proof?",
    "context": "This top answer claims that it is possible to prove that a constructive proof cannot exist.\nI think, if \"non-existence of constructive proof\" can be proven, then for some other questions, it is also possible to prove \"the existence of constructive proof\".\nConsider the following statement: \"existence of constructive proof without actually giving the construction algorithm.\"\nI am very curious if this statement is considered a \"constructive proof\" or \"existential proof\".\n",
    "proof": "You have a formal system equipped with some notion of \"constructive proof\" (note that \"constructive proof\" is nigh-impossible to pin down with any kind of generality - there are only a few standard proof systems such as Peano Arithmetic where the distinction between constructive and non-constructive proofs is more-or-less agreed upon and understood, or systems such as Heyting Arithmetic or Martin-Löf Type Theory which only admit constructive proofs by design).\nYou start with a sentence (formula, proposition-as-type, etc.) $\\psi$ of your chosen formal system, and consider a sentence (formula, etc.) $\\varphi$ which denotes something like \"$\\psi$ has a constructive proof (in the formal system under consideration)\". Then you ask:\n\nWould a non-constructive proof of $\\varphi$ be considered a constructive or non-constructive proof of $\\psi$?\n\nBy offering two alternatives, you unintentionally turned this into trick question! The short and correct answer is that a proof of $\\varphi$ is not considered a proof of $\\psi$ at all.\nFirst, the easy formalities. Each formal proof has a conclusion, the sentence (formula, etc.) $\\varphi$ that the proof actually establishes. This conclusion is unique: formally, a proof of $\\varphi$ is a proof of $\\varphi$, and not a proof of anything else. In particular, it's never a proof of some other formula, say $\\psi$.\nAt this point, you might think that you can get around these formal issues by rephrasing your question slightly, via some tricky phrasing such as \"can a proof of $\\varphi$ always be turned into a proof of $\\psi$ algorithmically...\"\nBut Gödelian obstacles prevent you from doing that. The statements $\\varphi$ and $\\psi$ are not related in a sufficiently strong way. For example, $PA+\\neg Con(PA)$ proves $\\varphi$ (no matter what $\\psi$ is), but (unless PA is inconsistent) it does not prove an arbitrarily chosen formula $\\psi$, say $0 = 1$. Basically, a sufficiently strong formal system won't go from a proof of \"there exists a proof of $\\psi$\" to a genuine proof of $\\psi$ unless it believes itself consistent: but then, by the second incompleteness theorem the formal system was gravely mistaken about its own consistency anyway.\nThis answers your question: a non-constructive proof of \"$\\psi$ has a constructive proof\" would not be considered a proof of $\\psi$ at all, much less a constructive proof of $\\psi$.\nFor further reading, assuming you already have a firm grasp of Gödel's theorems including Rosser's variant, you might want to read up on arithmetical soundness (if you can track down the first email of this FOM list discussion, it might be a good starting point; or see e.g. Boolos' The Logic of Provability), and the reflection theorem showing that PA is not finitely axiomatizable.\n",
    "tags": [
      "logic",
      "proof-writing",
      "proof-theory",
      "constructive-mathematics"
    ],
    "score": 5,
    "answer_score": 7,
    "is_accepted": false,
    "question_id": 4346202,
    "answer_id": 4346276
  },
  {
    "theorem": "Palindrome Fibonacci words",
    "context": "Let Fibonacci words over the alphabet $\\{0,1\\}$ be recursively defined by $\\omega_0=0$, $\\omega_1=01$, and $\\omega_n=\\omega_{n-1}\\omega_{n-2}$ for $n\\geq{2}$. I am trying to show that the word created by removing the last two letters of $\\omega_n$ is a palindrome. I am not sure how to go about proving this. I think that induction might work but I'm not sure what my statement would be.\n",
    "proof": "Well, the inductive formulation itself is pretty simple. Let's denote by $a_n$ the word you get by removing the last two letters of $w_n.$\nFirst, check the base cases: $w_2 = 010$, $w_3 =01001$ and so $a_1 = 0$, $a_2 = 010$ are indeed palindromes.\nSince, $a_n$ depends on all of the previous terms, it is probably better to use strong induction. That is, assume that for all $2\\leq k<n$, $a_k$ are palindromes. Then, we need to prove that:\n$$a_n = w_{n-1}a_{n-2}$$\nis a palindrome. But look at it closely,\n$$a_n = w_{n-1}a_{n-2} = a_{n-1}(xy)a_{n-2} = w_{n-2}a_{n-3}(xy)a_{n-2}=$$\n$$a_{n-2}(zt)a_{n-3}(xy)a_{n-2},$$\nwhich is a palindrome iff $z = y$ and $x = t,$ because our inductive hypothesis says $a_{n-3}$ is a palindrome. However, $xy$ is the last two digits of $w_{n-1}$, while $zt$ is the last two digits of $w_{n-2}.$ If you look at only the last two digits of the sequence $w_n, n\\geq 2:$\n$$10,01,10,01,...$$\nThis is very easily checked by an induction and so it's clear that $z = y$ and $x = t,$ which means we are done.\n",
    "tags": [
      "discrete-mathematics",
      "proof-writing",
      "recurrence-relations",
      "fibonacci-numbers",
      "palindrome"
    ],
    "score": 5,
    "answer_score": 6,
    "is_accepted": false,
    "question_id": 3142712,
    "answer_id": 3142787
  },
  {
    "theorem": "Can anyone clarify the rules for $\\forall$ intro and elimination, and $\\exists$ intro and elimination?",
    "context": "I am trying to better understand the introduction and elimination rules for quantifiers and in particular the syntax / proof system aspect. I'm currently using Fitch-style proofs.\nI asked a recent question with a sample proof and got an answer here https://math.stackexchange.com/a/3115673/525966 that I think is a good example.\nThe rules for $\\forall$ and $\\exists$ elimination and introduction still confuse me especially since there seems to be a lot of quirky rules about what letters we can use for the... variables, or predicates, or whatever. I honestly can't keep it all straight anymore because I'm so confused. Wikipedia also feels unclear to me. The jump from propositional logic feels massive and rules often seem unclear.\nIs there a good explanation somewhere explaining how we can use $\\forall$ and $\\exists$ introduction and elimination?\n",
    "proof": "Unfortunately, different formal proof systems will do things differently .... but here is how the book I am using ('Language, Proof, and Logic') defines these rules:\nFirst, the system makes the difference between constants and variables by declaring letters in the beginning of the alphabet ($a$, $b$, $c$, ...) to be constants, and letters at the end ($x$, $y$, $z$) to be variables.\nWith this difference in mind, we can immediately make sense of the following two formal inference rules:\nExistential Introduction\n$P(a)$\n$\\therefore \\exists x \\ P(x)$\nThis rule says that if some specific object has property $P$, then we know that there is some object that has property $P$ ($P$ here can of course be any property (formula) ... and the $a$ is any constant (variable-free term)\nUniversal Elimination\n$\\forall x \\ P(x)$\n$\\therefore P(a)$\nThis rule says that if everything has property $P$, then any specific one object will have property $P$\nOk, now let's do Universal Introduction. So, here is where you want to be able to prove that all objects have property $P$. How to do this? The strategy the LPL book uses is to say: \"Let $a$ be some arbitrary object from the domain. Now, if we can prove that this $a$ has property $P$, then given that we assumed nothing about this object other than that it is some object of the domain, it must be true that all objects have property $P$\"\nFormalized, this is what it looks like:\nUniversal Introduction\n$\\quad a$\n$\\quad ...$\n$\\quad P(a)$\n$\\forall x \\ P(x)$\nSo, notice that we are using a subproof structure here. Indeed, the line with $a$ is not a claim, hut rather a kind of assumption that effectively says \"let $a$ be an arbitrary object from the domain\". Also, to make sure that $a$ is indeed an arbitrary object, you have to make sure that when $a$ is 'introduced' at the start of the subproof, the $a$ is not used outside of that subproof; the use of $a$ is thus limited to the scope of the subproof.\nThe last rule is:\nExistential Elimination\n$\\exists x \\ P(x)$\n$\\quad a \\ P(a)$\n$\\quad ...$\n$\\quad Q$\n$Q$\nOk, so here we are also using a subproof, and again use that a constant $a$ whose use is limited to the scope of that subproof (i.e. does not occur before or after that subproof). As such, we are once again guaranteed that we are not assuming anything about $a$ other than that it is some object from the domain ... except in this subproof we are assuming that $a$ has property $P$. So, the first line of the subproof basically says: \"Let $a$ be one of those objects that have property $P$ (whose existence is guaranteed by the $\\exists x \\ P(x)$)\".\nNow, $Q$ is any arbitrary statement (except, it cannot contain any reference to $a$). And, since the subproofs demonstrates that $Q$ be inferred from the assumption that some object $a$ has property $P$, that means that $Q$ also follows from $\\exists x \\ P(x)$, because the arbitrariness of $a$ makes $P(a)$ not any stronger of a statement than $\\exists x \\ P(x)$\nHope that helps!\nHere are some sample proofs, btw:\n\\begin{array}{lll}\n1&\\forall x (P(x) \\to Q(x))&Assumption\\\\\n2&\\forall x (Q(x) \\to R(x))&Assumption\\\\\n3&\\quad a&\\\\\n4&\\quad P(a) \\to Q(a)&\\forall \\ Elim \\ 1\\\\\n5&\\quad Q(a) \\to R(a)&\\forall \\ Elim \\ 2\\\\\n6&\\quad \\quad P(a)&Assumption\\\\\n7&\\quad \\quad Q(a)&\\to \\ Elim \\ 4,6\\\\\n8&\\quad \\quad R(a)&\\to \\ Elim \\ 5,7\\\\\n9&\\quad P(a) \\to R(a)&\\to \\ Intro \\ 6-8\\\\\n10&\\forall x (P(x) \\to R(x))&\\forall \\ Intro \\ 3-9\\\\\n\\end{array}\nNote that in the above proof it is important to first introduce the cnstant used for the universal proof (line 3) and then instantiate the universals with that constant (lines 4 and 5); had you put lines 4 and 5 before line 3 then the $a$ would occur outside the scope of the subproof to which its use should be restricted.\n\\begin{array}{lll}\n1&\\exists x \\ P(x)&Assumption\\\\\n2&\\forall x (P(x) \\to Q(x))&Assumption\\\\\n3&\\quad a \\ P(a)&Assumption\\\\\n4&\\quad P(a) \\to Q(a)&\\forall \\ Elim \\ 2\\\\\n5&\\quad Q(a) & \\to \\ Elim \\ 3,4\\\\\n6&\\quad \\exists x \\ Q(x)& \\exists \\ Intro \\ 5\\\\\n7&\\exists x \\ Q(x)&\\exists \\ Elim \\ 1,2-6\\\\\n\\end{array}\nAlso in this proof we made sure to do the universal elimination (line 4) after the introduction of the temporary constant (line 3) used to set up the existential elimination. This is something you will learn very quickly: whenever presented with both an existential and a universal, instantiate the existential before the universal.\nOK! One more:\n\\begin{array}{lll}\n1&\\exists x \\ \\neg P(x)&Assumption\\\\\n2&\\quad \\forall x \\ P(x)&Assumption\\\\\n3&\\quad \\quad a \\ \\neg P(a)&Assumption\\\\\n4&\\quad \\quad P(a)&\\forall \\ Elim \\ 2\\\\\n5&\\quad \\quad \\bot & \\bot \\ Intro \\ 3,4\\\\\n6&\\quad \\bot & \\exists \\ Elim \\ 1,3-5\\\\\n7&\\neg \\forall x \\ P(x)&\\neg \\ Intro \\ 2-6\\\\\n\\end{array}\nThis one is interesting in that it shows that you can 'pull out' a contradiction using Existential Elimination.\nAll right, last one:\n\\begin{array}{lll}\n1&\\exists x \\ \\forall y \\ R(x,y)&Assumption\\\\\n2&\\quad a \\ \\forall y \\ R(a,y)&Assumption\\\\\n3&\\quad \\quad b & Assumption\\\\\n4&\\quad \\quad R(a,b) &\\forall \\ Elim \\ 2\\\\\n5&\\quad \\quad \\exists x \\ R(x,b)& \\exists \\ Intro \\ 4\\\\\n6&\\quad \\forall y \\ \\exists x \\ R(x,y)& \\forall \\ Intro \\ 3-5\\\\\n7&\\forall y \\ \\exists x \\ R(x,y)&\\exists \\ Elim \\ 1,2-6\\\\\n\\end{array}\nCool ... it uses all four quantifier rules!\n",
    "tags": [
      "logic",
      "proof-writing",
      "first-order-logic",
      "quantifiers",
      "natural-deduction"
    ],
    "score": 5,
    "answer_score": 9,
    "is_accepted": true,
    "question_id": 3115707,
    "answer_id": 3115741
  },
  {
    "theorem": "Proof $\\sum_{k=0}^\\infty \\binom{k}{n-k} = f_{n+1}$ where $f_n$ is the n-th Fibonacci-number",
    "context": "In our combinatorics script it is written, that\n$$\\sum_{k=0}^\\infty \\binom{k}{n-k} = f_{n+1}$$ where $f_n$ is the n-th Fibonacci-number.\nApparently this can be proven through the generating function \n$$A(x) = \\sum_{n=0}^\\infty x^n \\sum_{k=0}^\\infty \\binom{k}{n-k}$$\nI've looked on stackexchange math and on the internet, but I couldn't find a proof.\nI know that $$\\sum_{k\\ge0} \\binom{n-k}k=f_{n+1}$$\nand it can be proven through this\n$$\\sum_{k\\ge0} \\binom{n+1-k}k=\\sum_{k\\ge0} \\binom{n-k}k + \\sum_{k\\ge0} \\binom{n-k}{k-1}=\n\\sum_{k\\ge0} \\binom{n-k}k + \\sum_{k\\ge0} \\binom{n-1-(k-1)}{k-1}=\n\\sum_{k\\ge0} \\binom{n-k}k + \\sum_{j\\ge0} \\binom{n-1-j}{j}= f_{n+1}+f_n = f_{n+2}.$$\nBut I don't know how its done for the first case.\n",
    "proof": "\\begin{align}\nA(x)\n&= \\sum_{n\\ge 0} x^n\\sum_{k\\ge 0} \\binom{k}{n-k}\n\\\\&= \\sum_{k\\ge 0} \\sum_{n\\ge 0} \\binom{k}{n-k}x^n\n\\\\&= \\sum_{k\\ge 0} x^k\\sum_{n\\ge k} \\binom{k}{n-k}x^{n-k}\n\\\\&= \\sum_{k\\ge 0} x^k\\sum_{n\\ge 0} \\binom{k}{n}x^{n}\n\\\\&= \\sum_{k\\ge 0} x^k(1+x)^k\n\\\\&= \\frac1{1-(x+x^2)}\n\\end{align}\nAll that remains is to recall $\\sum_{n\\ge 0} f_nx^n=\\frac{x}{1-x-x^2}$. \n",
    "tags": [
      "combinatorics",
      "proof-writing",
      "binomial-coefficients",
      "fibonacci-numbers"
    ],
    "score": 5,
    "answer_score": 7,
    "is_accepted": true,
    "question_id": 3086355,
    "answer_id": 3086382
  },
  {
    "theorem": "If $a\\mid b$ and $a\\mid c$ prove $a\\mid a^2 + 3b - 2^{b} c$",
    "context": "So I have this problem for homework so I'm not looking for an answer, just some help moving forward.\nSo far, I have recognized that \n$$\\begin{align*}\na\\mid b &\\Rightarrow b = ak,\\\\a\\mid c &\\Rightarrow c = aj,\\\\a\\mid a &\\Rightarrow a = ah,\\end{align*}$$ where $k, j, h\\in\\mathbb{Z}$.\nFrom there I have $a + b + c = ak + aj + ah = a(k + j + h) = ai$ where $i\\in\\mathbb{Z}$.\nSo I have been able to prove that $a\\mid a + b + c$ but I am lost as to how to move onward to prove the desired statement.\nAny and all help or suggestions are very much appreciated!\n",
    "proof": "Hint. \nWhat you've done with $a+b+c$, you can do with $Ka+Lb+Mc$, for any integers $K$, $L$, $M$, right?\nNow choose $K=a$, $L=\\ldots$\n",
    "tags": [
      "discrete-mathematics",
      "proof-writing",
      "divisibility"
    ],
    "score": 5,
    "answer_score": 5,
    "is_accepted": true,
    "question_id": 2646405,
    "answer_id": 2646412
  },
  {
    "theorem": "Prove that the function below is discontinuous everywhere except at $x = 0$",
    "context": "$f(x)=\\begin{cases}0, \\; x \\notin \\mathbb{Q} \\\\   x , \\; x \\in \\mathbb{Q} \\end{cases} $\nI tried doing this proof through the epsilon delta definition using density of the rationals.\nThis is as far as I got:\nLet $a \\in \\mathbb{R} , a \\neq 0$ suppose $a \\in \\mathbb{Q} $ Let $ \\delta > 0$ by the densitiy of the irrationals there exists $x \\notin \\mathbb{Q}$ such that $|x - a| < \\delta$ then $f(x) = 0$ therefore $|f(a) - f(x)| = |f(a)| = |a|$\nI'm not sure what to do from this point  \n",
    "proof": "Let $a \\in \\mathbb{Q}$, $a \\not = 0$ and select $\\epsilon = \\dfrac {|a|}{2}$. In order for the limit to exist, there must exist a $\\delta$ such that $|f(x) - f(a)| < \\dfrac {|a|}{2}$ whenever $x \\in (a- \\delta, a+\\delta)  - \\{a\\}$.  Because of the density of irrationals, for any $\\delta$ there exists an irrational $x_0$ such that $x_0 \\in (a- \\delta, a+\\delta)  - \\{a\\}$. But this implies that $|f(x_0) - f(a)| < \\dfrac {|a|}{2} \\implies |0-a| < \\dfrac{|a|}{2}$, which is false.\n\nThe graph of $f$ is the blue line and the red line (with gaps in the of course). We can see that for any $a$, if we can find $\\epsilon$ small enough so that the blue line is outside of $(f(a)-\\epsilon, f(a) + \\epsilon)$, we could show that there's no $\\delta$ which satisfies the required proprety. \nIf $a=2$, we could pick any $\\epsilon$ smaller than $2$, such as $1.5$. But we needed to find an $\\epsilon$ that would work no matter what $a$ is, and $\\dfrac a2$ does the work. But you need to keep in mind that $a$ could be negative; since $\\epsilon>0$, we had to throw in the absolute value. \n",
    "tags": [
      "real-analysis",
      "proof-verification",
      "proof-writing"
    ],
    "score": 5,
    "answer_score": 3,
    "is_accepted": true,
    "question_id": 2287310,
    "answer_id": 2287328
  },
  {
    "theorem": "Any $n \\times n$ matrix $A$ can be written as $A = B + C$ with $B$ is symmetric and $C$ skew-symmetric.",
    "context": "How can I proof the following statement?\n\nAny $n \\times n$ matrix $A$ can be written as a sum\n  $$\n  A = B + C\n$$\n  where $B$ is symmetric and $C$ is skew-symmetric.\n\nI tried to work out the properties of a matrix to be symmetric or skew-symmetric, but I could not prove this.\nDoes someone know a way to prove it?\nThank you.\nPS: The question Prove: Square Matrix Can Be Written As A Sum Of A Symmetric And Skew-Symmetric Matrices may be similiar, in fact gives a hint to a solution, but if someone does not mind in expose another way, our a track to reach to what is mentioned in the question of the aforementioned link.  \n",
    "proof": "Suppose\n$$A=B+C$$\nIf $$B^T=B, $$\n$$C^T=-C,$$ then according to the known property of transposition of sum of matrices\n$$A^T=(B+C)^T=B^T+C^T=B+(-C)=B-C$$\nNow we have $$A=B+C \\tag 1\\\\ $$\n$$A^T=B-C\\tag 2\\\\$$\nAdding $(1)$ to $(2)$ gives $$B={(A+A^T)\\over 2}\\\\ $$\nSubtracting $(2)$ from $(1)$ gives $$C={(A-A^T)\\over 2}\\\\ $$\n",
    "tags": [
      "linear-algebra",
      "proof-writing",
      "alternative-proof",
      "proof-explanation"
    ],
    "score": 5,
    "answer_score": 4,
    "is_accepted": true,
    "question_id": 1817421,
    "answer_id": 1840358
  },
  {
    "theorem": "Explanation for the the number of trailing zeros in a factorial.",
    "context": "I was doing a programming problem that asked that I find the number of trailing zeros for a factorial, and I came up with this:\nfunction zeros (n) {\n  let numZeros = 0, power = 5;\n  while (power <= n) {\n    numZeros += Math.floor(n / power);\n    power *= 5;\n  }\n  return numZeros;\n}\n\nBasically through trial and error I found that the number of zeros in a given factorial was equal to:\n$$\\frac{n}{5} + \\frac{n}{25} +...+\\frac{n}{5^x}$$\nWhile $5^x$ was less than or equal to n, and $\\frac{n}{5^x}$ was rounded down to an integer value.\nI'd like to be able to write some kind of proof for this, but I don't know where to get started. I've never written a proof before.\n",
    "proof": "You add a zero every time that you multiply by $10$. Since the only prime factors of $10$ are $2$ and $5$, then clearly the trailing number of zeros in a number is the minimum of the two exponents in the prime factorization of that number.\nTo relate this to the formula you found, note that when computing a factorial, you will add a zero to the end every time that you multiply by a multiple of $5$—there's always an upaired factor of $2$ available to make $10$. When that multiple of $5$ is also a multiple of $25$, you’ll add an extra zero, three zeros when you hit a multiple of $5^3$, and so on.\n",
    "tags": [
      "proof-writing",
      "factorial"
    ],
    "score": 5,
    "answer_score": 5,
    "is_accepted": true,
    "question_id": 1543659,
    "answer_id": 1543704
  },
  {
    "theorem": "Proof by induction: inequality $n! &gt; n^3$ for $n &gt; 5$",
    "context": "I'm given a inequality as such: $n! > n^3$\nWhere n > 5, \nI've done this so far:\nBC: n = 6, 6! > 720 (Works)\nIH: let n = k, we have that: $k! > k^3$\nIS: try n = k+1, (I'm told to only work from one side)\nSo I have (k+1)!, but I'm not sure where to go from here.\nI've been told that writing out: $(k+1)! > (k+1)^3$ is a fallacy, because I can't sub in k+1 into both sides, but rather prove from one side only.  \nAny Help on how to continue from here, would be much appreciated. \n",
    "proof": "You have $(k+1)!=(k+1)k!>(k+1)k^3$ by the induction hypothesis. Now it suffices to show that if $k>5$ then $k^3 > (k+1)^2$. To do that, we write $(k+1)^2 = k^2 +2k+1 < k^2 + kk + k^2 = 3k^2 < kk^2 = k^3$, since we're assuming $k>5$.\n",
    "tags": [
      "inequality",
      "proof-writing",
      "induction",
      "factorial"
    ],
    "score": 5,
    "answer_score": 4,
    "is_accepted": true,
    "question_id": 1480832,
    "answer_id": 1480857
  },
  {
    "theorem": "Why is this proof for an arbitrary function constrained to a constant one?",
    "context": "Sorry if this seems trivial, I'm having some difficulty understanding a proof.\nI'm doing exercise 5.1.14 of Velleman's How to Prove It and a solution posted in this question, including the comments, does not make sense to me. I'll restate the problem here:\nSuppose $A$ is a nonempty set and $f:A\\longrightarrow A$. Also suppose that for all $g:A\\longrightarrow A, f\\circ g=f$. Prove that f is a constant function. Hint: What happens if $g$ is constant?\nProving $f$ is constant, if $g$ is constant was easy, but I don't understand how that helps solve the problem. The way I read the question, $g$ has to be an arbitrary function from $A$ to $A$, and restricting it to a constant one no longer makes it arbitrary. As an example, if $g=\\text{id}_A$, then $g$ is not constant, but still satisfies the conditions and $f\\circ g=f$.\nMy question boils down to: why can we discount all non-constant functions $g$?\n",
    "proof": "Here's a simpler statement along the same lines:\n\nWhat real $y$ satisfies that, for all other real numbers $x$, we have $yx=y$?\n\nWe could look at any $x$ we wanted to, but it just happens to be convenient to notice that $x=0$ causes us to conclude $0=y$. This suffices to show us \"if such a $y$ exists, it is $0$\". Then, all that's left to do is to check whether $y=0$ actually has the desired property for all other $x$.\nYou're in a similar situation: You've proven, by looking at only constant functions, that if such an $f$ exists, it is constant. Then, your only remaining task is to show all constant $f$ satisfy the given property. So, we're not discounting the other cases - we should still check them, but that's easy after we've leveraged the case of constant $g$ to show that $f$ must be constant.\n",
    "tags": [
      "functions",
      "proof-writing",
      "function-and-relation-composition"
    ],
    "score": 5,
    "answer_score": 4,
    "is_accepted": true,
    "question_id": 1373983,
    "answer_id": 1374003
  },
  {
    "theorem": "Proof of the Product Limit Law",
    "context": "Theorem:\n$$\\lim_{x \\to a} f(x) = L$$\n$$\\lim_{x \\to a} g(x) = M$$\nThen:\n$$\\lim_{x \\to a} f(x) g(x) = LM$$\nObviously,\n$$|f(x) - L| < \\epsilon$$\n$$|g(x) - M| < \\epsilon$$ \nBut multiplying these together doesnt get the desired:\n$$|f(x)g(x) - LM| < \\epsilon$$\nPlease, HINTS only!\n",
    "proof": "Hint:\n$$\\begin{align}|f(x)g(x)-LM|&=|f(x)g(x)-Lg(x)+Lg(x)-LM|\\\\&=|g(x)(f(x)-L)+L(g(x)-M)|\\\\&\\le|g(x)||f(x)-L|+|L||g(x)-M|\\end{align}$$\n",
    "tags": [
      "calculus",
      "real-analysis",
      "limits",
      "proof-writing",
      "epsilon-delta"
    ],
    "score": 5,
    "answer_score": 9,
    "is_accepted": true,
    "question_id": 1168905,
    "answer_id": 1168910
  },
  {
    "theorem": "Grade this proof of a surjective map from $\\mathbb{R}^3$ to $\\mathbb{R}^3$.",
    "context": "I just received this homework proof back in my abstract algebra class with a grade of 20%.  I feel very cheated, to say the least.  I present it here verbatim for your critiques.  Please tell me what is wrong here.\nprove: $f: \\mathbb{R}^3 \\rightarrow \\mathbb{R}^3$ defined by $f(x,y,z)=(x+y,y+z,x+z)$ is onto.\nwe want to show: for every $b \\in \\mathbb{R}^3$, there exists $a \\in \\mathbb{R}^3$ such that $f(a)=b$\nlet $b=(p,q,r) \\in \\mathbb{R}^3$\n$f\\left(\\frac{p}{2}-\\frac{q}{2}+\\frac{r}{2},\\frac{p}{2}+\\frac{q}{2}-\\frac{r}{2},-\\frac{p}{2}+\\frac{q}{2}+\\frac{r}{2}\\right)$\n$=\\left(\\frac{p}{2}-\\frac{q}{2}+\\frac{r}{2}+\\frac{p}{2}+\\frac{q}{2}-\\frac{r}{2},\\frac{p}{2}+\\frac{q}{2}-\\frac{r}{2}-\\frac{p}{2}+\\frac{q}{2}+\\frac{r}{2},\\frac{p}{2}-\\frac{q}{2}+\\frac{r}{2}-\\frac{p}{2}+\\frac{q}{2}+\\frac{r}{2}\\right)$\n$=\\left(\\frac{p}{2}+\\frac{p}{2},\\frac{q}{2}+\\frac{q}{2},\\frac{r}{2}+\\frac{r}{2} \\right)$\n$=\\left(p,q,r \\right)$\nhence, for every $b=(p,q,r) \\in \\mathbb{R}^3$, there exists $a \\in \\mathbb{R}^3$ defined as $a=\\left(\\frac{p}{2}-\\frac{q}{2}+\\frac{r}{2},\\frac{p}{2}+\\frac{q}{2}-\\frac{r}{2},-\\frac{p}{2}+\\frac{q}{2}+\\frac{r}{2}\\right)$ such that $f(a)=b$\ntherefore $f:\\mathbb{R}^3\\rightarrow\\mathbb{R}^3$ defined by $f(x,y,z)=(x+y,y+z,x+z)$ is onto\n$\\square$\n",
    "proof": "Here is your MSE stamp of approval: your solution is completely correct.\n",
    "tags": [
      "abstract-algebra",
      "proof-verification",
      "proof-writing"
    ],
    "score": 5,
    "answer_score": 5,
    "is_accepted": true,
    "question_id": 944956,
    "answer_id": 944963
  },
  {
    "theorem": "Proof of $x*0=0$ is invalid?",
    "context": "My professor told me today that while my logic was good and all my steps after the assumption were correct, my argument was invalid because I was assuming what I want to prove.  I don't see how.  Neither do any of my peers.  If someone could explain it to me, I would be greatly appreciative.\nUsing the axioms of ring theory, prove $b*0 = 0$.\nFirst, I proved that 0*1 = 0.  Then I did the following.  \n$Proof$: Suppose $a*b=a $ for all $b \\in \\mathbb{R}$.   (This is where he said I made the error)\nBy Additive Identity we have $(a+0)*b = a$.\nBy Distribitive Property we have $a*b + 0*b = a$.\nBy assumption, $a*b = a$, so we have $a+ 0*b = a$.\nSo $(-a)+a+0*b = (-a)+a$.\nBy Associativity, we have $(-a+a)+0*b = (-a+a)$.\nThen, $0+0*b = 0$ by Additive Inverses.\nSo $0*b = 0$ by Additive Identity.\nBy Commutativity of Multiplication, $0*b = b*0$, so $b*0 = 0$.\nAs I have been taught, we are allowed to make a basic assumption when writing proofs and then see what follows.  What I have done is prove the following statement.   \nIf $a*b =a$,  $\\forall b \\in \\mathbb{R} $, then $a=0$.\nI don't see how this is different than the hint we were given: If $a+b=a,$ then $b=0.$  In both cases, we are allowed to assume the if statement is true and then show that the result follows.  After talking to a lot of my peers, we are all of the same mind.   I think we would all benefit from a discussion as to why we cannot do this.\n",
    "proof": "You didn't show the asked property, all you've said is if there is an element \"a\" with such property then this element must be zero.\nHere is a simple proof of it in terms of basic ring properties:\n$$b\\cdot 0 = b(0 + 0)= b\\cdot 0 + b\\cdot 0$$ \n$$\\implies -(b\\cdot 0) + b\\cdot 0 = (-(b\\cdot 0) + b\\cdot 0) + b\\cdot 0$$ \n$$\\implies 0 = 0 + b\\cdot 0= b\\cdot 0$$ \n$$\\implies 0=b \\cdot 0$$\n",
    "tags": [
      "proof-verification",
      "proof-writing",
      "axioms",
      "fake-proofs"
    ],
    "score": 5,
    "answer_score": 5,
    "is_accepted": true,
    "question_id": 911490,
    "answer_id": 911515
  },
  {
    "theorem": "Prove! that $a+(1/a) ≥ 2$ and $a+(1/a) ≤2$",
    "context": "Let $a \\in R$ \n\nIf $a>0$, then $a+\\frac1a\\geq2$\nIf $a<0$, then $a+\\frac1a\\leq2$\n\nThis is how someone explained the first one to me but still not really sure about it.\nProof:\n$\\Longleftrightarrow$$a+\\frac1a\\geq2$ \n$\\Longleftrightarrow$ the square of any real number is non-negative\n  so we have $(a-1)^2\\geq0$ (don't understand this part)\n$\\Longleftrightarrow$ $a^2-2a+1\\geq0$\n$\\Longleftrightarrow$ $a^2+1\\geq2a$\n$\\Longleftrightarrow$ since $a>0$ then so is $\n  a+\\frac1a≥2$ if $a>0$\n",
    "proof": "Think about it in the other direction: If you square any real number you get a nonnegative result, so\n$$(a - 1)^2 \\ge 0$$\nExpand the left side:\n$$a^2 - 2a + 1 \\ge 0$$\nIf $a > 0$, we divide by $a$ to find\n$$a - 2 + \\frac 1 a \\ge 0$$\nor upon rearrangement, the desired inequality.\n\nIf $a < 0$, division by $a$ reverses the inequality.\n",
    "tags": [
      "proof-writing"
    ],
    "score": 5,
    "answer_score": 9,
    "is_accepted": false,
    "question_id": 657706,
    "answer_id": 657710
  },
  {
    "theorem": "Well-ordering principle on $\\mathbb N \\iff$ Principle of induction $\\iff$ Principle of strong induction: How to prove one of them?",
    "context": "\nWell-ordering principle on $\\mathbb N \\iff$ Principle of induction $\\iff$ Principle of strong induction\n\nHow to prove one of them ?\nOn Proofwiki there is an article proving the equivalence of the statements listed above. However the proof of an individual statement depends on the proof of one of the others, so in order to prove them all, one must prove one of them ?\nI know we can prove the Principle of induction by assuming there is a smallets $n \\in \\mathbb N$ such that $P(n)$ doesnt hold and then get a contradiction, since we have already proved $P(0) \\land P(n-1) \\Rightarrow P(n)$, so $P(n)$ is indeed true.\nHowever in this proof we are actually relying on the well-ordering principle on $\\mathbb N$ ? Otherwise how can one assume there is a smallets $n\\in \\mathbb N$ such that $P(n)$ is false ?\n",
    "proof": "These are principles, not theorems.  \nIf one principle holds, then they all must hold. If one principle fails to hold, they all fail to hold.  Hence, the principles are equivalent. (This is why we see the biconditional $\\iff$ being used.)\nPut another way, what we have is the following true assertion:\n\nWell-ordering principle on $\\mathbb N \\iff$ Principle of induction $\\iff$ Principle of strong induction.\n\nThis assertion claims nothing about the truth value of any one of the principles: it claims only that if one principle is true, they all must be true, and if one principle is false, they all must be false.  \nAnd hence, we adopt the well-ordering principle on $\\mathbb N$ as an axiom, or else derived from the principle of induction, which is itself basic (taken to be true).\n",
    "tags": [
      "proof-writing",
      "induction",
      "proof-verification"
    ],
    "score": 5,
    "answer_score": 6,
    "is_accepted": true,
    "question_id": 643982,
    "answer_id": 643988
  },
  {
    "theorem": "Prove that $2^n&gt;2n$ for all integral values of n greater than 2",
    "context": "Prove $2^n >2n$ for all integral values of n greater than 2.\nLet $p_n$ be the statement:\n$$2^n>2n\\ \\forall\\ n\\gt2$$ \nIf the inequality is valid for $n=k$ where $k>2$:\n$$p_k: 2^k>2k$$\nThen for $n=k+1$:\n$$p_{k+1} = 2^{k+1}>2(k+1)$$\nI don't know how to do the inductive step itself, I have only done series/recurrence relations inductions. Have I used the correct layout/notation? Is there more cool notation I could add to improve the mathematical-ness of the proof?\nThanks\n",
    "proof": "Don't forget the \"base case\": $p(3)$. It may seem obviously true, but formally, an inductive proof requires it.\nWith respect to the inductive step, note that $$2^{k+1} = \\underbrace{2 \\cdot 2^k \\gt 2\\cdot 2k}_{\\text{inductive step}} = 4k = \\underbrace{\\color{blue}{\\bf 2k+2k\\geq 2k+2}}_{\\text{for all }\\; k \\geq 1}=2(k+1)$$\n",
    "tags": [
      "algebra-precalculus",
      "proof-writing",
      "induction"
    ],
    "score": 5,
    "answer_score": 8,
    "is_accepted": true,
    "question_id": 480632,
    "answer_id": 480639
  },
  {
    "theorem": "How to prove or statements",
    "context": "How do I prove statements of the following types:\n$A \\text{ or } B \\implies$ C\n$A \\implies B \\text{ or } C$\nI don't know how to go about proving statements like this when they have \"or\". Can someone tell me different ways to prove such statements? And maybe give me some basic examples to help?\nThank-you.\n",
    "proof": "For the first, you can prove both $A\\Rightarrow C$ and $B\\Rightarrow C$.\nExample. If $x=1$ or $x=2$ then $x^2-3x+2=0$.\nProof: \nFirst assume $x=1$. Then $x^2-3x+2=1^2-3\\cdot 1+2=1-3+2=0$ as was to be shown.\nNext assume $x=2$. Then $x^2-3x+2=4-3\\cdot 2+2=4-6+2=0$ as was to be shown.\nSince both cases $x=1$ and $x=2$ lead to $x^2-3x+2$, we are done. $_\\square$\nFor the second, you can for example prove $(A\\land \\neg B)\\Rightarrow C$.\nExample. If $a\\cdot b=0$ then $a=0$ or $b=0$.\nProof: Assume $ab=0$ and $a\\ne 0$. Since $a\\ne 0$, we are allowed to divide both sides of $ab=0$ by $a$. By this we obtain $b=0$ as was to be shown. $_\\square$\n",
    "tags": [
      "logic",
      "proof-writing",
      "propositional-calculus"
    ],
    "score": 5,
    "answer_score": 5,
    "is_accepted": false,
    "question_id": 396253,
    "answer_id": 396261
  },
  {
    "theorem": "Number of straight line segments determined by $n$ points in the plane is $\\frac{n^2 - n}{2}$",
    "context": "How can we prove by mathematical induction that for all $n$, the number of straight line segments determined by $n$ points in the plane, no three of which lie on the same straight line, is $\\frac{n^2 - n}{2}$? (The line segment determined by two points is the line segment connecting\nthem.)\nI know we start with the base case, where, if we call the above equation $P(n)$, $P(0)$, for $0$ lines would be $0$. But I really have no idea how to begin the inductive step. How do we know what $k+1$ we're supposed to arrive at?\n",
    "proof": "Let $P(n)$ be the statement that the number of straight line segments determined by $n$ points in the plane no three of which lie on the same straight line is $\\frac{n^2-n}{2}$.\nWhen there is $1$ point, there are $0=\\frac{1^2-1}{2}$ line segments. Hence we have $P(1)$.\nNow suppose we have $P(k)$ for some positive integer $k$. Then $k$ points determine $\\frac{k^2-k}{2}$ line segments. When we add another point, we connect this point to each of the existing $k$ lines. So now we have $\\frac{k^2-k}{2}+k=\\frac{k^2-k+2k}{2}=\\frac{k^2+k}{2}=\\frac{(k+1)^2-(k+1)}{2}$ line segments. This means that we have $P(k+1)$.\nHence we have $P(n)$ for all positive integers $n$.\n",
    "tags": [
      "discrete-mathematics",
      "proof-writing",
      "induction"
    ],
    "score": 5,
    "answer_score": 3,
    "is_accepted": false,
    "question_id": 331383,
    "answer_id": 331389
  },
  {
    "theorem": "Prove that for $n \\in \\mathbb{N}, \\sum\\limits_{k=1}^{n} (2k+1) = n^{2} + 2n $",
    "context": "I'm learning the basics of proof by induction and wanted to see if I took the right steps with the following proof:\nTheorem: for $n \\in \\mathbb{N},  \\sum\\limits_{k=1}^{n} (2k+1)  =  n^{2} + 2n $\nBase Case: \nlet $$ n = 1 $$ Therefore $$2*1+1 = 1^{2}+2*1 $$ Which proves base case is true.\nInductive Hypothesis:\nAssume $$\\sum_{k=1}^{n} (2k+1)  =  n^{2} + 2n $$\nThen $$\\sum_{k=1}^{n+1} (2k+1)  =  (n+1)^{2} + 2(n+1) $$\n$$\\iff (2(n+1) +1)+ \\sum_{k=1}^{n} (2k+1)  =  (n+1)^{2} + 2(n+1) $$\nUsing inductive hypothesis on summation term:\n$$\\iff(2(n+1) +1)+ n^{2} + 2n  =  (n+1)^{2} + 2(n+1) $$\n$$\\iff 2(n+1) = 2(n+1)  $$\nHence for $n \\in \\mathbb{N},  \\sum\\limits_{k=1}^{n} (2k+1)  =  n^{2} + 2n $ Q.E.D.\nDoes this prove the theorem? Or was my use of the inductive hypothesis circular logic?\n",
    "proof": "Your proof looks fine, but if you know that \n$$1+2+...+n=\\frac{n(n+1)}{2}$$\nthen you can evaluate\n$$\\sum_{k=1}^n(2k+1)=2\\sum_{k=1}^n k+\\sum_{k=1}^n1=\\rlap{/}2\\frac{n(n+1)}{\\rlap{/}2}+n=n^2+2n$$\n",
    "tags": [
      "summation",
      "proof-writing",
      "induction"
    ],
    "score": 5,
    "answer_score": 5,
    "is_accepted": true,
    "question_id": 173120,
    "answer_id": 173126
  },
  {
    "theorem": "If a graph of $2n$ vertices contains a Hamiltonian cycle, then can we reach every other vertex in $n$ steps?",
    "context": "\nProblem: Given a graph $G,$ with $2n$ vertices and at least one triangle. Is it possible to show that you can reach every other vertex in $n$ steps if $G$ contains a Hamilton cycle (HC)?\nEDIT: Sorry, I forgot to mention, that $G$ is planar and 3-connected. A complete proof for $3$-regular graphs would also be accepted/rewarded.\n\nDoes the following work as proof?\n\nChoose a starting vertex $v_0$ and a direction.\n\nIf you walk along the HC you'll reach a vertex $v_{n-1}$ with maximal distance from $v_0$ in $n$ steps.\nYou'll reach $v_{n-2}$ by doing a round in the triangle and\n$v_{n-3}$ by stepping backwards at the last step.\nBy combining these moves, you'll reach half of all $v_k$.\nBy choosing the other direction at the beginning you'll reach the other half.\n$v_0$ is free to choose.\n\n\nShowing or disproving the \"only if\"-part would also be nice!\n",
    "proof": "The answer is no.\nQuestion: Let $G$ be a 3-connected, hamiltonian, planar graph with $2n$ vertices and at least one triangle. Is it true that for all vertex pairs $x,y$, that there is a walk of exactly $n$ steps from $x$ to $y$?\nThe following graph and vertex pair is a counter example\n\nIt is clear that the graph is planar and has a triangle. It can be easily verified that the graph is 3-connected. To show that the graph is hamiltonian, I have highlighted a hamiltonian cycle here\n\nSince the vertex has 16 vertices, we need to verify that there is no walk of length 8 from $x$ to $y$. Since $n$ is equal, we can not reach $y$ without using some of the four vertices on the right. Now it is easy to verify by hand, that there is no walk from $x$ to $y$ of length exactly 8.\n",
    "tags": [
      "graph-theory",
      "proof-writing",
      "open-problem"
    ],
    "score": 5,
    "answer_score": 1,
    "is_accepted": true,
    "question_id": 125159,
    "answer_id": 134114
  },
  {
    "theorem": "Need help with very simple set theoretic proofs",
    "context": "I am self studying Munkres' Topology book, and I'm having a hard time writing down proofs that relate to set theory. I can see why certain arguments are true, but constructing a formal proof seems to be difficult. Here's a very simple example of a problem in the book.\nLet $f:A \\rightarrow B$\n(a) Prove that $A_0 \\subset f^{-1}(f(A_0))$, show that $A_0 = f^{-1}(f(A_0))$ if $f$ is injective.\n(b) Prove that $f(f^{-1}(B_0)) \\subset B_0 $, show that $B_0 = f(f^{-1}(B_0))$ if $f$ is surjective.\nConceptually, the ideas are simple. In (a), let $S$ be the image set of $A_0$ under $f$ (sorry if I use non-conventional notation, I'm still somewhat new to this). This means that $S= \\{ b \\mid f(a)=b \\text{ for at least one } a \\}$. This we take the inverse we may end up with some of the $a \\in A^C_0$, where $A^C_0$ is the complement of $A_0$ in $A$. This happens because of the at least one in the definition of S. If we said exactly on instead, i.e. $f$ is injective, we can see that $f^{-1}(f(A_0))$ would give us $A_0$ back.\nIn (b) -- I'll scan through this one quickly -- let $P$ be the pre-image of $B_0$ under $f^{-1}$. Then $f^{-1}(B_0)$ gives us all points $a$ s.t. $f(a)=b \\in B_0$ However we cannot guarantee that every $b \\in B_0$ is the image of some a. Therefore $f(f^{-1}(B_0))$ only gives a subset of the original $B_0$. If we can guarantee that every $b \\in B_0$ has a matching $a \\in A$, that is $f$ is surjective, we get the entire $B_0$ back.\nNow that I've shown that I understand how to prove both (a) and (b), can someone help me put these proofs in elegant mathematical form?\n",
    "proof": "Here is a more precise way of phrasing your proof.\nLet $f : A \\to B$ be a function, $A_0 \\subseteq A$, $B_0 \\subseteq B$. Let $f_! : \\mathscr{P}(A) \\to \\mathscr{P}(B)$ be the function mapping subsets of $A$ to their images under $f$; that is, $f_! (A_0) = \\{ b \\in B : \\exists a_0 \\in A_0. \\, f(a_0) = b \\}$. Let $f^* : \\mathscr{P}(B) \\to \\mathscr{P}(A)$ be the preimage map, that is, $f^*(B_0) = \\{ a \\in A : \\exists b_0 \\in B_0 . \\, f(a) = b_0 \\}$. \n\nFollowing the definitions above, $f^* ( f_! (A_0)) = \\{ a \\in A : \\exists a_0 \\in A_0 . \\, f(a) = f(a_0) \\}$. It is immediate that $A_0 \\subseteq f^*(f_!(A_0))$. If $f$ is moreover injective, then $f(a) = f(a_0)$ if and only if $a = a_0$, so in that case $A_0 = f^*(f_!(A_0))$.\nSimilarly, $f_!(f^*(B_0)) = \\{ b \\in B : \\exists a \\in A . \\, \\exists b_0 \\in B_0 . \\, f(a) = b = b_0 \\}$. It is immediate that $f_!(f^*(B_0)) \\subseteq B_0$. If $f$ is moreover surjective, then for all $b$ in $B$, there exists an $a$ in $A$ such that $f(a) = b$, so it follows that $f_!(f^*(B_0)) = B_0$ in this case.\n\n\nBut here is what I think an elegant proof looks like. Observe that $f_!$ and $f^*$ are inclusion-preserving maps and moreover have the following adjunction property:\n$$f_!(A_0) \\subseteq B_0 \\text{ if and only if } A_0 \\subseteq f^*(B_0)$$\nThis is obvious from the definition. We say $f_!$ is the left adjoint of $f^*$, and of course, $f^*$ is the right adjoint of $f_!$. We obtain the following results:\n\n$A_0 \\subseteq f^*(f_!(A_0))$. [Set $B_0 = f_!(A_0)$ and use the adjunction property.]\n$f_!(f^*(B_0)) \\subseteq B_0$. [Set $A_0 = f^*(B_0)$ and use the adjunction property.]\nNow consider $f_!(f^*(f_!(A_0)))$. By (1), we have $f_!(A_0) \\subseteq f_!(f^*(f_!(A_0)))$, and by (2) we have $f_!(f^*(f_!(A_0))) \\subseteq f_!(A_0)$. So $f_!(A_0) = f_!(f^*(f_!(A_0)))$. But it is clear that $f_!$ is injective if $f$ is injective, so we can cancel $f_!$ from both sides of the equation to get $A_0 = f^*(f_!(A_0))$ in that case.\nSimilarly, $f^*(f_!(f^*(B_0))) = f^*(B_0)$. Observe that $f^*$ is injective if $f$ is surjective. [Confusing, but true!] So in that case we can cancel $f^*$ and get $f_!(f^*(B_0)) = B_0$.\n\nThe reason I think this is an elegant proof is because it highlights the symmetry underlying the two questions. Of course, you are free to disagree.\nExercise. Show that $f^* : \\mathscr{P}(B) \\to \\mathscr{P}(A)$ itself has a right adjoint $f_* : \\mathscr{P}(A) \\to \\mathscr{P}(B)$. [It is called the dual image map.] Using the only the adjunction properties between $f_!$, $f^*$, and $f_*$, prove the following:\n\n$f_!$ and $f^*$ preserve all unions and the empty set. [Use the fact that they are left adjoints.]\n$f^*$ and $f_*$ preserve all intersections and the full set. [Use the fact that they are right adjoints.]\n\n",
    "tags": [
      "functions",
      "elementary-set-theory",
      "proof-writing"
    ],
    "score": 5,
    "answer_score": 3,
    "is_accepted": true,
    "question_id": 55791,
    "answer_id": 55809
  },
  {
    "theorem": "Critique my proof of: P → (Q → R). Then &#172;R → (P → &#172;Q)",
    "context": "I'm a beginner at mathematical proof and I want to improve my proof writing skills. Any critique on correctness, structure, etc. is much appreciated. Below is a theorem and my proof of it.\nTheorem. Suppose $P → (Q → R).$ Then $¬R → (P → ¬Q).$\nProof. Suppose $¬R,$ $P,$ and $Q → R.$ Because $Q → R$ and $¬R,$ it must be the case that $¬Q$ because $P$ and $P → (Q → R).$ Therefore, because when $¬R,$ $P$ and $¬Q,$ it must be the case that $¬R→(P→ ¬Q).$\n",
    "proof": "Another approach is to write both propositions in conjunctive normal form.  The first one is\n$$\nP \\implies (\\lnot Q \\implies R) \\\\\n\\lnot P \\lor (Q \\lor R) \\\\\n\\lnot P \\lor Q \\lor R\n$$\nThe second one is\n$$\n\\lnot R \\implies (P \\implies \\lnot Q) \\\\\nR \\lor (\\lnot P \\lor Q) \\\\\nR \\lor \\lnot P \\lor Q \\\\\n\\lnot P \\lor Q \\lor  R\n$$\nIn fact, we have shown that the two propositions are equivalent, which is more than what was required.\n",
    "tags": [
      "proof-writing",
      "solution-verification",
      "propositional-calculus"
    ],
    "score": 5,
    "answer_score": 2,
    "is_accepted": true,
    "question_id": 4338662,
    "answer_id": 4338699
  },
  {
    "theorem": "Creating a new category that is the &#39;opposite&#39; of an existing category (Ex 3.1 from Aluffi Chapter 0)",
    "context": "This is the first exercise in the section that introduces Categories in Aluffi's Algebra text (Ch.1 Sec.3). Since this is my first exposure to categories, I wanted to know if:\n\nI'm properly understanding the basic definition of a category\nMy logic is correct\nThe language and notation in my proof are idiomatic (e.g. Aluffi seems to use set-theoretic notation like $\\in$ in the exposition).\n\nExercise 3.1:\nLet $\\mathsf{C}$ be a category. Consider a structure $\\mathsf{C}^{op}$ with\n\n$\\mathrm{Obj}(\\mathsf{C}^{op}) := \\mathrm{Obj}(\\mathsf{C})$\nfor $A,B$ objects of $\\mathsf{C}^{op}$ (hence objects of $\\mathsf{C}$), $\\mathrm{Hom}_{\\mathsf{C}^{op}}(A,B) := \\mathrm{Hom}_{\\mathsf{C}}(B,A)$.\n\nShow how to make this into a category (that is, define composition of morphisms in $\\mathsf{C}^{op}$ and verify the properties listed in $\\S3.1$).\nIntuitively, the 'opposite' category $\\mathsf{C}^{op}$ is simply obtained by 'reversing all the arrows' in $\\mathsf{C}$.\nMy Solution (original; see below for updated solution based on feedback):\nRemember that by definition, a category must have i) an identity morphism for all objects in the category and ii) a composition morphism for any pairs of morphisms. Additionally, the composition must satisfy two additional properties: a) associativity and b) unital (e.g. for $f: X \\to Y, f1_X = 1_Yf = f$).\ni) Identity morphism: If $A$ is an object in $\\mathsf{C}^{op}$, it also exists in $\\mathsf{C}$ (by definition of $\\mathrm{Obj}(\\mathsf{C}^{op})$). Since $\\mathsf{C}$ is a category, it satisfies the property of having an identity morphism for each object. Since we chose $A$ in $\\mathsf{C}^{op}$ arbitrarily, it follows that every object in $\\mathsf{C}^{op}$ has an identity morphism.\nii) Composition morphism: Let's define composition of morphisms as follows: for $f \\in \\mathrm{Hom}_{\\mathsf{C}^{op}}(A,B)$ and $g \\in \\mathrm{Hom}_{\\mathsf{C}^{op}}(B,C)$, define $fg \\in \\mathrm{Hom}_{\\mathsf{C}^{op}}(A,C)$ such that $fg = gf \\in \\mathrm{Hom}_{\\mathsf{C}}(C,A)$. We know $gf$ exists because $\\mathsf{C}$ is a category and thus satisfies the condition of having a composition morphism for all pairs of morphisms.\nTo prove that the composition morphism is both associative and unital:\na) Associativity: Let $f \\in \\mathrm{Hom}_{\\mathsf{C}^{op}}(A,B), g \\in \\mathrm{Hom}_{\\mathsf{C}^{op}}(B,C),$ and $h \\in \\mathrm{Hom}_{\\mathsf{C}^{op}}(C,D)$. Then $(fg)h = (gf)h = h(gf) = (hg)f = (gh)f = f(gh)$. Where the 1st, 2nd, 4th, and 5th equalities are due to the definition we chose for the composition of morphisms, and the 3rd equality is true because $\\mathsf{C}$ is a category and thus is associative.\nb) Unital: Let $f \\in \\mathrm{Hom}_{\\mathsf{C}^{op}}(A,B)$, then:\n\n$f1_A = 1_Af = f$\n$1_Bf = f1_B = f$\n\nWhere for both statements, the first equality is due to our definition of composition and the second equality is true because $\\mathsf{C}$ is a category and thus is unital.\nThank you\n\nSolution (updated):\nRemember that by definition, a category must have i) a composition morphism that satisfies associativity for any pairs of morphisms and ii) an identity morphism that is unital for all objects in the category (e.g. for $f: X \\to Y, f1_X = 1_Yf = f$).\ni) Composition morphism: Let's define composition of morphisms as follows: for $f \\in \\mathrm{Hom}_{\\mathsf{C}^{op}}(A,B)$ and $g \\in \\mathrm{Hom}_{\\mathsf{C}^{op}}(B,C)$, define $f \\circ{'} g \\in \\mathrm{Hom}_{\\mathsf{C}^{op}}(A,C)$ such that $f \\circ{'} g = g \\circ f \\in \\mathrm{Hom}_{\\mathsf{C}}(C,A)$. We know $g \\circ f$ exists because $\\mathsf{C}$ is a category and thus satisfies the condition of having a composition morphism for all pairs of morphisms.\nTo show that the composition morphism is associative, let $f \\in \\mathrm{Hom}_{\\mathsf{C}^{op}}(A,B), g \\in \\mathrm{Hom}_{\\mathsf{C}^{op}}(B,C),$ and $h \\in \\mathrm{Hom}_{\\mathsf{C}^{op}}(C,D)$. Then $(f \\circ{'} g) \\circ{'} h = (g \\circ f) \\circ{'} h = h \\circ (g \\circ f) = (h \\circ g) \\circ f = (g \\circ{'} h) \\circ f = f \\circ{'} (g \\circ{'} h)$. Where the 1st, 2nd, 4th, and 5th equalities are due to the definition we chose for the composition of morphisms, and the 3rd equality is true because $\\mathsf{C}$ is a category and thus its morphisms are associative.\nii) Identity morphism: If $A$ is an object in $\\mathsf{C}^{op}$, it also exists in $\\mathsf{C}$ (by definition of $\\mathrm{Obj}(\\mathsf{C}^{op})$). Since $\\mathsf{C}$ is a category, it satisfies the property of having an identity morphism for each object. Let's define $1_A$ for any $A$ in $\\mathsf{C}^{op}$ to be the same as $1_A$ for the same $A$ in $\\mathsf{C}$.\nTo show that the identity morphism is unital, let $f \\in \\mathrm{Hom}_{\\mathsf{C}^{op}}(A,B)$, then:\n\n$f \\circ{'} 1_A = 1_A \\circ f = f$\n$1_B \\circ{'} f = f \\circ 1_B = f$\n\nWhere for both statements, the first equality is due to our definition of composition and the second equality is true because $\\mathsf{C}$ is a category and thus its identity morphisms are unital.\n",
    "proof": "Seems generally correct, but you could be a bit more careful.\ni) An 'identity morphism' is just a distinguished element of $Hom_\\mathsf{C}(A,A)$. Given a choice of $1_A$ in the category $\\mathsf{C}$, it makes sense to define the identity morphism of $A$ in $\\mathsf{C}^{op}$ by choosing the same $1_A$, since you know $Hom_{\\mathsf{C}^{op}}(A,A) = Hom_\\mathsf{C}(A,A)$.\nA morphism being the identity is not an intrinsic property of some particular function, it is a structural property of a morphism in a category, so if you are building a new category you need to say what the identities are.\nii)b Take a morphism $f\\in Hom_{\\mathsf{C}^{op}}(A,B)$ corresponding to $f'\\in Hom_{\\mathsf{C}}(B,A)$. The morphism $f\\circ 1_A \\in Hom_{\\mathsf{C}^{op}}(A,B)$ is, by your definition of composition, the morphism corresponding to $1_A\\circ f'\\in Hom_{\\mathsf{C}}(B,A)$ which is $f'\\in Hom_{\\mathsf{C}}(B,A)$. Thus $f\\circ 1_A = f$.\nYou could just use the same letters for $f$ and $f'$, of course, I just wanted to emphasize that they 'go in different directions', i.e. 'live in different categories'.\nLogically speaking I would prefer you write ii) and ii)a together and put ii)b and i) together: the composition is a law for combining morphisms, calling something an identity only makes sense after defining composition. You cannot call $0$ an additive identity without knowing what addition is.\n",
    "tags": [
      "proof-writing",
      "category-theory",
      "solution-verification"
    ],
    "score": 5,
    "answer_score": 4,
    "is_accepted": true,
    "question_id": 4194415,
    "answer_id": 4194492
  },
  {
    "theorem": "Prove that if $S$ and $T$ are subspaces of $V$, then so is $S\\cap T$",
    "context": "I am asked to prove that \n\nIf $S$ and $T$ are subspaces of $V$, then so is $S\\cap T$.\n\nI have been thinking about this for a while but remain perplexed. I tried this:\nLet $\\{e_1, e_2, ..., e_k\\}$ be a basis for $S$ and let $\\{u_i, u_2, ..., u_k\\}$ be a basis for $T$.\nLet all $u_n=e_n$ be denoted $d_n$.\n$\\therefore$$\\{d_1, d_2, ..., d_p\\}$ is a basis for $S \\cap T$ if $S\\cap T$ is a linear space.\nNow all elements $x$ in $S$ have form $x=\\sum^{k}_{i=1}c_ie_i$ for some $c_i \\in \\Bbb R$ and all elements $y$ in $T$ have form $y=\\sum^k_{i=1}c_iu_i$ for some $c_i \\in \\Bbb R$.\nNow $S\\cap T$ contains all elements of $S$ and $T$ such that $x=y$.\n$\\therefore \\sum^k_{i=1}c_ie_i=\\sum^k_{i=1}c_iu_i$.\n$\\therefore e_i=u_i$.\n$\\therefore S\\cap T$ is a subspace with basis $\\{d_1, d_2, ..., d_p\\}$.\nI am not sure that this proof is valid, nor that it even really proves anything? Is there any merit to it? Otherwise, what would be a better approach to proving this?\n",
    "proof": "1) Since S, T are subspaces of V, the unique zero vector is contained in both, and so the zero vector is in the intersection.\n2) Let $a,b \\in S \\cap T$. That means a, b are in both, S and T. Since $S,T$ are subspaces, they are closed under addition and so $a+b$ is in S and in T and so in the intersection.\n3) Let c be an element of the reals. Let a be in the intersection of $S,T$. Since, $S,T$ are both subspaces, It follows that $ca$ is in both and therefore in the intersection.\n",
    "tags": [
      "linear-algebra",
      "proof-verification",
      "proof-writing"
    ],
    "score": 5,
    "answer_score": 6,
    "is_accepted": true,
    "question_id": 3287330,
    "answer_id": 3287333
  },
  {
    "theorem": "Examples where Strong Induction is more useful than &quot;Regular&quot; Induction?",
    "context": "The Principles of \"Regular\" and Strong Induction are equivalent (see for example here).\nBut are there any examples where something is more easily/elegantly proven with Strong rather than \"Regular\" Induction?\n(And if not, what use is Strong Induction -- why do we even bother mentioning it?)\n",
    "proof": "One common proof of the existence component of the fundamental theorem of arithmetic uses strong induction. \nSuppose that every natural number less than n factors as a product of primes. \nIf $n$ is prime then it factors uniquely as a product of primes. If $n$ is not prime then it can be written as the product of two numbers less than $n$ which by the hypothesis of strong induction, can both be written as the product of primes. Hence $n$ can be written as a product of primes.\nNotice that the argument wouldn't work with 'weak' induction.\n",
    "tags": [
      "proof-writing",
      "induction"
    ],
    "score": 5,
    "answer_score": 8,
    "is_accepted": true,
    "question_id": 2935204,
    "answer_id": 2935217
  },
  {
    "theorem": "Rigorous proof to show that the $15$-Puzzle problem is unsolvable",
    "context": "So this is supposedly a very popular puzzle by Sam Loyd. (I don't want answerers to provide solutions directly from some website etc. I mean, an ingenious solution is more welcome please.)\n\n\nNow, as you see all numbers are arranged in ascending order except the last two. The rule of the game is to move a numbered block which is adjacent to the blank space and create another blank place in its original position (I mean, you just shift the position of blank space around by sliding the numbers around) . \nNow the question goes as, can you arrange all the numbers in the correct ascending order? (The final outcome would be just interchange the positions of $15$ and $14$). \n\nI know of a solution using a very clever trick of invariance (the invariant seemed quite non-trivial to me ;-P). Can others here come up with some interesting solutions?\n",
    "proof": "This can be shown using some elementary abstract algebra.\nIn short, a puzzle configuration is not solvable if and only if it's an odd number of swaps away from the solved state, like the one in your image is (14 and 15 swapped = 1 swap).  I used this fact a few years ago to quickly generate random configurations for a 15-puzzle app I used to have in the App Store.\nReference:  https://kconrad.math.uconn.edu/blurbs/grouptheory/15puzzle.pdf\n",
    "tags": [
      "discrete-mathematics",
      "proof-writing",
      "recreational-mathematics",
      "puzzle",
      "invariance"
    ],
    "score": 5,
    "answer_score": 8,
    "is_accepted": true,
    "question_id": 2714774,
    "answer_id": 2714860
  },
  {
    "theorem": "A closed ball in a metric space is closed",
    "context": "\nProve that any closed ball in a metric space is closed.\n\n(Note that this is not a duplicate as this is a proof verification question and have a different proof, in my opinion, as compared to other proofs on this site)\nMy Attempted Proof:   Let $(X, d)$ be a metric space. Pick a closed ball $\\Phi = \\overline{B(x, r)} = \\{y \\in X \\ | \\ d(x, y) \\leq r\\}$. Note that at this moment in time $\\overline{B(x, r)}$ is just a notation, we don't know if $\\Phi$ is actually closed.\nWe now show that $\\Phi$ is closed. To do this we show that $X \\setminus \\Phi$ is open. Observe that $X \\setminus \\Phi = \\{y \\in X \\ | \\ d(x, y) > r\\}$. Pick $y \\in X \\setminus \\Phi$. For this $y$ we have $d(x, y) > r$ which implies that $d(x, y) = r + \\epsilon$ for some $\\epsilon > 0$. \nWe claim that $B(y, \\epsilon) \\subseteq X \\setminus \\Phi$. To prove this claim, pick $z \\in B(y, \\epsilon)$. We now show that $z \\in X \\setminus \\Phi$ by showing that $d(x, z) > r$. To that end observe that the triangle inequality gives us \n\\begin{align*} \n\t\t& d(x, y) \\leq d(x, z) + d(z, y) \\\\ \n\t\t& \\implies \\epsilon + r \\leq d(x, z) + d(y, z) \\\\\n\t\t& \\implies \\epsilon + r < d(x, z) + \\epsilon \\ \\ \\ \\ \\ \\  \\ \\ \\  \\text{since $d(y, z) < \\epsilon$} \\\\\n\t\t& \\implies r < d(x, z)\n\t\t\\end{align*}\nas desired. Hence $z \\in X \\setminus \\Phi$ and we have $B(y, \\epsilon) \\subseteq X \\setminus \\Phi$, and since $y$ was arbitrary we have that $X \\setminus \\Phi$ is an open set in $(X, d)$ and thus $\\Phi$ is a closed set. $\\square$\n\nIs this a rigorous and satisfactory proof? Can it be improved in any way?\n",
    "proof": "Your proof is flawless.\nThe only recommendation that I have is to change the notation on your closed ball. \n$\\overline{B(x, r)}$ is used for the closure of the open ball B(x, r) not the closed ball.\n$\\overline{B}(x,r)$ is a better notation for a closed ball.\nThe closure of an open ball is not necessarily the closed ball depending on the topology. \n",
    "tags": [
      "general-topology",
      "proof-verification",
      "metric-spaces",
      "proof-writing"
    ],
    "score": 5,
    "answer_score": 5,
    "is_accepted": true,
    "question_id": 2700749,
    "answer_id": 2700777
  },
  {
    "theorem": "Show that $2^{n+1}$ is $O(2^n)$",
    "context": "I just started an algorithm design and analysis class and i'm trying to learn how to properly prove things using big O. I was never taught much about big O in my previous CS classes other than the fact that the dominant term is the point of focus. I am trying to learn the proper way to prove things like this but everything I read confuses me even more. I know that:\n$2^{n+1} = 2 * 2^n = O(2^n)$\nBut I don't think this is not enough to prove this. From other examples i've seen, it seems I need to use a constant c to finish this proof. Is this the correct thought process? Are there other ways to prove something like this? What is the most efficient method of doing so?\nIf someone could help me understand the process, I would be so grateful. I've seen different ways of proving similar problems but i'm not sure which ways are correct.\nThanks so much in advance. \n",
    "proof": "Ok, so look at this page for the definition of Big O.\nWhat does it say? Well, $f(n)$ is said to be $O(g(n))$ if there exist constants $c$ and $k$ such that if $l \\geq k$ then $f(l) \\leq c \\times g(l)$.\nIn words : $f(n)$ is $O(g(n))$ if eventually, a constant multiple of $g$ is bigger than or equal to $f$.\nSo, why is it that $2^{n+1}$ is $O(2^n)$? Is it true, that eventually, a constant multiple of $2^n$ is always greater than $2^{n+1}$? Well, let's say we multiply $2^n$ by $2$,then it is $2^{n+1}$, which of course, is greater than or equal to $2^{n+1}$!\nSo with $c=2$ and $k=1$, we have $2 \\times 2^n \\geq 2^{n+1}$ for all $n \\geq 1$. Therefore , $2^{n+1}$ is $O(2^n)$. \nIf you have not understood , you may ask.\n",
    "tags": [
      "proof-writing",
      "asymptotics",
      "computer-science"
    ],
    "score": 5,
    "answer_score": 8,
    "is_accepted": true,
    "question_id": 2612966,
    "answer_id": 2612987
  },
  {
    "theorem": "Show that any integer can be expressed as $S^{k}_{n}$",
    "context": "Consider the following sequence: $a_1=6$, $a_2=4$, $a_3=1$, $a_4=2$, and $a_n=a_{n-4}$ for $n \\geqslant 5$.\nLet $S^{k}_{n}$ be the sum of $k + 1$ elements from $a_n$ of the previous sequence. For example:\n$\n  S^{0}_{3} = 1\n\\\\\n  S^{0}_{4} = 2\n\\\\\n  S^{1}_{3} = 2 + 1 = 3\n\\\\\n  S^{0}_{2} = 4\n\\\\\n  S^{1}_{2} = 4 + 1 = 5\n\\\\\n…\n\\\\\n  S^{6}_{2} = 4 + 1 + 2 + 6 + 4 + 1 + 2 = 20\n\\\\\n…\n\\\\\n$\nShow that any positive integer can be expressed as $S^{k}_{n}$.\n\nThis question was on an entrance exam for graduation. I wrote it from my memory, as the exam is not available online yet, maybe the wording is not 100% precise but I hope the examples are clear.\n",
    "proof": "As you notice, the sequence $a_n$ is by definition periodic: $6,4,1,2,6,4,1,2,6,4,1,2,...$\nThe sum of a whole \"period\" is $6+4+1+2=13$.\nYou can easily check that every number between $1$ and $12$ can be obtained as a sum of CONSECUTIVE numbers in the sequence $a_n$ (for instance, $9=1+2+6$). Then you just write any desired number $k$ (choose for example $k=139$) as a multiple of $13$ plus a number between $0$ and $12$ (in the example, $139 = 13 \\cdot 10 + 9$). Then it just suffices to choose the right starting point and the right number of elements (in the example: to get $9$ you need to start at some $1$ in the sequence, so that you get $1+2+6=9$; for example, $S_3^{42} = 1+2+6+4+1+2+6+4+...+1+2+6= 13 \\cdot 10 + 9 = 139$).\n",
    "tags": [
      "sequences-and-series",
      "proof-writing"
    ],
    "score": 5,
    "answer_score": 4,
    "is_accepted": true,
    "question_id": 2598836,
    "answer_id": 2598852
  },
  {
    "theorem": "Prove that $\\sum_{i=1}^{n} \\frac{n+i}{i+1} \\le 1 + n(n-1) \\ \\forall n \\in \\Bbb{N}$ (without calculus)",
    "context": "I'm trying to prove the following proposition (I'm not supposed to use calculus):\n$$\\sum_{i=1}^{n} \\frac{n+i}{i+1} \\le 1 + n(n-1) \\ \\forall n \\in \\Bbb{N}$$\n(I'm assuming that $0 \\notin \\Bbb{N}$)\nThis is what I've tried so far:\n$\\sum_{i=1}^{n} \\frac{n+i}{i+1} = \\sum_{i=1}^{n}[ \\frac{n}{i+1} + \\frac{i}{i+1} ] = [\\sum_{i=1}^{n} \\frac{n}{i+1}] + \\sum_{i=1}^{n} \\frac{i}{i+1} = n[\\sum_{i=1}^{n} \\frac{1}{i+1}] + \\sum_{i=1}^{n} \\frac{i}{i+1}$\nLet $j= i +1$ . So now we have:\n$\\sum_{i=1}^{n} \\frac{n+i}{i+1} =  n[\\sum_{j=2}^{n+1} \\frac{1}{j}] + \\sum_{j=2}^{n+1} \\frac{j-1}{j}$\n$= n[\\sum_{j=2}^{n+1} \\frac{1}{j}]  + \\sum_{j=2}^{n+1} \\frac{j}{j} - \\sum_{j=2}^{n+1} \\frac{1}{j}$\n$= n[\\sum_{j=2}^{n+1} \\frac{1}{j}]- \\sum_{j=2}^{n+1} \\frac{1}{j} + \\sum_{j=2}^{n+1} 1 $\n$= (n-1)[\\sum_{j=2}^{n+1} \\frac{1}{j}] + [\\sum_{j=1}^{n+1} 1 ] -1 $\n$= (n-1)[\\sum_{j=2}^{n+1} \\frac{1}{j}] + n+1 -1 $\n$= (n-1)[\\sum_{j=2}^{n+1} \\frac{1}{j}] + n$\nTherefore proving the following inequality is the same as proving the original:\n$$ (n-1)[\\sum_{j=2}^{n+1} \\frac{1}{j}] + n \\le 1 + n(n-1)$$\nIf $n=1$ then $ (n-1)[\\sum_{j=2}^{n+1} \\frac{1}{j}] + n = 1$ and $1 + n(n-1) = 1$\nNow I assume that $n>1$:\n$ (n-1)[\\sum_{j=2}^{n+1} \\frac{1}{j}] + n \\le 1 + n(n-1) \\iff  (n-1)[\\sum_{j=2}^{n+1} \\frac{1}{j}] + n \\le 1 + n^2 - n$\n$\\iff (n-1)[\\sum_{j=2}^{n+1} \\frac{1}{j}] \\le 1 + n^2 - 2n$\n$\\iff (n-1)[\\sum_{j=2}^{n+1} \\frac{1}{j}] \\le (n-1)^2$\n$\\iff \\sum_{j=2}^{n+1} \\frac{1}{j} \\le \\frac{(n-1)^2}{n-1}$\n$\\iff \\sum_{j=2}^{n+1} \\frac{1}{j} \\le n-1$\n$\\iff 1 + \\sum_{j=2}^{n+1} \\frac{1}{j} \\le n$\n$\\iff \\sum_{j=1}^{n+1} \\frac{1}{j} \\le n$\nAnd here is where I'm stuck. I know that an upper bound for the harmonic sum can be found using an integral test but I'm not supposed to use calculus. Is there a discrete way of proving that $\\sum_{j=1}^{n+1} \\frac{1}{j} \\le n$ for all natural $n \\ge 2$ ? Or maybe another way of proving the proposition without ending up with the harmonic sum?\n",
    "proof": "$$\\sum_{j=1}^{n+1} \\frac{1}{j} \\le n$$ is easy to prove of $n \\geq 2$. Note that the inequality fails for $n=1$.\nIndeed\n$$\\sum_{j=1}^{n+1} \\frac{1}{j}=\\sum_{j=1}^{n-1} \\frac{1}{j}+\\left( \\frac{1}{n}+\\frac{1}{n+1}\\right)<\\sum_{j=1}^{n-1} 1+\\left( \\frac{1}{2}+\\frac{1}{2}\\right)=n $$\n",
    "tags": [
      "elementary-number-theory",
      "inequality",
      "summation",
      "proof-writing"
    ],
    "score": 5,
    "answer_score": 4,
    "is_accepted": true,
    "question_id": 2560500,
    "answer_id": 2560512
  },
  {
    "theorem": "For any two sets, $A - B = B - A$ implies $A = B$",
    "context": "Is the following statement True or False:\n\nFor any two sets $A$ and $B$: If $A - B = B - A$ then $A = B$.\nIf it is true, prove it, otherwise provide a counterexample.\n\nI am unable to come up with a counter example. I think the statement is true but how do I prove it?\n",
    "proof": "If $A-B=B-A $ then for any $x\\in A-B=B-A $ we $x\\in A;x\\in B; x\\not \\in A; x\\not \\in B $.  That's a contradiction so $A-B=B-A $ is empty.\nThus there are no elements in $A $ that are not in $B$.  In other words $A $ is a subset of $B $.  Likewise there are no elements of $B $ that are in $A $.  So $B $ is a subset of $A $.\nSo $A=B $.\n",
    "tags": [
      "elementary-set-theory",
      "proof-writing"
    ],
    "score": 5,
    "answer_score": 3,
    "is_accepted": true,
    "question_id": 2333306,
    "answer_id": 2333376
  },
  {
    "theorem": "Prove if 2 divides $a^2$, then 2 divides $a$.",
    "context": "If 2 divides $a^2$, then 2 divides a.\nI know that 2 divides $a^2$ means there is some integer $n$ such that $a^2 = 2n$, \nand similarly, 2 divides $a$ means there is some integer $m$ such that $a = 2m$\nI thought I could rewrite $a^2 = 2n$ into this $=  a = 2(n/a)$ but I don't think that helps, because I'm not sure $n/a$ is an integer.  \nThank you for any help!\n",
    "proof": "$$RTP: 2|a^2\\implies 2|a$$\nOr equivalently using the fact that $A\\implies B$ is equivalent to $B^c\\implies A^c:$\n$$RTP:2\\not| a\\implies 2\\not| a^2$$\nSuppose $2\\not|a$. Then we can write $a=2k+1$ for some integer $k$.\n$\\implies a^2=(2k+1)^2 =4k^2+4k+1=2(2k^2+2k)+1\\equiv 1\\bmod 2\\implies2\\not|a^2\\quad\\text{as required}$\n",
    "tags": [
      "elementary-number-theory",
      "proof-writing"
    ],
    "score": 5,
    "answer_score": 2,
    "is_accepted": false,
    "question_id": 2270529,
    "answer_id": 2270535
  },
  {
    "theorem": "Prove: If $(g \\circ f)$ is bijective, is $f$ bijective?",
    "context": "I need to prove or disprove for a discrete mathematics assignment the following statement:\n$(g \\circ f)$ is bijective $\\rightarrow$ $f$ is bijective, $f: X \\rightarrow Y$ $\\hspace{.5cm} g:Y\\rightarrow Z$\nAll of the domains and codomains here are supposed to be the real numbers.\nI'm having a hard time understanding how to prove things about functions, which we just got into.\nI assume that I need to break this into two cases, those cases being to prove or disprove:\n\n$(g \\circ f)$ is injective $\\rightarrow$ $f$ is injective\n\n$(g \\circ f)$ is surjective $\\rightarrow$ $f$ is surjective\n\n\nI've been playing around with drawing different circle diagrams for $X$, $Y$ and $Z$. For $f(x)$, I drew diagrams where $|X|$ < $|Y|$, where $|X|$ = $|Y|$, and where $|X|$ > $|Y|$, and for $g(f(x))$, I did the same for $Y$ and $Z$.\nIt appears to me that when $(g \\circ f)$ is surjective, $f$ is not necessarily surjective, because you could draw the functions out like this:\n\nThis makes sense to me as a counterexample, but I'm not sure if that's right.\nNext, as for $f$ being injective when $(g \\circ f)$ is injective, I found a proof (see problem 3.3.7 part a at the top of the first page) that explained that this is true, but I didn't really understand the explanation. Also, when I drew it out myself trying to find an example for which it would be false, I found this:\n\nAnd it seems to me to show that when $(g \\circ f)$ is injective, $f$ is not necessarily injective, but that contradicts the proof that I found online.\nThe only two possibilities are that the proof I found was wrong, or that I'm drawing these functions and sets incorrectly and breaking some rule(s) that I'm unaware of.\nAny insight into what I'm doing wrong with my drawings, my logic, or into how to go about correctly solving this problem would be hugely appreciated. Thank you all.\n",
    "proof": "Neither diagram works properly.  For the first, $g \\circ f$ is not surjective as the middle element of $Z$ is not the image of any element of $X$.  This can be patched up by deleting the middle element of $Z$ and sending the middle element of $Y$ to one of the other two elements of $Z$.  For the second, $g \\circ f$ is not injective as $x_1$ and $x_2$ map to the same element.  It is true that if $g \\circ f$ is injective then $f$ must be.  If $f$ maps two elements to the same element of $Y$, $g \\circ f$ must necessarily map them both to the same element of $Z$.\n",
    "tags": [
      "functions",
      "elementary-set-theory",
      "proof-writing"
    ],
    "score": 5,
    "answer_score": 3,
    "is_accepted": true,
    "question_id": 2237479,
    "answer_id": 2237494
  },
  {
    "theorem": "Prove that $x^2+1$ cannot be a perfect square for any positive integer x?",
    "context": "I started this problem by trying proof by contradiction.\nI first noted that the problem stated that $x$ had to be a positive integer, and thus $x=0$ could not be a solution. I then assumed that $x^2+1=n^2$ for some integer $n$ other than $1$. From here I have tried various methods, to no avail:\n\nFactoring:\n\n$n^2-x^2=1\\implies(n+x)(n-x)=1$. It would be nice if I could say $(n+x)=(n-x)=1$ and $(n+x)=(n-x)=-1$. However, the issue here is that $(n+x)$ and $(n-x)$ could take on any value. For example, $(n+x)=2$ and $(n-x)=\\frac{1}{2}$, or $(n+x)=3$ and $(n-x)=\\frac{1}{3}$. Thus, I ruled out factoring.\n\nFor any number $n$, $n\\equiv 0 \\pmod4$ or $n\\equiv 1 \\pmod 4$.\n\nThe trouble with this is that I would have to prove the above statement, so I ruled this out.\nDoes anyone have any tips on how to continue? I feel like this should be an easy proof, but no solutions are coming to me, without having to prove something else.\n",
    "proof": "Your factoring is abosolutely correct. However, since $x,n$ are integers, this gives us  $x-n$ is a natural number. Thus $x-n = \\pm 1$. \nAn alternate proof would use that a square can not exist between two consecutive squres and $$x^2 < n^2 < x^2+2x+1$$\n",
    "tags": [
      "number-theory",
      "proof-verification",
      "proof-writing",
      "square-numbers"
    ],
    "score": 5,
    "answer_score": 5,
    "is_accepted": true,
    "question_id": 1719001,
    "answer_id": 1719006
  },
  {
    "theorem": "Stating the induction hypothesis",
    "context": "I would like to ask about the best way to state the induction hypothesis in a proof by induction.\nJust to use a concrete example, suppose I wanted to prove that $n!\\ge 2^{n-1}$ for every positive integer $n$.\nAssuming that I have already verified the case $n=1$, which of the following statements of the induction hypothesis would be best to use, and, more importantly, are any of them unacceptable?\n1) Let $n\\in\\mathbb{N}$ with $n!\\ge2^{n-1}$.\n2) Let $n!\\ge2^{n-1}$ for some $n\\in\\mathbb{N}$.\n3) Assume that $n!\\ge2^{n-1}$ for some $n\\in\\mathbb{N}$.\n4) Let $k!\\ge2^{k-1}$.\n(I realize that this is partly a matter of taste and style, and please note that I am not asking how to finish the inductive step.)\n",
    "proof": "The Principle of Mathematical Induction says that for all \"properties\" $P$,\n$$\\left(P(0)\\land\\forall k\\in \\mathbb N\\left(P(k) \\implies P(k+1)\\right)\\right)\\implies \\forall n\\in \\mathbb N(P(n)).$$\nSo you're basically asking how to write the $\\forall k\\in \\mathbb N\\left(P(k)\\implies P(k+1)\\right)$ bit. \nIt's a universal statement. It's common to start those by \"Let $k\\in \\mathbb N$\". Then you want to prove the conditional statement $P(k)\\implies P(k+1)$. It's common to prove these by starting with \"suppose $P(k)$ holds\" (or some variation). \nWrapping it up, I'd write \"Let $k\\in \\mathbb N$ and suppose that $P(k)$\" or \"Let $k\\in \\mathbb N$ be such that $P(k)$ holds\" or some variation of this. This includes (1) and to some extent (4). I wouldn't use (2) or (3) because the word \"some\" strongly suggests existential quantification which isn't even present in the formulation of the Principle of Mathematical Induction used in this answer (which is the most common anyway).\n",
    "tags": [
      "soft-question",
      "proof-writing",
      "induction"
    ],
    "score": 5,
    "answer_score": 8,
    "is_accepted": true,
    "question_id": 1411900,
    "answer_id": 1411906
  },
  {
    "theorem": "Prove that for every rational number z and every irrational number x, there exists a unique irrational number such that x+y=z",
    "context": "This is a homework assignment, please tell me if my proof is correct!\nProve that for every rational number z and every irrational number x, there exists a unique irrational number such that x + y = z.\nAssume a and b are integers with GCD (a,b) =1, and that c is an irrational number.\nThere exists a number d = (a/b) + c.\nAssume that d is rational. Then d - (a/b) would be rational, what is a contradiction because c is irrational. Therefore, d is irrational.\nTo prove uniqueness we can use the fact that the addition of any two real numbers has only one result, then d is unique. Q.E.D.\n",
    "proof": "You don't need to restrict the integers forming the ratio to having a greater common denominator of 1.   Just assert that they exist.\n\nAny $z$ that is a rational number can be expressed as the ratio of two integers.  (For strictness we require the denominator integer to be non-zero.)\nAny $x$ is an irrational number cannot be so expressed as the ratio of two integers.\nFor any real numbers, $x$ and $z$, there is only a unique number $y$ where $x+y=z$.\nIf this $y$ were rational it could be expressed as the ratio of some two integers.  For any sum $x+y$ which can also be expressed as the ratio of two integers, it would then follow that $x$ could be expressed as the sum of two integers.  (By reason the product of any two integers is an integer.)\nBy contraposition: for any irrational $x$, and any rational $z$, the number $y$ where $x+y=z$, must be both unique and irrational.\n",
    "tags": [
      "proof-verification",
      "proof-writing"
    ],
    "score": 5,
    "answer_score": 5,
    "is_accepted": true,
    "question_id": 1146056,
    "answer_id": 1146083
  },
  {
    "theorem": "Proof of $\\exists x(P(x) \\Rightarrow \\forall y P(y))$",
    "context": "Exercise 31 of chapter 3.5 in How To Prove It by Velleman is proving this statement:\n$\\exists x(P(x) \\Rightarrow \\forall y P(y))$.\n(Note: The proof shouldn't be formal, but in the \"usual\" theorem-proving style in mathematics)\nOf course I've given it a try and came up with this:\nProof:\nSuppose $\\neg \\exists x(P(x) \\Rightarrow \\forall y P(y))$. This is equivalent to $\\forall x(P(x) \\wedge \\neg \\forall y P(y))$, and since the universal quantifier distributes over conjunctions, it follows that $\\forall x P(x)$ and $\\forall x \\neg \\forall y P(y)$. Thus, for any $x_0, \\neg \\forall y P(y)$. But this contradicts $\\forall x P(x)$, therefore $\\exists x(P(x) \\Rightarrow \\forall y P(y))$.\nI'm not sure if the condradiction is legal, so I'd like to know if there are any flaws in my proof.\nThanks!\n",
    "proof": "This is the drinker's paradox:\n\"In every (populated) bar there is a person such that, if that person is drinking, then everyone is drinking\".\nIt takes advantage of the 2 cases of vacuous implication:\n\n(1) \"False implies anything\"\n(2) \"Anything implies true\"\n\nSo divide the theorem into 2 cases: \nCase (1): Someone is not drinking.  Then that person is an example of vacuous implication; specifically, if a person who is not drinking is drinking, then anything follows.\nCase (2): Everyone is drinking.  That is the other case of vacuous implication, if \"anything\" then everyone is drinking.\nThere error in your given proof is that you haven't explicitly stated the domain of $x$.  In an empty universe, $\\forall x ~~(p(x))$ is true no matter what $p$ is.\n",
    "tags": [
      "logic",
      "proof-verification",
      "proof-writing",
      "predicate-logic"
    ],
    "score": 5,
    "answer_score": 5,
    "is_accepted": true,
    "question_id": 1098053,
    "answer_id": 1098057
  },
  {
    "theorem": "$P(x)=x^3+ax^2+bx+c$, Proof $e^{P(x)}=\\sin x$ has a solution.",
    "context": "Let $P(x)=x^3+ax^2+bx+c$\nProof : $e^{P(x)}=\\sin x$ has a solution.\nI thought about it, and still cannot find where to start.\nAny ideas?, Thanks!\n",
    "proof": "Hint: $P(x)$ is a cubic, so it must have at least one real root. There exists an $x_0$ such that $e^{P(x_0)}=1$. This means $$e^{P(x_0)}\\geq \\sin x_0$$\nThe leading coefficient of $P(x)$ is $1$, so $$\\lim_{x\\to-\\infty} P(x)=-\\infty$$\nThis means that $$\\lim_{x\\to-\\infty} e^{P(x)}=0$$\nNow, why must there be a solution to $e^{P(x)}=\\sin x$ in the interval $(-\\infty,x_0]$? More strongly, why must there be infinitely many solutions in this interval?\n",
    "tags": [
      "calculus",
      "real-analysis",
      "proof-writing"
    ],
    "score": 5,
    "answer_score": 8,
    "is_accepted": true,
    "question_id": 1087047,
    "answer_id": 1087055
  },
  {
    "theorem": "Closed form of a Definite Integral",
    "context": "I attempted to integrate the following function from a practice problem in my Calculus textbook:\n$$\\displaystyle \\int_{0}^{\\frac{\\pi}{2}}{\\frac{1}{1+\\tan^\\sqrt{2}(x)}} \\ {\\rm d}x$$\nI failed to find an indefinite integral, and I am assuming getting an indefinite integral is simply impossible. Using Wolfram|Alpha to estimate the definite integral, I got $0.785398$. I am assuming this is $\\frac{\\pi}{4}$, but I have no formal proof that this is the answer.\n",
    "proof": "The $\\sqrt{2}$ is a complete red herring. In fact consider \n$$I=\\int_0^{\\frac{\\pi}{2}}\\frac{1}{1+\\tan^{\\alpha}(x)}\\,dx$$\nwhere $\\alpha$ is any nonnegative real number. Then we have\n\\begin{align}\nI&=\\int_0^{\\frac{\\pi}{2}}\\frac{1}{1+\\tan^{\\alpha}(x)}\\,dx\n\\\\&=\\int_0^{\\frac{\\pi}{2}}\\frac{1}{1+\\frac{\\sin^{\\alpha}(x)}{\\cos^\\alpha(x)}}\\,dx\n\\\\&=\\int_0^{\\frac{\\pi}{2}}\\frac{\\cos^{\\alpha}(x)}{\\cos^{\\alpha}(x)+\\sin^{\\alpha}(x)}\\,dx\n\\\\&=\\int_0^{\\frac{\\pi}{2}}\\frac{\\sin^{\\alpha}(y)}{\\cos^{\\alpha}(y)+\\sin^{\\alpha}(y)}\\,dy\n\\end{align}\nwhere in the last step we have used the substitution $y=\\dfrac{\\pi}{2}-x$.\nNow we can combine the final two steps to get\n\\begin{align}\n2I&=\\int_0^{\\frac{\\pi}{2}}\\frac{\\cos^{\\alpha}(x)}{\\cos^{\\alpha}(x)+\\sin^{\\alpha}(x)}\\,dx+\\int_0^{\\frac{\\pi}{2}}\\frac{\\sin^{\\alpha}(x)}{\\cos^{\\alpha}(x)+\\sin^{\\alpha}(x)}\\,dx\n\\\\&=\\int_0^{\\frac{\\pi}{2}}\\frac{\\cos^\\alpha(x)+\\sin^{\\alpha}(x)}{\\cos^{\\alpha}(x)+\\sin^{\\alpha}(x)}\\,dx\n\\\\&=\\int_0^{\\frac{\\pi}{2}} 1\\,dx\n\\\\&=\\frac{\\pi}{2}\n\\end{align}\nHence $I=\\dfrac{\\pi}{4}$.\n",
    "tags": [
      "definite-integrals",
      "proof-writing"
    ],
    "score": 5,
    "answer_score": 3,
    "is_accepted": true,
    "question_id": 1073951,
    "answer_id": 1073987
  },
  {
    "theorem": "Show that lcm$(a,b)= ab$ if and only if gcd$(a,b)=1$",
    "context": "Not sure how to begin. If gcd$(a,b)=1$ what can I deduce from that?\n",
    "proof": "It is known that:\n$$gcd(a,b) \\cdot lcm(a,b)=a \\cdot b $$\n\nIF $gcd(a,b)=1$:\n$$ gcd(a,b) \\cdot lcm(a,b)=a \\cdot b  \\Rightarrow 1 \\cdot lcm(a,b)=a \\cdot b \\Rightarrow lcm(a,b)=a \\cdot b$$\nIF $lcm(a,b)=a \\cdot b$:\n$$gcd(a,b) \\cdot lcm(a,b)=a \\cdot b \\Rightarrow gcd(a,b) \\cdot a \\cdot b=a \\cdot b \\Rightarrow gcd(a,b)=1 $$\n\n",
    "tags": [
      "abstract-algebra",
      "elementary-number-theory",
      "proof-writing"
    ],
    "score": 5,
    "answer_score": 1,
    "is_accepted": true,
    "question_id": 920978,
    "answer_id": 920981
  },
  {
    "theorem": "Fields - Proof that every multiple of zero equals zero",
    "context": "This is one of my first proofs about fields.\nPlease feed back and criticise in every way (including style and details).\nLet $(F, +, \\cdot)$ be a field.\nNon-trivially, $\\textit{associativity}$ implies that any parentheses are meaningless.\nTherefore, we will not use parentheses.\nTherefore, we will not use $\\textit{associativity}$ explicitly.\nBy $\\textit{identity element}$, $F \\ne \\emptyset$.\nNow, let $a \\in F$.\nIt remains to prove that $0a = 0$.\n\\begin{equation*}\n\\begin{split}\n0a &=  0a +  0      && \\quad \\text{by }\\textit{identity element }(+    ) \\\\\n   &=  0a +  a + -a && \\quad \\text{by }\\textit{inverse element  }(+    ) \\\\\n   &=  0a + 1a + -a && \\quad \\text{by }\\textit{identity element }(\\cdot) \\\\\n   &= (0 + 1)a + -a && \\quad \\text{by }\\textit{distributivity   }        \\\\\n   &= (1 + 0)a + -a && \\quad \\text{by }\\textit{commutativity    }(+    ) \\\\\n   &=      1 a + -a && \\quad \\text{by }\\textit{identity element }(+    ) \\\\\n   &=        a + -a && \\quad \\text{by }\\textit{identity element }(\\cdot) \\\\\n   &=  0            && \\quad \\text{by }\\textit{inverse element  }(+    )\n\\end{split}\n\\end{equation*}\nQED\nPS: Is \"Let $(F, +, \\cdot)$ be a field.\" ok?\nBesides, I would not want to call $F$ a field, because $F$ is just a set.\nAlso, what do you think about using adverbs like \"Now\"? How would you have said the associativity-thing?\n",
    "proof": "A few pointers:\n\nYou don't have to use \"Now\". You could just say \"Let $a\\in F$.\"\nDon't say \"meaningless\". Rather, phrase it like so:\n\nNon-trivially, associativity implies that any parentheses are redundant. Hence, parenthesis will be suppressed and we will thus not explicitly employ associativity.\n\nIt's not wrong to say that $(F,+,\\cdot)$ is a field, unless the question goes something like this: \"Let $(F,+,\\cdot)$ be a field, and let $0\\in F$. Then show that every multiple of zero equals zero, i.e., for any $a\\in F$, $0a=0$\". Considering the way you phrased your proof, I don't think that this is how the question was phrased (correct me if I'm wrong).\n\nNow, your proof is not wrong, but it's not the shortest either. Your proof could go something like this: $0a=(0+0)a=0a+0a$; hence $0a=0$. Alternatively, you could go like so: $0a=(0+0)a=0a+0a$. But $0a=0a+0$. Hence, $0a+0a=0a+0$, implying that $0a=0$. If I was a teacher, I would personally prefer the latter. However, do not take this personally; your proof is also very nice, but a tad bit longer than what I think is the conventional proof of this fact. Mathematicians are lazy; they prefer the shortest proofs (or at least that's what I think. I'm not a professional!)!\n",
    "tags": [
      "abstract-algebra",
      "soft-question",
      "field-theory",
      "proof-verification",
      "proof-writing"
    ],
    "score": 5,
    "answer_score": 2,
    "is_accepted": true,
    "question_id": 911075,
    "answer_id": 911120
  },
  {
    "theorem": "If $R$ is a transitive relation, then $R\\circ R\\subseteq R$",
    "context": "Here's the question I'm struggling with:\nLet R be a transitive relation on a set A.  Prove the R composed with R is a subset of R.\nI'm kind of lost on how to prove this.\nI've started with saying:\n\"If R is transitive, then R is the subset of A such that (a,b) is in R and (b,c) is in R, and, due to transitivity, (a,c) is in R when (a,b) and (b,c) have the same b for all a, b, c in A.\"\nNow, for R composed with R, I understand that there is an (a,b) in R and an (b,c) in R, and R composed with R is the set of all (a,c) in A such that (a,b) and (b,c) have the same b.\nSo to continue from here, this is what I've written, continuing from the previous quotation:\n\"The composition of R and R on the set A is the subset of all (a,c) in A such that (a,b) in R and (b,c) in R have the same b.\"\nAnd now I'm lost - I'm not sure how to show that R composed with R is a subset of R from here.\nI was going to write out my attempt but I realized I was simply repeating myself and didn't make much sense, so I'd like to ask for some direction before I try again.\nThanks for any help!\n",
    "proof": "You are very close. For brevity, denote $R$ composed with $R$ by $R\\circ R$.  \nBy the definition of composition, the pair $(a,c)$ is in $R\\circ R$ if and only if there exists a $b$ such that $(a,b)$ and $(b,c)$ are both in $R$. \nBut if there is such a $b$, then by transitivity $(a,c)$ is in $R$. Thus if  $(a,c)$ is in $R\\circ R$, then $(a,c)$  is in $R$. It follows that $R\\circ R\\subset R$.  \n",
    "tags": [
      "discrete-mathematics",
      "elementary-set-theory",
      "proof-writing",
      "relations",
      "function-and-relation-composition"
    ],
    "score": 5,
    "answer_score": 6,
    "is_accepted": false,
    "question_id": 876621,
    "answer_id": 876639
  },
  {
    "theorem": "Proof for a graph distance",
    "context": "For Graph $G$, there are several $(x, y)$-paths; the shortest among them have length $2$. Thus $d(x, y) = 2$. Prove that graph distance satisfies the triangle inequality. That is, if $x,y,z$ are vertices of a connected graph $G$, then\n$d(x, z) ≤ d(x, y) + d(y, z).$\nSo basically the graph is a pentagon with one vertex in the center.  I understand the logic of how to prove it, but I don't know how to write it in proof form.\nBasically, I was going to say \nAssume $(x,z ∈ V)$ Then $d(x,z) = 1$, therefore $d(y,z) = 2.$ Therefore $1 \\le 2 + 1$ is true.\nFor the other case,\nAssume $(y,z ∈ V)$ Then  $d(y,z) = 1$, therefore $d(x,z) = 2.$ Therefore $3 \\le 2 + 1$ is true.\n",
    "proof": "I would approach this using a proof by contradiction. Suppose $d(x, y) + d(y, z) < d(x, z)$. By definition, $d(x, y)$ is the length of the shortest path from $x$ to $y$, and $d(y, z)$ is the length of the shortest path from $y$ to $z$. However, $d(x, z)$ is the length of the shortest path from $x$ to $z$, so $d(x, y) + d(y, z)$ can't be less than $d(x, z)$. So we have a contradiction.\nNotice how I generalize here, rather than using specific examples. Remember that example is not valid proof, unless you are using a counterexample to disprove something.\n",
    "tags": [
      "graph-theory",
      "proof-writing"
    ],
    "score": 5,
    "answer_score": 8,
    "is_accepted": true,
    "question_id": 737487,
    "answer_id": 737495
  },
  {
    "theorem": "Why does this step work in this proof?",
    "context": "I'm trying to learn discrete math and am brushing up on proofs by reading Richard Hammack's Book of Proof. I'm tripped up on this proof... I understand that it's contrapositive, and why contrapositive is the best approach, but I'm not sure why you should multiply both sides of $y-x > 0$ by the positive value $x^2+y^2$. Can anyone explain this to me?\n\nThanks!\n",
    "proof": "This is an example of reverse engineering.\nWe first realize that in the original problem, we have $y^3+yx^2$, which could be factored as $y(x^2+y^2)$ and $x^3 + xy^2$ could be factored as $x(x^2+y^2)$.\nHence, to prove the contrapositive of the statement, we take that $y>x$ and multiply throughout by $x^2+y^2$ to get our desired conclusion.\n",
    "tags": [
      "discrete-mathematics",
      "proof-writing"
    ],
    "score": 5,
    "answer_score": 6,
    "is_accepted": true,
    "question_id": 283814,
    "answer_id": 283820
  },
  {
    "theorem": "Proving that a metric space is compact",
    "context": "Let $H^\\infty$ be the set of real sequences such that each element in each sequence has $|a_n|\\leq 1$. The metric is defined as\n$$d(\\{a_n\\}, \\{b_n\\}) = \\sum_{n=1}^\\infty \\frac{|a_n - b_n|}{2^n}.$$\nProve that $H^\\infty$ is a compact metric space. \nTo prove this, I want to show that every sequence in $H^\\infty$ has a convergent subsequence. I know that if we have a sequence $\\{\\{a_n\\}^{(k)}\\}$ in $H^\\infty$, then for all $i$, the real sequence $\\{a_i^{(k)}\\}$ has a convergent subsequence, since it is bounded by 1. So we can get a convergent subsequence $\\{a_1^{(k_j)}\\}$, and then a convergent subsequence of $\\{a_2^{(k_j)}\\}$, and continue taking subsequences of subsequences until we have a convergent subsequence of $(a_1, a_2, a_3, ..., a_n)^{(k)}$ with $n$ some positive integer if we stop taking subsequences at the $nth$ subsequence; this gives a sequence $\\{x_n\\}$ where $x_n$ is the limit of the $n$th convergent subsequence of $\\{a_n^{(k)}\\}$. Ideally, we could show that the sequence in $H^\\infty$ converges to $\\{x_n\\}$.\nI know that if we have the $nth$ subsequence of $\\{\\{a_i\\}^{(k)}\\}$ defined in the way described above, then for any $\\epsilon > 0$ there exist $N_1, ..., N_n$ such that if $k\\geq \\max_{i\\leq n}\\{N_i\\}$, then for $1\\leq i\\leq n$, $|a_i^{(k)} - x_i| < \\epsilon/2n$. By choosing $n$ sufficiently large that $\\sum_{i=n+1}^\\infty |a_i^{(k)} - x_i|/2^i\\leq \\sum_{i=n+1}^\\infty 1/2^{i-1} < \\epsilon/2$, we can ensure that $d(\\{a_n\\}^{(k)}, \\{x_n\\}) < \\epsilon$. But the problem here is that for each $\\epsilon$, we end up choosing a different convergent subsequence of the first $n$ terms (since we need to choose $n$, which determines how many subsequences of subsequences we take). Any idea on how to proceed?\n",
    "proof": "One way is to take first a subsequence $(a_1^i)_i$ such that the first coordinate converges. Now take a subsequence of this subsequence $(a_2^i)_i$ such that the second coordinate converges. Proceeding this way you obtain a sequence of nested subsequences $(a_{k+1}^i)_i \\subset (a_k^i)_i$. Now take the sequence $(a_k^k)_k$. By the last property every coordinate of this sequence converges, and this is equivalent to convergence in your metric. (Note that for each $k,i$ that $a_k^i$ is a sequence of real numbers so we have three layers of sequences...). By the way this is called a diagonal argument, and it's a really useful trick to have available.\nAnother way to do this would be to note that your space is basically $\\prod_{n=1}^{\\infty} [0,1]$ with the product topology, so an appeal to Tychonoff's theorem gives the conclusion. \n",
    "tags": [
      "real-analysis",
      "analysis",
      "metric-spaces",
      "proof-writing",
      "compactness"
    ],
    "score": 5,
    "answer_score": 8,
    "is_accepted": false,
    "question_id": 177819,
    "answer_id": 177826
  },
  {
    "theorem": "Proof of Bolzano&#39;s Theorem",
    "context": "I know one proof of Bolzano's Theorem, which can be sketched as follows:\nSet\n\n$f$ a continuous function in $[a,b]$ such that ${f(a)<0<f(b)}$.\n${A=\\{x:a<x<b \\text{ and } f <0\\in[a,x] \\}}$ \n\n$A \\neq \\emptyset $\n$\\exists \\delta : a\\leq x < a+\\delta \\Rightarrow x \\in A\n$\n$ b$ is an upper bound and $\\exists \\delta :b-\\delta <x \\leq b $ and $x$ is another upper bound of $A$.\n\n\nFrom the previous observations, $A$ has a supremum $\\alpha$, from which we show $f(\\alpha) =0$ ad absurdum. \nSuppose $f(\\alpha) <0$. Then \n$$\\exists \\delta : \\alpha - \\delta <x<\\alpha +\\delta \\Rightarrow f(x) <0 $$\nSince $\\alpha$ is the l.u.b., $$\\exists x_0 : \\alpha - \\delta <x_0<\\alpha$$ or else $\\alpha$ wouldn't be the l.u.b. \nThen $f<0$ in $[a,x_0]$. But if $\\alpha < x_1 < \\alpha +\\delta$ then $f$ is also negative in $[x_0,x_1]$. Thus $f$ is negative in $[a,x_1]$, so $x_1 \\in A$, which can't happen since $\\alpha$ was the supremum.\n\nThe same procedure is used to rule out $f(\\alpha) >0$, from where it is concluded that $f(\\alpha) =0$.\nMy main concerns are:\n\nIs the theorem necesserailly proven using the lub property of $\\mathbb R$? (I suppose so).\nHow could another proof be constructed?\n\n",
    "proof": "You need the least upper bound property of $\\mathbb R$, because since $\\mathbb R$ is essentially $\\mathbb Q$ closed under the operation of \"taking l.u.b.\"'s, to see the necessity of it, you only need to notice that the result is not true over $\\mathbb Q$ : take a continuous function $f : \\mathbb R \\to \\mathbb R$ that has a unique zero $\\alpha$ where $\\alpha \\notin \\mathbb Q$ and $f(\\alpha) = 0$, and then restrict your function to $\\mathbb Q$, i.e. consider the same function but now $f : \\mathbb Q \\to \\mathbb R$. Your function is still continuous over $\\mathbb Q$, but it has no zeros anymore. Therefore, the l.u.b. property is necessary to do this proof.\nI believe this answers your second question as well.\nHope that helps,\n",
    "tags": [
      "real-analysis",
      "proof-writing",
      "continuity"
    ],
    "score": 5,
    "answer_score": 3,
    "is_accepted": true,
    "question_id": 144316,
    "answer_id": 144319
  },
  {
    "theorem": "Is this proof wrong?",
    "context": "\nFor $n>1$, let $a_1, a_2, \\dots, a_n$ be $n$ distinct integers. Prove that the \n  polynomial $$f(x)=(x-a_1)(x-a_2)...(x-a_n) - 1$$ cannot be written as the\n  product of two nonconstant polynomials with integer coefficients.\n\nMy Proof (or attempt)\nAssume that $f(x)$ can be written as $h(x)\\cdot g(x)$. Note for any $x$, $f(x)$ must be prime. This means either $h(x)$ or $g(x)$ must equal $1$, but since these polynomials must be non-constant, we have a contradiction, and we are done.\nSource: Art of Problem Solving Vol. 2\n",
    "proof": "Suppose $f=gh$. Then $-1=f(a_i)=g(a_i)h(a_i)$. Since $g(a_i)$ and $h(a_i)$ are integers, we have $g(a_i)=-h(a_i)$ for all $i$. If both $g$ and $h$ have degree less than $n$, this implies that $g=-h$. But then $f=-g^2$, which cannot happen because the leading coefficients cannot match.\n",
    "tags": [
      "polynomials",
      "proof-writing"
    ],
    "score": 5,
    "answer_score": 8,
    "is_accepted": true,
    "question_id": 95472,
    "answer_id": 95482
  },
  {
    "theorem": "Is this a valid proof that $A \\cup (B \\cap C) \\not= (A \\cup B) \\cap C$?",
    "context": "I'm currently taking an introductory proof-writing course, and was wondering whether the following proof is incorrect:\nProve or disprove the following statement: if $A$, $B$ and $C$ are sets then $A \\cup (B \\cap C) = (A \\cup B) \\cap C$.\nProof. The statement is false. Suppose we have some $x \\in A$ such that $x \\not\\in B$ and $x \\not\\in C$. Then $x$ is an element of $A \\cup (B \\cap C)$, but is not an element of $(A \\cup B) \\cap C$.\nIs there some issue with \"supposing\" such an $x$ exists in this way? This question was marked wrong because it is \"not reasonable\", I asked for further clarification and this is the response I got:\nThe question asked you to prove but not just to provide explanation. You can use digits or numbers to prove this.  For example, A=(1,2,3), B=(3, 4), C=(2) to prove this question which is more detailed rather than just to explain.\nI'm open to being wrong here, I'm just having a hard time understanding the TA's explanations and would appreciate some outside feedback. Thanks!\n",
    "proof": "There is an issue with supposing that there is an $x$ such that $x \\in A$ but $x \\notin B$ and $x \\notin C$.\nIf at some point in a proof you assume that a certain object exists, then you need to ensure that the object does exist, otherwise you've only proved that if such and such an object exists, then some statement is true.\nEDIT:\nNow in this case, what you are assuming is correct, even if it could be phrased a bit better, e.g. \"Suppose there are sets $A$, $B$, $C$ with $x \\in A$, $x \\notin B$ and $x \\notin C$.\" But until you've established that it is correct, e.g. in this case, produced a concrete example, you haven't produced a proof.\nThe reason why it's called a \"proof-writing course\" is that there is a big difference between having some kind of heuristic argument that something is correct versus having a formally correct proof. Indeed, there have been many cases in the history of mathematics where convincing heuristic arguments have turned out to be subtly mistaken. The course is teaching you the skill of how to approach converting heurstic arguments into formal proofs.\nThis might seem pedantic or tedious when dealing with questions about elementary set theory. But when you move on to courses on more subtle topics like real analysis, where you have to be much more careful when you assume that such and such an object exists, you will need this skill to have become second nature.\n",
    "tags": [
      "elementary-set-theory",
      "solution-verification",
      "proof-writing"
    ],
    "score": 5,
    "answer_score": 4,
    "is_accepted": false,
    "question_id": 4860208,
    "answer_id": 4860216
  },
  {
    "theorem": "How to prove that the $\\angle CED$ of the triangle below is equal to $\\frac{1}{2} \\angle\\alpha\\;?$",
    "context": "Here is the whole problem:\n\nIn the triangle $ABC$, it is known that $AC > AB$, and the angle at the vertex $A$ is equal to $\\alpha$. On the side $AC$, point $M$ is marked so that $AB=MC$. Point $E$ is the midpoint of the segment $AM$, point $D$ is the midpoint of the segment $BC$. Find the angle $CED$.\n\nThe textbook answer to the problem is that $\\angle CED = \\dfrac{\\alpha}{2}$. The tip on how get to it is to mark a point $K$ on the $AB$ such, that $AK = KB$, draw the midline $KD$ of the triangle $ABC$ and then prove that the $KDE$ triangle is isosceles.\nAt that point I've tried everything(even chapgpt, which, as it turned out, is bad at math), the best I've gotten so far is if I mark a midpoint F on the $CM$, then I get parallelogram $EKDF$ $\\bigg(KD = \\dfrac{1}{2}AC = EF$, and $KD\\parallel AC\\bigg)$. And triangles $KDE$ and $FED$ are congruent, but it's not even close to what I need to prove. How would you do that ?\nHere is the final figure:\n\nPlease, take into account that this is a problem from an $\\mathbf{8^{th}}$ grade math textbook, the topic is \"Midline of a triangle\", which means I'm not allowed to use any angle functions or similar features that were not introduced yet in the curicullumn.\n\nEDIT: The textbook's hint seems to be wrong, thanks @Vasili for the solution. The actual triangle that needs to be proven to be isosceles is another one, not the $KDE$.\n",
    "proof": "Here is how I arrived to the solution but also without completely using the hint. Draw $BM$ and $NE$. $NE$ is a midline in $\\triangle ABM$ so $AKNE$ is a paralellogram, $NE=AK$. $ND$ is a midline in $\\triangle BMC$ so $ND=\\frac{MC}{2}=AK=NE \\implies \\triangle END$ is isosceles.\n\n",
    "tags": [
      "geometry",
      "proof-writing",
      "triangles"
    ],
    "score": 5,
    "answer_score": 5,
    "is_accepted": true,
    "question_id": 4690832,
    "answer_id": 4690944
  },
  {
    "theorem": "Writing proofs and solutions completely but concisely",
    "context": "I have a university lecturer that puts a lot of emphasis on the writing quality of an answer, not just if its correct. He wants our answers to be 5 things, Clear, Complete, Concise, Coherent and Correct. He says he doesn't believe in perfect answers so can be very nit-picky over our solutions, but it also means he can be contradictory. For example in one of my solutions he said i had been repetitive but firstly, the repeated line (which was something like, \"by the law of total variance\") was there to help the narrative of the answer - which he puts so much emphasis on - and it was repeated from part 1 of a question after quite a few lines of integration. But secondly and more importantly, in his very own solution, he had the exact same repetition. And he said my answer was very good and he was being really nitpicky but that i could at some point write more detail, like write \"the pdf of X is f(x)=...\" rather than just go straight into \"f(x)=...\"\nSo my question is how do you strike a balance between explaining enough and becoming too laborious? Also what are some good connecting words in proofs and 'show me' questions and when do you use which? Because i just tend to stick in 'and', 'because', 'so' 'thus' etc, when prehaps sometimes it isn't strictly true (but does get the point across)\nAn example of such a question: Let X~Gamma($\\alpha,\\beta$) where $\\beta$ is a rate parameter. Find the MGF of X and\nuse this to show that $\\Bbb{E}(X)=\\frac{\\alpha}{\\beta}$ and $var(X)=\\frac{\\alpha}{\\beta^2}$\nLet X~Gamma($\\alpha , \\beta$)\nThen the PDF of X is:\n$$f(x)=\\frac{\\beta^\\alpha}{\\Gamma(\\alpha)}x^{(\\alpha-1)}e^{-\\beta x}dx $$ ,for $x>0$ and 0 otherwise, where $$\\Gamma(\\alpha)=\\int_0^\\inf t^{\\alpha-1}e^{-t}dt$$\n$$M_X(t)=\\mathbb{E}(e^{tX})=\\int_0^\\inf e^{tx}f(x)dx$$\n$$=\\frac{\\beta^\\alpha}{\\Gamma(\\alpha)}\\int_0^\\inf e^{tx}x^{\\alpha-1}e^{-\\beta x}dx$$\n$$=\\frac{\\beta^\\alpha}{\\Gamma(\\alpha)}int_0^\\inf x^{\\alpha-1}e^{-(\\beta-t) x}dx$$\nLet $$u=(\\beta-t)x$$\n$$\\iff x=\\frac{u}{\\beta-t}$$\n$$\\implies dx=(\\beta-t)du$$\nSo $$M_X(t)=\\frac{\\beta^\\alpha}{\\Gamma(\\alpha)}\\int_0^\\inf (\\frac{u}{\\beta-t})^{\\alpha-1}e^-u(\\beta-t)du$$\n$$=\\frac{\\beta^\\alpha}{\\Gamma(\\alpha)}(\\frac{1}{\\beta-t})^{\\alpha}\\int_0^\\inf u^{\\alpha-1}e^{-u}du$$\n$$\\frac{\\beta^\\alpha}{(\\beta-t)^\\alpha}$$\nTo show that $\\Bbb{E}(X)=\\frac{\\alpha}{\\beta}$ and $var(X)=\\frac{\\alpha}{\\beta^2}$:\nUsing differentiation gives us:\n$$M'(t)=\\frac{\\alpha\\beta^\\alpha}{(\\beta-t)^{\\alpha+1}}$$ and $$ M''(t)=\\frac{\\alpha(\\alpha+1)\\beta^\\alpha}{(\\beta-t)^{\\alpha+21}}$$\nIt follows that\n$$\\Bbb{E}(X)=M'(0)=\\frac{\\alpha\\beta^\\alpha}{\\beta^{\\alpha+1}}=\\frac{\\alpha}{\\beta}$$\nand\n$$\\Bbb{E}(X^2)=M''(0)=\\frac{\\alpha(\\alpha+1)\\beta^\\alpha}{\\beta^{\\alpha+21}}=\\frac{\\alpha(\\alpha+1)}{\\beta^2}$$\nHence\n$$ var(X)=\\Bbb{E}(X^2)-\\Bbb{E}(X)^2$$\n$$=\\frac{\\alpha(\\alpha+1)}{\\beta^2}-\\frac{\\alpha^2}{\\beta^2}$$\n$$=\\frac{\\alpha}{\\beta^2}$$ as required\nI know i need connecting words before i calculate the M(t) but i have no idea what to put, plus I'm not sure if where I've put \"it follows that\" is strictly true, also i feel like where i've used \"So\" there might be a better word to use but i can't think what\n",
    "proof": "Based on your lecturer's feedback, it seems like you are already doing a fairly good job! Most pieces of writing could be improved; maybe if you asked your lecturer about the \"contradictory\" remark, he'd say that his own answer could also be better.\nIf you're starting out writing proofs, it helps to have some rules of thumb to look at when you're writing. Many textbooks about proofs have a list; for example, here (section 5.3 on Mathematical Writing) is a list from Richard Hammack's Book of Proof.\nThe first point on that list explains why it is better to write \"The PDF of $X$ is $f(x)=\\dots$\" rather than just \"$f(x) = \\dots$\": beginning a sentence with a symbol is bad style (and sometimes confusing, because it doesn't look the way we expect a sentence to begin). If you didn't explain that $f(x)$ is the PDF of $X$, you could also be running afoul of point 8: always explain each new symbol you introduce.\nPoints 10 and 11 give you some guidance on how to use connecting words. If you want these to come more naturally to you, I suggest reading more proofs, and imitating the style of those that seem clearest to you.\n\nHere is some concrete feedback on your writeup. Using words between equations is important, but the key phrase is using words; not just adding them. A single \"so\" doesn't add anything; you can and should say more. Two of the goals words can serve are:\n\nSeparate different steps in your proof, so that it's not a surprise (for example) when you switch from talking about $\\Gamma(\\alpha)$ to talking about $M_X(t)$.\nExplain what you're about to do.\n\nI would rewrite the first half of your proof as follows:\n\nThen the PDF of $X$ is $$f(x) = \\frac{\\beta^\\alpha}{\\Gamma(\\alpha)} x^{\\alpha-1} e^{-\\beta x}\\,dx$$ for $x>0$ and $0$ otherwise, where $\\Gamma(\\alpha) = \\int_0^\\infty t^{\\alpha-1} e^{-t}\\,dt$.\n\n(Some commentary: not every equation needs its own line. Here, it makes sense to put $\\Gamma(\\alpha)$ inline, since it's not as important: you're explaining something the reader should already know.)\n\nSubstituting this into the definition of $M_X(t) = \\mathbb E(e^{t X})$, we get \\begin{align} M_X(t) &= \\int_0^\\infty e^{tx} f(x)\\,dx \\\\ &= \\frac{\\beta^\\alpha}{\\Gamma(\\alpha)} \\int_0^\\infty e^{tx} x^{\\alpha-1}e^{-\\beta x}\\,dx \\\\ &= \\frac{\\beta^\\alpha}{\\Gamma(\\alpha)} \\int_0^\\infty x^{\\alpha-1} e^{-(\\beta-t)x}\\,dx.\\end{align}\n\n(I explain what we're going to be doing in this step: this is just substitution and some simplification. Note that it helps to align the equations to make it clear that all of this is just different expressions for $M_X(t)$.)\n\nNow perform a $u$-substitution where $u = (\\beta-t)x$ and, accordingly, $x = \\frac{u}{\\beta-t}$ and $dx = \\frac1{\\beta-t}\\,du$. The result is \\begin{align}M_X(t) &= \\frac{\\beta^\\alpha}{\\Gamma(\\alpha)} \\int_0^\\infty \\left(\\frac{u}{\\beta-t}\\right)^{\\alpha-1} e^{-u} \\frac1{\\beta-t}\\,du \\\\ &= \\frac{\\beta^\\alpha}{\\Gamma(\\alpha)} \\left(\\frac{1}{\\beta-t}\\right)^\\alpha \\int_0^\\infty u^{\\alpha-1} e^{-u}\\,du \\\\ &= \\frac{\\beta^\\alpha}{(\\beta-t)^{\\alpha}}.\\end{align}\n\n(Again, equations like $u = (\\beta-t)x$ aren't complicated enough to need their own line. On the other hand, explaining that we're going to do a $u$-substitution is worthwhile! Also, I corrected a mathematical error: $dx = (\\beta-t)\\,du$ should have been $dx = \\frac1{\\beta-t}\\,du$. If I were to add anything else, I would add a sentence at the end explaining that we replace $\\displaystyle \\int_0^\\infty u^{\\alpha-1} e^{-u}\\,du$ by $\\Gamma(\\alpha)$.)\n",
    "tags": [
      "probability",
      "proof-writing",
      "soft-question"
    ],
    "score": 5,
    "answer_score": 7,
    "is_accepted": false,
    "question_id": 4571532,
    "answer_id": 4571546
  },
  {
    "theorem": "if $a^5+b^5&lt;1$ and $c^5+d^5&lt;1$ then prove that ${a^2}{c^3}+{b^2}{d^3}&lt;1$",
    "context": "if $a^5+b^5<1$ and $c^5+d^5<1$ then prove that ${a^2}{c^3}+{b^2}{d^3}<1$ given $a,b,c,d$ are non  negative real numbers\nMy try:\nIt is easy to deduce that $a,b,c,d<1$ ,thus\n$$a^2c^3+b^2d^3<a^5c^5+b^5d^5<(a^5+b^5)(c^5+d^5)<1$$\nI want to know if there is a more cleaner method to solve this problem and if possible with calculus. I would also  want to know if my proof has a mistake anywhere.\n",
    "proof": "Your proof is wrong in the first step: $$a^2c^3+b^2d^3\\geq a^5c^5+b^5d^5.$$\nBy Holder $$1>(a^5+b^5)^2(c^5+d^5)^3\\geq(a^2c^3+b^2d^3)^5.$$\nWe can prove the last inequality by the following way.\nIf $abcd=0$, so our inequality is obvious.\nLet $abcd\\neq0$, $a=xb$ and $c=yd$.\nThus, we need to prove that $$(x^5+1)^2(y^5+1)^3\\geq(x^2y^3+1)^5$$ or $f(x)\\geq0,$ where $$f(x)=2\\ln(x^5+1)+3\\ln(y^5+1)-5\\ln(x^2y^3+1).$$\nBut $$f'(x)=\\frac{10x^4}{x^5+1}-\\frac{10xy^3}{x^2y^3+1}=\\frac{10x(x^3-y^3)}{(x^5+1)(x^2y^3+1)},$$\nwhich gives $x_{min}=y$, $$f(x)\\geq f(y)=0$$ and we are done!\n",
    "tags": [
      "multivariable-calculus",
      "inequality",
      "proof-writing"
    ],
    "score": 5,
    "answer_score": 2,
    "is_accepted": true,
    "question_id": 3787529,
    "answer_id": 3787559
  },
  {
    "theorem": "Proving the boundary of a disc is a circle",
    "context": "As the title suggests, I already know that the boundary of a disk is the circle with the same radius, but what I'm interested in is actually proving that fact.\nI recently got around to the section of my topology book that deals with closure, interior, and boundaries of a subset and I've been having a great deal of trouble actually finding any of these for a given subset. I know what they are, but actually finding these things for a given subset is fairly difficult for me. \nI want to emphasize that when I say it's difficult, I don't mean that I have no idea what they are in reference to that subset, but that I don't know how to rigorously find them or prove one set is the closure/boundary/interior of the other.\nI used the disk with circular boundary mostly as a springboard example, if you know of another example which may elucidate this a little more, feel free to use that one.\nI'm aware there is no one process that will definitely find it every time, but if there is some way of thinking about the question that can help make it a little easier or some useful theorems to aid in finding these things then I'd be immensely grateful.\n",
    "proof": "I find the following characterizations extremely helpful for both intuition and also computations: \n\nThe interior of a set $A$ is the largest open set that is contained in $A$. \nThe closure of a set $A$ is the smallest closed set containing $A$.\n\nSo for instance, when you try to argue that the disk $$D=\\{x \\in \\mathbb{R}^n \\mid  \\Vert x \\Vert \\leq1 \\}$$ is the closure of the set $$U=\\{x \\in \\mathbb{R}^n \\mid \\Vert x \\Vert < 1 \\},$$ you can ask yourself the following questions: \"Is $D$ closed? And if yes, are there any other intermediate sets $U \\subseteq B \\subsetneq D$ that are closed?\" If you can check that the answer of the latter is no, then you are done by 2.\nIn order to conclude that the boundary is $$S=\\{x \\in \\mathbb{R}^n \\mid \\Vert x \\Vert=1\\}$$ you just need to observe that the interior of $U$ is $U$, because $U$ is already open (see 1.).\n",
    "tags": [
      "general-topology",
      "proof-writing",
      "examples-counterexamples"
    ],
    "score": 5,
    "answer_score": 5,
    "is_accepted": true,
    "question_id": 2674352,
    "answer_id": 2674361
  },
  {
    "theorem": "Show that $\\lim_{x \\rightarrow \\infty} f(x)$ exists by the given condition.",
    "context": "Suppose $f$ is a twice differentiable function such that $f''(x) \\ge 0$ for all $x \\in \\mathbb R$. Let $0 \\le f(x) \\le 1$ for $x \\ge 0$. Then show that $\\lim_{x \\rightarrow \\infty} f(x)$ exists.\nI have tried it but I fail. Please give me some hint to proceed in the right way.\nThank you in advance. \n",
    "proof": "$f''>0$ therefore $f'$ is increasing.\n\nFirst case: $\\forall x, f'(x)\\le0$. Then $f$ is always decreasing, and since it's bounded it converges.\nSecond case: Likewise, $\\forall x, f'(x)\\ge0$. Then $f$ is always increasing, and since it's bounded it converges.\nThird case: $f'$ is negative until some $x_0$ where it becomes positive (remember, $f'$ is increasing). Then we're back to case 2 and $f$ converges.\n\nEdit:\nAs pointed out by @MatthewLeingang, cases 2 and 3 are not actually necessary. Indeed, a function that verifies $f''(x)>0,f'(x)>0$ for all $x\\ge x_0$ cannot converge.\nThis can be proved easily: suppose that $\\lim_{x\\to\\infty}f(x)=l<\\infty$. By definition for any $\\epsilon>0$ there exists $x_1\\ge x_0$ such that $|l-f(x_1)|=l-f(x_1)<\\epsilon$. ($l>f(x_1)$ since $f$ is increasing)\nHowever, since $f'$ is also increasing, we have $$f(x_1+\\frac{2\\epsilon}{f'(x_1)})\\ge f(x_1)+2\\epsilon>l$$ which is absurd. Hence $f$ does not converge.\n",
    "tags": [
      "real-analysis",
      "limits",
      "derivatives",
      "proof-writing"
    ],
    "score": 5,
    "answer_score": 5,
    "is_accepted": false,
    "question_id": 2321183,
    "answer_id": 2321195
  },
  {
    "theorem": "The $n$th derivative of $\\frac{1}{5x^5 + 1}$ at $x=0$ is $0$ for all $n$ which is not a multiple of $5$?",
    "context": "How can I show this? This is part of an assigned task for which I am to find the Taylor Series expansion of $f(x) = 1 / (5x^5 + 1)$ around $0$. I have noticed by studying the $n$th derivatives of this function that for $n$ is not a multiple of $5$, the derivative evaluates to $0$.\nI believe this is useful but how can I prove this? I believe it may be related to the fact that odd derivatives of even functions at $x=0$ are $0$. Is there a theorem for the function that is $1/(5x^5 + 1)$ that says its derivatives evaluate to $0$ around $0$?\nCheers!\n",
    "proof": "Let \n$$f(x) = \\frac{1}{5x^5+1}$$\n$$g(x) = \\frac{1}{x+1}$$\nso that $f(x)=g(5x^5)$. Due to our knowledge of geometric series, we know that (when $\\vert x \\vert < 1$)\n$$g(x)=\\sum_{n=0}^\\infty (-1)^nx^n = 1-x+x^2-x^3+...$$\nand so\n$$f(x) = g(5x^5) = 1-5x^5+25x^{10}-125x^{15}+...$$\nSo, we can see that whenever $n=5k$, we have \n$$f^{(n)}(0) = n!\\cdot 5^kx^n$$\nand whenever $n$ is not a multiple of $5$, we have $f^{(n)}(0) = 0$.\n",
    "tags": [
      "real-analysis",
      "proof-writing",
      "taylor-expansion"
    ],
    "score": 5,
    "answer_score": 3,
    "is_accepted": true,
    "question_id": 2303469,
    "answer_id": 2303475
  },
  {
    "theorem": "Any Implications of Fermat&#39;s Last Theorem?",
    "context": "In our discourse FLT is Fermat's Last Theorem.\nI am unaware of any theorems or conjectures that begin assuming FLT is true, or otherwise use FLT as a starting point or tool. The small amount of literature review I've done on this question reveals nothing.\nMy question is: Where can I find a work requiring FLT, or some useful implication of FLT? Even an implication of a polynomial inequality, that may not be FLT, may be a good answer to this question, as I'll likely try to use it to find something regarding FLT.\nThe following is not acceptable as an answer to this question:\n$$ (a^{x_{1}}_{1} + b^{x_{1}}_{1} - c^{x_{1}}_{1}) \\ldots (a^{x_{n}}_{n} + b^{x_{n}}_{n} - c^{x_{n}}_{n}) \\not= 0 : x_{i} > 2 $$\nand it's expansions imply (something trivial)\nAnother acceptable answer to this question would be a proof requiring FLT to be false.\nThanks and please let me know if I can ask this question in a way more fitting math.se (new user). \n",
    "proof": "I don't know if this helps. But you can consider this for fun in the meantime while you search for something significant. More of a \"nuking the mosquito\"\nTo prove  $2^\\frac{1}{n}$ is an irrational number when $n\\ge3$. \n$$2^\\frac{1}{n}=\\frac{p}{q}$$\n$$p^n=q^n+q^n$$\nwhich contradicts the $FLT$, therefore proving $2^\\frac{1}{n}$ is indeed irrational.\nOn a side note, $FLT$ is not strong enough to prove the irrationality of $2^\\frac{1}{n}$ for case $n=2$\n",
    "tags": [
      "number-theory",
      "proof-writing",
      "diophantine-equations",
      "alternative-proof"
    ],
    "score": 5,
    "answer_score": 7,
    "is_accepted": true,
    "question_id": 1887675,
    "answer_id": 1887692
  },
  {
    "theorem": "Show that $\\mathbb{Q}$ is not locally compact with a characterization of local compactness",
    "context": "I know this question has been asked to death, but I wish to prove that $\\mathbb{Q}$ is not locally compact with a use of a lemma I have not seen used in the other proofs.\nLem:\n\nGiven $(X, \\mathfrak{T})$ Hausdorff, then it is locally compact iff\n  every point is contained in an open set with compact closure\n\nMy attempt:\nSince $\\mathbb{Q}$ is a subspace of $\\mathbb{R}$, therefore $\\mathbb{Q}$ is Hausdorff given that Hausdorffness is arbitrarily hereditary. \nTo show that $\\mathbb{Q}$ is not locally compact, we wish to produce a point $x \\in \\mathbb{Q}$ that is contained in an open set without a compact closure. \nLet $U = (a,b) \\subset \\mathbb{R}$. Then $U \\cap \\mathbb{Q}$ is an open set in the subspace topology. Pick $x \\in U \\cap \\mathbb{Q}$, then $x \\in U \\cap \\mathbb{Q} \\subseteq \\overline {U \\cap \\mathbb{Q}}$. (Alarm bells: $\\overline {A \\cap B}$ might not be equal to $\\overline A \\cap \\overline B$)\nNow we need to show that $ \\overline {U \\cap \\mathbb{Q}}$ is not compact...however intuitively this set feels like $[a,b] \\subset \\mathbb{R}$...\nWhat!\n\nDoes anyone see how to resolve this? Thanks so much\n\nAlso there is another answer using a much simpler characterization Why Q is not locally compact, connected, or path connected? Can we see that the lemma I am using and the one given https://math.stackexchange.com/a/650270/174904 is equivalent?\n",
    "proof": "You need to find a point so that none of its open neighborhoods has compact closure.\nLet's consider $0$. Suppose $U$ is an open neighborhood of $0$ (in $\\mathbb{Q}$) that has compact closure. Then $U\\supseteq(-1/n,1/n)\\cap\\mathbb{Q}$, for some integer $n>0$. Therefore the closure of $(-1/n,1/n)\\cap\\mathbb{Q}$ (in $\\mathbb{Q}$) is compact as well.\nLet $C$ be the closure (in $\\mathbb{Q}$) of $(-1/n,1/n)\\cap\\mathbb{Q}$.\nTake your favorite irrational number $r\\in(0,1/n)$ and an increasing sequence $(r_n)$ in $(0,1/n)\\cap\\mathbb{Q}$ that converges to $r$. Then $(r_n)$ is a sequence in $C$, but no subsequence is convergent (in $\\mathbb{Q}$), a contradiction.\n",
    "tags": [
      "general-topology",
      "proof-verification",
      "proof-writing",
      "compactness",
      "rational-numbers"
    ],
    "score": 5,
    "answer_score": 7,
    "is_accepted": true,
    "question_id": 1886830,
    "answer_id": 1886854
  },
  {
    "theorem": "If $A$ is an invertible skew-symmetric matrix, then prove $A^{-1}$ is also skew symmetric",
    "context": "\nLet $A$ be an invertible skew-symmetric $(2n \\times 2n)$-matrix. Prove that $A^{-1}$ is also skew-symmetric. (You may assume that $(AB)^T = B^TA^T$).\n\nI did this with a $2 \\times 2$ matrix and got that it worked, but I don't know how to show it for a general $2n \\times 2n$ matrix, as it is a little harder to calculate the inverse of that. Obviously the hint comes into play somehow but I can't see how. \nI have the definition of a skew symmetric bileanr function to be $B(u,v) = - B(v,u)$, but again, I can't see how to put this into matrix form and use that.\nCan someone give me some hints please?\n",
    "proof": "$(A^T)^{-1}=(A^{-1})^T$ and according to Wikipedia, a skew-symmetric matrix is a matrix that satisfies the condition $A^T=-A$. So $(A^{-1})^T=(A^T)^{-1}=(-A)^{-1}=-A^{-1}$\nWhy do you need $2n\\times 2n$ condition?\n",
    "tags": [
      "linear-algebra",
      "matrices",
      "proof-writing",
      "inverse"
    ],
    "score": 5,
    "answer_score": 7,
    "is_accepted": true,
    "question_id": 368123,
    "answer_id": 368131
  },
  {
    "theorem": "Proof that a strongly connected digraph has an irreducible adjacency matrix",
    "context": "I need to prove that a strongly connected digraph has an irreducible adjacency matrix. If anybody would be willing to give an advice on how to tackle this problem I would be thankful.\n",
    "proof": "I guess, you are using the following definition:\n\nThe matrix $A$ is irreducible if it is not reducible.\n\nIn such a case, assume that the matrix is reducible, that is the state space of the graph allows for the decomposition $V = V_1\\cup V_2$ where sets $V_1$ and $V_2$ are disjoint and\n$$\n  a_{ij} = 0 \\tag{1}\n$$\nfor all $i\\in V_1$ and $j\\in V_2$. Since $V$ is strongly connected, there exists a path $i_1i_2\\dots i_n$ where $i_1$ is some point in $V_1$ and $i_2$ is some point in $V_2$. Show that it contradicts $(1)$.\n",
    "tags": [
      "matrices",
      "graph-theory",
      "proof-writing"
    ],
    "score": 5,
    "answer_score": 5,
    "is_accepted": true,
    "question_id": 350994,
    "answer_id": 350999
  },
  {
    "theorem": "Resources for Proof practice",
    "context": "I am a Math fan and a self-learner. I try to look at least once a day at a Linear Algebra or Calculus problem to keep myself in shape and to learn.\nI also like Analysis, Abstract Algebra and Discrete Math, but I feel I need to and would like to get proficient at proofs. \nI have much harder time finding good online sources of solved problems and step-by-step guides for practicing proofs (induction, contradiction).\nI would ideally like to have a list of solved proofs that progress in difficulty.\nSomething like this but in the area of analysis and abstract algebra:\nhttp://archives.math.utk.edu/visual.calculus/0/domain.1/index.html\nhttp://archives.math.utk.edu/visual.calculus/\nPlease suggest any resources and thank you in advance.\n",
    "proof": "Daniel Solow has an excellent book on proofs that my undergraduate mathematics program used.\nSolow, Daniel. How to Read and Do Proofs: An Introduction to Mathematical Thought Processes. Hoboken, NJ: Wiley, 2009. Print.\n",
    "tags": [
      "abstract-algebra",
      "proof-writing"
    ],
    "score": 5,
    "answer_score": 3,
    "is_accepted": false,
    "question_id": 206934,
    "answer_id": 206938
  },
  {
    "theorem": "Why do proofs using continuity use the Bolzano-Weierstrass theorem?",
    "context": "This may just be goofy of me but there's a lot of proofs I've been seeing recently while learning real analysis that keep on invoking the Bolzano-Weierstrass theorem, and while I understand how it's being used and it makes sense, I don't quite understand why it needs to be used at all.\nFor example, this was a proof of the extreme value theorem,\nIf $f$ is continuous, then $f$ is bounded by the previous theorem. Thus, the set\n$E = \\{f(x) | x \\in [a, b]\\}$ is bounded above. Let $L = \\sup E$. Then,\n\n$L$ is an upper bound for $E$, i.e. $\\forall x \\in [a, b], f(x) \\le L$.\nThere exists a sequence $\\{f(x_n)\\}$ with $x_n \\in [a, b]$ such that $f(x_n) \\to L$.\nBy the Bolzano-Weierstrass theorem, there exists a subsequence $\\{x_{n_k}\\}$ of ${x_n}$ and $d \\in [a, b]$ such that $x_{n_k} \\to d$ as $k \\to \\infty$. Hence, $f(d) = \\lim_{k \\to \\infty} f(x_{n_k}) = \\lim{n \\to \\infty} f(x_n) = L$ by the continuity of $f$. Thus, $f$ achieves an absolute maximum at $d$.\n\nI understanding creating a sequence in $[a,b]$ such that $f(x_n)$ approaches $L$. I also understand that for the sequence you can generate a subsequence such that $x_{n_k}$ approaches $d$. I'm just a little lost on why we can't just say that ${x_n}$ approaches $d$, and why we need to create a whole subsequence just to justify it. If there's anyone who can help me out here, many thanks.\n",
    "proof": "Here's an example: let $f(x) = |x|$ on $[-1,1]$. The sequence $x_n = (-1)^n(1-1/n)$ satisfies $f(x_n) \\to 1$, but the sequence is not convergent. Nothing in the proof rules out $x_n$ being a sequence like that, which is why you need to take a subsequence.\n",
    "tags": [
      "real-analysis",
      "sequences-and-series",
      "proof-writing",
      "proof-explanation",
      "extreme-value-theorem"
    ],
    "score": 5,
    "answer_score": 6,
    "is_accepted": false,
    "question_id": 4967058,
    "answer_id": 4967067
  },
  {
    "theorem": "Is there a rigorous proof for this beautiful property of the function of type $f(x) \\sin x$?",
    "context": "I just noticed today that the graph of $x \\sin x$ is like a $\\sin x$ trapped in $x$ and $-x$. Upon this realisation, I tried to plot some graphs by hand, others by desmos. I tried to investigate this property according to which always $\\sin x$ would be trapped inside the $+f(x)$ and $-f(x)$ for a function $g(x)=f(x) \\sin x$ and its shape would change in order to fit the function at varying x coordinates. But, rather than doing induction I wanted to prove that this type of property will always be valid.\n I defined a function;\n$$g(x) = f(x) \\sin x$$\n$$-f(x)\\le g(x) \\le f(x)$$\n$$-1 \\le \\sin x\\le 1$$\nNow we can argue that $f(x)$ will act like a varying amplitude for $\\sin x$ wave(/graph) and thus it should be trapped. But this is not satisfactory enough.\nThus my question is, “Is there a rigorous proof for this sort of property?”\n\nFollowing are the graphs I tried to analyse the property off of:\n1. $x \\cdot \\sin x$ \n\n \n2. $x^2 \\sin x$ \n\n\n3. $x^3 \\sin x$ \n\n\n4. $\\frac{\\sin x}{x^2+1}$ \n\n\n5. $\\ln x \\cdot \\sin x$ \n\n\n6. $(3x^2-2x^3) \\sin x$ \n\n\n7. $((1-x^{\\frac{2}3})^{\\frac{3}2}) \\sin x$ \n\n\n8. $\\sqrt{(x-1)(x-2)(x-3)} \\cdot \\sin x $ \n\n\n9. $x \\sqrt{\\frac{x+5}{x-5}} \\cdot \\sin x $ \n\n",
    "proof": "Notice that $\\sin(x)$ is bounded, indeed $|\\sin(x)|\\leq 1$. This gives us: $$|f(x)\\sin(x)|\\leq|f(x)|\\cdot|\\sin(x)|\\leq|f(x)|\\cdot 1=|f(x)|.$$\nIn other words, an element of $g(x)$ can never go above the graph of $f(x)$ or below the graph of $-f(x)$. Can you try to generalize this for a function $h(x)$ such that $|h(x)|\\leq n$ for $n$ a natural number? What would happen to the graph of $h(x)f(x)$?\n",
    "tags": [
      "proof-writing",
      "graphing-functions"
    ],
    "score": 5,
    "answer_score": 6,
    "is_accepted": true,
    "question_id": 4164829,
    "answer_id": 4164838
  },
  {
    "theorem": "Question about how to interpret arbitrary elements",
    "context": "I have a question about how to interpret arbitrary elements.\nWhen we say:\n\n\"let $x$ be an arbitrary integer\"\n\n\"let $x$ be any integer\"\n\n\"let $x$ be an integer\"\n\n\nat the beginning of proofs, are we saying:\n\n\"let the symbol $x$ represent one of the integers, although it is not clear which specific integer the symbol $x$ represents\".\n\nIf not, please let me know how I should think about the above statements.\nThank you for your time.\n\nEdit:\nI had a follow up question to the one I asked earlier. What do mathematicians mean by “arbitrary element” or “any element”. I can understand when mathematicians say “let $x$ be $4$” as this means “the symbol $x$ represents $4$”. Could someone please elaborate on why the specific terms “arbitrary” and “any” are used?\n",
    "proof": "Your statement is somewhat correct, but maybe not that useful.\nConsider a statement like the following, which we'll call Statement 1:\n\nLet $x$ be an integer. The number $4x+3$ is odd.\n\nThe power of this statement is its universality: it applies to every integer $x$. If I wanted, I could say Statement 2:\n\nIf $x=-26$, then $4x+3$ is odd.\n\nThis is a less general statement that only applies to a single integer $x$. I could also say Statement 3:\n\nIf $x=55$, then $4x+3$ is odd.\n\nThe power of a construction like Statement 1 is that it implies Statements 2 and 3: once we've proven it, we can use it for any integer we'd like. Statement 1 is not intended to be used for a specific value of $x$ that we decided not to name -- it can be used for any integer $x$ we want.\n\nA side-note is that we can also apply such a statement to other \"generic\" integers. Consider Statement 4:\n\nIf $t$ is a positive integer, then $2^{t-1}$ is an integer.\n\nWe can combine the two to say that, if $t$ is a positive integer, then $2^{t-1}$ is an integer, and so if we set $x=2^{t-1}$ in Statement 1, we get that $4\\cdot 2^{t-1}+3$ is an integer. So, Statement 4 and Statement 1 can be combined to give another general statement:\n\nIf $t$ is a positive integer, then $2^{t+1}+3$ is odd.\n\nIf we only cared about Statement 1 for a single integer $x$, doing something like the above wouldn't really be possible.\n",
    "tags": [
      "discrete-mathematics",
      "proof-writing"
    ],
    "score": 5,
    "answer_score": 6,
    "is_accepted": false,
    "question_id": 3954362,
    "answer_id": 3954533
  },
  {
    "theorem": "Finding the supremum of $\\left\\{n^{\\frac{1}{n}}\\;\\middle\\vert\\;n\\in\\mathbb{N}\\right\\}$",
    "context": "$\\left\\{n^{\\frac{1}{n}}\\;\\middle\\vert\\;n\\in\\mathbb{N}\\right\\}$\nWhat is the Supremum of the above set?\nI consider the function $f(x)= x^{\\frac{1}{x}}$, and show that $f(x)$ is maximum when $x=e$.\nBut here the domain of the set is $\\mathbb{N}$. So how can I find the supremum of the above set?\nPlease anyone help me solve it. Thanks in advance. \n",
    "proof": "You know that as a real function, $x^{1/x}$ is increasing on $(0, e)$ and decreasing on $(e, \\infty)$. The same must be true if we consider it a function on the integers. That means that the maximum among integers must be at either $2$ or at $3$ (since any other integer input must give a function value strictly smaller than one of these two). Now just check those two input values, and you're done.\n",
    "tags": [
      "real-analysis",
      "proof-writing",
      "supremum-and-infimum"
    ],
    "score": 5,
    "answer_score": 5,
    "is_accepted": true,
    "question_id": 3236971,
    "answer_id": 3236973
  },
  {
    "theorem": "$8^n-3^n$ Divisible by 5 - Proof Verification.",
    "context": "Statement: $\\frac{8^k-3^k}{5}=M, M\\in\\mathbb{N}$\nBase case: $P(1): \\frac{8-3}{5}=1\\in\\mathbb{N}$\nAssume $P(n): \\frac{8^n-3^n}{5}=N$\nThen, $P(n+1)=8^{n+1}-3^{n+1}=5K$, where $K$ is in terms of $M$\nWriting LHS in terms of $N$: \n$8^n-3^n=5N \\to 8\\cdot8^n-8*3^n=40N$ \n$8^{n+1}-3^{n+1}=40M+5\\cdot3^{n+1}$\nDividing through by $5$:\n$\\frac{8^{n+1}-3^{n+1}}{5}=8M+3^{n+1}=K$\n, where $K$ is clearly an integer, as $M$ and $N$ Are defined to be integers.\n———————————————————————————————\nQuestions:\n\nIs this proof valid?\nAdding to that, is that last deduction of $K\\in\\mathbb{N}$ fair?\n\n",
    "proof": "Yes, your proof is correct.  Below I explain how to view the arithmetical essence of the matter more conceptually as the result of a product rule, first using congruences, and later using bare divisibility (in case you don't know congruences).\nConceptually the induction follows very simply by multiplying the first two congruences below using  CPR = Congruence Product Rule, $ $ \n$$\\begin{align}\\bmod 5\\!:\\qquad \\color{#c00}{8}\\ &\\equiv\\ \\color{#c00}{3}\\\\\n8^{\\large n}&\\equiv 3^{\\large n}\\quad\\ \\ \\ P(n)\\\\\n\\Rightarrow\\ \\ \\color{#c00}{8}\\,8^{\\large n}&\\equiv 3^{\\large n}\\color{#c00}{3}\\quad\\ P(n\\!+\\!1),\\ \\ \\rm by \\ CPR\\end{align}\\qquad  $$\ni.e. the proof is a special case of the  (inductive) proof of the Congruence Power Rule. Note how the use of congruences  highlights innate arithmetical structure allowing us to reduce the induction to an easy one $\\,a\\equiv b\\,\\Rightarrow\\, a^n\\equiv b^n,\\,$ with obvious inductive step: multiply by $\\,a\\equiv b\\,$ via the product rule.\nIf you don't know congruences we can preserve this arithmetical essence by using an analogous divisibility product rule  (DPR), $ $ where  $\\ m\\mid n\\ $ means $\\,m\\,$ divides $\\,n,\\,$ namely\n$\\!\\!\\begin{align}\n5&\\mid\\  \\color{#c00}{8\\,\\ \\ -\\  3}\\\\\n5&\\mid\\ \\  \\ 8^{\\large n} -\\ 3^{\\large n}\\quad\\ P(n)\\\\\n\\Rightarrow\\ \\ 5&\\mid\\ \\color{#c00}{8}8^{n}\\! -\\!\\color{#c00}33^{\\large n}\\quad\\ \\ P(n\\!+\\!1),\\ \\ \\rm by\\ the\\ rule\\ below\\\\[.8em]\n{\\bf Divisibility\\ Product\\ Rule}\\ \\ \\ \\  \nm&\\mid\\  a\\ -\\ b\\qquad {\\rm i.e.}\\quad \\   a\\,\\equiv\\, b\\\\\nm&\\mid \\ \\ A\\: -\\: B\\qquad\\qquad \\ A\\,\\equiv\\, B\\\\\n\\Rightarrow\\ \\ m&\\mid aA - bB\\quad  \\Rightarrow\\quad aA\\equiv bB\\!\\pmod{\\!m}\\\\[.4em]\n{\\bf Proof}\\,\\ \\   m\\mid (\\color{#0a0}{a\\!-\\!b})A + b(\\color{#0a0}{A\\!-\\!B}) \\ \\ \\ &\\!\\!\\!\\!=\\, aA-bB\\ \\ \\text{by $\\,m\\,$ divides $\\rm\\color{#0a0}{green}$ terms by hypothesis.}\\end{align}$\nRemark $ $ The proof in Jose's answer is nothing but a (numerical) special case of the prior proof - see here where I explain that at length. Further discussion on related topics is in  many prior posts.\n",
    "tags": [
      "proof-verification",
      "proof-writing",
      "induction",
      "divisibility"
    ],
    "score": 5,
    "answer_score": 2,
    "is_accepted": true,
    "question_id": 3177988,
    "answer_id": 3178208
  },
  {
    "theorem": "Showing the composition of invertible functions $g \\circ f$ is invertible with inverse $f^{-1} \\circ g^{-1}$",
    "context": "\nLet $ f : X \\rightarrow Y$ and $g : Y \\rightarrow Z$ be invertible functions. Prove that $g \\circ f : X \\rightarrow Z$ is invertible and that $(g \\circ f ) ^{-1} = f^{-1} \\circ g^{-1} $ \n\nWould this suffice as a proof? \n$$\\begin{align}\n(g \\circ f) \\circ (f^{-1} \\circ g^{-1}) &= g \\circ ((f \\circ f^{-1}) \\circ g^{-1})\\\\\n&= g \\circ (I_Y \\circ g^{-1})\\\\\n&= g \\circ g^{-1}\\\\\n&= I_X\\\\\n(f^{-1} \\circ g^{-1}) \\circ (g \\circ f) &= (f^{-1} \\circ (g^{-1} \\circ g)) \\circ f\\\\\n&= (f^{-1} \\circ I_Y) \\circ f\\\\\n&= f^{-1} \\circ f\\\\\n&= I_Z\n\\end{align}$$\n",
    "proof": "I would accept this as a proof on an undergraduate homework assignment.\nThe only comment I would make is that it needs some English sentences to provide context to what you're doing and why.\nTry opening with \n\nConsider the function $f^{-1} \\circ g^{-1}$, which is guaranteed to exist because (...)\n\nbefore moving on with\n\nNow we verify that $f^{-1} \\circ g^{-1}$ inverts $g\\circ f$. First consider the composition from the left, which can be done because (...)\n\nand then\n\n(...) now consider the composition from the right, which can be done because (...)\n\nand then a statement with your conclusion.\nGenerally blocks of mathematics should be delimited with English statements. You have two main mathematical \"thoughts\" in what you wrote, so need to wrap them both, as well as the unspoken \"thought\" about using/examining $f^{-1} \\circ g^{-1}$ in the first place.\n",
    "tags": [
      "discrete-mathematics",
      "proof-verification",
      "elementary-set-theory",
      "proof-writing",
      "proof-explanation"
    ],
    "score": 5,
    "answer_score": 6,
    "is_accepted": true,
    "question_id": 3135683,
    "answer_id": 3135700
  },
  {
    "theorem": "Prove $\\forall n\\in\\mathbb{N}$, $\\exists m\\in\\mathbb{N}$ s.t. $m&gt;n$ and $m$ is prime",
    "context": "There are two parts I am having trouble getting started.\nA. Prove that $n_1, n_2,...,n_k\\in\\mathbb{N}$ are each at least $2$ then $n=n_1n_2...n_k+1$ is not divisible by any numbers $n_1, n_2,...,n_k$. \nB. Prove that the truth of the negation leads to a contradiction. (Use theorem: For all $a,b\\in\\mathbb{N}$ there exist a unique quotient $q$ and remainder $r$ in $\\mathbb{Z^+}$ such that we have both $a=qb+r$ and $0\\leq r<q$.)\nFor part A, I started with, given $k\\in\\mathbb{N}$ and $n_1, n_2,...,n_k\\geq1$, I'll show that $\\forall i$, $n_i \\nmid n=n_1n_2...n_k+1$ to set it up, but I'm not sure how to actually go about starting it.\nFor part B, I know that the negation is $\\exists n\\in\\mathbb{N}$ s.t. $\\forall m\\in\\mathbb{N}$ either $m\\leq n$ or $m$ is not prime, but again I'm not sure what I should do to start the proof or exactly how to incorporate that theorem.\n",
    "proof": "This is a proof by contradiction: assume there is a largest prime\nFor part (A), we construct $N=n_1n_2n_3...n_k+1$ for $n_1,n_2,n_3,...\\in\\mathbb{N}$ and n_k the largest prime number. \nFrom there, we need only one fact: if $q\\in\\mathbb{N}$, and if $q|N-1$, then $q\\nmid N$. \nThis is easily proven with modular arithmetic, but one can see it logically (if q|N-1, then the next number it will divide is the next multiple of $q$, or $N-1+q>N$ for $q>1$). Alternatively stated with modular arithmetic, \n$q|N-1\\Longleftrightarrow N-1\\equiv 0\\mod q\\Longleftrightarrow N\\equiv 1\\not\\equiv 0\\mod q\\Longrightarrow q\\nmid N$.\nFor part (B), we note that $n_1|N-1$, $n_2|N-1$, $n_3|N-1$, ... From the above proven fact, $n_1\\nmid N$, $n_2\\nmid N$, $n_3\\nmid N$, ..., and $n_k\\nmid N$. But because $n_k$ is the largest prime, and all numbers below $n_k$ also do not divide $N$ no know prime number divides N. \nHowever, it is a basic fact that all integers must have at least one prime factor. Therefore, either $N$ is prime or has prime factors which are all larger than the $n_k$.Therefore, we have a contradiction; there is no largest prime $n_k$. \nThis is equivalent to the statement you're trying to prove. \n",
    "tags": [
      "proof-writing",
      "prime-numbers"
    ],
    "score": 5,
    "answer_score": 4,
    "is_accepted": false,
    "question_id": 3104091,
    "answer_id": 3104152
  },
  {
    "theorem": "Does every theorem have a short proof?",
    "context": "This question is somehow based on my belief that every theorem has a short and simple proof. By \"proof\" I mean:\n\nProving an statement\nDisproving a statement\nProving that a statement is undecidable\n\nOnce we have formalized what we understand for a \"step\" in a proof, could it be proven that every theorem has a proof consisting of less than $n$ steps? If so:\n\nWhat would be the (minimal) value of $n$?\nSuch a proof would be about all proofs so what would it say about itself?\nCould there be (in some sense) proofs with a non-integer number of steps?\n\n",
    "proof": "Let $f(n)$ be any computable (total) function. Then there must be a theorem $T$ of length $N$ such that the shortest proof of $T$ is longer than $f(N)$ steps.\nThis is because, if such a $T$ did not exist, we could solve known unsolvable questions.\nThis result assumes the following basic fact about the \"length\" of proofs:\n\nGiven any $m$, there are only finitely many proofs of length at most $m$.\nThere exists a program which can take $m$ as input an return the list of proofs of length $m$.\n\n",
    "tags": [
      "logic",
      "proof-writing",
      "predicate-logic"
    ],
    "score": 5,
    "answer_score": 4,
    "is_accepted": true,
    "question_id": 1916617,
    "answer_id": 1916667
  },
  {
    "theorem": "Proof of a statement about eigenvalues and eigenvectors.",
    "context": "How can i proof the following:\nLet $\\mathbb L: V\\rightarrow V $ be a linear mapping. Let $v_1,v_2,..,v_n$ non-zero eigenvectors with eigenvalues $c_1,c_2,..,c_n$ respectively, also let the eigenvalues be pairwise differents, show that $v_1,v_2,..,v_n$ are linearly independents.\nI tried starting to write 0 as a linear combination of $v_1,v_2,..,v_n$ and then do the mapping, and use induction to proof but i could not finish, have another way to proof? \n",
    "proof": "Your idea sounds good. I don't know where you got stuck, but here's something that should help you for a proof by induction :\nYou can prove that if $v_1,\\dots,v_n$ are linearly dependent, then $v_1,\\dots,v_{n-1}$ are linearly dependent. Indeed, suppose\n$$\\alpha_1v_1+\\dots+\\alpha_n v_n=0\\tag{A}\\label{A}$$\nwith $\\alpha_i\\neq 0$. Applying $\\mathbb{L}$, you get that\n$$\\alpha_1c_1v_1+\\dots+\\alpha_n c_nv_n=0.\\tag{B}\\label{B}$$\nMultiply \\eqref{A} by $c_n$ and take the difference with \\eqref{B}; you get\n$$\\alpha_1(c_1-c_n)v_1+\\dots+\\alpha_{n-1} (c_{n-1}-c_n)v_{n-1}=0\\tag{C}\\label{C}$$\nand all $c_i-c_n\\neq 0$.\n",
    "tags": [
      "linear-algebra",
      "proof-writing",
      "alternative-proof",
      "proof-explanation"
    ],
    "score": 5,
    "answer_score": 4,
    "is_accepted": true,
    "question_id": 1814531,
    "answer_id": 1814551
  },
  {
    "theorem": "Use of max and min &quot;functions&quot;",
    "context": "\"Let a,x,y be elements of R. If a < min(x,y) then a < x and a < y\"\n\nThe above is a theorem that I encountered in my Real Analysis course. I do not want to ask about what it means, but rather concerning the \"min(x,y)\" being used. The nature of it I am not clear about, even after a conversation with my lecturer about it. This is within the context of proving limits for the reals. \nMy current understanding: this allows us to ensure that we have a positive real number, that min(x,y) is assuredly so. \nI have also seem a similar thing in the context of epsilon-N proofs (sequences) , but min is replaced with max.  \n",
    "proof": "Let $x,y \\in \\mathbb{R}$. If $x<y$, then $\\min \\{x,y\\} := x$; if $x > y$, then $\\min \\{x,y \\} := y$; if $x = y$, then $\\min \\{x,y\\} := x = y$.\nSo $\\min$ eats a pair of real numbers and vomits the smaller one. I think you can now sense how $\\max$ is similarly defined.\n",
    "tags": [
      "real-analysis",
      "limits",
      "proof-writing"
    ],
    "score": 5,
    "answer_score": 3,
    "is_accepted": false,
    "question_id": 1437513,
    "answer_id": 1437523
  },
  {
    "theorem": "Prove: the countable product of regular topological spaces is regular.",
    "context": "Prove: the countable product of regular topological spaces is regular.\nLabel the countable product of $X_i$ as $X$. Given $x \\in X$ and $U$ a closed set s.t. $ x \\notin U$, let's find disjoint neighborhoods of $x$ and $U$. Because $U$ is closed in $X$, it's closed in each $X_i$ (label these closed sets as $U_i$. Also, each coordinate of $x$ is disjoint in each $U_i$, and from $X_i$'s regularity we get that there are open disjoint neighborhoods around $x$ and $U_i$ in $X_i$. \nIf we take the product of these neighborhoods we get what we want.\nIs this proof correct? I'm new to the idea of product spaces so I'm not quite sure what I'm doing.\n",
    "proof": "There are multiple mistakes in your proof. First of all, the set $U$ is a subset of $X$, not a a subset of $X_i$. This mistake also cannot be fixed by considering the projection of $\\pi_i(U)$ onto $X_i$, as $\\pi_i(x)$ may be inside of $\\pi_i(U)$. For example, consider a circle in $\\mathbb{R}^2$ and the origin.\nAlso, I assume you want to consider the product topology on the product of spaces. However, the infinite product of open sets is in general not open in the product topology (see also Box Topology).\nIf you want a hint as to how to prove the assertion, take a look at this answer on MO.\n",
    "tags": [
      "general-topology",
      "proof-verification",
      "proof-writing",
      "separation-axioms"
    ],
    "score": 5,
    "answer_score": 3,
    "is_accepted": true,
    "question_id": 1393053,
    "answer_id": 1393071
  },
  {
    "theorem": "Is Alfred Tarski&#39;s Introduction to Logic still helpful for self study?",
    "context": "I am trying to setup a self study path to enhance my knowledge of mathematical logic.  I haven't taken a logic course for a few years and my confidence on mathematical proofs is unnerving.\nI am planning to attend college in a year to study physics and would perfer not to sit idle when I could be learning on my own. (I perfer self study anyway.)\nI am currently going through the following books:\n\nThe Feynman Lectures on Physics Vol. 1\nBasic Mathematics by Serge Lang (Checking on anything I'm not solid on before attempting calculus which I've only slightly dipped into)\nGodel, Escher, Bach: An Eternal Golden Braid\n\nI would like to supplement this all with a refresher on logic and then perhaps a second course level book on mathematical logic.\nI picked up Alfred Tarski's Introduction to Logic and so far it has been great however I don't see many people talking about it online.  It seems other books are more highly recommended especially in the \"Teach Yourself Logic\" pdf by Peter Smith.\nI'm wondering if it would be a better use of my time to get a book that will be self sufficient as a preface to higher mathematical logic.  I wouldn't want to read through Tarski's book if there was another introductory book that presents the same material alongside more valuable information.\nOr am I needlessly worrying?\n",
    "proof": "You won't be surprised to learn that in the last seventy-plus years since Tarski's book was first published in English, many other books have been appeared which will perhaps serve better as introductions to modern logic. And if you have downloaded my Teach Yourself Logic, you will have seen my \"entry-level\" suggestions on formal logic at the beginning of that Guide. And also my warm recommendation of Velleman's How to Prove It if you want to improve confidence in understanding modes of mathematical argument.\nStill, Tarski's 1941 book is something of a classic -- a discursive and readable introduction at an elementary level to a range of topics in logic. If you are enjoying it, don't stop reading. It will teach you some important basic concepts in an accessible way. And at a pre-college level, it probably doesn't matter hugely what you read: finding things that you enjoy, that you are getting something out of, and that work well for self-study for you is more important. (After all, you are going to have years of the disciplined tread-mill of a fixed curriculum!)\n",
    "tags": [
      "logic",
      "proof-writing",
      "self-learning"
    ],
    "score": 5,
    "answer_score": 6,
    "is_accepted": true,
    "question_id": 1348807,
    "answer_id": 1348969
  },
  {
    "theorem": "How much does Proof writing improve over the years?",
    "context": "This is a very soft question.     \nJust a bit of background:\n I'm a junior in high school taking Analysis I and II out of Baby Rudin at a very well-recognized university. I find quite a few of his exercises a bit difficult, and am always left astonished when a grad student tutor seems to arrive at the same conclusion I have in a fifth of the time. As such, it's prompted me to ask:    \nHow much does one's mathematical ability improve year by year? \nFor example, how, in your personal experience, has your ability to write proofs and see through a problem increased over the years. Say, how was your first year of undergraduate proofs in analysis, to your second year, to your first year as a grad student. Naturally the growth will slow quite a bit eventually, but I suppose I'm curious to know whether this occurs right away, or 10,15,30 years off into someone's career.\nIt often feels as though, while doing a particularly difficult problem, that no matter how much you learn or practice, your ability would not allow you to see a solution. I want to know how much of this fear is myth and how much of it is truth. \nI received an A in analysis I, but I often felt that if this, just undergraduate analysis, was difficult, then why bother to even try to do more difficult work.  \nAny comments appreciated, feel free to share personal experience, or to speak in a general sense.\n",
    "proof": "If it's alright, I'll just offer my own personal experience.\nLike you, I also studied real analysis out of Rudin as a high school junior.  I have since learned that Rudin's text -- while very efficient -- is notoriously concise, and that there are far more student-friendly texts out there.\nLike you, I was often very frustrated with my inability to solve problems, create proofs, and understand the proofs given in the text.  Though I ended up receiving A's in nearly all of my math classes, I sweated and toiled through each one.  Very little came easily.\nAnd like you, I clearly remember going to a second- or third-year graduate student's office hours, and sitting there dumbfounded as he easily breezed through problems that had taken me hours.  I simplistically assumed that he was brilliant, that I was not, that things were simply going to be this way, and that I would never attain that level of fluency.\nIt's now 8 years later, and I've attained that fluency.\n\nFor me, there were two points at which my ability to prove things and \"see through a problem\" significantly improved.  The first was when I completed the standard undergraduate sequence of courses (real/complex analysis, algebra, topology, etc.), about four years later.  The second was roughly three years after that, when I had finished the graduate sequence of courses (and started reading papers).\nI don't know why I had those mental growth spurts.  Some part of me thinks that it has something to do with having a broader perspective, and seeing the bigger picture.  But just as much of me thinks that it's about the sheer number of hours I'd committed to math, in various forms.\n\nOn a somewhat more personal note, I remember it used to bother me when professors and older students would insist that \"seeing the big picture\" would somehow, magically make problems clearer -- especially when my own on-the-ground experience felt counter to that.  At the time, I felt a sharp division between technical mastery and conceptual understanding.\nAnd indeed, I do think that there's a difference: technical ability and abstract conceptual ability can be different things.  But lately (and only lately), I've found the two to be merging for me.  On the one hand, I'm seeing just how big the big picture really is, and have been using it to solve problems quicker.  At the same time, a friend's exam-time recommendation that I \"focus on the methods\" has led me to understand certain concepts better.\nIn short, many things which seemed magic and completely, utterly out of reach eight years, seven years, ..., and even as recently as three years ago, no longer seem to be so now.\nHope this helps.\n",
    "tags": [
      "real-analysis",
      "soft-question",
      "proof-verification",
      "proof-writing"
    ],
    "score": 5,
    "answer_score": 4,
    "is_accepted": true,
    "question_id": 1232264,
    "answer_id": 1234170
  },
  {
    "theorem": "Show that the sum of the $x$-coordinates of three points on the graph of $y = x^2$ whose normal lines intersect at a common point is $0$.",
    "context": "Suppose that three points on the graph of $y = x^2$ have the property that their normal lines intersect at a common point. Show that the sum of their $x$-coordinates is $0$.\nI've done a bit of work in trying to explain this, but I'm not so certain I can visualise it to begin with.\n\nThe the line tangent to the curve $y = x^2$ at a point $T(a, a^2)$ is $$y - a^2 = 2a(x - a) \\implies y = a^2 + 2ax - 2a^2 \\implies y = 2ax - a^2$$\nIn order for the lines tangent to two points to intersect at a common point $P(x, y)$, which will be along the $y$-axis, their $y$-intercepts must be equivalent, and $x = 0$.\nThe two points must therefore be equidistant from the origin, such $|c| = |a|$ in order for the lines tangent to their coordinates $T_2(c, c^2)$ and $T_1(a, a^2)$ will intersect at point $(x, y)$, as their $y$-intercepts are expressed by $-c^2$ and $-a^2$, respectively.\n\nI feel a bit as though I'm rambling, to a certain extent. I'm having trouble thinking of a way to describe this scenario as it would seem that the normal line to each point would have to pass through the origin in order for the lines to meet at one point, as a linear function $f(x)$ may only meet another linear function $g(x)$ at one point assuming that $f(x) \\neq g(x)$ and $m_f \\neq m_g$ (which would mean infinitely many points of intersection, and no points of intersection, respectively).\n\nEdit: \nLet there be three points $A(a,a^2)$, $B(b,b^2)$, $C(c,c^2)$ whose normal lines intersect at a point $P(x_p, y_p)$\nThe tangent to the curve at $A$ is $y - a^2 = 2a(x - a)$\nThe normal to the curve at $A$ is $y - a^2 = -\\frac{1}{2a}(x - a) \\implies x+2ay=2a^3+a$ \n$$\n\\begin{align*}\n\t\\\\ &\\left[y = \\dfrac{2a^3 + a - x}{2a} \\right] \\text{ } \\left[y = \\dfrac{2b^3 + b - x}{2b} \\right] \\text{ } \\left[y = \\dfrac{2c^3 + c - x}{2c} \\right]\n\t\\\\\n\t\\\\ &\\left[x = 2a^3 + a - 2ay \\right] \\left[ x = 2b^3 + b - 2by \\right] \\left[ x = 2c^3 + c - 2cy \\right]\n\\end{align*}\n$$\n$I_{A \\cdot B} = $ the intersection of $line_{normal_A}$ and $line_{normal_B} \\implies$ $I_{A \\cdot B}([-2ab(a+b)], [a^2+ab+b^2+\\frac{1}{2}])$\n$I_{A \\cdot C} = $ the intersection of $line_{normal_A}$ and $line_{normal_C} \\implies$ $I_{A \\cdot C}([-2ac(a+c)], [a^2+ac+c^2+\\frac{1}{2}])$\n\nFrom here, I'm not certain how to proceed.\n",
    "proof": "Suppose that three points on the graph of $y = x^2$ have the property that their normal lines intersect at a common point. Show that the sum of their $x$-coordinates is $0$.\nLet the points be $A(a,a^2)$, $B(b,b^2)$, $C(c,c^2)$\nThe tangent to the curve at $A$ is $y - a^2 = 2a(x - a)$\nThe normal to the curve at $A$ is $y - a^2 = -\\frac{1}{2a}(x - a) \\implies x+2ay=2a^3+a$ \n\\begin{align*}\n\\\\ &\\left[y = \\dfrac{2a^3 + a - x}{2a} \\right] \\text{ } \\left[y = \\dfrac{2b^3 + b - x}{2b} \\right] \\text{ } \\left[y = \\dfrac{2c^3 + c - x}{2c} \\right]\n\\\\ &\\left[x = 2a^3 + a - 2ay \\right] \\left[ x = 2b^3 + b - 2by \\right] \\left[ x = 2c^3 + c - 2cy \\right]\n\\end{align*}\n$I_{A \\cdot B} = $ intersection of the normal lines of $A$ and $B$ $\\implies$ $I_{A \\cdot B}([-2ab(a+b)], [a^2+ab+b^2+\\frac{1}{2}])$\n$I_{A \\cdot C} = $ intersection of the normal lines of $A$ and $C$ $\\implies$  $I_{A \\cdot C}([-2ac(a+c)], [a^2+ac+c^2+\\frac{1}{2}])$\n$$\n\\\\ \\begin{align}\n\\\\ I_{A \\cdot B} = I_{A \\cdot C} \\implies x_{I_{A \\cdot B}} = x_{I_{A \\cdot C}} \\implies \\frac{x_{I_{A \\cdot B}}}{x_{I_{A \\cdot C}}} = 1 \\implies \\dfrac{-2ab(a+b)}{-2ac(a+c)} &= 1\n\\\\\n\\\\ b(a + b) &= c(a + c)\n\\\\ ba+b^2 &= ca + c^2 \n\\\\ a(b - c) &= c^2 - b^2\n\\\\ a(b - c) &= (c + b)(c - b)\n\\\\ a(b-c) &= (c+b)(b-c)(-1) \n\\\\  \\frac{a(b-c)}{(b-c)} &= -c - b\n\\\\ a + b + c &= 0\n\\\\\n\\\\\\end{align}\n$$\nThis shows the sum of $a+b+c$ is $0$.\n",
    "tags": [
      "calculus",
      "real-analysis",
      "proof-writing"
    ],
    "score": 5,
    "answer_score": 1,
    "is_accepted": true,
    "question_id": 1193916,
    "answer_id": 1195845
  },
  {
    "theorem": "Idea of a proof by contradiction",
    "context": "Is the idea of a proof by contradiction to prove that the desired conclusion is both true and false or can it be any derived statement that is true and false (not necessarily relating to the conclusion)? Or can it simply be an absurdity that you know is false but through your derivation comes out true?\n",
    "proof": "It can be both: take the following simple problem. Is is possible to cover an $8\\times8$ chessboard which has had its two white corners removed using domino-like pieces of size $2\\times1$?\nLet us assume it is possible and we have somehow managed to do it. Notice we  covered $32$ black squares and $30$ white squares. Also notice a domino-like piece will cover 1 black and 1 white square.\nSince each domino-like piece covers 1 black square and we covered $32$ white pieces the number of dominoes we used is $16$.\nOn the other hand  Since each domino-like piece covers 1 white square and we covered $30$ white pieces the number of dominoes we used is $15$.\nTherefore $16=15\\dots$ wait wut? In this case it wasn't a direct contradiction. It just lead to something that clearly can't happen.\n",
    "tags": [
      "logic",
      "proof-writing"
    ],
    "score": 5,
    "answer_score": 3,
    "is_accepted": true,
    "question_id": 819830,
    "answer_id": 819837
  },
  {
    "theorem": "Image of closed unit ball under a compact operator",
    "context": "Let $X,Y$ be Banach spaces and $A\\in\\mathcal L(X,Y)$ . The task is to prove the following:\n$A$ is compact if and only if the image of the closed unit ball in $X$ is compact in $Y$.\nI have proven this when $X$ is a reflexive space.\nProof. Let $X$ be a reflexive space, $\\bar B$ the closed unit ball in $X$, and $A$ a compact operator. Let further $y_n=Ax_n$ be a sequence in $A(\\bar B)$.\nIn reflexive spaces $\\bar B$ is weakly compact, so there exists a subsequence $x_{n_j} \\to x$ weakly.\nBecause $A$ is compact, $Ax_{n_j}\\to Ax$ strongly. \nOn the other side, $A(\\bar B)$ is relatively compact, so there exists $z_k=Ax_{n_{j_k}}$ that converges strongly to $y\\in Y$.\nBut $z_k\\to Ax$ strongly. So by unicity of the limit $y=Ax$ and the image is compact.\nIt's easily proved that if the image is compact, the operator is also compact.\nBut I don't know what to do in case of nonreflexive spaces. Is there any counterexample or proof in such case?\n",
    "proof": "It is false in general that the image of the closed unit ball under a compact operator is closed (and hence compact). Here is an easy example:\nConsider $X = C[0,1]$ with the uniform norm, and the compact operator $A \\in B(X)$ defined by the formula:\n$\\displaystyle\\qquad Af(x) = \\int_0^x f(t)\\,dt$.\nCompactness of $A$ is easily proven using Arzelà–Ascoli. Our operator $A$ produces an anti-derivative of any input given to it, and the image of the closed unit ball of $X$ under $A$ is the set\n$\\displaystyle\\qquad \\{f \\in C^1[0,1] \\mathrel: f(0)=0,\\ \\lVert f'\\rVert \\leq 1\\}$\nwhich certainly is not a closed subset of $X$. \n",
    "tags": [
      "functional-analysis",
      "proof-writing"
    ],
    "score": 5,
    "answer_score": 7,
    "is_accepted": true,
    "question_id": 404516,
    "answer_id": 404644
  },
  {
    "theorem": "How to exactly write down a proof formally (or how to bring the things I know together)?",
    "context": "I've always had trouble with this: I know everything I need to know but I can't connect everything I know in the way it's being wanted from me.\nThis is what I have to do:\n\nProve for $f : M → N$ the following statement.$f$ is surjective if and only if the following is true: If $g, h: N → L$ are any functions with $g ∘ f = h ∘ f$, then $g = h$ applies.\n\nThe thing is, I am pretty sure I know what they want from me, but I have no idea how to show it. This is what I know:\n\n$f: x ∈ M → f(x) ∈ N$$g: f(x) ∈ N → g(f(x)) ∈ L$  $h: f(x) ∈ N → h(f(x))∈L$  and of course:  $f$  is surjective when: $∀y ∈ N ∃ x ∈ M : f(x) = y$\n\nPretty simple, but I don't know if it is necesary to write this down. I continue:\n\n$f$ is surjective if: $g ∘ f = h ∘ f $$⇔ g(f(x)) = h(f(x))$$⇔g(y) = h(y)$ would this be enough prove about that $g = h$ part?\n\nI'm not even sure if this is going to the right direction, because I'm afraid I don't even know if I know what I am doing.\nI'm open to every kind of help, because this is something I want to apply to all the proves of this kind.\n",
    "proof": "Since this is an \"if and only if\" kind of proof, slow down and try to write down both implications one at a time. Try to understand every step. For example:\nLet's show that $f$ surjective implies the other thing. Assume that $g \\circ f = h \\circ f$. This means that for any element $m \\in M$, $g(f(m)) = h(f(m))$. We need to show that for any element $n \\in N$, we have $g(n) = h(n)$. But $f$ is surjective, so any $n$ has a preimage $m$: $$\\exists m: n = f(m)$$ Thus we in fact have $g(n) = g(f(m)) = h(f(m)) = h(n)$, as required!\nCan you try to do the other direction?\nHere's what you have to do: assume that $g \\circ f = h \\circ f \\implies g = h$. Now, take any element $n \\in N$, and show that there exists a $m \\in M$ such that $f(m) = n$.\nEdit: This last part is perhaps the less straightforward of the two, so let's see if I can be a little more helpful. Assume on the contrary that $f$ is not surjective. Thus, there exists $n_0 \\in N$ that possesses no preimage. Now, take $L = \\{a,b\\}$, any two-element set. Set $g(n) = a$, and\n$$ h(n) = \\begin{cases}a & n \\neq n_0 \\\\ b & n = n_0 \\end{cases}$$\nNow can you find a contradiction?\n",
    "tags": [
      "functions",
      "proof-writing"
    ],
    "score": 5,
    "answer_score": 3,
    "is_accepted": false,
    "question_id": 371593,
    "answer_id": 371610
  },
  {
    "theorem": "Proving an algorithm&#39;s correctness in determining the number of 1 bits in a bit string",
    "context": "   procedure bit count(S: bit string)\n    count := 0\n          while S != 0\n              count := count + 1\n              S := S ∧ (S − 1)\n    return count {count is the number of 1s in S}\n\nHere S-1 is the bit string obtained by changing the rightmost 1 bit of S to a 0 and all the 0 bits to right of this to 1s.\nSo I understand why this is correct, and I have write a rough explanation;\nAfter every iteration, the rightmost 1 bit in S, as well as all the bits to the right of it, is set equal to 0. Thus, after each iteration, the next right-most 1 is accounted for and set to 0, until the entire string is 0's and the loop breaks with the count equal to the number of 1's. \nI know this kind of answer won't pass in any mathematics community, so I was hoping to write a formal proof, but I don't know how to go about doing that.  My proof skills are particularly shoddy, so an explanation of the techniques involved would be greatly appreciated.\n",
    "proof": "Here’s one way to make your informal explanation a bit more formal.\nLet $\\sigma=b_1b_2\\dots b_n$ be a bit string of length $n$. For convenience let $[n]=\\{1,\\dots,n\\}$. If $\\sigma$ is not the $0$ string, let $k=\\max\\{i\\in[n]:b_i=1\\}$, so that $b_k=1$ and $b_i=0$ for $i>k$. Then $\\sigma-1$ is the bit string $a_1a_2\\dots a_n$ such that for each $i\\in[n]$\n$$a_i=\\begin{cases}\nb_i,&\\text{if }i<k\\\\\n0,&\\text{if }i=k\\\\\n1,&\\text{if }i>k\\;.\n\\end{cases}$$\nThen $$\\sigma\\land(\\sigma-1)=(b_1\\land a_1)(b_2\\land a_2)\\ldots(b_n\\land a_n)\\;,$$ and for $i\\in[n]$ we have\n$$b_i\\land a_i=\\begin{cases}\nb_i,&\\text{if }i<k\\\\\n0,&\\text{if }i\\ge k\\;:\n\\end{cases}$$\n$b_k\\land a_k=1\\land 0=0$, and $b_i\\land a_i=0\\land 1=0$ for each $i>k$. In other words, if $$\\sigma\\land(\\sigma-1)=c_1c_2\\dots c_n\\;,$$ then \n$$c_i=\\begin{cases}\nb_i,&\\text{if }i<k\\\\\n0,&\\text{if }i\\ge k\\;.\n\\end{cases}$$\nRecalling that $b_k=1$ and $b_i=0$ for $i>k$, we see that $\\sigma$ and $\\sigma\\land(\\sigma-1)$ differ only in the $k$-th bit, where $\\sigma$ has a $1$ and $\\sigma\\land(\\sigma-1)$ has a $0$. \nIf $|\\sigma|_1$ is the number of $1$’s in $\\sigma$, we’ve just shown that\n$$|\\sigma\\land(\\sigma-1)|_1=|\\sigma|_1-1\\;.$$\nThus, each pass through the loop adds $1$ to the count and reduces the number of $1$’s in the string by $1$. (In other words, $\\text{count}+|\\sigma|_1$ is a loop invariant for this loop.) Since $\\text{count}$ starts at $0$ and $|\\sigma|_1$ at the number of $1$’s in the input bit string, it’s now clear that after $|\\sigma|_1$ passes through the loop, the input string will be the zero string, and $\\text{count}$ will be the number of $1$’s in the input string. And since the input is now the zero string, we exit the loop with no further change in $\\text{count}$.\n",
    "tags": [
      "algorithms",
      "discrete-mathematics",
      "proof-writing",
      "induction"
    ],
    "score": 5,
    "answer_score": 3,
    "is_accepted": false,
    "question_id": 251370,
    "answer_id": 251376
  },
  {
    "theorem": "Suppose $A$ is a $7\\times 7$ matrix with all entries less than $1$ in magnitude. Prove that $\\det (100I+A) &gt; 0$",
    "context": "\nSuppose $A$ is a $7\\times 7$ matrix with all entries less than $1$ in magnitude. Prove that $\\det (100I+A) > 0$.\n\nThis is my first post on the mathematics stack exchange, so forgive me if it’s not in the right format or if it’s been asked before (I couldn’t find anything like it, though).\nMy elementary linear algebra professor posted this question in the beginning of class this morning and left it up to us to discuss independently.\nI can gather that the determinant of $100 I_7$ is $100^7$ and every element in $A$ is in between $-1$ and $1$ (given) but other than that, I’ve got no clue where to start.\nThank you!\n",
    "proof": "The product of elements in the main diagonal is $100^7$. Estimate the sum of all other summands in the determinant: notice, that every other permutation contains $100$ at most five times. How many permutations are there all together? It is $7!=5040$. But $100^7>100^5\\cdot 5040$, which means that all the other permutations cannot be as negative as how big that one permutation is alone.\nEdit: Thank you user1551 for pointing out the small mistake: we only know that the product of elemnts in the main diagonal is at least $99^7$, while when bounding the negative part, we should use $101$, not $100$. But the proof still works as $99^7>101^5\\cdot 5040$.\n",
    "tags": [
      "linear-algebra",
      "proof-writing",
      "determinant"
    ],
    "score": 5,
    "answer_score": 5,
    "is_accepted": true,
    "question_id": 4884063,
    "answer_id": 4884081
  },
  {
    "theorem": "If $a_n&gt;0;\\ a_n\\to 0,$ is there an enumeration $\\{ x_n \\}_{n\\in\\mathbb{N}}$ of $\\mathbb{Q}\\cap (0,1]$ such that $a_1 x_1 &gt; a_2 x_2 &gt; a_3 x_3\\ldots\\ ?$",
    "context": "Suppose $\\ a_n>0\\ \\forall n\\in\\mathbb{N}\\ $ with $\\ a_n\\to 0.\\ $\nProposition: There exists a bijective enumeration $\\ \\{ x_n \\}_{n\\in\\mathbb{N}}\\ $ of $\\ \\mathbb{Q}\\cap (0,1],\\ $ such that $\\ a_1 x_1 > a_2 x_2 > a_3 x_3 \\ldots\\ .$\nI have many ideas of how to prove this, although I am finding it tricky to actually write out a proof. I was wondering if there are particularly simple/straightforward proofs of this. I'm also just interested in seeing different ideas and methods for how to prove this...\nI'll start posting one or more of my proofs once I've formalised them... but they are to do with the peaks and troughs of $\\ a_n\\ $ like in the Lemma to the proof of B-W. The peaks form a decreasing sequence which gives a good starting point for a proof. The troughs also give a decreasing sequence which gives a good starting point for a proof.\nBy \"trough\" here, I mean, a point $\\ a_k\\ $ such that $\\ i<k\\implies a_i>a_k.$\n",
    "proof": "Proof outline:\n\nLet $\\ \\{a_{k_n}\\}_n\\ $ be the subsequence of all the troughs of $\\ \\{a_n\\}_n.\\ $ Note that $\\ \\{a_{k_n}\\}_n\\ $ is strictly decreasing sequence $\\ \\to 0^+.\\ $ Note also that $\\ a_{k_1} = a_1,\\ $ because $\\ a_1\\ $ is the first trough by vacuous truth.\n\nLet $\\ x_{k_1} = x_1 = 1.\\ $ Note that $\\ k_1=1.\\ $\n\nLet $\\ \\{y_n\\}_n\\ $ be an enumeration of $\\ \\mathbb{Q}\\cap (0,1).$\n\n\nFor each $\\ i\\in\\mathbb{N},\\ $ carry out the following procedure:\n\nLet $\\ x_{k_{i+1}}\\ $ be equal to the earliest member of $\\ \\{y_n\\}_n\\ $ that hasn't been used yet in (previous iterations of) this procedure. $\\quad (1)$\n\nSince $\\ a_{k_n}\\overset{n\\to \\infty}{\\to} 0^+,\\ $ we can always find $\\ \\large{k_{j_{(i+1)}}>k_{j_i}}\\ $ such that $\\ \\Large{{a_{k_{j_{{(i+1)}}}}< \\frac{x_{k_i} a_{k_{j_i}}}{x_{k_{(i+1)}}}}}.\\ $\n\nWe then have the desired $\\ \\large{x_{k_{(i+1)}} a_{k_{j_{(i+1)}}} < x_{k_i} a_{k_{j_i}} }\\ $ as well as $\\  \\large{ a_{k_{j_{(i+1)}}}< a_{k_{j_i}} }.$\n\nNext, choose any members $\\ y_{n_1},\\ y_{n_2},\\ \\ldots,\\ y_{n_\\left({k_{j_{(i+1)}}-k_{j_i}-1}\\right)}\\ $ of $\\ \\{ y_n\\}_n\\ $ that: (a) $\\ y_p\\neq y_q\\ $ if $\\ p\\neq q,\\ $ (b) None of them are equal to $\\ x_{k_{(i+1)}},\\ $ (c) none of them have already been chosen from $\\ \\{ y_n\\}_n\\ $ so far at any prevoius stage in the procedure, and (d) they satisfy: $$\\ \\large{ x_{k_i} a_{k_{j_i}}>y_{n_1} a_{(k_{j_{i}}+1)}>y_{n_2} a_{(k_{j_{i}}+2)}>\\ldots> y_{n_\\left({k_{j_{(i+1)}}-k_{j_i}-1}\\right)} a_{(k_{j_{(i+1)}}-1)} > x_{k_{(i+1)}} a_{k_{j_{(i+1)}}} } $$\n\n\n$\\qquad $ and set $\\ x_{(k_i+m)} = y_{n_m}\\ $ for each $\\ m \\in \\{ 1,\\ 2,\\ \\ldots,\\ k_{j_{(i+1)}} - k_{j_i} - 1 \\}. $\nIn our procedure, $(1)\\ $ ensures that $\\ \\mathbb{Q}\\cap (0,1] \\subset \\{ x_n\\}_n,\\ $ and I think that the density of $\\ \\mathbb{Q}\\ $ ensures we don't have any problems with choosing the $\\ y_{n_m}.\\ $ Indeed, I think this means we can replace $\\ \\mathbb{Q}\\cap (0,1]\\ $ with any dense subset of $\\ (0,1].\\ $ Finally, it is easily verified that our procedure produces: $\\ x_1 a_1 > x_2 a_2 > x_3 a_3 > \\ldots\\ $.\n",
    "tags": [
      "real-analysis",
      "general-topology",
      "proof-writing",
      "problem-solving",
      "alternative-proof"
    ],
    "score": 5,
    "answer_score": 2,
    "is_accepted": true,
    "question_id": 4352841,
    "answer_id": 4355182
  },
  {
    "theorem": "Uniform convergence implies pointwise convergence",
    "context": "I am having trouble proving a simple proposition regarding uniform convergence and pointwise convergence in Real Analysis.\nProblem:\nSuppose that $\\left(f_{n}\\right)$ is a sequence of functions $f_{n}: A \\rightarrow \\mathbb{R}$ such that $\\left(f_{n}\\right)$ converges uniformly to $f: A \\rightarrow \\mathbb{R}$. Prove that $\\left(f_{n}\\right)$ also converges pointwise to $f: A \\rightarrow \\mathbb{R}$\nRelevant definitions/Notations:\nOne says that $f_{n} \\rightarrow f$ uniformly on $A$ if, for every $\\epsilon>0,$ there exists $N \\in \\mathbb{N}$ such that $n>N$ implies that $$\\left|f_{n}(x)-f(x)\\right|<\\epsilon$$ for all $x \\in A$.\nFinally, we say that $f_{n} \\rightarrow f$ pointwise on $A$ if, given $\\epsilon>0,$ for each $x \\in A$ there exists $N \\in \\mathbb{N}$ such that $n>N$ implies that $$\\left|f_{n}(x)-f(x)\\right|<\\epsilon$$\nAttempts:\nI tried to write down the definition of uniform convergence and then arguing that, in particular, since $N \\in \\mathbb{N}$ from uniform convergence works for any given point in $x \\in A$, then it must work for a given point and from that conclude pointwise convergence.\nI also checked some proofs like the one that states that uniform continuity implies continuity and write something similar, but i just dont know how to do it.\nI would highly appreciate a detailed proof regarding this fact, i am trying to become proeficient at proof writing.\nThanks in advance, Lucas\n",
    "proof": "Your attempt looks fine. Take $x\\in A$; you want to prove that $\\lim_{n\\to\\infty}f_n(x)=f(x)$. Now, take $\\varepsilon>0$; you want to prove that there is some $N\\in\\Bbb N$ such that$$n\\geqslant N\\implies\\bigl|f_n(x)-f(x)\\bigr|<\\varepsilon.\\tag1$$So, take $N\\in\\Bbb N$ such that$$(\\forall a\\in A)(\\forall n\\in\\Bbb N):n\\geqslant N\\implies\\bigl|f_n(a)-f(a)\\bigr|<\\varepsilon.$$It follows from this that, for such a $N$, $(1)$ holds.\n",
    "tags": [
      "real-analysis",
      "sequences-and-series",
      "proof-writing",
      "solution-verification",
      "uniform-convergence"
    ],
    "score": 5,
    "answer_score": 3,
    "is_accepted": true,
    "question_id": 3980377,
    "answer_id": 3980391
  },
  {
    "theorem": "Deforming the torus without a point to $S^1 \\lor S^1$",
    "context": "Let $T$ be the topological torus, given by taking a rectangle with parallel sides oriented in the same direction, and glueing together each pair of parallel sides along the given direction.\n\nTake a point $P\\in T$ and remove it, I want to show that $S^1 \\lor S^1$ is\na deformation retract of $T-\\{P\\}$ which is given by glueing two\ncircles at some point.\n\nIntuitively:\n\nThe point $P$ divides the inner area of the rectangle in four parts, those points lying above $P$ or below $P$, and left and right of $P$.\nThese parts will be the triangles delimited by the segments connecting the point $P$ to the vertices.\nNow I can define a retraction collapsing each point of the rectangle to the corresponding point on the base of the triangle it belongs to.\nThis is not well defined for points lying on the segments connecting $P$ to the vertices, but each choice will give some retraction, and the retraction will be homotopic to the identity because the rectangle is a convex set (edit: I think this point is wrong, because if I remove a point I lose the convexity).\nSo we have produced a retraction of the rectangle onto its sides.\nPassing to the quotient which identifies the parallel sides, this composition is still a retraction if we quotient also the rectangle by the same relation making it a torus.\nWe conclude by observing that quotienting the boundary of the rectangle by the above relation gives a space homeomorphic to two circles glued at a point, and we are done.\n\n\nCan we make this argument more rigorous?\nWhat I am not satisfied about my argument is that it does not make\nclear why it is important to remove a point from the torus. What is a\nrigorous way to make clear that removing a point is necessary to make\nthis argument work?\n\nMy main question is:\n\nWhere precisely in the argument above am I using the fact that I removed the point $P$?\n\n",
    "proof": "Let me answer your main question by first considering a different situation.\nDeforming the sphere $S^2$ with a point to a point: Take a point $P \\in S^2$ and remove it, and show that what's left deformation retracts to a point. To do this let's choose $P$ to be the north pole $P=(0,0,1)$, and we'll define a deformation retraction of $S^2 - \\{P\\}$ to the south pole $Q = (0,0,-1)$.\nIntuitively, the deformation restriction moves each point of $S^2 - \\{P\\}$ southward, along the longitude line through that point, to the south pole $Q$.\nWhere precisely in this argument did we use the fact that we removed the point $P$?\nThere is no well-defined longitude line through the north pole; in some sense the north pole lies on every longitude line. Thus, we had to remove the north pole before our deformation retraction could be well-defined.\nWhat is a rigorous way of doing this?\nUse spherical coordinates in $\\mathbb R^3$, whose required properties, including apprpriate continuity properties, are known to you from your knowledge and expertise of analytic geometry. Using spherical coordinates, write down a formula for the deformation retraction\n$$h : (S^2 - \\{P\\}) \\times [0,1] \\to S^2 - \\{P\\}\n$$\nThe formula for $h$ that you write down should have the effect that the latitude coordinate (usually in $[0,2\\pi]$, with $0$ and $2\\pi$ identified) does not change as the time parameter $t \\in [0,1]$ increases from $0$ to $1$. But the longitude coordinate (usually in $[-\\pi/2,\\pi/2]$ with $-\\pi/2$ as the south pole and $+\\pi/2$ as the north pole) should decrease at constant speed from its initial value in $[-\\pi/2,\\pi/2)$, moving along its latitude line to the final value $-\\pi/2$.\nNotice: the north pole had to be omitted because it does not lie on a well-defined longitude, and so there is no way to extend the formula for $h$ continuously. Intuitively, we cannot continuously choose a longitude line along which the north pole moves down toward the south pole. While it's also true that the longitude line at the south pole is not well-defined, the south pole does not move under the deformation retraction.\nBut,  for full rigor you must actually write down the formula for $h$, and check all of its the required properties for the desired deformation retraction.\n\nNow, on to the torus.\nModel the torus $T$ as the quotient of the square $R = [-1,+1] \\times [-1,+1]$ with respect to the equivalence relation generated by $(x,-1) \\sim (x,+1)$ and $(-1,y) \\sim (+1,y)$. In place of cylindrical or spherical coordinates in $S^2$, use radial coordinates on the square $R$. Its boundary $\\partial R$ is the union of the four sides $\\{-1\\} \\times [-1,+1]$, $\\{+1\\} \\times [-1,+1]$, $[-1,+1] \\times \\{-1\\}$, $[-1,+1] \\times \\{+1\\}$. We are going to remove the point $\\mathcal O = (0,0)$. Using our knowledge and expertise in plane analytic geometry, each point $x \\in R - \\{\\mathcal O\\}$ can be written uniquely in the form\n$$r(x) \\cdot b(x)\n$$\nwhere\n\\begin{align*}\nr(x) &= \\frac{1}{\\max\\{x_1,x_2\\}} \\\\\nb(x) = \\frac{x}{r(x)}\n\\end{align*}\nWe have removed the point $\\mathcal O$ in order that these expressions $r(x)$ and $b(x)$ be well-defined and continuous as functions of $x \\in R - \\mathcal O$.\nNow use the coordinates to define the formula for the deformation retraction\n$$h : (R - \\mathcal O) \\times [0,1] \\to R - \\mathcal O\n$$\nIntuitively, the formula for $h$ keeps the boundary coordinate $b(x)$ constant, whereas the \"radial\" coordinate increases linearly from its initial value $r(x) \\in (0,1]$ to its final value $1$, as $x$ moves outward along its radial segment towards $\\partial R$.\nNotice: the central point $\\mathcal O$ had to be removed, because it does not lie on a well-defined radial segment and therefore there is no way to extend $h$ continuously. Now, under the identification of $\\partial R$ to a wedge of two circles, a point in that wedge does not correspond to a well-defined point of $\\partial R$, instead it correspond to either $2$ or $4$ points of $\\partial R$; however, this does not matter because those points do not move under the deformation retraction.\n",
    "tags": [
      "general-topology",
      "algebraic-topology",
      "proof-writing",
      "differential-topology",
      "geometric-topology"
    ],
    "score": 5,
    "answer_score": 3,
    "is_accepted": true,
    "question_id": 3786184,
    "answer_id": 3786437
  },
  {
    "theorem": "Properties of centralization in group theory",
    "context": "Recall Let $C\\subseteq G$ where $G$ is a group. If $x\\in G$ for any $c\\in C,$ $xc=cx,$ then $x$ centralizes $C$\n$$C_G(C) := \\{x \\in G : \\text{ for any } x\\in C, xc=cx \\}$$\nClearly,\n$$C_G(C)=\\bigcap_{c\\in C}C_G(c)$$\n\nProve that\na) $C\\subseteq C_G(C_G(C))$\nb) If $C\\subseteq D$ then $C_G(D)\\subseteq C_G(C)$\nc) $C_G(C_G(C_G(C)))=C_G(C)$\n\nMy Proof-trying:\na) Let $c\\in C$. We will show $c\\in C_G(C_G(C)).$ We need to show for any $x\\in C_G(C),$ $xc=cx.$\nNote that $x\\in C_G(C)$ $\\iff$ for any $c\\in C$ $xc=cx$ by definition.\nHence $c\\in C_G(C_G(C)).$ Therefore $C\\subseteq C_G(C_G(C)).$\nb) Assume $C\\subseteq D.$ Let $x\\in C_G(D).$ Then for any $d\\in D$, we have $dx=xd.$ Since $C\\subseteq D$, then for any $c\\in C$, we have $cx=xc.$ So we are done.\nc)I couldn't show it.\nCan you check my proof, if there is a false, can you edit? Can you help for c)? Thanks...\n",
    "proof": "The proofs for a) and b) are OK.\nFor c), first, apply b) to the inclusion a) to obtain one inclusion.\nSecond, to obtain the reverse inclusion, apply a)  to $C_G(C)$.\n",
    "tags": [
      "group-theory",
      "proof-writing"
    ],
    "score": 5,
    "answer_score": 4,
    "is_accepted": true,
    "question_id": 3750950,
    "answer_id": 3750960
  },
  {
    "theorem": "Proving that a finite simple group (order &lt; 100) is either abelian or has order 60",
    "context": "As the title suggests, I'm asked to \n\nProve that a finite simple group $G$ of order less than $100$ is either abelian or has order $60.$\n\nI approached the problem by saying that $G$ could either have a prime order or a non-prime order and have already proven that, if $G$ has order prime, it has to be abelian.\nHowever, I'm stuck on what to do for the second case. I've seen examples online where they prove that the order of a finite simple nonabelian group $G$ is less than $60$, but how do I prove that there is no other order that $G$ can be if it is nonabelian that is between $61$ and $100?$\n",
    "proof": "This is a classic problem which is an exercise in casework.  I won't do the casework for you, but here are some key observations that will make your casework a lot easier. Let $G$ be a simple non-abelian group.\n\n$G$ can't have a prime power order (consider its center)\n$G$ is isomorphic to a subgroup of the alternating group on the left coset space of $G/H$ for any proper subgroup $H$.\nThe Sylow number equals the index of the Sylow normalizer for any $p$ prime.\nFor any $H\\subseteq G$, the order of $G$ divides $\\frac{[G:H]!}2$\n$G$'s order can't be $p\\cdot q\\cdot r$, where $p,q,r$ are distinct primes.\n$A_5$ is a simple non-abelian group.\n\n",
    "tags": [
      "abstract-algebra",
      "group-theory",
      "proof-writing",
      "simple-groups"
    ],
    "score": 5,
    "answer_score": 5,
    "is_accepted": false,
    "question_id": 3422459,
    "answer_id": 3422474
  },
  {
    "theorem": "Help me with this interesting and challenging to prove &quot;fill the chessboard&quot; problem",
    "context": "A $15×15$ chessboard was covered by $3×3$ and $2×2$ plates in such a way that the plates don't stick out of the chessboard, they don't overlap each other and every field of the chessboard is covered. Find the smallest number of $3×3$ plates used, for which it is possible.\nWhile analyzing this problem i came to the conclusion that the number of nine $3×3$ plates is enough for this to work, but I have no idea how to prove it correctly\n",
    "proof": "Very fun question! I was not satisfied with any of the solutions so far, because they do not generalize to larger boards. But together with a friend I found the general solution to the question of how many $3\\times3$ tiles are necessary for an $n\\times m$ board with $n,m\\geq2$.\nFirst of all, if $n$ and $m$ are even, then we can tile the board with $2\\times2$ tiles only. So let us assume without loss of generality that $n$ is odd from now on.\nLemma: For all $i\\geq0$ with $3i<m$ we need a $3\\times3$ tile with its top on row $3i$. (Use zero-based indexing from top to bottom.)\nProof of lemma: Let $t_j$ denote the number of $3\\times3$ tiles with their top on row $j$. First note that $t_j=0$ holds for $j<0$ and for $j>m-3$, because plates are not allowed to stick out of the chessboard. Furthermore, the total number of $3\\times3$ tiles on row $j$ is $t_j+t_{j-1}+t_{j-2}$. We only have $2\\times2$ and $3\\times3$ tiles and a $2\\times2$ tile has an even width and a $3\\times3$ tile has an odd with. Since all rows $0\\leq j<m$ are $n$ wide and since $n$ is odd, we need to have an odd number of $3\\times3$ tiles in all rows. So $t_j+t_{j-1}+t_{j-2}$ is odd for all $0\\leq j<m$.\nBy induction we will use this to prove that $t_{3i}$ is odd for all $i\\geq0$ with $3i<m$. The base case follows from the case $j=0$. Assume $t_{3i}$ is odd for some $i\\geq0$ with $3i+3<m$. Using the cases $j=3i+2$ and $j=3i+3$, we find that both $t_{3i}+t_{3i+1}+t_{3i+2}$ and $t_{3i+1}+t_{3i+2}+t_{3i+3}$ are odd. From this, we get $t_{3i}\\equiv t_{3i+3}\\ (\\mbox{mod}\\ 2)$, so $t_{3i+3}$ is odd.\nSince $t_{3i}$ is odd for all $i\\geq0$ with $3i<m$, we certainly have $t_{3i}\\geq1$. This proves the lemma.\nCorollary: We need $m$ to be divisible by $3$. This is, again, because plates are not allowed to stick out of the chessboard.\nThere are two cases to consider. Either $m\\equiv0\\ (\\mbox{mod}\\ 6)$ or $m\\equiv3\\ (\\mbox{mod}\\ 6)$.\nCase $m\\equiv0\\ (\\mbox{mod}\\ 6)$: Write $m=6\\ell$. From the lemma, we know the amount of $3\\times3$ tiles needed is at least $2\\ell$. But this can also easily be achieved. We fill the left $3$ columns with $3\\times3$ tiles and the rest can be filled with $2\\times2$ tiles. This is possible, because $n-3$ and $m$ are both even.\nCase $m\\equiv3\\ (\\mbox{mod}\\ 6)$: This is the interesting case. We write $m=6\\ell+3$. By symmetry, from the corollary we find that $n$ is also a multiple of $3$, so we write $n=6k+3$. First note that $2k+2\\ell+1$ is an upper bound on the necessary amount of $3\\times3$ tiles, because we can fill the top $3$ rows and left $3$ columns with $3\\times3$ tiles and then fill the rest with $2\\times2$ tiles. This is possible because $n-3$ and $m-3$ are both even. The claim is that this is optimal.\nWe colour the tiles of the chessboard with the colours $1$, $2$, $3$ and $4$. Rows $0,2,...$ will alternate $1,2,1,2,...$ and rows $1,3,...$ will alternate $3,4,3,4,...$. Note that any $2\\times2$ tile covers every colour exactly once. Also note that there are $(3k+2)(3\\ell+2)$, $(3k+1)(3\\ell+2)$, $(3k+2)(3\\ell+1)$ and $(3k+1)(3\\ell+1)$ tiles of the colours $1$, $2$, $3$ and $4$ respectively.\nLet $a$, $b$, $c$ and $d$ denote the numbers of $3\\times3$ tiles with their center on a tile with colour $1$, $2$, $3$ and $4$ respectively. By counting the amount of tiles a $3\\times3$ tile covers of every colour, and using the fact that $2\\times2$ tiles cover every colour exactly once, we find the following equations.\n\\begin{align*}\n3(d-a)&=(3k+2)(3\\ell+2)-(3k+1)(3\\ell+1)=3k+3\\ell+3\n\\\\3(c-b)&=(3k+1)(3\\ell+2)-(3k+2)(3\\ell+1)=3k-3\\ell\n\\end{align*}\nSo $d=a+k+\\ell+1$ and $c=b+k-\\ell$. Note that we can assume without loss of generality that $k\\geq\\ell$.\nBy applying the lemma to all odd $i$, we find that $a+b\\geq\\ell$. Using this, we also get $c+d=a+b+2k+1\\geq2k+\\ell+1$. Combining the two, we finally get $a+b+c+d\\geq2k+2\\ell+1$, which proves the claim.\n",
    "tags": [
      "proof-writing",
      "plane-geometry",
      "chessboard"
    ],
    "score": 5,
    "answer_score": 3,
    "is_accepted": false,
    "question_id": 3374518,
    "answer_id": 3407656
  },
  {
    "theorem": "Minimum value of definite integral.",
    "context": "\n$f(x)$ is continuous in $[a,b]$, and $f(x)>0$. $$g(m)=\\int_a^b |f(x)-m|\\,dx.$$\n  Claim: $g(m)$ has a minimum when $$m=\\frac{\\int_a^b f(x)\\,dx}{b-a}.$$\n\nIs this claim is true? If it is true, how to prove it? I tried using $$F(x)=\\int f(x)\\,dx$$ as an increasing function, but it didn't work well...\n",
    "proof": "We note that\n$$g(m)=\\int_a^b\\sqrt{(m-f(x))^2}\\,dx.$$\nIt follows that\n\\begin{align*}\ng'(m)&=\\frac{d}{dm} \\int_a^b\\sqrt{(m-f(x))^2}\\,dx \\\\\n&=\\int_a^b\\frac{\\partial}{\\partial m}\\sqrt{(m-f(x))^2}\\,dx \\\\\n&=\\int_a^b\\frac{m-f(x)}{\\sqrt{(m-f(x))^2}}\\,dx \\\\\n&=\\int_a^b\\operatorname{sgn}(m-f(x))\\,dx.\n\\end{align*}\nThis integral is actually easier to evaluate using the Lebesgue integral (if the Riemann integral exists, the Lebesgue integral exists and they're equal). The result would be\n$$\\int_{[a,b]}\\operatorname{sgn}(m-f(x))\\,d\\mu =\\mu(\\{x\\in[a,b]:m-f(x)> 0\\})-\\mu(\\{x\\in[a,b]:m-f(x)<0\\}). $$\nWe want this to be zero, and we can see that this will happen when, in the interval $[a,b],$ $m$ is chosen so that as much area is above $m$ (that is, above $m$ and below $f$) as is below $m$. That is the average you have written out above.\n",
    "tags": [
      "definite-integrals",
      "proof-writing"
    ],
    "score": 5,
    "answer_score": 5,
    "is_accepted": false,
    "question_id": 3267758,
    "answer_id": 3267774
  },
  {
    "theorem": "Taking a natural number different from zero as a base case in an inductive proof",
    "context": "Principle of mathematical induction states that if a subset $S$ of a successor set $\\omega$ is also a successor set, then $S=\\omega$. In primitive terms, it is formulated as:\nif $S \\subset \\omega$, if $0 \\in S$, and if $n^+ \\in S$ whenever $n \\in S$, then $S=\\omega$.\nNow, I wonder what if a statement that I want to prove holds for numbers starting from $b \\in \\omega$ for example. Would it be correct to introduce\n\na set $L$ that contains all natural numbers that satisfy the\n    statement\na function $s: \\omega \\to \\omega, s(n) = n^+$\na function $r: \\omega \\to \\omega,\n\\begin{cases}\nr(0) = b\\\\\nr(n^+) = s(r(n))\\\\\n\\end{cases}$\na set $S$ that\n    contains all natural numbers for which $r(n)\\in L$\n\nand show that $0$, $n$ and $n^+$ are in $S$? Because I can't just start proving inductively from $b$ and say that it holds for all numbers starting from $b$, as the principle of induction clearly states that $0$ has to be in $S$ too. So, I introduce a trick, i.e. mapping to perform induction.\nIs it correct to do so? If not, then how to formally show that I can start from any number?\n",
    "proof": "You can either generalize the principle of induction to:\n\"For all $b \\in \\omega$: if $S \\subset \\omega$, if $b \\in S$, and if $n^+ \\in S$ whenever $n \\in S$, then $S=\\{ n \\in \\omega | n \\geq b \\}$.\"\nOr, you can use the principle as stated and prove the base and step for the property $P(n)$ defined as $n \\geq b \\to P'(n)$, where $P'(n)$ is the 'real' property you are interested in, i.e. the property you want to prove all numbers $n \\geq b$ to have. This works, since $P(n)$ will be trivially true for all $n < b$.\n",
    "tags": [
      "proof-writing",
      "induction",
      "peano-axioms"
    ],
    "score": 5,
    "answer_score": 5,
    "is_accepted": true,
    "question_id": 3256468,
    "answer_id": 3256518
  },
  {
    "theorem": "Proving $n \\leq 3^{n/3}$ for $n \\geq 0$ via the Well-Ordering Principle",
    "context": "I'm attempting to prove:\n$$n \\leq 3^{n/3} \\quad \\text{for }n \\geq 0$$\nI'm having a little trouble continuing. This is what I have so far:\n\nSuppose for a contradiction there is a subset of nonnegative integers $S$ such that $x > 3^{x/3}$ for $x \\in S$. By the Well-Ordering Principle, there is some least element $m \\in S$. It also means that for some $n < m$, $n \\leq 3^{n/3}$ must apply, and since $n = 0$ holds we can conclude that $m > 0$. If follows that $m - 1 \\geq 0$ and so $m - 1 \\leq 3^{(m-1)/3}$ applies:\n$$\n\\begin{align}\nm - 1 \\leq 3^{(m-1)/3} &\\equiv (m-1)^3 \\leq 3^{m-1} \\\\\n&\\equiv 3(m-1)^3 \\leq 3^m \\\\\n&\\equiv 3(m-1)^3 \\leq 3^m < m \\\\\n&\\equiv 3(m-1)^3 < m\n\\end{align}\n$$\n\nBut I'm not sure how to show now that this is a contradiction. How do I continue?\n",
    "proof": "Using contradiction, if there exist a $n \\ge 0 $ such that\n$$\n\\begin{align}\nn &> 3^{n/3}\n\\end{align}\n$$\nThen, because of the WOP, there will be a $n=m$ which is the least value of the set that satisfies the inequality above.\nIt is verifiable that $n \\le 3^{n/3}$ holds true for $0 \\le n \\le 4$. Thus, $m \\ge 5$\nsince $m-3 \\lt m$\n$$\n\\begin{align}\n(m-3) &\\le 3^{(m-3)/3}\n\\end{align}\n$$\nThus,\n$$\n\\begin{align}\n(m-3) &\\le 3^{m/3}\\cdot3^{-1}\\\\\n3(m-3) &\\le 3^{m/3}\n\\end{align}\n$$\nIt is known that $m \\lt 3(m-3)$ for any $m \\ge 5$, thus\n$$\n\\begin{align}\n m \\lt 3(m-3) &\\le 3^{m/3}\\\\\n m &\\lt 3^{m/3}\n\\end{align}\n$$\nWhich contradicts the premise $n > 3^{n/3}$.\n",
    "tags": [
      "proof-writing",
      "induction",
      "natural-numbers"
    ],
    "score": 5,
    "answer_score": 3,
    "is_accepted": false,
    "question_id": 3013815,
    "answer_id": 3514207
  },
  {
    "theorem": "What are some good examples of _Proof by Mutual Induction_?",
    "context": "Sometimes programs rely on mutual recursion to do things. For example, here is an Agda program (taken from here that proves ∀ {m n : ℕ} → even m → even n → even (m + n) and ∀ {m n : ℕ} → odd m → even n → odd (m + n) by using one fact to prove the other during induction. This example is also explained here \nimport Relation.Binary.PropositionalEquality as Eq\nopen Eq using (_≡_; refl; cong; sym)\nopen import Data.Nat using (ℕ; zero; suc; _+_)\n\ndata even : ℕ → Set\ndata odd : ℕ → Set\n\ndata even where\n  zero : even zero\n  suc : ∀ {n : ℕ} → odd n → even (suc n)\n\ndata odd where\n  suc : ∀ {n : ℕ} → even n → odd (suc n)\n\ne+e≡e : ∀ {m n : ℕ} → even m → even n → even (m + n)\no+e≡o : ∀ {m n : ℕ} → odd m → even n → odd (m + n)\n\ne+e≡e zero     en = en\ne+e≡e (suc om) en = suc (o+e≡o om en)\n\no+e≡o (suc em) en = suc (e+e≡e em en)\n\nAnother example is this (in Haskell) (from here). Suppose you want to identify sequences which keep increasing or decreasing alternately.\nalternating :: [Int] -> Bool\nalternating l = (updown l) || (downup l)\n\nupdown :: [Int] -> Bool\nupdown [] = True\nupdown [x] = True\nupdown (x:y:ys) = (x < y) && (downup (y:ys))\n\ndownup:: [Int] -> Bool\ndownup [] = True\ndownup [x] = True\ndownup (x:y:ys) = (x > y) && (updown (y:ys))\n\nWhat are some other examples of mathematical proofs which are more naturally expressed with mutual induction?\nThe most non-trivial example I can think of is the proof of Strong Normalization Theorem for Simply Typed Lambda Calculus (see page 42, section 6.2 here). However, this is a very technical property that I suppose mainstream mathematicians are not really interested in.\n",
    "proof": "In my opinion, the strong normalization theorem for simply typed $\\lambda$-calculus is not a negligible result, at least in theoretical computer science (which is much closer to mathematics than software development or web programming).\nIn general, proofs by mutual induction are quite common in theoretical computer science, which deals with discrete objects often defined by induction or mutual induction. In general, a proof by mutual induction of a statement $A$ consists in proving a stronger statement than $A$, usually a statement of the form $A \\land B$ (i.e. $A$ and $B$), where $B$ can be seen as an \"auxiliary statement\". The advantage is that now one can use a stronger induction hypothesis than in a proof of the mere statement $A$ by simple induction.\nI guess you  are interested in examples of proofs by mutual induction concerning ordinary objects of mathematics. No famous result in ordinary mathematics whose proof makes use of mutual induction in a crucial way crosses my mind. Anyway, the following is a nice example of a property about natural numbers that cannot be proved by simple induction (at least, not in a natural way), but the proof by mutual induction is very easy, because it gives a stronger induction hypothesis.\nLet $f, g, h \\colon \\mathbb{N}\\to \\{0,1\\}$ be functions defined as follows:\n\\begin{align}\nf(n) &= \n\\begin{cases}\n0 &\\text{if } n = 0 \\\\\ng(n - 1)  &\\text{otherwise}\n\\end{cases}\n&\ng(n) &= \n\\begin{cases}\n1 &\\text{if } n = 0 \\\\\nf(n - 1)  &\\text{otherwise}\n\\end{cases}\n\\\\[0.5em]\nh(n) &= \n\\begin{cases}\n0 &\\text{if } n = 0 \\\\\n1- h(n - 1) &\\text{otherwise}\n\\end{cases}\n\\end{align}\n\nProposition. For every $n \\in \\mathbb{N}$, one has $h(n) = f(n)$.\n\nIt does not seem possible to prove the proposition by simple induction, the induction hypothesis in this case is too weak. But the proof is quite easy if you prove the following stronger statement, by (mutual) induction on $n \\in \\mathbb{N}$:\n\nProposition'. For every $n \\in \\mathbb{N}$, one has $h(n) = f(n)$ and $h(n) = 1 - g(n)$.\n\nProof. By induction on $n \\in \\mathbb{N}$.\nBase case. $f(0) = 0 = h(0)$ and $h(0) = 0 = 1 - 1 = 1- g(0)$.\nInductive step. By induction hypothesis (i.h.), $f(n) = h(n)$ and $h(n) = 1 - g(n)$. Hence, \n\\begin{align}\nf(n+1) &= g(n) \\overset{i.h.}{=} 1 - h(n) = h(n+1) \\\\\nh(n+1) &= 1 - h(n) \\overset{i.h.}{=} 1 - f(n) = 1 - g(n+1). \\qquad \\square\n\\end{align}\nNote that in the inductive step, to prove that $f(n+1) = h(n+1)$ we use the inductive hypothesis about the second statement, i.e. $h(n) = 1 - g(n)$.\nConversely, to prove that $h(n+1) = 1 - g(n+1)$ we use the inductive hypothesis about the first statement, i.e. $f(n) = h(n)$.\nThis is the essence of mutual induction. \n",
    "tags": [
      "logic",
      "proof-writing",
      "soft-question",
      "induction"
    ],
    "score": 5,
    "answer_score": 3,
    "is_accepted": false,
    "question_id": 2929632,
    "answer_id": 2929857
  },
  {
    "theorem": "Existence of a ball whose images under two continuous functions are disjoint.",
    "context": "Let $f,g: X \\to \\mathbb{R}^{n}$ be a continuous functions at point $a$. If $f(a)\\neq g(a)$, show that exists an open ball B with center $a$ such that $\\forall x,y \\in B \\Rightarrow f(x)\\neq g(y)$.\nMy thoughts:\n$b=|f(a)-g(a)|\\neq 0.$ For $\\epsilon=b/2$, there is $\\delta_{1}>0$ such that $|x-a|<\\delta_{1} \\Rightarrow |(f(x)-g(x))-(f(a)-g(a))|<\\epsilon = b/2$. Then $|f(x)-g(x)\n|>b/2$.\nNow $|f(x)-g(y)|+|g(y)-g(a)| \\geq |f(a)-g(a)|=b>0$\n$c=|g(a)|$. For $\\epsilon =\\min\\{c/2,b/2\\}=d$, there is $\\delta_{2}$ such that $|x-a|<\\delta_{2} \\Rightarrow |g(a)-g(x)|<d$.\nIf $x,y \\in B_{\\delta}(a), \\delta=\\min\\{\\delta_{1},\\delta_{2}\\}$, then $|f(x)-g(y)|>b-d>0$.\nI believe it is correct, but I'm not sure.\n",
    "proof": "It can also be proved by contraposition. Suppose that in every ball $B$ centered at $a$, there exists points $x, y\\in B$ such that $f(x)=g(y)$. Taking balls of radius $\\dfrac{1}{n}$, $n\\geq 1$, we then get a sequence of points $x_n$ and $y_n$ such that $|x_n-a|, |y_n-a|<\\dfrac{1}{n}$ and $f(x_n)=g(y_n)$. Obviously, $x_n\\rightarrow a$ and $y_n\\rightarrow a$ as $n\\rightarrow\\infty$. Since $f$ and $g$ are continuous, so $$\\lim_{n\\rightarrow \\infty}f(x_n)=f(a),\\;\\;\\; \\lim_{n\\rightarrow \\infty}g(y_n)=g(a)$$ and therefore $f(a)=g(a)$. \n",
    "tags": [
      "real-analysis",
      "proof-verification",
      "proof-writing"
    ],
    "score": 5,
    "answer_score": 2,
    "is_accepted": true,
    "question_id": 2816388,
    "answer_id": 2816622
  },
  {
    "theorem": "Proving that softmax converges to argmax as we scale x",
    "context": "For a vector $\\mathbb{x}$, the softmax function $S:\\mathbb{R}^d\\times \\mathbb{R}\\rightarrow \\mathbb{R}^d$ is defined as\n$$\nS(x;c)_i = \\frac{e^{c\\cdot x_i}}{\\sum_{k=1}^{d} e^{c\\cdot x_k}}\n$$\nConsider if we scale the softmax with constant $c$,\n$$\nS(x;c)_i = \\frac{e^{c\\cdot x_i}}{\\sum_{j=1}^{d} e^{c\\cdot x_j}}\n$$\nNow since $e^x$ is an increasing and diverging function, as $c$ grows, $S(x)$ will emphasize more and more the max value. At $c \\rightarrow \\infty$, $S(x)$ outputs a one-hot vector with 1 at the position of the maximum element. Now this is my intuition, but how do I prove this?\n",
    "proof": "Put $|\\mathbb{x}| = n$ for convenience since this proof assumes $x \\in\\mathbb R^n$ for some $n$,\n$$\nS(x_i) = \\frac{e^{x_i}}{\\sum_{k=1}^{n} e^{x_k}}\n$$\nscaling by constant $c$,\n$$\nS(x_i, c) = \\frac{e^{x_i (c)}}{\\sum_{k=1}^{n} e^{x_k (c)}}\n$$\nLet $\\hat x = max(x_i)$ and divide multiply $S(x_i, c)$ by $\\frac{e^{-\\hat xc}}{e^{-\\hat xc}}$:\n$$\\lim_{c \\to \\infty} S(x_i, c) = \\lim_{c \\to \\infty} \\frac{e^{-(\\hat x -x_i) (c)}}{\\sum_{k=1}^{n} e^{-(\\hat x -x_k) (c)}}$$\nNotice that $\\Delta_i = \\hat x -x_i > 0$ if $x_i \\not = \\hat x$ and $\\hat \\Delta = \\hat x -x_i = 0$ if $x_i = \\hat x$\n$$\\implies \\lim_{c \\to \\infty} S(x_i, c) = \n\\begin{cases}\n\\lim_{c \\to \\infty} \\frac{e^{-(\\Delta_i) (c)}}{(\\sum_{x_k \\not = \\hat x} e^{-(\\Delta_k) (c)}) + 1} \\text{, if $x_i \\not = \\hat x$}\\\\\n\\lim_{c \\to \\infty} \\frac{1}{(\\sum_{x_k \\not = \\hat x} e^{-(\\Delta_k) (c)}) + 1} \\text{, if $x_i = \\hat x$} \n\\end{cases}\n$$\n$$\\implies S(x_i, c) \\to \\begin{cases}\n\\frac{0}{1} = 0 \\text{, if $x_i \\not = \\hat x$}\\\\\n\\frac{1}{1} = 1 \\text{, if $x_i = \\hat x$} \n\\end{cases}$$\nas $c \\to \\infty$\nTherefore, softmax $\\to$ argmax as $x$ is scaled.\n",
    "tags": [
      "probability",
      "proof-writing"
    ],
    "score": 5,
    "answer_score": 5,
    "is_accepted": true,
    "question_id": 2656231,
    "answer_id": 2656236
  },
  {
    "theorem": "Proving Statement $A$ implies Statement $B$",
    "context": "I'm trying to wrap my head around logical implications, equivalences, etc. and I made up two random statements. This is my attempt at proving one implies the other:\nStatement $A$\n$∀a,b,c∈Z,a<b<c⇒0<f(a)<f(b)<f(c)$\nStatement $B$\n$∀x,y,z∈Z,x<y∧z≠x∧z≠y⇒f(x)+f(y)+f(z)>0$\nProve $A⇒B$\nAssume Statement $A$ and the antecedent of Statement $B$\nDividing Statement $B$ into cases:\nCase 1: $z<x<y$\nFrom $A$ we can conlude that $0<f(z)<f(x)<f(y)$, meaning $f(x)+f(y)+f(z)>0$\nCase 2: $x<z<y$ and Case 3: $x<y<z$\n(Same reasoning as in case 1)\nBy showing that all three possible cases are true given the assumption that $A$ is true, can I say that Statement $A$ implies Statement $B$? \nAnd if possible, how would you prove that $B$ implies $A$?\n",
    "proof": "This is interesting.  Your argument that the truth of $A$ forces the truth of $B$ is correct.  You have divided it into sensible cases and correctly argued the cases.  As an exercise in logic or proof-writing, it's fine.\nThere is, however, quite a bit of fat that can be trimmed out of all this to get to the real meat of the problem.  \nYour statement $A$ immediately implies that $f$ is a positive function, so that $f(x) > 0$ for all $x \\in \\mathbb{Z}$.  Take any $a$, and let $b=a+1$ and $c=a+2$.  Then you can conclude $f(a) > 0$ from $A$.  Since $a$ is arbitrary, we're done.  Now that $f$ is positive, $B$ follows immediately:  the sum of three positive integers is again positive.  \n",
    "tags": [
      "proof-verification",
      "logic",
      "proof-writing",
      "predicate-logic"
    ],
    "score": 5,
    "answer_score": 3,
    "is_accepted": true,
    "question_id": 2580236,
    "answer_id": 2580292
  },
  {
    "theorem": "Proof by induction and contrapositive",
    "context": "When I was in highschool I was taught how to prove statements using Proof by contrapositive. I also learned how to  prove statements using mathematical induction.\nNow I realize that, as the inductive step is a conditional statement, it might be proved using proof by contrapositive. However, I cannot find an example that uses this technique but in an easier way than regular induction.\nIs there a case where this is useful? (Using both concepts)\n",
    "proof": "As a concrete example, consider the following two Peano Axioms:\n$PA 1: \\forall x \\ \\neg s(x) = 0$\n$PA 2: \\forall x \\forall y (s(x) = s(y) \\rightarrow x = y$\nThese two axioms are about the successor function $s$, normally understood as $s(x) = x+1$\nUsing induction and contraposition, you can now prove that $\\forall x \\ s(x) \\not = x$:\nBase: $x=0$.  By $PA1$, we have $s(0) \\not = 0$. Check!\nStep: Take some arbitrary $n$. We want to show the conditional $s(n) \\not = n \\rightarrow s(s(n)) \\not = s(n)$\nWell, we can do this by contraposition: By $PA2$ we immediately get $s(s(n))=s(n) \\rightarrow s(n)=n$. Check! \n===\nAs a more general comment, I think you might also be interested in learning about the proof technique of Proof by Infinite Descent, which combines ideas from induction and proof by contradiction.\nIn infinite Descent, you prove that no natural number has a certain property by proving that if there is a number with that property, then there will always be a smaller number with that property.  But since such a 'descent' along the natural numbers has to come to a stop at some point, no 'infinite descent' is possible.\nAs it turns out, Infinite Descent is very closely related (in fact, it is equivalent) to Strong Induction, but it has a different conceptual flavor.\n",
    "tags": [
      "logic",
      "proof-writing",
      "induction"
    ],
    "score": 5,
    "answer_score": 4,
    "is_accepted": true,
    "question_id": 2509136,
    "answer_id": 2510029
  },
  {
    "theorem": "$P$ is a prime ideal of $R$ iff $R/P$ is an integral domain. : $P≠R$",
    "context": "Here is the theorem and proof in my textbook.\n\n\nThm.  Let $P$ be an ideal of a commutative ring $R$ with identity $1$.  Then $P$ is a prime ideal of $R$ if and only if $R/P$ is an integral domain.\n\nProof.\n\nSuppose that $P$ is a prime ideal of a commutative ring $R$ with $1$. \nThen $P$≠$R$ implies $1+P≠0+P$.\nHence $R/P$ is a commutative ring $R$ with identity. \nAssume that $(a+P)(b+P)=0+P$. \nThen $ab+P=0+P$ and $ab∈P$. \nBy the definition of a prime ideal P we get $a∈P$ or $b∈P$. \nThat is, $a+P=0+P$ or $b+P=0+P$. \nThus $R/P$ is an integral domain.\n\nConversely, if $R/P$ is an integral domain, then $1+P≠0+P$ and $R/P$ is a commutative ring $R$ which has no zero divisors. \nHence $P≠R$. \nAssume $ab∈P$. \nThen $ab+P=0+P$ and $(a+P)(b+P)=0+P$. \nSince $R/P$ is an integral domain, we get $a+P=0+P$ or $b+P=0+P$. \nSo $a∈P$ or $b∈P$. \nThus $P$ is a prime ideal.\n\nI am having problems understanding proof with \"Conversely, if $R/P$ is an integral domain, then $1+P≠0+P$.\".\nI know that $R/P$ has an identity. (cause it is ID)\nBut, why doesn't \"$P$\" have an identity?\nIsn't there a case $P=R$?\nPlease help me understanding.\nI think $P≠R$ should be with condition of Thm..\nThank you in advance.\n",
    "proof": "\nBut, why doesn't \"$P$\" have an identity?\n\nWho says it can't? If $F$ is a field, then $P=\\{0\\}\\times F$ is a prime ideal of $F\\times F$, and $P$ has an identity element $(0,1)$.\n\nIsn't there a case $P=R$?\n\nIf you are assuming the condition on the left that $P$ is prime, then no: prime ideals are defined to be proper ideals of their rings.\nIf you are assuming the condition on the right ($R/P$ an integral domain) then again no: integral domains are defined to have a nonzero multiplicative identity, so this quotient ring must have at least two elements. Saying that $R=P$ would imply that the quotient has only one element.\nI say both these things from the position of standard definitions of \"prime ideal\" and \"integral domain.\" Of course you can always make nonstandard definitions and change the answer here, but I think the point is to know what the mainstream reasoning is.\n",
    "tags": [
      "abstract-algebra",
      "proof-writing",
      "ideals",
      "maximal-and-prime-ideals",
      "integral-domain"
    ],
    "score": 5,
    "answer_score": 4,
    "is_accepted": true,
    "question_id": 2453484,
    "answer_id": 2454347
  },
  {
    "theorem": "Group Theory symmetry questions",
    "context": "Is it possible to draw a figure that has exactly one reflection symmetry (flip) and one (or more) non-trivial rotational symmetry?  (Note:  The trivial symmetry is the 0 degrees or 360 degreesrotation).  \nI am doing this problem for a homework assignment, and was assuming that it wasn't possible, but my proof may not be rigorous enough or even proof at all. This is what i think. \nif you assume that something has one reflectional symmetry and one non-trivial rotational symmetry you can rotate it and another reflectional symmetry will become present which contradicts the original assumption of a single reflective symmetry. furthermore any figure with a single reflective symmetry will have atleast 2 rotational non-trivial symmetries. \ncan someone help me word this so it makes more sense? or steer me on the right path?\n",
    "proof": "You can use orientation of symmetries here: Per definition a symmetry $\\sigma$ is a reflection if and only if $\\det\\sigma$ is negative. So if there's a rotation $\\sigma$ and a reflection $\\rho$ in some symmetry group, then $\\det(\\sigma\\rho)=\\det\\sigma\\cdot\\det\\rho<0$ and $\\sigma\\rho$ is a reflection as well.\n",
    "tags": [
      "group-theory",
      "proof-verification",
      "proof-writing",
      "proof-explanation",
      "symmetric-groups"
    ],
    "score": 5,
    "answer_score": 0,
    "is_accepted": true,
    "question_id": 2351726,
    "answer_id": 2351963
  },
  {
    "theorem": "Show $\\dim(U) = d_1^2 + d_2^2 + \\cdots + d_k^2$, where $U$ is the set of matrices that commute with a diagonalizable matrix $A$",
    "context": "I asked this question last night, but I have made some additional progress where I have another question.\n\nLet $A$ be an $n \\times n$ diagonalizable matrix with distinct eigenvalues $\\lambda_1, \\ldots, \\lambda_k$ with corresponding multiplicities $d_1, \\ldots, d_k$. Show that $$\\dim(U) = d_1^2 + d_2^2 + \\cdots + d_k^2,$$ where $U = \\{B \\in M_n \\,|\\, AB = BA \\}$.\n\nPROOF: Let $A$ be an $n \\times n$ diagonalizable matrix with eigenvalues $\\lambda_1, \\ldots, \\lambda_k$ and respective multiplicities $d_1, \\ldots, d_k$. Since $A$ is diagonalizable, $D = P^{-1}AP$ is a diagonal matrix.\nObserve that for any $B \\in U$, $$P^{-1}AP = D \\implies P^{-1}ABP = D(P^{-1}BP)$$ and likewise $$P^{-1}BAP = (P^{-1}BP)D.$$ This implies that we need only determine matrices $C$ such that $CD = DC$.\nLet $$D = \\begin{pmatrix}\\lambda_1I_1 & & & \\\\ & \\lambda_2I_2 & & \\\\ & & \\ddots & \\\\ & & & \\lambda_kI_k\\end{pmatrix}.$$ Note that $I_i$ is the identity matrix of size $d_i \\times d_i$. Partition $C$ as such: $$C = \\begin{pmatrix}C_{11} & C_{12} & \\cdots & C_{1k} \\\\ C_{21} & C_{22} & & C_{2k} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ C_{k1} & C_{k2} & \\cdots & C_{kk}\\end{pmatrix}$$ Because $C$ and $D$ should commute, $CD = DC$ and so for any $i, j = 1, \\ldots, k$, we have $$\\lambda_iC_{ij} = C_{ij}\\lambda_j \\implies (\\lambda_i - \\lambda_j)C_{ij} = O.$$\nBecause each eigenvalue is distinct, it is not possible that $\\lambda_i - \\lambda_j = 0$ (for $i = j$), which leaves that $C_{ij} = O$ where $i \\ne j$. So $C$ is block diagonal.\nHere's where I'm stuck. I'm not sure what comes next to deduce $\\dim(U)$. I think I need to find the dimension of each $C_{ii}$. How can I find the dimension of each $C_{ii}$? Surely it isn't as easy as saying that each $C_{ii}$ is $d_i \\times d_i$, is it?\n",
    "proof": "It is surely that easy. Each diagonal block $C_{ii}$ interacts only with the $\\lambda_iI_i$ part of $D$, both when calculating $CD$ and when calculating $DC$, and that part of the product clearly commutes without any restrictions on the entries of $C_{ii}$. The shape of  the $C_{ii}$ block is $d_i\\times d_i$, and thus it contributes a term $d_i^2$ to the dimension of $U$.\n",
    "tags": [
      "linear-algebra",
      "vector-spaces",
      "proof-writing",
      "eigenvalues-eigenvectors"
    ],
    "score": 5,
    "answer_score": 5,
    "is_accepted": true,
    "question_id": 1924980,
    "answer_id": 1925001
  },
  {
    "theorem": "Show that the one point compactification of $\\mathbb{Q}$ is not Hausdorff",
    "context": "https://en.wikipedia.org/wiki/Alexandroff_extension\nDefinition: one point compactification\n\nLet $X$ be any topological space, and let   $ \\infty$  be any object\n  which is not already an element of $X$. Put $ X^{*}=X\\cup \\{\\infty\n \\}$, and topologize $X^*    $ by taking as open sets all the open\n  subsets $U$ of $X$ together with all subsets $V$ which contain $\\infty\n $  and such that $X\\setminus V$ is closed and compact\n\nShow that the one point compactification of $\\mathbb{Q}$ which is $\\mathbb{Q}^*$ is Not Hausdorff.\nWhat to do? What...\nMy Attempt:\nSuppose to the contrary $\\mathbb{Q}^*$ is Hausdorff, then we take two points $x$, $\\infty \\in \\mathbb{Q}^*$, $x \\neq \\infty$, and produce disjoint open sets $U,V$ such that $x \\in U$ and $\\infty \\in V$\nBy definition, we know that $\\mathbb{Q} \\backslash V$ is a closed and compact space such that $x \\in U \\subseteq \\mathbb{Q} \\backslash V$\nWe wish to produce a contradiction such that $\\mathbb{Q} \\backslash V$ is not closed, or not compact. But we know that $\\mathbb{Q} \\backslash V$ has to be closed since $V$ is open, therefore we need to show $\\mathbb{Q} \\backslash V$ is not compact.\nLet $\\mathcal{U}$ be an open cover of $\\mathbb{Q} \\backslash V$. Since  $\\mathbb{Q} \\backslash V$ is claimed to be compact, then $\\mathcal{U}$ has a finite subcover $\\{U_i|i \\in F\\}$, $F$ is finite in  $\\mathbb{Q} \\backslash V$. Then for all $x \\in \\mathbb{Q}\\backslash V$, $\\exists i \\in F$ s.t. $x \\in U_i$ (...Ugh everything seems fine...)\nCan someone provide me with some help as to how to go on with this proof? Thanks a bunch.\n",
    "proof": "Given your open sets $U$ and $V$ in $\\Bbb Q^*$, you can continue the argument as follows.\n$U$ is an ordinary open nbhd of $x$ in $\\Bbb Q$, so it contains an open nbhd of $x$ of the form $(a,b)\\cap\\Bbb Q$ for some irrational $a,b\\in\\Bbb R$. Let $W=(a,b)\\cap\\Bbb Q$; clearly $W\\cap V=\\varnothing$, so in particular $\\infty\\notin\\operatorname{cl}_{\\Bbb Q^*}W$. Thus, \n$$\\operatorname{cl}_{\\Bbb Q^*}W=\\operatorname{cl}_{\\Bbb Q}W=[a,b]\\cap\\Bbb Q=W\\;.$$\nBut this is clearly impossible, since $W$ is not compact. \nIf you want an explicit example of an open cover of $W$ with no finite subcover, let $\\langle a_n:n\\in\\Bbb N\\rangle$ and $\\langle b_n:n\\in\\Bbb N\\rangle$ be sequences in $(a,b)$ such that\n\n$\\langle a_n:n\\in\\Bbb N\\rangle$ is strictly decreasing and converges to $a$,  \n$\\langle b_n:n\\in\\Bbb N\\rangle$ is strictly increasing and converges to $b$, and  \n$a_0<b_0$.\n\nLet $U_n=(a_n,b_n)\\cap\\Bbb Q$ for $n\\in\\Bbb N$; then $\\{U_n:n\\in\\Bbb N\\}$ is the desired cover.\n",
    "tags": [
      "general-topology",
      "proof-verification",
      "proof-writing",
      "compactness",
      "compactification"
    ],
    "score": 5,
    "answer_score": 4,
    "is_accepted": true,
    "question_id": 1886801,
    "answer_id": 1887330
  },
  {
    "theorem": "Proving the nested interval theorem",
    "context": "\nTheorem:\nLet $\\{I_n\\}_{n \\in \\mathbb N}$ be a collection of closed intervals with the following properties:\n\n$I_n$ is closed $\\forall \\,n$, say $I_n = [a_n,b_n]$;\n$I_{n+1} \\subseteq I_n$$\\forall \\,n$.\nThen $\\displaystyle\\bigcap_{n=1}^{\\infty} I_n \\ne \\emptyset$.\n\n\nPf: Let $I_n $ be intervals that satisfy 1 and 2. Say $I_n = [a_n, b_n] \\forall n\\ge 1$.\nLet the sets $A$ and $B$ be defined by $A = \\{a_n\\}$ and $B = \\{b_n\\}$.\nTherefore $\\forall n,k \\ge 1$, $a_k \\le b_n$.\nCase 1: $k \\le n$\nThen $[a_n,b_n] \\subset [a_k, b_k]$ therefore $b_n \\in [a_k,b_k]$ and $a_k \\le b_n \\le b_k$ therefore $a_k \\le b_k$.\nCase 2: $k>n$\nTherefore $I_k \\subset I_n$. By nestedness, $[a_k,b_k] \\subset [a_n,b_n]$, therefore $a_k \\le b_n$.\nClaim: $\\sup A \\le \\inf B$.\nProof of claim: Let $A$ and $B$ be sets such that for all $a \\in A$ and for all $b \\in B$, $a \\le b$ Therefore $\\sup A \\le b$ and $a \\le \\inf B$ therefore $\\sup A \\le \\inf B$.\nNow we must prove that either $\\bigcap_{n=1}^{\\infty} I_n = [\\sup A, \\inf B]$ or $\\bigcap_{n=1}^{\\infty} I_n = \\emptyset$.\nFirst we will show $[\\sup A, \\inf B] \\subset \\bigcap I_n$. Let $x \\in [\\sup A, \\inf B]$. Therefore $\\sup A \\le x \\le \\inf B$ and $\\forall n $, $a_n \\le \\sup A \\le x \\le \\inf B \\le b_n$ or $a_n \\le x \\le b_n$ and thus $x \\in I_n$.\nNow we will show that $\\bigcap I_n \\subset [\\sup A, \\inf B]$. Let $y \\in \\bigcap I_n$. Show $\\sup A \\le y \\le \\inf B$. We know that $\\forall n \\ge 1$, $a_n \\le y \\le b_n$. Since $a_n \\le y$, we see that $\\sup A \\le y$.\nSimilarly since $y \\le b_n$, we see that $y \\le \\inf B$.\n\nAs you can see, this proof is very long. Does anyone have any advice to shorten this?\n\n",
    "proof": "Here is a non-constructive proof.\nConstruct a sequence by choosing an element $x_n \\in I_n$ for every $n$; you can do this however you like. Since this sequence is bounded then the Bolzano-Weierstrass theorem says there exists a convergent subsequence $x_{n_k}$ converging to some real number $c$. Suppose $c \\not\\in \\bigcap_{n=1}^{\\infty} I_n$. Then there exists some $N$ for which $c \\not\\in I_n$ for all $n > N$, and so there is some number $\\varepsilon >0$ such that $|x_{n_k} - c| \\geq \\varepsilon$ for all $n_k > N$ contradicting convergence. Thus $c \\in \\bigcap_{n=1}^{\\infty} I_n$ and so it is nonempty.\n",
    "tags": [
      "proof-writing",
      "self-learning",
      "cauchy-sequences",
      "supremum-and-infimum"
    ],
    "score": 5,
    "answer_score": 3,
    "is_accepted": false,
    "question_id": 1475538,
    "answer_id": 1475581
  },
  {
    "theorem": "Proof that group is commutative if every element is its inverse (feedback wanted)",
    "context": "This is one of my first proofs about groups. Please feed back and criticise in every way (including style & language).\nAxiom names (see Wikipedia) are italicised. $e$ denotes the identity element.\n\nLet $(G, \\cdot)$ be a group.\nWe assume that every element is its inverse.\nIt remains to prove that our group is commutative.\nNon-trivially, $\\textit{associativity}$ implies that parentheses are unnecessary.\nTherefore, we do not use parentheses,\nwe will not use $\\textit{associativity}$ explicitly.\nBy $\\textit{identity element}$, $G \\ne \\emptyset$.\nNow, let $a, b \\in G$.\nBy assumption, $$aa = e \\text{ and } bb = e. \\quad \\text{(I)}$$\nBy $\\textit{closure}$, $ab \\in G$.\nSo, by assumption, $$abab = e.\\quad \\text{(II)}$$\nIt remains to prove that $ab = ba$.\n\\begin{equation*}\n\\begin{split}\nab &= aeb && \\quad\\text{by }\\textit{identity element} \\\\\n   &= aababb && \\quad\\text{by (II)} \\\\\n   &= ebabb && \\quad\\text{by (I)} \\\\\n   &= ebae && \\quad\\text{by (I)} \\\\\n   &= bae && \\quad\\text{by }\\textit{identity element} \\\\\n   &= ba && \\quad\\text{by }\\textit{identity element}\n\\end{split}\n\\end{equation*}\nQED\n",
    "proof": "It is correct, however, you might see the way similarly as: $$a(ab)b=a^2b^2=e=(ab)^2=a(ba)b.$$\n",
    "tags": [
      "abstract-algebra",
      "group-theory",
      "soft-question",
      "proof-verification",
      "proof-writing"
    ],
    "score": 5,
    "answer_score": 5,
    "is_accepted": true,
    "question_id": 910640,
    "answer_id": 910646
  },
  {
    "theorem": "Difference between bound and free variable",
    "context": "What is the difference between \n$\\forall x (P(x)\\implies Q(x))$   and $P(x)\\implies Q(x)$\nI know in the first one the variable x is bound but in the second one the variable is free. What are the consequences of this when proving it(Is the proof of the first statement different from the second)?\nAlso $ x>2 \\implies x>4$ and $\\forall x (x>2 \\implies x>4)$ What is the difference(When proving how are the proofs different) ?\nAlso, in limits $\\forall \\epsilon >0 \\exists \\delta >0 \\forall x ( |x-a|<\\delta \\implies |f(x)-L|<\\epsilon)$. Are $\\epsilon, \\delta,x$ bound and $a,f(x)$ free?\n",
    "proof": "In first-order logic typical rules governing the quantifiers are :\n\n$${\\Gamma \\vdash \\forall x \\alpha \\over \\Gamma \\vdash \\alpha [x/t] }\\, \\, (\\text{where} \\, t \\, \\text{is substitutable for} \\, x \\text{in} \\alpha)$$\n$${\\Gamma \\vdash \\alpha [x/y] \\over \\Gamma \\vdash \\forall x \\alpha }\\, \\, (y \\, \\text{not free in} \\, X \\cup \\alpha)$$\n\nThus, in your example, having proved :\n\n$∀x[P(x) \\rightarrow Q(x)]$\n\nwe can derive :\n\n$P(t) \\rightarrow Q(t)$\n\nfor every term $t$, included $x$ itself.\nFor the other \"direction\", if we have proved :\n\n$\\Gamma \\vdash P(y) \\rightarrow Q(y)$,\n\nprovided that $y$ is not free in any formula in $\\Gamma$, we can conclude :\n\n$\\Gamma \\vdash \\forall x[P(x) \\rightarrow Q(x)]$.\n\nIn the above proof $\\Gamma$ is a set of formulae, and we can have $\\Gamma = \\emptyset$.\nThus, as a particular case of the above meta-theorem (usually called Generalization Theorem) we have :\n\nif $\\vdash \\alpha[x/y]$, then $\\vdash \\forall x \\alpha$.\n\nThe proviso is necessary in order to avoid fallacies.\nIf we assume $x = 0$, we can write the following derivation :\n$x = 0 \\vdash x = 0$;\nfrom it, mis-applying the above rule [because $\\Gamma = \\{ x = 0 \\}$ and $x$ is free in it]:\n$x = 0 \\vdash \\forall x (x = 0)$.\nBy use of Deduction Theorem we can conclude :\n$\\vdash x = 0 \\rightarrow \\forall x (x = 0)$.\nBut this may not be because, by soundeness of first-order logic (i.e. : if $\\vdash \\alpha, then \\vDash \\alpha$) we expect that only valid formulae are provable, and the above formula is not valid.\nTho show that it is not, consider an interpretation with domain the set $\\mathbb N$ of natural numbers and consider an assignement $s$ of value to the free variables [i.e. a function $s : Var \\rightarrow \\mathbb N$] such that $s(x)=0$.\nClearly :\n$(x = 0 \\rightarrow \\forall x (x = 0))[s] = FALSE$\nbecause $(x = 0)[s]$ is $0 = 0$, which is true, while $\\forall x (x = 0)$ is false.\n",
    "tags": [
      "logic",
      "proof-writing",
      "quantifiers"
    ],
    "score": 5,
    "answer_score": 2,
    "is_accepted": false,
    "question_id": 801448,
    "answer_id": 801458
  },
  {
    "theorem": "Let $a,b \\in R$ where $ a &lt; b$. Prove that there exist a rational number $c$ and an irrational number $d$ such that $ a &lt;c&lt;b$ and $ a&lt;d&lt;b$.",
    "context": "Question : Let $a,b \\in R$ where $ a < b$. Prove that there exist a rational number $c$ and an irrational number $d$ such that $ a <c<b$ and $ a<d<b$. Hint: consider decimal expansions of $a$ and $b$\nAttempt: Theorem 2.7.5 states that a real number is rational if and only if its decimal expansion terminates or has an infinitely repeating sequence of digits. \nSet $I$ is irrational numbers \n$I =[x \\in R: x \\notin Q]$\nSet $Q$ is rational numbers\n$Q = [ \\frac{a}{b}:a,b, \\in Z$ and $b \\neq 0]$\nIf the division process terminates, then we are done. Otherwise, since each digit of the quotient \ndetermines in turn its successor, and since there are at most $b-1$ possible remainders when dividing \nby $b$ (by the division algorithm), some digit of the remainder must show up again, forcing a sequence \nof digits to repeat forever. The length of the repeating cycle is at most $b-1$\nSuppose the decimal expansion of r will terminate. Therefore, for $r=0$, we have $a_1,a_2,...a_k$ where \neach $a_i \\in [0,1,2,3,4,5,6,7,8,9]$ and $a_k \\neq 0$. Then \n$ r = \\frac{a_110^{k-1}+a_210^{k-2}+...+a_k}{10^k}$\nsatisfies the definition of a rational number. Now, suppose the decimal expansion of r has an \ninfinitely repeating sequence of digits that begins immediately after the decimal point:\n$r=0.b_1b_2...b_k$\nThe sequence of digits are repeated forever. Therefore,\n$10^kr=b_1b_2...b_k.b_1b_2...b_kb_1b_2...b_k$\nso,\n$$10^kr-r=b_1b_2...b_k$ \ngiving\n$r = \\frac{b_1b_2...b_k}{10^k-1} \\in Q$\nSuppose $r$ has an initial sequence of digits before the repeating sequence: $r = 0.a_1a_2...a_lb_1b_2...b_k$. Then we let $r' = 0.b_1b_2...b_k$. By the previous case $r' \\in Q$. It is \neasy to verify that \n$r=\\frac{r'+a_1a_2...a_l}{10^l}$\nAs a result, $ r \\in Q$\nSo my question is do I apply the decimal expansion to $c$ and $d$. Assuming I can, then the decimal \nexpansion of $c$ will terminate since it's rational by theorem 2.7.5.\nThen,  for $r=0.c_1c_2...c_k$ where each $ c_i \\in [0,1,2,3,4,5,6,7,8,9]$ and $c_k \\neq 0$. Then \n$ r = \\frac{c_110^{k-1}+c_210^{k-2}+...+c_k}{10^k}$\nSuppose $d$ is an irriational number, then the decimal expansion won't terminate. \n$10^kr=d_1d_2...d_k.d_1d_2...d_kd_1d_2...d_k$\nso,\n$10^kr-r=d_1d_2...d_k$ \ngiving\n$r = \\frac{d_1d_2...d_k}{10^k-1} \\in Q$.\nIf we let $r = 0.c_1,c_2...c_ld_1d_2...d_k$ and $r' =0.d_1d_2...d_k$, then $r' \\in Q$ and $r=\\frac{r'+c_1c_2...c_l}{10^l}$\nI could've sworn that this is going to work for $c$ only, but not for $d$ because as I mentioned earlier $c$'s decimal expansion will stop and $d$ will go on forever. \n",
    "proof": "Using the decimal expansion for this purpose is at least cumbersome, and while we can do it by observing that $a$ and $b$ must differ at some decimal for the first time, one has to be careful with lots of $9$s and $0$s causing trouble.\nAnyway you have a lot of freedon with \"late\" decimals, so you can make things periodic or aperiodic at will.\nIt is much simpler to let $\\epsilon=b-a>0$ and then note that there is at least one onteger $n$ with $n>\\frac1\\epsilon$ and then at least one integer $m$ with $na<m<nb$ (because $nb-na>1$) and one integer $m'$ with $n(a-\\sqrt 2)<m'<n(b-\\sqrt 2)$. Then $c=\\frac mn$ and $d=\\frac {m'}n+\\sqrt 2$ do the trick.\n",
    "tags": [
      "proof-writing",
      "irrational-numbers",
      "rational-numbers"
    ],
    "score": 5,
    "answer_score": 5,
    "is_accepted": false,
    "question_id": 767117,
    "answer_id": 767138
  },
  {
    "theorem": "Proving the Obvious",
    "context": "Is it normal that I have the hardest time when I'm trying to prove statements that are blatantly obvious on a visual and/or intuitive level?\nFor instance, how does one go about formally proving the following statement? \nGiven a set $P$ of points on the real plane that are not all collinear, prove that there is a subset of $P$ that corresponds to the convex hull of $P$. Furthermore, that this polygon is unique (up to collinear points).\nAn intuitive 'proof' would be \"Stretch a rubber band such that it contains all the points, and release it.\" This, at least to me, makes it obvious that the above statement is true, but of course it's not very rigorous.\n",
    "proof": "\nIs it normal that I have the hardest time when I'm trying to prove\n  statements that are blatantly obvious on a visual and/or intuitive\n  level?\n\nYes, this is quite common.\n\nThe Jordan curve theorem is a classic example of a geometrically obvious theorem that is true, but quite hard to prove.\nThe idea that there do not exist space-filling curves is a classic example of a geometrically obvious \"theorem\" that is in fact false.\n\nNow, you also asked a specific question, namely:\n\nGiven a set P of points on the real plane that are not all collinear,\n  prove that there is a subset of P that corresponds to the convex hull\n  of P. Furthermore, that this polygon is unique (up to collinear\n  points).\n\nThis is result is quite easy to prove, but only if you know the \"trick\" (otherwise, you'll have no idea how to even get started). Anyway, to see that every set $P \\subseteq \\mathbb{R}^2$ has a convex hull:\n\nLet $P$ denote a subset of $\\mathbb{R}^2$.\nLet $K$ denote the collection of all convex subsets $Q$ of $\\mathbb{R}^2$ with $P \\subseteq Q$.\nShow that the intersection of $K$ is itself convex, and define that this intersection is the convex hull of $P$.\n\n",
    "tags": [
      "geometry",
      "proof-writing",
      "convex-analysis",
      "self-learning"
    ],
    "score": 5,
    "answer_score": 4,
    "is_accepted": true,
    "question_id": 694949,
    "answer_id": 694964
  },
  {
    "theorem": "Every first countable space is a moscow space.",
    "context": "First countable space $X$ is an example of moscow spaces.\nLet $U$ is an open subset of $X$ and $x\\in \\overline{U}$. If $\\overline{U}$ is open or even a nbhood of $x$ this proposition is immediately proved by definition of first countability of space $X$.\nBut now how can I prove this?\nThanks.\n",
    "proof": "In a first countable space each point is a $G_\\delta$, so every set is a union of $G_\\delta$ sets. In particular, then, $\\operatorname{cl}U$ is a union of $G_\\delta$ sets for any $U\\subseteq X$. (Note that this argument really needs only that $X$ have countable pseudocharacter.)\n",
    "tags": [
      "general-topology",
      "proof-writing"
    ],
    "score": 5,
    "answer_score": 5,
    "is_accepted": true,
    "question_id": 321130,
    "answer_id": 321134
  },
  {
    "theorem": "Why would the author ask if I used the Associative Law to prove + is not equiv. to *?",
    "context": "I just started reading An Introduction to Mathematical Analysis by H.S. Bear and problem 1 goes as follows:\n\nProblem 1: Show that + and * are necessarily different operations. That is, for any system (F, +, *) satisfying Axioms I, II, and III, it cannot happen that x + y = x * y for all x, y. Hint: You do not know there are any numbers other than 0 and 1, so that your argument should probably involve only these numbers. Did you use Axiom II? If not, state explicitly the stronger result that you actually proved.\n\nIn this book, Axiom I is commutativity of + and *, Axiom II is associativity of + and *, and Axiom III is existence of identities (x+0=x, x*1=x, 0 does not equal 1).\nMy question: Simply why would the author specifically ask the reader if he/she used Axiom II (associativity) and what exactly do they mean by \"If not, state explicitly the stronger result you actually proved\"? Why not not include those last two sentences?\nFWIW, here is my solution:\nTo prove:\n\nRestated:\n\n\nAnd I justified 5 by citing Axiom III since Axiom III includes the statement that 0 does not equal 1.\n",
    "proof": "The basic idea is as follows. From the neutral Axioms III, and commutativity of addition we have\n$$\\begin{eqnarray}\\rm x = 0 + x &\\rm \\\\\n\\rm y *\\: 1 &=\\rm y \\end{eqnarray}$$\nIf $\\ +\\, =\\, *\\ $ then aligned terms are unified for $\\rm\\:y = 0,\\ x = 1,\\:$ yielding\n$$\\rm\\ 1 = 0 + 1 = 0 * 1 = 0 $$\ncontra hypothesis $\\rm\\:1 \\ne 0.\\:$ Thus $\\rm\\: +\\: \\ne\\: *\\:$ because they take different values at the point $\\rm\\:(0,1)$.\nNote that the proof does not use associativity, and doesn't use commutativity if you state the neutral axioms as above. In any case, only one of the commutative axioms is needed, so that the neutral axioms can be ordered so the above unification is possible. In particular, the inference works in noncomutative rings, i.e. rings where multiplication is not necessarily commutative. Further, because the proof did not use associativity, it will also work in nonassociative rings.\nNote $\\ $ This method of deriving consequences by unifying terms in identities is a basic method in equational reasoning (term rewriting), e.g. google Knuth-Bendix or Grobner basis algorithms. \n",
    "tags": [
      "field-theory",
      "proof-writing",
      "axioms"
    ],
    "score": 5,
    "answer_score": 4,
    "is_accepted": true,
    "question_id": 154310,
    "answer_id": 154326
  },
  {
    "theorem": "How do we formalize the derivation of the ideal gas law from Boyle&#39;s Law, Charles&#39;s Law, and Avogadro&#39;s Law?",
    "context": "Given:\n\n$V \\propto 1/P$\n$V \\propto T$\n$V \\propto n$\n\nProve that:\n$V \\propto \\frac{nT}{P}$\nHere is the intuitive argument I have: We can see that\n\n$V = \\langle \\text{mystery meat} \\rangle  1/P$\n$V = \\langle \\text{mystery meat} \\rangle  T$, and\n$V = \\langle \\text{mystery meat} \\rangle  n$\n\nbut it's possible to interpret it so that the mystery meat in (1) is actually equal to $nT$, and the mystery meat in (2) is equal to $\\frac{n}{P}$, and the mystery meat in (3) is equal to $\\frac{T}{P}$ and so it ends up looking like the blind men touching the elephant.\n...But I don't know how to make this intuitive argument formal. How would this be turned into a formal proof?\n",
    "proof": "In my humble opinion, I think that we must start with historical data\n\nIn year $1662$, Boyle experimentally observed that, at constant temperature, the product $PV$ is almost a constant\n\nIn year $1787$, Charles experimentally observed that, at constant pressure, the change of volume is proportional to the change of temperature.\n\nIn year $1834$, Clapeyron combined the two facts and proposed $PV=kT$ and, at that time, $k$ was supposed to be component dependent before becoming the universal $R$.\n\nVolume is an extensive property.\n\n\nThese are experimental facts which have been turned into the first equation of state ($PV=RT$ is not a law).\nIt is from here that all theoretical work started.\nIf you are concerned by the next steps, I could add (even a lot !).\nEdit\nVery early, thermodynamicists point ted out that this was not good since $PV=RT$\n\nfirst aof all, traduces only repulsion between molecules (and, if there is repulsion, there is attration somewhere !)\n\nwould lead to a zero volume at constant $T$ and infinite $P$ which is not possible.\n\n\nAlready, in year $1865$, Dupré introduced the concept of the covolume (the limit of volume at infinite  pressure and constant temperature) and proposed o rewrite\n$$P(V-b_T)=RT$$\nBack to experimantal, in these years, in order to have nice plots, physicists used to plot pressure as polynomials of the reciprocal of volume\n$$P=\\sum_{n=1}^p \\frac{a_n(T)}{V^n}$$ which was already called as virial expansion.\nBack to physics, the jey year is $1873$ when Van der Waals combined all of the above and proposed\n$$\\large\\color{blue}{P=\\frac{RT}{V-b}-\\frac a{V^2}}$$ which, expanded as a series, gives\n$$P=\\frac {R T}V-\\frac{a-b R T}{V^2}+RT \\sum_{n=3}^\\infty \\frac{b^{n-1}}{V^n}$$ which, written as\n$$P=\\frac {R T}V+\\sum_{n=2}^\\infty \\frac{A_n(T)}{V^n}$$ is the standard form of the virial equation of state.\nVan der Waals equation of state is so simple that all modern developments of cubic equations of state started from here.\nTo be continued if you wish.\n",
    "tags": [
      "algebra-precalculus",
      "proof-writing",
      "education"
    ],
    "score": 5,
    "answer_score": 4,
    "is_accepted": true,
    "question_id": 4796214,
    "answer_id": 4798288
  },
  {
    "theorem": "Proving that $x(x^2-1)(x^2-10)=c$ cannot have five integer solutions for any real $c$",
    "context": "I found this question that caught my attention at MSE and  I did a solution, but I suspect something is wrong with the solution.\n\nOriginal problem says:\n\nProve that for any real values of $c$, the equation $x(x^2-1)(x^2-10)=c$ can't have $5$ integer solutions.\n\n\nThe things I have done:\nI find stronger (?) results in this answer:\n\nIf $c=0$, then we have $3$ integer solutions.\n\nIf $c>0$ or $c<0$, then we have only $1$ integer solution.\n\n\nThis means,\n\nThe number of integer solutions is always less than $4$.\n\n\nLet,\n$$f(x)=x(x^2-1)(x^2-10)$$\nand\n$$x(x^2-1)(x^2-10)=c$$\nwhere $c\\in\\mathbb R, x\\in\\mathbb Z$.\nIn fact we need only $c\\in\\mathbb Z$. Because, if $x\\in\\mathbb Z$ then $c\\in\\mathbb Z$.\nWe see that $c=0$ is trivial.\n$\\underline{\\text{Case}-1:~c>0}$\n$$\\begin{align}&x(x^2-1)(x^2-10)>0 ,x\\in\\mathbb Z \\\\ &\\iff x\\in \\left\\{-3, -2\\right\\}∪[4,+\\infty)\\end{align}$$\nThen suppose that, $x_1≥4, x_1\\in\\mathbb Z $ is a solution.\nIf $x_2>x_1≥4$ then $f(x)$ is strictly increasing and if $4≤x_2<x_1$, then $f(x)$ is strictly decreasing. This means, if $x_1≥4$, then we have one positive integer solution.\nThen, we see that $f(-3)<f(-2)<f(4)$ and $f(x)$ is strictly increasing for $x≥4$.\nThis implies, if $c>0$, then we have only one integer solution.\n\n$\\underline{\\text{Case}-2:~c<0}$\nLet's multiply both sides of the equation by $(-1).$\n$$-x(x^2-1)(x^2-10)=-c, c<0$$\nLet, $-x=t$ then\n$$t(t^2-1)(t^2-10)=-c,-c>0$$\nThis means, for $c<0$ we have also one integer solution.\nThus, we conclude the number of integer solutions is always less than $4$.\n\nDoes my solution contain any errors?\nPlease, don't post the correct solution.\nThank you for reviewing.\n",
    "proof": "If $c$ is bigger than the local maximum, $y$ value about\n$3.723601932658154682760729230  ,$  then there are, at most, three real solutions to $x(x^2-1)(x^2-10) = c.$\nJust draw a horizontal line at $y=c$ for some $c$ value of interest. Then, when $|c| \\leq 3.723601932658154682760729230,$  we do get five real roots, but these are not all integers as the largest is strictly between 3 and 4.\n\n",
    "tags": [
      "algebra-precalculus",
      "elementary-number-theory",
      "polynomials",
      "proof-writing",
      "solution-verification"
    ],
    "score": 5,
    "answer_score": 4,
    "is_accepted": false,
    "question_id": 4126066,
    "answer_id": 4126107
  },
  {
    "theorem": "If $f$ is uniformly continuous on the dense set $E \\subset X$ then a continuous extension of $f$ exists on $X$.",
    "context": "\nLet $E \\subset X$ which is a dense subset of $X$.The function $f$ has its range in $\\mathbb{R}$.We need to show that there exists a continuous extension from $E$ to $X$.\n\nMy attempt :\nlet $x \\in X$ and we choose a cauchy sequence {$e_n$} $\\to x$.Now, since $\\{e_n\\}$ is a convergent sequence so it is a cauchy sequence. As $f$ is a uniformly continuous function, then $\\{f(e_n)\\}$ is also a cauchy sequence and so it is convergent to a point,we define that to be $f(x)$[As $\\mathbb{R}$ is a complete metric space].\nNow, we choose another sequence $\\{(e_n)'\\} \\to x$.We see that\n$d(e_n,x)<\\frac{\\delta}{2}$ for all $n \\ge K_1$ and $d((e_n)',x)) < \\frac{\\delta}{2}$ for all $n \\ge K_2$\nThen, we see that $d_y(f((e_n)'),f(e_n)) \\le \\frac{\\epsilon }{2}$ when $d(e_n,(e_n)')\\le \\delta$ for all $n \\ge max\\{K_1,K_2\\}$.Using this equation we can show that $f((e_n)') \\to f(x)$.\nTo prove:$f$ is uniformly continuous. We choose $d(x_m,x_n) < \\frac{\\delta}{3}$.\nLet $\\{(e_n^m)\\} \\to x_m$ and $\\{(e_n^k)\\} \\to x_k$.Now,  $\\{f(e_n^m)\\} \\to f(x_m)$ and $\\{f(e_n^k)\\} \\to f(x_k)$.So,\n$d_y(f(e_n^k),f(x_k)) < \\frac{\\epsilon}{3}$ for all $n \\ge K_1$ and\n$d_y(f(e_n^m),f(x_m)) < \\frac{\\epsilon}{3}$ for all $n \\ge K_2$.\nNow by some manipulations we can find a $N$ such that when $n_1,n_2 \\ge N$ we have $d((e_{n_1}^m), (e_{n_2}^k))  < \\delta $ and by choosing such a delta we can have $d_y(f(e_{n_1}^k),f(e_{n_2}^m) )< \\frac{\\epsilon}{3}$ for all $n_1,n_2\\ge N$.Also we see that,\n$d_y(f(x_m),f(x_k)) \\le d_y(f(e_n^k),f(x_k)) + d_y(f(e_n^m),f(x_m)) + d_y(f(e_n^k),f(e_n^m))< \\epsilon$ for all $n \\ge max\\{K_1,K_2,N\\}$\nThis has been an attempt. Can someone go through my proof?I am not sure about the part where I am proving uniform continuity.\n",
    "proof": "\nYour proof is mainly correct, although the notation might seem a bit confusing at times. I will try to write a cleaner version of what you did in the last paragraph, where you proved the uniform continuity of the extension of $f$, which is denoted exactly the same. Of course, I shall assume that $(X, d)$ is a complete metric space and by abuse of notation, I shall also denote the standard euclidian distance on $\\mathbb{R}$ by $d.$\nLet $\\varepsilon > 0.$ Since $f$ is uniformly continuous on the dense set $E,$ there is $\\delta_\\varepsilon > 0$ such that $d(f(e_1), f(e_2)) < \\frac{\\varepsilon}{3}$ whenever $d(e_1, e_2) < \\delta_\\varepsilon$ and $e_1, e_2 \\in E.$ Take now $x, y \\in X$ such that $d(x, y) < \\frac{\\delta_\\varepsilon}{3}.$ From the definition of the extension (thus implicitly the density of $E$), we infer that there are $e_x, e_y \\in E$ such that $d(e_x, x) < \\frac{\\delta_\\varepsilon}{3} > d(y, e_y)$ and $d(f(x), f(e_x)) < \\frac{\\varepsilon}{3} > d(f(e_y), f(y)).$ In particular, it follows from the triangle inequality that we also have that $d(e_x, e_y) < \\delta_\\varepsilon.$ Hence we have that $d(f(e_x), f(e_y)) < \\frac{\\varepsilon}{3}.$ Finally, applying once again the triangle inequality, we deduce that $d(f(x), f(y)) \\leq d(f(x), f(e_x)) + d(f(e_x), f(e_y)) + d(f(e_y), f(y)) < \\varepsilon.$ This proves the claim that the extension of $f$ is uniformly continuous.\n",
    "tags": [
      "real-analysis",
      "proof-writing",
      "solution-verification"
    ],
    "score": 5,
    "answer_score": 2,
    "is_accepted": true,
    "question_id": 4009247,
    "answer_id": 4009499
  },
  {
    "theorem": "Question 6 of the 2nd round of the 15th Iran mathematical olympiad.",
    "context": "I came across this question the other day on an old Iran MO. The question goes as follows,\n\nLet $P$ be the set of all points in $R^n$ with rational coordinates. For points $A,B ∈ P$, one can move from $A$ to $B$ if the distance $|AB|$ is $1$. Prove that every point in $P$ can be reached from any other point in $P$ by a finite sequence of moves if and only if $n ≥ 5$.\n\nI have tried a few things such as polar coordinates and vectors but didn't make a huge amount of progress. Any advice or a solution would be greatly appreciated.\n",
    "proof": "We first show that not every point can be reached from $(0,0)$ for $n \\le 4$. That it can be done for $n \\ge 5$ will be shown afterwards. That this solves the general problem follows from the symmetry of the \"move from A to B\" definition.\nPart 1:  $n \\le 4$\nLet the rational units be defined as\n$$RU_n=\\{r=(r_1,\\ldots,r_n) \\in P| \\text{ satisfying } |r|^2=r_1^2+\\ldots+r_n^2=1\\}.$$\nA move can be made from $A\\in P$ to $B \\in P$ iff $B-A \\in RU_n$, where the difference is made component-wise. That means the points in $P$ that can be reached from $(0,0)$ in a finite series of moves are exactly those that can be expressed as a finite sum\n$$x=\\sum_{i=1}^k x_i, \\text{ where } \\forall i \\in \\{1,\\ldots,k\\}: x_i \\in RU_n. \\tag{*}\\label{eq0}$$\nWe'll now show no component of a rational unit can have a denominator divisible by $4$ (in reduced form) if $n \\le 4$.\nAssume otherwise, then there exists a $r \\in RU_n$ and writing $\\forall i=1,\\ldots,n: r_i=\\frac{a_i}{b_i}$ in reduced form, we assume w.l.o.g that $b_1$ contains (one of) the highest powers of $2$ among the $b_i$, so $b_1=2^kb'_1, 2\\nmid a_1, 2 \\nmid b'_1, k \\ge2 $. Note that $d=lcm (b_1,\\ldots,b_n)$ is then of the form $d=2^ko$ with odd $o$. Using the definition of rational units we get\n$$\\sum_{i=1}^n \\frac{a_i^2}{b_i^2}=1$$\nand if we multiply this with $d^2$ we get\n$$ \\sum_{i=1}^n c_i^2=d^2, \\text { where } c_i=a_i\\frac{d}{b_i}\\in \\mathbb Z. \\tag{1} \\label{eq1}$$\nAs we noted above $2\\nmid a_1$ and since $\\frac{d}{b_1}=\\frac{o}{b'_1}$ is also odd, so is $c_1$! Since $n \\le 4$, we can add $0^2$ terms to the left side of \\eqref{eq1} to fill up to 4 summands if necessary, sowie finally get\n$$\\sum_{i=1}^4 c_i^2 = d^2=16d'^2 \\tag{2} \\label{eq2}$$\nConsidering \\eqref{eq2} mod 4, and remembering that $0$ and $1$ are the only possible quadratic residues mod 4, the only solutions are $c_1^2\\equiv c_2^2 \\equiv c_3^2 \\equiv c_4^2 \\equiv 0 \\pmod 4$ and $c_1^2\\equiv c_2^2 \\equiv c_3^2 \\equiv c_4^2 \\equiv 1 \\pmod 4$. Since we know that $c_1$ is odd, only the second option is possible, so all the $c_i$ are odd.\nConsidering \\eqref{eq2} now mod 16, and remembering that the only odd quadratic residues mod 16 are $1$ and $9$, we see that \\eqref{eq2} is impossible to fullfil with all odd $c_i$ mod 16, because the only options are $4\\times 1, 3\\times 1 + 1\\times 9, 2\\times 1 + 2\\times 9,1\\times 1 + 3\\times 9$ and $4\\times 9$, which are $\\equiv 4, 12, 4, 12, 4 \\pmod {16}$ in that order, so never $0$.\nOk, so we proved that no component of a rational unit has a denominator that is a multiple of $4$. But \\eqref{eq0} shows that each component of a point that is reachable from $(0,0)$ is the sum of components of rational units. But such a sum can never be $\\frac14$. Because if it were, then\n$$\\frac14=\\sum_{i=1}^k \\frac{a_i}{b_i},$$\nwhere $\\frac{a_i}{b_i}$ is the reduced form of a component of a rational unit, so $4\\nmid b_i$ and hence $4\\nmid lcm(b_1,\\ldots,b_k)$. Multiplying the above equation with $lcm(b_1,\\ldots,b_k)$ gives an integer result on the right hand side, but not an integer on the left hand side, as $lcm(b_1,\\ldots,b_k)$ cannot \"cancel\" the $4$ in the denominator.\nSo any point in $P$ where one cordinate is $\\frac14$ cannot be reached from $(0,0)$ for $n \\le 4$.\nThis concludes the proof for part 1.\nPart 2: $n \\ge 5$\nWe use an old theorem from number theory, Lagrange's four-square theorem, that every natural number can be represented as the sum of four integer squares.\nThat means for every integer $k \\ge 1$ we have for integers $a_k, b_k, c_k, d_k$ such that\n$$a_k^2 + b_k^2+ c_k^2 + d_k^2 = 4k^2 -1,$$\nand thus\n$$\\left(\\frac{1}{2k}\\right)^2 + \\left(\\frac{a_k}{2k}\\right)^2 + \\left(\\frac{b_k}{2k}\\right)^2 + \\left(\\frac{c_k}{2k}\\right)^2 + \\left(\\frac{d_k}{2k}\\right)^2  = 1.$$\nThat means for $n\\ge 5$ there exists a rational unit $r_1$ with $\\frac1{2k}$ as first component (if $n > 5$, just fill up with zeros). Since the definition of the rational unit only uses the square of each component, changing any number of signs in a rational unit makes the resulting number also a rational unit. Let $r_2$ be the rational unit obtained from $r_1$ by flipping all the signs, expcept on the first component. That means\n$$r_1+r_2=(\\frac1k,0,0,\\ldots).$$\nObviously $((-r_1) + (-r_2)=(-\\frac1k,0,0,\\ldots)$, and similar by permutating the components rational units can be found such that the sum of 2 of them produces the zero vector except for a given component, where the component is $\\frac1k$ or $-\\frac1k$.\nBut each vector from $p\\in P$ can be obtained as a finite sum of vector with only one component.\n$$p=(p_1,\\ldots,p_n) = (p_1,0,\\ldots,0) + (0,p_2,0,\\ldots) + \\ldots + (0,\\ldots,0,p_n), \\tag{3}\\label{eq3}$$\nand if $p_1=\\frac{a}{b}, b > 0$, then\n$$(p_1,0,\\ldots,0)=\\sum_{i=1}^{|a|}(\\pm\\frac1b,0,\\ldots,0),$$\nwith the signs before the fraction chosen the same as $a$. The same can of course be done for the other summands in \\eqref{eq3}. But the summands thus calculated are exactly of the type that we proved are the sum of 2 rational units, so each $p=(p_1,\\ldots,p_n) \\in P$ is the sum of a finite number of rational units, so can be reached from $(0,0)$.\nThis concludes part 2 of the proof.\n",
    "tags": [
      "proof-writing",
      "contest-math"
    ],
    "score": 5,
    "answer_score": 4,
    "is_accepted": true,
    "question_id": 3998851,
    "answer_id": 3999297
  },
  {
    "theorem": "Prove complex numbers $a$ and $b$ are antipodal under stereographic projection $\\iff a \\overline{b} = -1$",
    "context": "I'm trying to prove the following statement:\n\nGiven $a, b \\in \\mathbb{C}$, prove that $a$ and $b$ correspond to antipodal points on the Riemann sphere under stereographic projection if and only if $a \\overline{b} = -1$\n\n\nMy attempt\nI wanted to make a proof where all my implications were reversible to avoid making a proof of each implication separately. As previous knowledge, I know that if a have a point $a \\in \\mathbb{C}$, then the stereographic projection $f: \\mathbb{C} \\to S^2$ is given by\n$$\nf(a) = \\left(\\frac{a + \\overline{a}}{1 + |a|^2},\\frac{a - \\overline{a}}{i\\left(1 + |a|^2\\right)},\\frac{|a|^2-1}{|a|^2+1}\\right)\n$$\nNow, given that $P,Q\\in S^2$ are antipodal if and only if $P =-Q$, I get the following:\n\\begin{align}\nf(a) = -f(b) &\\iff\n\\begin{cases}\n\\frac{a + \\overline{a}}{1 + |a|^2} = \\frac{-b - \\overline{b}}{1 + |b|^2} \\\\\n\\frac{a - \\overline{a}}{i\\left(1 + |a|^2\\right)} = \\frac{\\overline{b}-b}{i\\left(1 + |b|^2\\right)} \\\\\n\\frac{|a|^2-1}{|a|^2+1} = \\frac{1-|b|^2}{|b|^2+1} \\\\\n\\end{cases}\\\\\n&\\iff\\begin{cases}\na + \\overline{a}+a|b|^2 +\\overline{a}|b|^2 = -b - \\overline{b}-b|a|^2 -\\overline{b}|a|^2 \\\\\na - \\overline{a}+a|b|^2 -\\overline{a}|b|^2 = -b + \\overline{b}-b|a|^2 +\\overline{b}|a|^2 \\\\\n|ab|^2+|a|^2-|b|^2-1 =-|ab|^2+|a|^2-|b|^2+1 \\\\\n\\end{cases}\\\\\n&\\iff\\begin{cases}\na +a|b|^2 = -b -b|a|^2  \\\\\n\\overline{a} +\\overline{a}|b|^2 = -\\overline{b} -\\overline{b}|a|^2  \\\\\n|ab|^2=1 \\\\\n\\end{cases}\\\\\n&\\iff\\begin{cases}\na +b +a|b|^2+b|a|^2 =0 \\\\\n|a||b|=1 \\\\\n\\end{cases}\\\\\n\\end{align}\nWhere here I use brackets to indicate that all those equations are true simultaneously. On this last step is where I ran into trouble because I couldn't find a way to show that both conditions in the last step are equivalent to $b =- \\frac{1}{\\overline{a}}$.\nIs my attempt correct (up to what I have already written)? And if so, does somebody know how I could conclude the proof of equivalence? Any help would be greatly appreciated. Thank you!\n",
    "proof": "For the direct implication, one could also use the inverse function of $f$, $\\phi$:\n$$ \\phi (x,y,u) = \\frac{x+iy}{1-u}$$\nfor $(x,y,u)\\not= (0,0,1)$, $x^2+y^2+u^2=1.$\nIf $ P = (x,y,u)$ and $Q=(-x,-y,-u)$, then\n$$ \\phi(P)\\overline{\\phi(Q)} =  \\frac{x+iy}{1-u} \\cdot \\frac{-x+iy}{1+u}  = -\\frac{x^2+y^2}{1-u^2} = -1$$\nThe indirect implication is straightforward. For example:\n$$\\frac{a + \\bar{a}}{1+|a|^2} =   \\frac{-\\bar{b}^{-1} -b^{-1}}{1+|b|^{-2}} = -\\frac{b + \\bar{b}}{1+|b|^2}.$$\nEdit: Note that\n$$ a + b + a|b|^2  + b|a|^2 = 0$$\nis equivalent to\n$$ a(1+|b|^2) = - b(1+|a|^2) $$\nMultiplying by $\\bar{b}$, we get:\n$$ a\\bar{b}(1+|b|^2) = - |b|^2(1+|a|^2) $$\nwhich implies that $ a\\bar{b}$ is real negative.\n",
    "tags": [
      "complex-analysis",
      "complex-numbers",
      "proof-writing",
      "solution-verification",
      "analytic-geometry"
    ],
    "score": 5,
    "answer_score": 4,
    "is_accepted": true,
    "question_id": 3773836,
    "answer_id": 3773871
  },
  {
    "theorem": "Property of a semicyclic quadrilateral",
    "context": "This question has already been asked yesterday by the user @anonymus. I tried to solve it unsuccessfully after leaving a longer comment to persuade the $OP$ to include personal thoughts in the post. Since nothing has happened up to this moment, I voted to close it and ask the same question here including my attempt.\nHere it goes:\n\nLet $ABCD$ be a quadrilateral inscribed in a circle, where $|DC|<|AB|$ and $DC\\nparallel AB$. Let $X$ be the intersection point of the diagonals $\\overline{AC}$ and $\\overline{BD}$. And $Y$ be the foot of the perpendicular from $X$ on the edge $\\overline{AB}$. If $XY$ bisects the angle $\\measuredangle{DYC}$, prove that $\\overline{AB}$ is the diameter of the (circum)circle, i.e. $ABCD$ is a semicyclic quadrilateral.\n\n\nMy attempt:\nIf $XY$ bisects $\\measuredangle DYC$, then $\\measuredangle DYX=\\measuredangle XYC$.\n$$\\color{red}{\\measuredangle AYD}=90^{\\circ}-\\measuredangle DYX=90^{\\circ}-\\measuredangle XYC=\\color{red}{\\measuredangle CYB}$$\n$$\\measuredangle C'YA=\\measuredangle AYD$$\nWhen drawing, I noticed that $X$ is the center of the circle inscribed in $\\Delta DYC$\n$$\\implies\\color{green}{\\measuredangle CDB=\\measuredangle BDY}\\;\\&\\;\\color{blue}{\\measuredangle YCA=\\measuredangle ACD}$$\nI tried using the following:\n$$\\color{purple}{\\Delta ABX\\sim\\Delta CDX}\\;\\&\\;\\Delta AXD\\sim\\Delta CXB$$\nMy reasoning is circular. \nI'm not sure if I should already assume $\\color{brown}{\\measuredangle{BDA}=\\measuredangle{BCA}=90^{\\circ}}$.\nThen there's no point in stating that $BCXY$ is also a cyclic quadrilateral.\nHow can I continue and improve what I've written so far? Thank you in advance!\n\nUpdate:\nFor all those wondering, thanks to @Blue in the comment section, I will read more on the topic: Incircle and excircles of a triangle.\n\nPicture:\n\n",
    "proof": "Here's an approach that may be unnecessarily-complicated.\n\nIn the figure, $\\angle BAC\\cong\\angle BDC$ and $\\angle ABD\\cong\\angle ACD$, since each pairs of angles subtend the same arcs. A little angle-chasing gives $\\angle YCA=90^\\circ-\\alpha-\\theta$ and $\\angle YDB=90^\\circ-\\beta-\\theta$.\nBy the trigonometric form of Ceva's Theorem (see an alternative below), we have\n$$1 = \\frac{\\sin\\angle CYX}{\\sin\\angle XYD}\\cdot\\frac{\\sin\\angle DCX}{\\sin\\angle XCY}\\cdot\\frac{\\sin\\angle YDX}{\\sin\\angle XDC} = 1\\cdot\\frac{\\sin\\beta}{\\sin(90^\\circ-\\alpha-\\theta)}\\cdot\\frac{\\sin(90^\\circ-\\beta-\\theta)}{\\sin\\alpha} \\tag{1}$$\nso that\n$$\\sin\\alpha\\cos(\\alpha+\\theta) = \\sin\\beta\\cos(\\beta+\\theta) \\quad\\to\\quad \\sin(\\alpha-\\beta)\\cos(\\alpha+\\beta+\\theta) =  0 \\tag{2}$$\nSince $\\alpha$, $\\beta$, $\\theta$ are positive and acute, we have either that $\\alpha=\\beta$ or $\\alpha+\\beta+\\theta=90^\\circ$. The former would make $\\overline{AB}\\parallel\\overline{CD}$, which violates an assumption; thus, the latter holds. Substituting into the expressions for $\\angle YDB$, we find this equal to $\\alpha$, and is thus also equal to $\\angle YDX$. This makes $\\square XYAD$ a cyclic quadrilateral whose opposite angles at $Y$ and $D$ must be supplementary. The result follows. $\\square$\n\nNote. I like to throw in trigonometric Ceva whenever possible, because I don't think it gets enough attention, but it isn't necessary to get to $(2)$.\nInstead, we can define, say, $x := |XY|$ and use straightforward trig to give expressions in $x$, $\\alpha$, $\\beta$, $\\theta$ for the lengths of the subsegments of the diagonals of $\\square ABCD$. Then, we can use the similarity $\\triangle AXB\\sim\\triangle DXB$ (or, equivalently, the chord-chord aspect of the Power of a Point theorem) to write\n$$|XA|\\cdot|XC| = |XB|\\cdot|XD| \\tag{3}$$\nand manipulate the result into $(2)$.\n",
    "tags": [
      "geometry",
      "proof-writing",
      "euclidean-geometry",
      "circles",
      "quadrilateral"
    ],
    "score": 5,
    "answer_score": 4,
    "is_accepted": true,
    "question_id": 3597559,
    "answer_id": 3599496
  },
  {
    "theorem": "Correcting the set in my proof.",
    "context": "The question was:\nFind $\\int_{[0, \\pi/2]} f$ if  $$f(x) = \\begin{cases} \n      \\sin{x}, & if  \\cos(x) \\in \\mathbb{Q}, \\\\\n      \\sin^2{x}, &  if \\cos(x) \\not\\in \\mathbb{Q}.\n   \\end{cases}$$\nMy answer was:\nAssume that $0 \\leq x \\leq \\pi/2$, then taking the cosine of this, we get $\\cos 0 \\geq \\cos x \\geq \\cos (\\pi/2),$ so, $1 \\geq \\cos (x) \\geq 0$ (because $\\cos (x)$ is a decreasing function in this interval.)\\\nNow, by monotonicity of measure $$m\\{x \\in [0, \\pi/2] \\mid \\cos(x)\\in \\mathbb Q\\} \\subseteq m\\{[0, \\pi/2] \\cap  \\mathbb{Q}\\}.$$\nBut,$m\\{[0, \\pi/2] \\cap  \\mathbb{Q}\\} = 0.$\\\nThis is because $\\mathbb{Q}$ is countable and hence its measure is 0 and $\\{[0, \\pi/2] \\cap  \\mathbb{Q}\\} \\subset \\mathbb{Q}$, then by monotonicity of measure $m\\{[0, \\pi/2] \\cap  \\mathbb{Q}\\} = 0.$\\ \nAnd since the integral of  each integrable function $f$ on a set of measure equal to $0$ is $0$, we have:\\\n$\\int_{[0, \\pi/2]} f = \\int_{[0, \\pi/2] \\cap  \\mathbb{Q}} f + \\int_{[0, \\pi/2] \\cap  \\mathbb{Q}^c} f = 0 + \\int_{[0, \\pi/2] \\cap  \\mathbb{Q}^c} f = \\int_{[0, \\pi/2] \\cap  \\mathbb{Q}^c}  \\sin^2{x}  = \\int_{[0, \\pi/2] \\cap  \\mathbb{Q}}  \\sin^2{x} + \\int_{[0, \\pi/2] \\cap  \\mathbb{Q}^c}  \\sin^2{x} dx = \\int_{[0, \\pi/2]}  \\sin^2{x} dx,$\\\nWhere in the last equality we have changed the Lebesgue integral over  $[0, \\pi/2]$ into Riemann integral over $[0, \\pi/2]$ because our function $\\sin^2{x}$ is Riemann integrable and bounded by $[0,1]$ and the domain of integration is closed and bounded interval then by \\textbf{ Theorem 3, on page 73} the Lebesgue integral is the Riemann integral.\\\nNow we can compute this integral:\\\n$$\\int_0^{\\pi/2}f(x)\\,\\mathrm d x=\\int_0^{\\pi/2}\\sin^2(x)\\,\\mathrm d x = \\int_0^{\\pi/2} \\{ \\frac{1 - \\cos{2x}}{2} \\} d x = \\frac{\\pi}{4} - ( \\frac{1}{4} \\times 0) = \\frac{\\pi}{4}. $$ \nBut it turns out that:\nMy justification in this step:\n$$m\\{x \\in [0, \\pi/2] \\mid \\cos(x)\\in \\mathbb Q\\} \\subseteq m\\{[0, \\pi/2] \\cap  \\mathbb{Q}\\}.$$ was wrong, could anyone help me correct this step please? \n",
    "proof": "It's better to directly prove that $X=\\{x\\in [0,\\pi/2]\\mid \\cos(x) \\in\\Bbb{Q}\\}$ is countable.\nThere is a natural bijection between $X\\to \\Bbb{Q} \\cap [0,1]$ given by $f(x) = \\cos(x) $ since $\\cos$ is injective (1 on 1) on $[0,\\pi/2]$ then so it is on $X\\subseteq [0,\\pi/2]$.\nSince $\\cos x$ is surjective (onto) from $[0,\\pi/2]$ to $[0,1]$ then for each element from $[0,1]\\cap\\Bbb{Q}$ there is a corresponding element $y\\in [0,\\pi/2]$ such that $\\cos y=x$ but by definition $\\cos y=x\\in\\Bbb{Q} $ so $y\\in X$. \n",
    "tags": [
      "real-analysis",
      "analysis",
      "measure-theory",
      "proof-writing"
    ],
    "score": 5,
    "answer_score": 2,
    "is_accepted": true,
    "question_id": 3388787,
    "answer_id": 3388897
  },
  {
    "theorem": "Rhombus in a cyclic quadrilateral",
    "context": "Let $ABCD$ be a cyclic quadrilateral whose opposite sides are not parallel. The lines $AB$ and $CD$ intersect at point $P$. The lines $AD$ and $BC$ intersect in point $Q$. The bisector of the angle $\\angle DPA$ cuts the line segment $BC$ and $DA$ in the points $E$ and $G$, respectively.  The bisector of the angle $\\angle AQB$ cuts the line segments $AB$ and $CD$ in the points $H$ and $F$.\nNow it seems as if the quadrilateral $EFGH$ is a always a rhombus. I intend to prove this.\n\nMaybe anyone has a checklist or any idea to begin with.\n",
    "proof": "We first show that $EG \\perp HF$. Let $E'$ and $G'$ be the intersections of line $GE$ with the circle so that $G$ is between $G'$ and $E$. Similarly, define $H'$ and $F'$. For any two points $X,Y$ on the circle, let $XY$ denote the radian measure of the shorter arc connecting them (sorry, I couldn't figure out the arc command here). By the assumptions, we have \n$$\\begin{align}\\newcommand{arc}[1]{\\overset{\\mmlToken{mo}{⏜}}{#1}}\\arc{H'E'}+\\arc{G'F'}&=\\arc{H'B}+\\arc{BE'}+\\arc{G'D}+\\arc{DF'}=(\\arc{H'B}+\\arc{DF'})+(\\arc{BE'}+\\arc{G'D})\\\\&=(\\arc{CF'}+\\arc{AH'})+(\\arc{E'C}+\\arc{AG'})=\\arc{E'C}+\\arc{CF'}+\\arc{H'A}+\\arc{AG'}\\\\&=\\arc{E'F'}+\\arc{H'G'},\\end{align}$$\nwhich implies our claim. \nLet $S$ be the intersection of $EG$ and $HF$.\nNow, in triangle $\\triangle PHF$ the angle bisector of $\\angle P$ is perpendicular ot $HF$, hence it is an isosceles triangle, hence $S$ is the midpoint of the side $HF$. Similarly, $S$ is the midpoint of side $EG$. So the quadrilateral $EFGH$ has perpendicular diagonals that bisect each other. This happens only if $EFGH$ is a rhombus.\n",
    "tags": [
      "geometry",
      "proof-writing",
      "euclidean-geometry",
      "circles",
      "quadrilateral"
    ],
    "score": 5,
    "answer_score": 2,
    "is_accepted": false,
    "question_id": 3002705,
    "answer_id": 3002792
  },
  {
    "theorem": "If $C\\subseteq [0,1]$ is uncountable then $C\\cap[\\alpha,1]$ is uncountable",
    "context": "I am asked to prove or disprove the following:\n\nLet $C\\subseteq[0,1]$ be uncountable, and let $A$ be the set of all values $a\\in(0,1)$ such that $C\\cap[a,1]$ is uncountable. Define $\\alpha=\\sup A$. Is $C\\cap[\\alpha ,1]$ also uncountable?\n\nI haven’t been able to come up with a proof, but I have the following counter example to disprove the statement:\nFix $b\\in(0,1)$, and let $C=\\lbrace x\\in\\mathbb{R} | x\\in[0,b)\\rbrace\\cap \\lbrace x\\in\\mathbb{Q} | x\\in[b,1]\\rbrace$. Then, $C\\cap[\\alpha ,1]$ is countable.\nI have the following questions: \n$1)$ Can I prove this geberally without the use of a counterexample?\n$2)$ Why is the choice for $a$ restricted to $(0,1)$?\n",
    "proof": "\nNo need to prove anything, one counterexample suffice to prove a statement is false. \nBecause for $a=1$, we have $[a, 1]=\\{1\\} $ countable, and for $a=0$, it's $[0,1]\\cap C=C$ uncountable by hypothesis. \nHowever, if we take $\\inf$ instead of $\\sup$, the statement becomes true. Can you prove it? \n\n",
    "tags": [
      "real-analysis",
      "proof-writing"
    ],
    "score": 5,
    "answer_score": 2,
    "is_accepted": true,
    "question_id": 2790513,
    "answer_id": 2790520
  },
  {
    "theorem": "Mathematical proof that $m^4+8$ is not a cube of an integer if $13$ does not divide $m$ (Table proof provided)",
    "context": "My question is how to express my solution to $m^4+8$ is not a cube of an integer if $13$ does not divide $m$ as a mathematical proof.\nI understand that to prove by contradiction, the result is to be a cube of an integer:\n\\begin{align}\n m^4 + 8 & = n^3 \\\\\n m^4 &= n^3 - 8 \\\\\n \\therefore m^4 &\\equiv n^3-8\\pmod{13}\\\\\n  \\\\\n\\end{align}\nThat equivalence is however impossible, because of the following table:\nWe see that $m^4 \\equiv n^3-8\\pmod{13}$ can only occur when m $\\equiv 0 \\pmod{13}$ and n = 2, 5 or 6.\nThis shows that since $13$ can not divide $m$, $m^4+8$ is not a cube of an integer. I am now sure how to express this mathematically.\nAny help would be appreciated!\n",
    "proof": "Your method is perfectly fine for this. Just for fun, I'm giving an alternative method in which you can make do with only a table of squares mod 13.\nn     0  1  2  3  4  5  6  7  8  9 10 11 12\nn^2   0  1  4  9  3 12 10 10 12  3  9  4  1\n\n$$m^4+8 \\equiv n^3 \\mod 13\\\\\n(m^4+8)^4 \\equiv n^{12} \\mod 13$$\nFrom Fermat's little theorem we know that $n^{12}\\equiv0,1 \\mod 13$, so we then get:\n$$(m^4+8)^4 \\equiv 0,1 \\mod 13\\\\\n(m^4+8)^2 \\equiv 0,1,12 \\mod 13\\\\\nm^4+8 \\equiv 0,1,5,8,12 \\mod 13\\\\\nm^4 \\equiv 0,4,5,6,10 \\mod 13$$\nNote that $5$ and $6$ are not squares, so they are certainly not fourth powers.\n$$m^4 \\equiv 0,4,10 \\mod 13\\\\\nm^2 \\equiv 0,2,6,7,11 \\mod 13$$\nNote that the only one of these that is a square is $0$ so we are just left with\n$$m^2 \\equiv 0 \\mod 13\\\\\nm \\equiv 0 \\mod 13$$\n",
    "tags": [
      "number-theory",
      "elementary-number-theory",
      "proof-writing"
    ],
    "score": 5,
    "answer_score": 3,
    "is_accepted": false,
    "question_id": 2642805,
    "answer_id": 2643053
  },
  {
    "theorem": "Is the proof I am using, sufficient/ correct for the system of equation?",
    "context": "I want to ask MSE to confirm the correctness of the alternate solution and its mistake. \nI know possible solution: https://math.stackexchange.com/a/2557094/456510\nIf $x,y,z\\in {\\mathbb R}$, Solve the system equation:\n\n$$\n\\left\\lbrace\\begin{array}{ccccccl}\n    x^4 & + & y^2 & + & 4         & = & 5yz\n\\\\[1mm]\ny^{4} & + & z^{2} & + & 4 & = &5zx\n\\\\[1mm]\nz^{4} & + & x^{2} & + & 4 & = & 5xy\n\\end{array}\\right.\n$$\n\nI wrote a solution myself (after more work).\nMy attempts / solution:\nIt is obvious that, if $x>0,y>0,z>0$ are solutions, $x<0,y<0,z<0$ are also solutions and it is obvious $x≠0,y≠0,z≠0$.\nIf the equations have a solution, then $ x = y = z $ should be.\n\nProof:\nI will accept $x,y,z\\in {\\mathbb R^+}$ \na-1)\nLet $x≥z>y$\nWe can write :\n$z^4>y^4 \\\\ x^2≥z^2 \\\\ z^4+x^2+4>y^4+z^2+4 \\\\ 5xy > 5zx \\\\ y>z$\nWe get the contradiction : $y>z$\nBecause, it must be $z>y$\na-2)\nLet $x>z≥y$\nWe can write:\n$z^4≥y^4 \\\\ x^2>z^2 \\\\ z^4+x^2+4>y^4+z^2+4 \\\\ 5xy > 5zx \\\\ y>z$\nWe get the same contradiction : $y>z$\nBecause, it must be $z≥y$\nb)\n$y≥x>z$ \nWe can write:\n$x^4>z^4 \\\\ y^2≥x^2 \\\\ x^4+y^2+4>z^4+x^2+4 \\\\ 5yz > 5xy \\\\ z>x$\nBut, this is contradiction, because it must be $z<x$.\nWe get the same contradiction for : $y>x≥z$\nc)\n$y>z≥x$\nWe can write:\n$y^4>z^4 \\\\ z^2≥x^2 \\\\ y^4+z^2+4>z^4+x^2+4 \\\\ 5zx > 5xy \\\\ z>y$\nBut, this is contradiction, because it must be $z<y$.\nWe get the same contradiction for : $y≥z>x$\nd)\n$z>x≥y$\nWe can write:\n$z^4>x^4 \\\\ x^2≥y^2 \\\\ z^4+x^2+4>x^4+y^2+4 \\\\ 5xy > 5yz \\\\ x>z$\nBut, this is contradiction, because it must be $z>x$.\nWe get the same contradiction for : $z≥x>y$\ne)\n$z≥y>x$\nWe can write:\n$y^4>x^4 \\\\ z^2≥y^2 \\\\ y^4+z^2+4>x^4+y^2+4 \\\\ 5zx > 5yz \\\\ x>y$\nBut, this is contradiction, because it must be $x<y$.\nWe get the same contradiction for : $z>y≥x$\nf)\n$x>y≥z$\nWe can write:\n$x^4>y^4 \\\\ y^2≥z^2 \\\\ x^4+y^2+4>y^4+z^2+4 \\\\ 5yz > 5zx \\\\ y>x$\nBut, this is contradiction, because it must be $x>y$.\nWe get the same contradiction for : $x≥y>z$\nThen, solution must be $x=y=z$ (if there is a solution).\nThe proof is completed.\n\nFinally, \n\n$$x^4+x^2+4-5x^2=0 \\Rightarrow x^4-4x^2+4=0 \\Rightarrow (x^2-2)^2=0 \\Rightarrow x=±\\sqrt2\\Rightarrow x=y=z=±\\sqrt2 .$$\n\nIs my proof/ solution correct?\nThanks. \n",
    "proof": "Good job. Just too verbose.\nYou are correct into saying you can assume $x$, $y$ and $z$ all positive (there will be a corresponding solution with their negatives). The case when two are positive and one negative cannot appear, nor can the case of two negative and one positive, because the positivity of the left-hand sides forces positivity of the right-hand sides, so all three numbers must share the sign.\nHowever, there is another simplification, namely, you can also assume $x$ is the maximum solution, because the equations are cyclic. Thus\n$$\nx\\ge y\\ge z \\qquad\\text{or}\\qquad x\\ge z>y\n$$\nYou have already excluded the second case, so we can concentrate on the first.\nIn order to show that for a solution you need $x=y=z$, you just have to exclude $x>y$ and $y>z$.\nIn the case $x>y\\ge z$, we have, according to your method,\n$$\nx^4>y^4\n\\qquad\ny^2\\ge z^2\n$$\nThen\n$$\n5yz=x^4+y^2+4>y^4+z^2+4=5zx\n$$\nwhich implies $y>x$: a contradiction.\nIn the case $x\\ge y>z$ we have\n$$\ny^2>z^2\n\\qquad\nx^4\\ge y^4\n$$\nwhich implies\n$$\n5yz=x^4+y^2+4>y^4+z^2+4=5zx\n$$\nimplying $y>x$, again a contradiction.\nWe saw that assuming either $x>y$ or $y>z$ leads to a contradiction. Since $x\\ge y\\ge z$ by assumption and we cannot have neither $x>y$ nor $y>z$, we deduce that $x=y$ and $y=z$.\nNow, finding what's the common value is easy: we have\n$$\nx^4-4x^2+4=0\n$$\nso $x^2=2$ and $x=\\pm\\sqrt{2}$. The problem has exactly two solutions.\n",
    "tags": [
      "algebra-precalculus",
      "proof-verification",
      "proof-writing",
      "contest-math",
      "systems-of-equations"
    ],
    "score": 5,
    "answer_score": 4,
    "is_accepted": true,
    "question_id": 2559814,
    "answer_id": 2565241
  },
  {
    "theorem": "If $a\\cos(θ)=b$ $\\cos( θ+2π/3)=c\\cos(θ+4π/3)$, prove that $ab+bc+ca=0.$",
    "context": "THE PROBLEM: If $a\\cos(θ)=b$ $\\cos( θ+2π/3)=c\\cos(θ+4π/3)$, prove that $ab+bc+ca=0.$\nMY THOUGHT PROCESS: We have to prove that $ab+bc+ca=0$. \nOne method using which we can do this is, if we can somehow obtain the equation $k(ab+bc+ca)=0$ we can deduce that $ab+bc+ca=0$ using the zero product rule. \nAnother method to do this would be to obtain an expression which has $(ab+bc+ca)$ in it. The identity $(a+b+c)^2=a^2+b^2+c^2+2(ab+bc+ca)$ has $2(ab+bc+ca)$ in it. Therefore if we can somehow show that $(a+b+c)^2=a^2+b^2+c^2$ our task would be over.\nMY ATTEMPT: I have proved the result using the first approach. I tried to do it using the second one but could not proceed far. I was facing problems showing that $(a+b+c)^2=a^2+b^2+c^2$. If i get any help i shall be very grateful.\n",
    "proof": "Using the addition formulas:\n$$2\\cos\\left(\\theta+\\frac{2\\pi}{3}\\right)=-\\cos\\theta-\\sqrt3\\sin\\theta$$\n$$2\\cos\\left(\\theta+\\frac{4\\pi}{3}\\right)=-\\cos\\theta+\\sqrt3\\sin\\theta$$\nThen on the one hand\n$$\\cos\\left(\\theta+\\frac{2\\pi}{3}\\right)\\cos\\left(\\theta+\\frac{4\\pi}{3}\\right)+\\cos\\theta\\cos\\left(\\theta+\\frac{4\\pi}{3}\\right)+\\cos\\theta\\cos\\left(\\theta+\\frac{2\\pi}{3}\\right)=$$\n$$\\frac{\\cos^2\\theta-3\\sin^2\\theta}{4}+\\frac{-\\cos^2\\theta+\\sqrt3\\sin\\theta\\cos\\theta}{2}+\\frac{-\\cos^2\\theta-\\sqrt3\\sin\\theta\\cos\\theta}{2}=$$\n$$\\frac{\\cos^2\\theta-3\\sin^2\\theta}{4}-\\cos^2\\theta=\\frac{\\cos^2\\theta-3\\sin^2\\theta-4\\cos^2\\theta}{4}-\\cos^2\\theta=\\frac{-3\\cos^2\\theta-3\\sin^2\\theta}{4}=-\\frac{3}{4}$$\nOn the other hand\n$$\\left(\\cos\\left(\\theta+\\frac{2\\pi}{3}\\right)\\cos\\left(\\theta+\\frac{4\\pi}{3}\\right)\\right)^2+\\left(\\cos\\theta\\cos\\left(\\theta+\\frac{4\\pi}{3}\\right)\\right)^2+\\left(\\cos\\theta\\cos\\left(\\theta+\\frac{2\\pi}{3}\\right)\\right)^2=$$\n$$\\left(\\frac{\\cos^2\\theta-3\\sin^2\\theta}{4}\\right)^2+\\left(\\frac{-\\cos^2\\theta+\\sqrt3\\sin\\theta\\cos\\theta}{2}\\right)^2+\\left(\\frac{-\\cos^2\\theta-\\sqrt3\\sin\\theta\\cos\\theta}{2}\\right)^2=$$\n$$\\frac{\\cos^4\\theta-6\\sin^2\\cos^2+9\\sin^4}{16}+\\frac{\\cos^4\\theta-2\\sqrt3\\sin\\theta\\cos^3\\theta+3\\sin^2\\theta\\cos^2\\theta}{4}+\\frac{\\cos^4\\theta+2\\sqrt3\\sin\\theta\\cos^3\\theta+3\\sin^2\\theta\\cos^2\\theta}{4}=$$\n$$\\frac{\\cos^4\\theta-6\\sin^2\\cos^2+9\\sin^4}{16}+\\frac{2\\cos^4\\theta+6\\sin^2\\theta\\cos^2\\theta}{4}=$$\n$$\\frac{\\cos^4\\theta-6\\sin^2\\cos^2+9\\sin^4+8\\cos^4\\theta+24\\sin^2\\theta\\cos^2\\theta}{16}=$$\n$$\\frac{9}{16}(\\cos^4\\theta+2\\sin^2\\theta\\cos^2\\theta+\\sin^4\\theta)=\\frac{9}{16}\\left(\\cos^2\\theta+\\sin^2\\theta\\right)^2=\\frac{9}{16}.$$\nIf we square the first equation and subtract it from the second we get $0$. \nSince\n$$\\cos\\left(\\theta+\\frac{2\\pi}{3}\\right)\\cos\\left(\\theta+\\frac{4\\pi}{3}\\right)(a+b+c)=$$\n$$a\\left(\\cos\\left(\\theta+\\frac{2\\pi}{3}\\right)\\cos\\left(\\theta+\\frac{4\\pi}{3}\\right)+\\cos\\theta\\cos\\left(\\theta+\\frac{4\\pi}{3}\\right)+\\cos\\theta\\cos\\left(\\theta+\\frac{4\\pi}{3}\\right)\\right)$$\nand\n$$\\left(\\cos\\left(\\theta+\\frac{2\\pi}{3}\\right)\\cos\\left(\\theta+\\frac{4\\pi}{3}\\right)\\right)^2(a^2+b^2+c^2)=$$\n$$a^2\\left(\\left(\\cos\\left(\\theta+\\frac{2\\pi}{3}\\right)\\cos\\left(\\theta+\\frac{4\\pi}{3}\\right)\\right)^2+\\left(\\cos\\theta\\cos\\left(\\theta+\\frac{4\\pi}{3}\\right)\\right)^2+\\left(\\cos\\theta\\cos\\left(\\theta+\\frac{2\\pi}{3}\\right)\\right)^2\\right)$$\nwe conclude\n$$(a+b+c)^2-(a^2+b^2+c^2)=0$$\nand $$ab+bc+ca=0$$\n",
    "tags": [
      "trigonometry",
      "proof-writing"
    ],
    "score": 5,
    "answer_score": 1,
    "is_accepted": true,
    "question_id": 2273226,
    "answer_id": 2273414
  },
  {
    "theorem": "Is there ever a case where a two-dimensional limit exists but not the one dimensional limits?",
    "context": "I'm just learning some proof-based multivariable calculus, and in Apostol Volume 2 chapter 8.5 exercise number 2 there is the following statement.\nIf \n$$\n\\lim_{(x,y)\\to (a,b)}f(x,y)=L\n$$\nand $\\lim_{x\\to a}f(x,y)$ and $\\lim_{y\\to b}f(x,y)$ both exist, prove that \n$$\n\\lim_{x \\to a}[\\lim_{y \\to b}f(x,y)] = \\lim_{y \\to b}[\\lim_{x \\to a}f(x,y)]=L.\n$$\nThis is easy enough to believe. My question is whether it is redundant to state that the single variable limits exist, given that the two variable limit exists. Is there ever a case where the two dimensional limit exists but the individual one dimensional limits don't? Thanks.\n",
    "proof": "Consider \n$$f(x,y)=\\begin{cases} -x, & y\\in\\Bbb Q \\\\ x, & y\\notin\\Bbb Q \\end{cases}.$$\nThen  $\\lim\\limits_{y\\to 0} f(x,y)$ does not exist when $x\\ne 0$, but $\\lim\\limits_{(x,y)\\to (0,0)} f(x,y) = 0$. \n",
    "tags": [
      "real-analysis",
      "multivariable-calculus",
      "proof-writing"
    ],
    "score": 5,
    "answer_score": 4,
    "is_accepted": true,
    "question_id": 2225558,
    "answer_id": 2225574
  },
  {
    "theorem": "Proving $ \\forall r, s \\in \\mathbb{R^+} \\sqrt {r. s} = \\sqrt r . \\sqrt s$",
    "context": "I am new to writing proofs, I have written down a proof for $ \\forall r, s \\in \\mathbb{R^+}  \\sqrt {r\\cdot s}  = \\sqrt r \\cdot  \\sqrt s$\nI know I am a bit messy with my proof, I would love to get some feedback. Also please verify if my proof is good enough. Here is my approach,\nProof -  Let, $\\sqrt r = p$ and $\\sqrt s = q$ where  $p, q \\in \\mathbb{R^+}$.\nThen,\n$\\sqrt r \\sqrt s  = p\\cdot q$ ... (1)\nSquaring both the sides we get,\n$(\\sqrt r \\sqrt s)^2  = (p\\cdot q)^2$   ...(2)\n$(\\sqrt r \\sqrt s)^2 = (r^{1/2}\\cdot s^{1/2})^2$  [as $\\sqrt a = a^{1/2}$]\n\nLemma 1 - $\\forall a, b \\in  \\mathbb{R} [(a\\cdot  b )^2 = a^2\\cdot  b^2] $\nProof - By definition of squaring, $ a^2 = a\\cdot a$\nIt follows from the definition that, $(a \\cdot b)^2 = (a \\cdot b ) (a\\cdot b) = a^2\\cdot b^2$ $\\blacksquare$\n\nFrom lemma 1,\n$(r^{1/2}\\cdot s^{1/2})^2 = r^{2 / 2}\\cdot  s^{2/2} = r\\cdot s$\nTherefore,\n$(\\sqrt r \\sqrt s)^2 =  r\\cdot s$\nFrom equation 2, we have,\n$r\\cdot s = (p\\cdot q)^2 $  ...(3)\nIf the equality holds the LHS of the equation must yield the same result as equation 1,\nNow from equation 3,\n$ \\sqrt {r\\cdot  s} = \\sqrt{(p\\cdot q)^2}$\n$\\sqrt{(p\\cdot q)^2} = ((p\\cdot q)^2)^{1/2}$ [as $\\sqrt a = a^{1/2}$]\nNow from the power rule $(a^b)^c = a^{b\\cdot c}$, it follows that\n$((p\\cdot q)^2)^{1/2} = (p\\cdot q)^{2/2}= p\\cdot q$ .. (4)\nFrom equation (1) and equation (4). As both sides yield the same result, it is true that, $ \\forall r, s \\in \\mathbb{R^+}  \\sqrt {r. s}  = \\sqrt r \\cdot \\sqrt s$ $\\blacksquare$\nI have one concern with this proof, by this same reasoning that I have followed, I can even prove the statement for $\\forall r, s \\in \\mathbb{R^-}$, which is not true, so I must be wrong somewhere.\n",
    "proof": "Since, as you say, ''you are new to writing proofs'', I think that you want some advice to improve your proof.\nThe first step is to start with a good definition of the mathematical objects that you are using. In this case we need a definition of $\\sqrt{a}$ for $a\\in \\mathbb{R}^+$. This definition is:\n\n$1) \\qquad \\sqrt{a}$ is a non negative real number  $x\\ge 0$ such that $x^2=a$\n\nThe second step is to well define what we want to prove, and in this case it is the identity: $\\sqrt{ab}=\\sqrt{a}\\sqrt{b}$, where ( by the definition $1)$) we have: $\\sqrt{a}=x\\ge 0$ , $\\sqrt{b}=y\\ge 0$ and $\\sqrt{ab}=z\\ge 0$, such that $z^2=ab$, $y^2=b$ and $x^2=a$ (note that this exclude the possibility that $a,b$ (and $ab$) can be negative numbers).\nNow, for the proof, we can start from the LHS or from the RHS of this identity. \nFrom the LHS:\n$$\n\\sqrt{ab}=z \\Rightarrow z^2=ab=x^2y^2\n$$\nSo, using commutativity of the product in $\\mathbb{R}$:\n$$\nz^2=(xy)^2\n$$\nand, since $xy\\ge 0$ we can write\n$$\nxy=\\sqrt{ab}=z \n$$\nthat is:\n$$\n\\sqrt{a}\\sqrt{b}=\\sqrt{ab}\n$$\nNote that we don't need fractional exponents, but only the definition of the square root. in particular we did not use the rule $(a^b)^c=a^{bc}$ that is not valid, in general, for a fractional exponent.\nIn your proof you  started from the RHS,\nI leave to you the writing of the proof in this case.\n",
    "tags": [
      "algebra-precalculus",
      "proof-verification",
      "proof-writing"
    ],
    "score": 5,
    "answer_score": 2,
    "is_accepted": true,
    "question_id": 2193339,
    "answer_id": 2193416
  },
  {
    "theorem": "Is my proof correct? If $f$ has a finite number of discontinuities on $[a, b]$, then it is integrable on $[a, b]$",
    "context": "Question:\nSuppose a function $f(x)$ over the interval $[a, b]$ is bounded and has only a finite number of discontinuous points on $[a, b]$.  I intend to prove that it must be integrable on $[a, b]$.\nIs my proof below correct?\nMy Answer (Is it correct?):\nSince $f(x)$ has only a finite number of discontinuous points, call these points $p_1, p_2, ..., p_n$.  Now let $r$ be some number greater than $0$ such that for all $\\epsilon > 0$, $2r < \\frac{\\epsilon}{4M}$.  Now let \n$M_i =$ sup$_{x \\in [p_i - r, p_i +r]}f(x)$ and $m_i =$ inf$_{x \\in [p_i - r, p_i +r]}f(x)$. Then $M_i - m_i \\leq 2M$.\nNow then $f$ must be continuous on each interval $[a, p_1 - r], [p_1 + r, p_2 - r], ...,[p_n + r, b]$ and thus $f$ must be integrable on each one of these intervals.  Therefore there exist partitions $P_1, P_2,..., P_{N+1}$ of each of these intervals such that $U(P_k, f) - L(P_k, f) < \\frac{\\epsilon}{2(N + 1)}$. $U(P_k, f)$ and $L(P_k, f)$ are the upper and lower sums of $f$ over their respective partitions.\nLet $P$ be the partition given by the $P_1 \\cup P_2 \\cup ... \\cup P_{N+1}$.\nThen $U(P, f) - L(P, f) = U(P_1, f) - L(P_1, f) + U(P_2, f) - L(P_2, f) + ... U(P_{N + 1}, f) - L(P_{N + 1}, f) + M_1 - m_1 + M_2 - m_2 + ... M_N - m_N < 2M(\\frac{\\epsilon}{4M}) + \\frac{\\epsilon}{2(N + 1)}(N + 1) = \\frac{\\epsilon}{2} + \\frac{\\epsilon}{2} = \\epsilon$.\nThus for all $\\epsilon > 0$, there exists a partition $P$ of $[a, b]$ such that $U(f, P) - L(f, P) < \\epsilon$.\nTherefore $f(x)$ is integrable over $[a, b]$.\nIs this proof correct ?\n",
    "proof": "You have the correct approach. Just clean up a few details.\nYou are trying to show that given any $\\epsilon > 0$, there exists a partition $P_\\epsilon$ such that $U(P_\\epsilon,f) - L(P_\\epsilon,f) < \\epsilon$.  \nFirst define $M$,\n$$M := \\sup_{x \\in [a,b]} |f(x)|.$$\nThen it follows that for $i = 1, 2, \\ldots, n$ we have\n$$M_i - m_i \\leqslant 2M,$$ \nand the contribution to the difference in upper and lower sums from intervals $[p_i - r, p_i +r]$ is\n$$\\sum_{i=1}^n(M_i - m_i)[p_i + r - (p_i-r)] \\leqslant 4Mnr.$$\nDon't choose $r < \\epsilon/4M$ for any $\\epsilon$. For a given $\\epsilon > 0,$ choose a particular $r = r(\\epsilon)$ such that $r < \\epsilon/ (8Mn)$ and $[p_i-r,p_i+r] \\subset (a,b).$\nChoose partitions $P_k$ of the other $n+2$ intervals needed to cover $[a,b]$ such that\n$$U(P_k,f)-L(P_k,f) < \\frac{\\epsilon}{2(n+2)}.$$\nAlso you are proving this for $p_i \\in (a,b)$. The same argument applies with some slight modification if $p_1 = a$ or $p_n = b.$\n",
    "tags": [
      "calculus",
      "real-analysis",
      "integration",
      "proof-verification",
      "proof-writing"
    ],
    "score": 5,
    "answer_score": 4,
    "is_accepted": false,
    "question_id": 1757564,
    "answer_id": 1757649
  },
  {
    "theorem": "Show that $r$ is the rank of the $n$x$n$ matrix $A\\iff A$ has a nonsingular $r$x$r$ submatrix",
    "context": "Show that $r$ is the rank of the $n$x$n$ matrix $A\\iff A$ has a nonsingular $r$x$r$ submatrix, but any larger square submatrix of $A$ is singular. \nI know that to be nonsingular, det $ \\neq 0$ \nI can see this to be true by writing out examples but I am unsure how to approach writing a proof for it. \n",
    "proof": "Sketch (using that the column and row ranks are the same):\nSince the rank of $A$ is $r$, there are $r$ independent columns in $A$.  Consider the submatrix $B$ of $A$ formed by those $r$ columns.  Then, the rank of $B$ is $r$ because the columns of $B$ are independent.  Then, since the dimension of the row space of $B$ is $r$, there are $r$ independent rows.  Form the submatrix $C$ by using those rows.  This is an $r\\times r$ submatrix which is nonsingular.\nIf there were a larger invertible submatrix $D$, then the columns of $A$ that include the columns of $D$ must be independent.  This means that the rank of $A$ is larger than $r$, which is impossible.\n",
    "tags": [
      "linear-algebra",
      "matrices",
      "proof-writing"
    ],
    "score": 5,
    "answer_score": 4,
    "is_accepted": true,
    "question_id": 1721062,
    "answer_id": 1721102
  },
  {
    "theorem": "Proof using induction: $n! &gt; n^2$, for $n\\geq4$",
    "context": "Proof using induction: $n! > n^2$, for $n\\geq4$\n\nBasis:\nFor n = 4, we have:\n$4! > 4^2$\n$24 > 16$ (TRUE)\nInductive step:\nBy the induction hypothesis:\n$k! > k^2$\n$(k+1)k! > (k+1)k^2$\n$(k+1)! > k^3 + k^2$\nBut, $k^3 + k^2 > k^2 + 2k + 1$, for $k\\geq4$\n\nSo, \n$(k+1)! > k^3 + k^2 > k^2 + 2k + 1$\n$(k+1)! > (k+1)^2$ ---> What we want to proof\nDoes this serve as a proof for my sentence?\n",
    "proof": "@Vinicius No need to make that so complicated.\nAssuming $k! > k^2$.\n$$(k+1)!>(k+1)^2$$\n$$(k+1)k! > (k+1)(k+1)$$\n$$k!>k+1$$\nThe last statement requires little proof for $k \\in \\{4, 5, 6, \\cdots\\}$.\n",
    "tags": [
      "proof-writing",
      "induction"
    ],
    "score": 5,
    "answer_score": 1,
    "is_accepted": true,
    "question_id": 1694180,
    "answer_id": 1694318
  },
  {
    "theorem": "Open sets and measure zero",
    "context": "Show that no non-trivial open set in $R^n$ can have measure zero in $R^n$. \nAttempt at the solution: I am having a lot of difficulty attempting this question, I have read a lot of material on measure zero and almost all of the questions posted on this forum regarding the topic, but I still can't seem to understand how to attempt this problem. I have written a proof that somewhat makes intuitive set to me, but I am pretty terrible at proof writing and trying to improve so please don't be sparing in your recommendations and criticism. \nLet A be open set in $R^n$ of the form (a,b) s.t a $\\not=$ b, we can choose intervals I for any $x_i$ $\\in$ A :[$x_i - \\epsilon $, $x_i + \\epsilon$], we can make these intervals arbitrarily small, the problem occurs at the points a and b, we will need an interval that covers a and closest point to a that is included in the set, the smallest interval of this form would be [a-$\\epsilon$, $x_1 + \\epsilon$], this interval does not have measure zero by the definition of open sets ( since if a is arbitrarily close to $x_1$ we would arrive at a contradiction i.e A will no longer be open), which gives us that no non trivial open set can have measure zero. \nThanks in advance \n",
    "proof": "Your proof has the right idea, but could be made cleaner.  If I understand your approach, for a fixed $x$, you are constructing a closed $n$-square around $x$ that is contained in $A$.  Then, since the measure of $A$ is greater than or equal to the measure of the $n$-square, which is nonzero, the measure of $A$ is not zero.\nHint: It may be easier to start with $x\\in A$.  Since $x\\in A$ and $A$ is open, there is some $\\varepsilon>0$ such that $B(x,\\varepsilon)\\subseteq A$.  Now, it is easy to construct a closed $n$-square within the open ball, for instance there is an $n$-square with side length $\\frac{\\sqrt{2}\\varepsilon}{\\sqrt{n}}$ centered at $x$ which is contained within the ball.\n",
    "tags": [
      "real-analysis",
      "proof-verification",
      "proof-writing",
      "lebesgue-measure"
    ],
    "score": 5,
    "answer_score": 4,
    "is_accepted": true,
    "question_id": 1589749,
    "answer_id": 1589755
  },
  {
    "theorem": "Number of Taxicab routes in a triangular city",
    "context": "I am assuming a triangle that is \"almost\" half a rectangular city with taxicab geometry. I am trying to find the number of paths in this triangular city.\nAssuming that the ride starts from the corner of the city. If we move p steps in one direction, and q steps in the perpendicular direction, the number of paths in case of a rectangular city is known and is given by:\n$$\\binom{p+q}{p} ~ or ~ \\binom{p+q}{q}$$\nFor example, assume the 45-degree rotated city in the following figure (left).\nComplete and half cities\n\nIf we start from the point where the arrow is pointing, the numbers at the cross-points refer to the number of possible paths from the start point to each cross-point.\nNow, assume the figure on the right side. Again, the numbers at the cross-points refer to the number of possible paths from the start point to each cross-point. I obtained these numbers using a combination of counting and observation.\nThe main observation is that the number of paths in the triangular city is a ratio of the rectangular city. Take for example the 7'th row, we find the following (Can you explain why?):\n924/(7/1)=132\n462/(7/2)=132\n210/(7/3)=90\n84/(7/4)=48\n28/(7/5)=20\n7/(7/6)=6\n1/(7/7)=1\nThis applies to all rows.\nNow to my question. Assume the following shape of a city, where the inlets are at the left edge, and the outlets are at the bottom edge.\nMy problem\n\nWhat I want to do is to find the number of paths from any of the inlets to any of the outlets. Hopefully a formula, and a proof.\nIn Figure 2 is what I got so far. If the outlet is less than or equal to the input, this can be directly obtained using the formula for the rectangular case.\nAssuming that this is correct, note that up to the diagonal, numbers are following the rules of Pascal's triangle. After the diagonal, which represents the boundary of the city, it does not follow the same rules, but there is a pattern.\n1st diagonal after the half (subtract 1)\n6= (6+1)-1\n20=(6+15)-1\n34=(15+20)-1\n2nd (subtract 6)\n20=(20+6)-6\n48=(20+34)-6\n62=(34+34)-6\n3rd (subtract 20)\n4th (subtract 48)\n5th (subtract 90)\n6th (subtract 132)\nWhich are the numbers in the row corresponding to inlet 7 (Again, can you explain why?).\n",
    "proof": "Imagine connecting your \"inlets\" together to the left of your city and the \"outlets\" at the bottom, like this (shown for 4 of each instead of 7 of each):\n*\n|\n+-->:\n|   ::.\n+-->::::\n|   :::::. <- dotted area corresponds to your drawing\n+-->:::::::\n|   ::::::::.\n+-->::::::::::\n    v  v  v  v\n    |  |  |  |\n    +--+--+--+--*\n\nThen each route from some inlet to some outlet corresponds to a route from * to * in the expanded graph. But the expanded graph is just a \"half-city\" of size 2 more than the one you started with, except that the bottom left intersection is removed.\nSo the number of solutions is one less than the number from top to bottom in a \"half-city\" of size $n+2$.\nThe number of paths through a \"half-city\" of size $n$ is the $n$th Catalan number -- what you're counting is the number of strings of $n$ \"left\"s and $n$ \"right\"s such that no prefix contains more rights than lefts.\n",
    "tags": [
      "geometry",
      "proof-writing"
    ],
    "score": 5,
    "answer_score": 2,
    "is_accepted": false,
    "question_id": 1549304,
    "answer_id": 1549319
  },
  {
    "theorem": "Help getting started on a proof",
    "context": "I honestly have no idea where to start with the following proof, and I was wondering if anyone could help me get started. I don't want the whole idea, I just need to know where to start with this proof. \nThe question:\nLet x be a real number. Suppose 1 < x. Given any M > 0; prove there is a natural number N such that n $\\geq$ N implies M $\\leq$ $x^n$.\nMy book has absolutely no examples (it's a custom text), and I honestly have just stared at this problem for a few hours now. If someone would be willing to just help me get going, I think I should be able to work out the rest from there. I appreciate any help anyone would be willing to offer, thank you.\nedit: Here is the solution I came up with after the generous help I received below.\nLet M > 0 and let x = 1 + t. Since x > 1, we have 1 + t > 1 if and only if t > 0. We then have $x^n$ = $(1 + t)^n$. Using the Bernoulli Inequality, we have $(1 + t)^n$ $\\geq$ 1 + nt for all n $\\geq$ 0. We then have $x^n$ $\\geq$ 1 + nt > nt, thus $x^n$ > nt for all n $\\geq$ 1. Let N be a natural number with N = $\\frac{M}{t}$. Then n $\\geq$ N = $\\frac{M}{t}$ implies nt $\\geq$ M. Since $x^n$ > nt, we have that n $\\geq$ N implies $x^n$ > M.\nI know my proof-writing skills are very bad and I'm sure there are a million ways I can improve what I've done, but I am very thankful for the help I received here from Andre Nicolas.\n",
    "proof": "A start: Let $x=1+t$. Then $t\\gt 0$ and $x^n=(1+t)^n$. Now use the Bernoulli Inequality \n$$(1+t)^n \\ge 1+nt.$$ This can be proved by induction, or by using the Binomial Theorem.  \n",
    "tags": [
      "proof-writing"
    ],
    "score": 5,
    "answer_score": 4,
    "is_accepted": true,
    "question_id": 942358,
    "answer_id": 942361
  },
  {
    "theorem": "Prove the Correctness of Horner&#39;s Method for Evaluating a Polynomial",
    "context": "I am currently studying the Skiena `Algorithm Design Manual' and need a little help with a proof of correctness. The problem goes as follows:\n\nProve the correctness of the following algorithm for evaluating a polynomial.\n  $P(x)=a_nx^n+a_{n-1}x^{n-1}+\\ldots+a_1x+a_0$\nfunction horner($A,x$)\n\n$p=A_n$\n  \nfor $i$ from $n-1$ to $0$\n  \n$p=p*x+A_i$\n\nreturn $p$\n\n\n\nIt is intuitively obvious, that this algorithm gives the right result. But as I want a proof of correctness, I have to make sure this becomes obvious. My idea is proof by induction. We want to use, rigorously, that the inductive hypotheses is true up to and including step $n-1$.\nTo consider the algorithm, let $(a_i)_{i\\in\\mathbf{N}_0}$ be some sequence of real numbers and let $P$ be a function on $\\mathbf{N}_0\\times\\mathbf{R}$ into $\\mathbf{R}$ given by $$\\forall (n,x)\\in \\mathbf{N}_0\\times\\mathbf{R} , \\quad P(n,x)=\\sum_{j=0}^n a_jx^j.$$\nSo let us consider the base case of $n=0$. This case is no problem, and the algorithm exites with $P(0,x)=a_0$ for all $x\\in\\mathbf{R}$.\nNow, for the inductive step, assume the algorithm holds for all steps up to and including step $n-1$ for some arbitrary $n\\in\\mathbf{N}$. I want to express the iterative scheme symbolically, but of course, it is a nonsense to write $p=p*x+A_i$, since this just yields a function given by $p=A_i/(1-x)$ (when the division is well-defined). So I will give up the notation of the algorithm.\nFor this arbitrary $n$, what we want to show is that\n$$\\{[(a_nx+a_{n-1})x+a_{n-2}]x+\\ldots\\}x+a_0=\\sum_{j=0}^n a_jx^j.$$\nIt is, as I said above, intuitively clear that this equation holds true, but I feel the left hand side is a bit too ambiguous. My problem is that I do not know how to express the left hand side of nested multiplication and summation in unambiguous notation (a product-operator and summation-operator). This is where I would like some help,\nThanks in advance\n",
    "proof": "\nInduction Hypothesis:\n\n\n\n$\\mathbf{Consider}$ the polynomial of $\\mathbf{n^{th}}$ degree $\\mathbf{P_n}$, such that $$\\mathbf{P_n} = a_nx^n + a_{n-1}x^{n-1} + ... + a_1x + a_0$$\n\nSuch that $\\mathbf{A_n} = \\{a_n, a_{n-1}, a_{n-2},..., a_1, a_0\\}$ is the set of all coefficients of $\\mathbf{P_n}$\n\n\n$\\mathbf{Then}$ we can define the $\\mathbf{Honer}$ function of $\\mathbf{P_n}$ as follows:\n\n$$\n\\mathbf{horner(A_{n-i}, x)} =\n\\begin{cases}\na_n,  & \\text{; $i = n$} \\\\\n\\mathbf{horner(A_{n-i-1}, x)} \\cdot x + a_i, & \\text{; $\\forall$ i such that $0 \\leq i \\lt n $}\n\\end{cases}\n$$\n\n\n\nBase case:\n\n\n$ \\mathbf{horner(A_1, x)} = \\mathbf{P_1} =  a_nx + a_0 $\nAssume this is true up till $\\mathbf{A_n}$, then $$ \\mathbf{horner(A_n, x)} = (... (a_nx + a_{n-1} )x + a_{n-2})x + ... + a_1)x + a_0 ) $$\n\nInduction step:\n\n\n\nYou can show that $\\mathbf{horner(A_{n}, x)} \\implies \\mathbf{horner(A_{n+1}, x)}$ in two ways.\n\n\nThe first one, is by using the induction hypothesis definition. So, starting with $i = 0$\n\\begin{align}\n\\mathbf{horner(A_{n+1}, x)} & = {\\color{red}{\\mathbf{horner(A_{(n+1)-1}, x)}}} \\cdot x + a_0 \\\\\n& = {\\color{red}{(\\mathbf{horner(A_{(n+1)-2}, x) \\cdot x + a_1)}}} \\cdot x + a_0 \\\\\n& = \\vdots \\text{ for k-times} \\\\ \n& = (\\mathbf{horner(A_{(n+1)-{\\color{red}{k}}}, x) \\cdot x + a_{{\\color{red}{k}}-1})} \\cdot x + ... + a_1)x + a_0 \\\\\n\\end{align}\n\nAssume $ \\mathbf{horner(A_{(n+1)-k}, x)} = \\mathbf{horner(A_0, x)} $ \nThen $ (n+1)-k = 0 \\implies (n+1) = k $ \nSubstitute in back in the equation gives,\n\n\\begin{align}\n\n\\mathbf{horner(A_{n+1}, x)} & = (\\mathbf{horner(A_{{\\color{red}{k}}-k}, x) \\cdot x + a_{{\\color{red}{n+1}}-1})} \\cdot x + ... + a_1)x + a_0 \\\\\n& = ( {\\color{red}{\\mathbf{horner(A_{0}, x)}}} \\cdot x + a_{n}) \\cdot x + ... + a_1)x + a_0 \\\\\n& = ({\\color{red}{a_{n+1}}} \\cdot x + a_{n}) \\cdot x + ... + a_1)x + a_0 \\\\\n\n\\end{align}\n\n\n\n\nThe other way is using the fact that \n$ \\mathbf{horner(A_{n+1}, x)} = \\mathbf{P_{n+1}} $ \nConsider $\\mathbf{P_{n+1}}$, we can define it as follows:\n\n\\begin{align}\n\n\\mathbf{P_{n+1}} & = {\\color{red}{a_{(n+1)}x^{(n+1)} + a_{n}x^{n}}} + a_{(n-1)}x^{(n-1)} + ... + a_1x + a_0 \\\\\n& = {\\color{red}{(a_{(n+1)}x + a_n)x^n}} + a_{(n-1)}x^{(n-1)} + ... + a_1x + a_0 \\\\\n& = {\\color{red}{((a_{(n+1)}x + a_n)x + a_{(n-1)})x^{(n-1)}}} + a_{(n-2)}x^{(n-2)} + ... + a_1x + a_0 \\\\\n& = \\vdots \\text{ keep factoring out k-times} \\\\\n& = {\\color{red}{(...(a_{(n+1)}x + a_n)x + a_{(n-1)})x}} + ... + a_{(n-k)})x^{(n-k)} + ... + a_1x + a_0 \\\\\n\n\\end{align}\n\nIt turns out that the non-red highlighted terms are nothing but $\\mathbf{horner(A_{n-k}, x)}$\n\n\\begin{align}\n\n\\mathbf{P_{n+1}} & = {\\color{red}{(...(a_{(n+1)}x + a_n)x + a_{(n-1)})x}} + ... + \\mathbf{horner(A_{n-k}, x)} \\\\\n\n\\end{align}\n\nAssume $ \\mathbf{horner(A_{n-k}, x)} = \\mathbf{horner(A_1, x)} $ \nThen $ n-k = 1 $ \nSubstitute in back in the equation gives,\n\n\\begin{align}\n\n\\mathbf{P_{n+1}} & = {\\color{red}{(...(a_{(n+1)}x + a_n)x + a_{(n-1)})x}} + ... + \\mathbf{horner(A_{1}, x)} \\\\\n& = {\\color{red}{(...(a_{(n+1)}x + a_n)x + a_{(n-1)})x}} + ... + a_1x + a_0 \\\\\n\n\\end{align}\n\nSince $\\mathbf{horner(A_{n+1}, x)} = \\mathbf{P_{n+1}}$\nThen $\\mathbf{horner(A_{n+1}, x)} = (...(a_{(n+1)}x + a_n)x + a_{(n-1)})x + ... + a_1x + a_0$\n\n\nConclusion\n$$\\mathbf{horner(A_n, x)} \\implies \\mathbf{horner(A_{n+1}, x)}$$\n",
    "tags": [
      "proof-writing"
    ],
    "score": 5,
    "answer_score": 2,
    "is_accepted": false,
    "question_id": 436730,
    "answer_id": 3520298
  },
  {
    "theorem": "Proof of Bezout&#39;s Lemma using Euclid&#39;s Algorithm backwards",
    "context": "I've seen it said that you can prove Bezout's Identity using Euclid's algorithm backwards, but I've searched google and cannot find such a proof anywhere. I found another proof which looks simple but which I don't understand. \nBezout's lemma is:\nFor every pair of integers a & b there are 2 integers s & t such that as + bt = gcd(a,b)\n\nEuclid's algorithm is:\n1. Start with (a,b) such that a >= b\n2. Take reminder r of a/b\n3. Set a := b, b := r so that a >= b\n4. Repeat until b = 0\n\nSo here's the proof by induction that I found on the internet:\nBase case:\n\nb = 0 \ngcd (a,b) = a\ns = 1 \nt = 0\n\nInductive Step:\n\nAssume Bezout's Lemma is true for all pairs of b < k.\n\na = qb + r with 0 =< r < b = k\n\ngcd (a,b) = gcd (b,r)\n\nBy the inductive hypothesis there are integers x and y with bx and ry = gcd(a,b)\n\nbx + ry = bx + (a - qb)y = b(x - qy) + ay = gcd(a,b)\n\nSet t = (x - qy) and s = y. This establishes the identity for a pair (a,b) with b = k and completes the induction. \n\nI only followed the proof up to the base case. I don't see how it proved b = k from b < k. Could you please explain this to me? \nThanks. \nEDIT: After two days of trying to figure it out I still don't understand the proof. I conclude that either I lack the requisite knowledge or the intelligence or both. Thanks for trying to help. \n",
    "proof": "Below is a simple conceptual proof of Bezout's identity for the gcd.\nThe set $\\rm\\:S\\:$ of all integers of form $\\rm\\:a\\:x + b\\:y,\\,\\ x,y\\in \\mathbb Z,\\:$ is closed under subtraction so, by Lemma below, every positive $\\rm\\:n\\in S\\:$ is divisible by $\\rm\\:d = $ least positive $\\rm\\in S,\\:$ so $\\rm\\:a,b\\in S\\:$ $\\Rightarrow$ $\\rm\\:d\\:|\\:a,b.\\:$ Thus $\\rm\\:d\\:$ is a common divisor of $\\rm\\:a,b,\\:$ necessarily greatest, $ $ by $\\rm\\:c\\:|\\:a,b\\:$ $\\Rightarrow$ $\\rm\\:c\\:|\\:d = a\\:x\\!+\\! b\\:y\\:$ $\\Rightarrow$ $\\rm\\:c\\le d.$ \nLemma $\\ $ If a nonempty set of positive integers $\\rm\\: S\\:$ satisfies both $\\rm\\ n > m\\: \\in\\, S \\ \\Rightarrow\\  n-m\\, \\in\\, S$  then every element of $\\rm\\:S\\:$ is a multiple of the least element $\\rm\\:m_{\\circ} \\in\\, S.$\nProof $\\,\\bf 1$ $\\  $ If not there is a least nonmultiple $\\rm\\:n\\in S,\\,$ contra  $\\rm\\:n-m_{\\circ} \\in S\\:$ is a nonmultiple of $\\rm\\:m_{\\circ}$\nProof $\\,\\bf 2$ $\\rm\\ \\ S\\,$ closed under subtraction $\\rm\\:\\Rightarrow\\:S\\,$ closed under remainder (mod), when it is $\\ne 0,$ since mod may be computed by repeated subtraction, i.e. $\\rm\\: a\\ mod\\ b\\, =\\, a - k b\\, =\\, a-b-b-\\cdots -b.\\:$ Hence $\\rm\\:n\\in S\\:$ $\\Rightarrow$ $\\rm\\: n\\ mod\\ m_\\circ = 0,\\:$ else it is $\\rm\\,\\in S\\,$ and smaller than $\\rm\\:m_\\circ\\,$ contra mimimality of $\\rm\\:m_\\circ$ \nRemark $\\ $ In a nutshell, two applications of induction yield the following inferences\n$$\\!\\rm\\begin{eqnarray} S\\ closed\\ under\\ subtraction &\\!\\Rightarrow\\:&\\rm S\\ closed\\ under\\ mod \\!=\\! remainder = repeated\\ subtraction \\\\\n&\\!\\Rightarrow\\:&\\rm S\\ closed\\ under\\ gcd\\! =\\! repeated\\ remainder\\!\\!: Euclid'\\!s\\ algorithm \\end{eqnarray}$$\nInterpreted constructively, this yields the extended Euclidean algorithm for the gcd. Namely, $ $ starting from the two elements of $\\rm\\,S\\,$ that we know: $\\rm\\ a \\,=\\, 1\\cdot a + 0\\cdot b,\\ \\ b \\,=\\, 0\\cdot a + 1\\cdot b,\\ $ we search for the least element of $\\rm\\,S\\,$ by repeatedly subtracting elements to produce smaller elements of $\\rm\\,S\\,$ (while keeping track of every elements linear representation in terms of $\\rm\\,a\\,$ and $\\rm\\,b).\\:$ This is essentially the subtractive form of the Euclidean algorithm (vs. the mod/remainder form).\n",
    "tags": [
      "elementary-number-theory",
      "proof-writing",
      "gcd-and-lcm"
    ],
    "score": 5,
    "answer_score": 4,
    "is_accepted": false,
    "question_id": 310637,
    "answer_id": 310655
  },
  {
    "theorem": "$d(x,A)=0\\iff $ every neighborhood of $X$ contains a point of $A$",
    "context": "Mendelson, Introduction to Topology, p.52\n\n$(8)$. Let $A$ be a non-empty subset of a metric space $(X,d)$. Let $x\\in X$. Prove that $d(x,A)=0$ if, and only if, every nieghborhood $V$ of $x$ contains a point of $A$.\n\nDEFINITION Given a subset $A$ of a metric space $X$, and $x\\in X$, the distance of $x$ to $A$ is defined as:\n$$d(x,A)=\\inf\\{d(x,a):a\\in A\\}$$\nCOROLLARY 5.9 Let $(X,d)$ be a metric space, $a\\in X$ and $A$ a non-empty subset of $X$. Then there is a sequence $\\{a_n\\}$ of points of $A$ such that $\\lim \\; d(a,a_n)=d(a,A)$\nPROOF\n$(\\Rightarrow)$ \nSuppose every neighborhood of $x$ contains a point of $A$. We must prove that $\\inf\\{d(x,a):a\\in A\\}=0$ But since every neighborhood of $x$ contains a point of $A$, then there is a sequence of points $\\{a_n\\}$ of $A$ such that $\\lim \\;a_n=x$. It follows that $\\lim \\; d(x,a_n)=0$, and since $\\{a_n\\}\\subset A$, $\\inf\\{d(x,a):a\\in A\\}=0$ since $d(x,a)\\geq 0$ for any $a,x$.\n$(\\Leftarrow)$ Suppose $d(x,A)=0$. It follows by 5.9 that there is a sequence of points $\\{a_n\\}$ in $A$ such that  $\\lim \\; d(x,a_n)=0$. But given a point $a\\in X$, the function $f:X\\to \\Bbb R\\;/\\;f(x)=d(x,a)$ is continuous (just take $\\epsilon =\\delta$). Thus $\\lim \\; d(x,a_n)= d(x,\\lim \\;a_n)=0$. But $d(x,a)=0\\iff x=a$, so $\\lim \\;a_n=x$. This means that for any neighborhood $V$ of $x$ there exists an $N$ such that $a_n \\in V$ whenever $n>N$, so every neighborhood of $x$ contains some $a\\in A$.\nIs this alright? Is there any gap or circularity I'm missing?\n",
    "proof": "You can get a cleaner proof when you avoid sequences altogether:\n\nSuppose that every neighborhood of $x$ contains a point of $A$.\n  Obviously, $d(x,A)\\geq 0$. Now let $\\epsilon>0$. Then there exists by\n  assumption $a\\in A$ with $d(x,a)<\\epsilon$. Since $\\epsilon$ was arbitrary,\n  $d(x,A)=0$.\nSuppose that $d(x,A)=0$. Let $\\epsilon>0$. By assumption, there is\n  $a\\in A$ with $d(x,a)<\\epsilon$. Hence, every open $\\epsilon$-ball\n  around $x$ contain an element of $A$. Since every neighborhood of $x$\n  is a superset of such a ball, we are done.\n\nAfficionados might note that one avoids having to make arbitrary choices in the sequence-free proof. \n\nEdit: On your own proof. The original proof is correct except for your argument that $\\lim d(x,a_n)=0$ implies $\\lim a_n=x$. The result is true, but there is a subtle flaw in the argument. You seem to use the sequence characteriation of continuity there, so that $f$ is continuous at $y$ if $f(y_n)$ converges to $f(y)$ whenever $y_n$ converges to $y$. To apply that in your case, you need to assume that the sequence $(a_n)$ is convergent. It is, but that is something you have to prove first. \n",
    "tags": [
      "metric-spaces",
      "proof-writing"
    ],
    "score": 5,
    "answer_score": 3,
    "is_accepted": true,
    "question_id": 171671,
    "answer_id": 171677
  },
  {
    "theorem": "Can vacuously true statements be proved using proof by contradiction?",
    "context": "I'd always thought that it's valid to prove vacuously true statements using proof by contradiction, but now I'm not so sure.\nFor example, to prove the vacuously true statement $$\\forall (x \\in \\emptyset)\\; x + 1 = 0,$$ I assume for the sake of contradiction that $$\\exists (x \\in \\emptyset)\\;x + 1 \\neq 0.$$ This contradicts $x \\notin \\emptyset$ for all objects $x.$\nBut the property $x + 1 \\neq 0$ does not make sense for any arbitrary object $x$ in $\\emptyset$, does it? (Refer to my confusion regarding properties only being defined for certain sets of objects.) Is there a flaw in my reasoning?\n",
    "proof": "Proof By Contradiction has this general template :\nWe want to Prove $P$\nWe assume the negation : $\\lnot P$\nWe then derive some falsity [[ Eg1 \"$\\lnot \\lnot P$\" : Eg2 \"$1 = 0$\" : Eg3 \"$\\sqrt{2}$ is rational\" : Eg4 \"$8$ is irrational\" ]]\nHence we conclude that the assumption was wrong\nHence the negation of the assumption is true : $\\lnot [ \\lnot P ] \\equiv [ P ]$ is true\nIn OP Case , that template works :\nWe want to Prove : $P \\equiv [ \\forall x \\in \\emptyset, x + 1 = 0 ]$\nWe assume the negation : $\\lnot [\\forall x \\in \\emptyset, x + 1 = 0]$\nThat is : $\\exists x \\in \\emptyset, \\lnot [x + 1 = 0]$\nThat is : $\\exists x \\in \\emptyset, [x + 1 \\not = 0]$\nThat is a falsity : there is no $\\exists x \\in \\emptyset$\nHence we conclude that the assumption was wrong\nHence the negation of the assumption is true : $\\lnot [ \\lnot P ] \\equiv [ P ]$ is true\nProof By Contradiction work out well here !!\n",
    "tags": [
      "elementary-set-theory",
      "logic",
      "proof-writing",
      "first-order-logic",
      "intuition"
    ],
    "score": 5,
    "answer_score": 1,
    "is_accepted": false,
    "question_id": 4906979,
    "answer_id": 4906998
  },
  {
    "theorem": "Intermediate level proof based math book.",
    "context": "I am studying linear algebra. I find elementary level linear algebra; i.e, concepts like vector spaces,basis, span dimension, solving determinants, finding Eigenvalues ,using Cayley Hamilton theorem, etc very easy. I am reading Sheldon Axler's Linear Algebra done right. I have completed exercises upto Chapter 3 exercise D. But I find the exercises from the next section onwards, which are proof based and focus on abstract vector spaces very hard. I am unable to solve it without help. Part of the reason is I have never encountered this sort of mathematics before. I would appreciate if anyone can recommend books which introduce  proof based mathematics and abstract mathematical concepts, the books need not be based on linear algebra only, they can be based on other topics also.\n",
    "proof": "I am also studying Sheldon Axler's Linear Algebra done right. The site https://linearalgebras.com/ is great because you can find the solutions for all exercises in the Linear Algebra done right. Before this book, I have read A Logical Introduction to Proof by D. Cunningham. However, How to Prove It: A Structured Approach by D. Velleman is better because you can find the solutions for all exercises in the web.\n",
    "tags": [
      "linear-algebra",
      "proof-writing"
    ],
    "score": 5,
    "answer_score": 3,
    "is_accepted": true,
    "question_id": 4804249,
    "answer_id": 4804267
  },
  {
    "theorem": "How to prove $\\phi \\models \\psi$ iff $\\phi \\cup \\{\\neg \\psi\\}$?",
    "context": "I want to prove $\\phi \\models \\psi$ iff $\\phi \\cup \\{\\neg \\psi\\}$ unsatisfiable.\nWhat's the meaning of the union operator w.r.t. satisfiability in FOL?\nDoes it mean that all such interpretations $\\mathcal{I}$ of the LHS also have to model $\\phi$ and $\\{\\neg \\psi\\}$?\nSo $\\phi \\cup \\{\\neg \\psi\\} \\Leftrightarrow  \\phi \\wedge \\neg \\psi$ ?\nI read a few other related posts.\nFirst one which I changed w.r.t. my proof:\nLet $\\phi$ be a set of propositional formulas and $\\psi$ a propositional formula.\n$\\Rightarrow:$\nAssume $\\phi\\models \\psi$ and $\\phi \\cup \\{\\neg \\psi\\}$ satisfiable\nThen there is a variable assignment s.t. $\\forall \\varphi:(\\beta \\models \\varphi) \\wedge (\\beta \\models \\neg\\psi)$\nIt follows that $\\beta \\nvDash \\psi$. Therefore $\\psi$ not a consequence of $\\phi$.\n$\\Leftarrow$:\nAssume $\\psi \\cup \\{\\neg \\psi\\}$ not satisfiable and $\\beta$ any variable assignment s.t. $\\beta \\models \\phi$.\nTo be shown $\\beta \\models \\psi$.\nFor contradiction assume $\\beta \\nvDash \\psi$.\nThen $\\beta \\models \\neg \\psi$, whch implies $\\beta \\models \\phi \\cup \\{\\neg \\psi\\}$.\nSo $\\phi \\cup \\{ \\neg \\psi\\}$ satisfiable, which is a contradiction.\nIs this correct?\nSource of first proof\nAnother proof with natural deduction.\n$\\frac{\\psi \\cup \\{\\phi\\} \\models \\bot}{\\phi \\models \\neg \\psi}$\nAnd\n$\\frac{\\phi \\models \\psi}{\\phi \\cup \\{ \\neg \\psi\\}\\models \\bot}$\n$\\Rightarrow:$\nIf $\\phi \\models \\psi$ then $\\phi \\cup \\{\\neg \\psi\\} \\models \\psi$.\nBut $\\phi \\cup \\{\\neg \\psi\\} \\models \\{ \\neg \\psi\\}$.\nCan't be satisfied, because it proves contradiction.\n$\\Leftarrow:$\nIf $\\phi \\cup \\{ \\neg \\psi\\} \\models \\phi$ can't be satisfied, then it has no models and so $\\phi \\cup \\{ \\neg \\psi\\} \\models \\bot$\nBy usage of semantic completeness theorem we get $\\phi \\cup \\{ \\neg \\psi \\} \\models \\bot$.\nWhich by usage of natural deduction results in $\\phi \\models \\psi$.\nIn the deductions I changed $\\vdash$ to $\\models$, is this still correct?\nSource of second proof\n",
    "proof": "\nI want to prove $\\phi \\models \\psi$ iff $\\phi \\cup \\{\\neg \\psi\\}$ unsatisfiable.\nWhat's the meaning of the union operator w.r.t. satisfiability in FOL?\n\nWe have to be careful with symbols. $a\\models b$ could mean one of two things:\n\n$a$ is a model, and the formula $b$ is true for $a$.\n$a$ is a set of formulas, and for every model $\\mathcal M$ that satisfies $a$, it must satisfy $b$.\n\nIn your case, it seems you mean $\\phi$ is either a formula or a set of formulas. If $\\phi$ is a set of (well-formed) formulas that are understood as axioms, then $\\cup$ is literally the union of the two sets, that is to add $\\neg\\psi$ as a new axiom.\nNow the statement is almost trivial: if $\\phi\\models \\psi$, i.e. for any model $\\mathcal M$ that satisfies $\\phi$, it must satisfy $\\psi$, hence it cannot satisfy $\\neg\\psi$, so $\\phi\\cup\\{\\neg\\psi\\}$ cannot be satisfied. If $\\mathcal M$ satisfies $\\phi\\cup\\{\\neg\\psi\\}$, it satifies $\\phi$ but not $\\psi$, which contradicts the meaning of $\\phi\\models\\psi$.\nThis is trivial precisely because $\\vDash$ is about semantics. Only \"$\\phi\\vdash\\psi$ iff $\\phi\\cup\\{\\neg\\psi\\}$ cannot be satisfied\" needs the Godel completeness theorem. See the difference between $\\vdash$ and $\\vDash$.\nThere seem to be many notational problems. Just to name a few, but you probably should read entire sections of your study materials more carefully.\n\nSo $\\phi \\cup \\{\\neg \\psi\\} \\Leftrightarrow  \\phi \\wedge \\neg \\psi$ ?\n\n$\\phi\\wedge\\neg\\psi$ makese no sense, unless $\\phi$ is a single formula instead of a (potentially infinite) set of formulas.\n\n$\\forall \\varphi:(\\beta\\models \\varphi) \\wedge (\\beta \\models \\neg\\psi)$.\n\n\"$\\forall, \\wedge$\" are reserved symbols in the FOL, so better to avoid using them on the meta-level. If $\\beta$ is actually a model/interpretation (together with assignment for free variables if necessary), it's correct to use $\\vDash$ here, but this $\\vDash$ has a different meaning from the one in $\\phi\\vDash\\psi$. Also, are you suggesting $\\varphi\\in\\phi$?\n\nIf $\\phi \\cup \\{ \\neg \\psi\\} \\models \\phi$ can't be satisfied, then it has no models and so $\\phi \\cup \\{ \\neg \\psi\\} \\models \\bot$\n\nIt makes no sense to say whether \"$a\\vDash b$\" can be satisfied, which is either true or false objectively. Only a set of sentences can be satisfied or not.\n",
    "tags": [
      "logic",
      "proof-writing",
      "computer-science",
      "first-order-logic"
    ],
    "score": 5,
    "answer_score": 3,
    "is_accepted": true,
    "question_id": 4773896,
    "answer_id": 4773924
  },
  {
    "theorem": "Critique my proof of: Suppose $A$, $B$, $C$, and $D$ are sets. Prove that $(A \\times B) \\cup (C \\times D) \\subseteq (A \\cup C) \\times (B \\cup D)$.",
    "context": "Critique my proof on correctness, structure, etc.\nProof. Let $p = (x,y) \\in (A \\times B) \\cup (C \\times D)$. Thus, $(x, y) \\in (A \\times B)$ or $(x, y) \\in (C \\times D)$.\nCase #1\nSuppose $(x, y) \\in (A \\times B)$. By definition of cartesian product, $x \\in A$ and $y \\in B$. It follows that $x \\in (A \\cup C)$ and $y \\in (B \\cup D)$ by definition of union.\nCase #2\nSuppose $(x, y) \\in (C \\times D)$. By definition of cartesian product, $x \\in C$ and $y \\in D$. It follows that $x \\in A \\cup C$ and $y \\in C \\cup D$ by definition of union.\n$\\therefore$ Because $(x, y)$ is an arbitrary element of $(A \\times B) \\cup (C \\times D)$, $(A \\times B) \\cup (C \\times D) \\subseteq (A \\cup C) \\times (B \\cup D)$.\n",
    "proof": "This proof is very tight and if I was grading it I'd give it full credit.\nIf you want to be more concise you could say that since the union operator is commutative, that proving only case one is necessary since case two is the same argument just relabeling the sets.\nSomething along the lines of\n\nWithout loss of generality, assume that $(x,y)\\in A\\times B$. This means that $x\\in A$ and $y\\in B$ which, by definition of the union means that $x\\in A\\cup C $ and $y\\in B\\cup D \\implies (x,y)\\in(A\\cup C)\\times (B\\cup D)$. The same argument can be made if $(x,y)\\in C\\times D$.\n\n\n$\\therefore$ since all $p\\in (A\\times B)\\cup(C\\times D)\\implies p\\in (A\\cup C)\\times(B\\cup D)$ we can say $(A\\times B)\\cup(C\\times D)\\subseteq  (A\\cup C)\\times(B\\cup D)$. Q.E.D.\n\nHowever, perhaps I should not be encouraging people to be lazy in proofs like me\n",
    "tags": [
      "elementary-set-theory",
      "proof-writing",
      "solution-verification"
    ],
    "score": 5,
    "answer_score": 3,
    "is_accepted": true,
    "question_id": 4343144,
    "answer_id": 4343190
  },
  {
    "theorem": "Additive but not $\\sigma$-additive measure $\\mathbb{Q} \\cap I$",
    "context": "I read in an old book the following example of a measure:\n\nFor the set $M=\\mathbb{Q} \\cap[0,1]$ denote with $S$ the set system of subsets of $M$ of the form $\\mathbb{Q} \\cap I$, where $I$ is any interval in $[0,1]$. Let us define the function $\\mu: S \\rightarrow \\mathbb{R}$ as follows: for any set $A \\in S$ of the form $A=\\mathbb{Q} \\cap I$ we set $\\mu(A)=\\ell(I)=b-a .$\n\nThen it said without proof that $\\mu$ is finitely additive, but not $\\sigma$-additive.\nAs I did not get why I tried to prove it by myself and I tried to show that $S$ is a semi-ring, I guess that is important before I start with the other proof.\nWe have $\\emptyset \\in \\mathbb{Q}$ and furthermore $(\\mathbb{Q} \\cap I_1)\\cap (\\mathbb{Q} \\cap I_2)=\\mathbb{Q} \\cap I_1 \\cap I_2$ and the union of two closed intervals is either an interval or the disjoint union of two intervals.\nThen $(\\mathbb{Q} \\cap I_1)\\setminus (\\mathbb{Q} \\cap I_2)$ is also an interval or the disjoint union of two intervals.\nNow the proof. I do not quite understand how it cannot be $\\sigma$-additive. Does it have something in common with Cantor sets? I don't know how to start the proof here. Any help or explanation (maybe an idea for the beginning of a proof) is appreciated. If there is a proof...\n",
    "proof": "If $\\mu$ is $\\sigma$-additive then for disjoint suitable sets $A_n$ it must satisfy: $$\\mu\\left(\\bigcup_{n=1}^\\infty A_n\\right)=\\sum_{n=1}^{\\infty}\\mu(A_n)$$\nFrom this it can be deduced that for suitable sets $B_n$ (not necessarily disjoint) it must satisfy:$$\\mu\\left(\\bigcup_{n=1}^\\infty B_n\\right)\\leq\\sum_{n=1}^{\\infty}\\mu(B_n)$$\nNow for every $r\\in M$ choose an interval $I_r$ with $r\\in S_r=\\mathbb Q\\cap I_r\\in\\mathcal S$.\nIf $\\mu$ is indeed $\\sigma$-additive then:$$1=\\mu(M)=\\mu\\left(\\bigcup_{r\\in M}S_r\\right)\\leq\\sum_{r\\in M}\\mu(S_r)=\\sum_{r\\in M}l(I_r)$$\nHowever it is easy to choose the intervals $I_r$ in such a way that:$$\\sum_{r\\in M}l(I_r)<1$$and doing so we run into a contradiction.\n",
    "tags": [
      "measure-theory",
      "proof-writing"
    ],
    "score": 5,
    "answer_score": 3,
    "is_accepted": true,
    "question_id": 4287072,
    "answer_id": 4287090
  },
  {
    "theorem": "Proving $19 \\mid 2^{2^n} + 3^{2^n} + 5^{2^n}$",
    "context": "Theorem. $19 \\mid 2^{2^n} + 3^{2^n} + 5^{2^n}$, for all positive integers $n$.\n\nI'm tasked with proving the given theorem by induction. Here's where I've gotten so far...\nProof. Clearly, the theorem is true for $n=1$, establishing the base case. Moreover, it can easily be shown that the theorem works for $n=2,3,4,5,\\ldots, 18$.\nSuppose, now, that $k$ is an integer for which the given theorem holds. Consider, then, $k+18$.\n$$2^{2^{k+18}} + 3^{2^{k+18}} + 5^{2^{k+18}}$$\nBy Fermat's Little Theorem, if $n \\equiv m \\pmod{p-1}$, then $a^m \\equiv a^n \\pmod{p}$. Clearly, $k+18 \\equiv k \\pmod{18}$. Then,\n$$2^{2^{k+18}} \\equiv 2^{2^k} \\pmod{19}$$\n$$3^{2^{k+18}} \\equiv 3^{2^k} \\pmod{19}$$\n$$5^{2^{k+18}} \\equiv 5^{2^k} \\pmod{19}$$\nHence, $2^{2^{k+18}}$ may be expressed as $\\;2^{2^k} + 19n$ for some integer $n$, $\\;3^{2^{k+18}}$ as $\\;3^{2^k}+ 19m$ for some integer $m$, and $\\;5^{2^{k+18}}$ as $\\;5^{2^k} + 19q$ for some integer $q$.\nThus, $2^{2^{k+18}} + 3^{2^{k+18}} + 5^{2^{k+18}} = (2^{2^{k}} + 3^{2^{k}} + 5^{2^{k}}) + 19(m+n+q)$, a sum of integers divisible by $19$, and thus clearly divisible by $19$.\nBy the principle of induction, we've thus shown that $19 \\mid 2^{2^n} + 3^{2^n} + 5^{2^n}$, for all positive integers $n$, concluding the proof.\n$\\blacksquare$\n\nIs this approach valid?\n",
    "proof": "Your approach seems valid, but, rather than checking $18$ cases,\nyou could note that $a^{2^7}=a^{128}=(a^{18})^7a^2\\equiv a^2\\bmod19$ when $\\gcd(a,19)=1$,\nso $a^{2^8}=(a^{2^7})^2\\equiv a^{2^2},$ etc., so you would have to check only $n=1,2,3,4,5,6.$\n",
    "tags": [
      "elementary-number-theory",
      "proof-writing",
      "solution-verification",
      "modular-arithmetic",
      "divisibility"
    ],
    "score": 5,
    "answer_score": 2,
    "is_accepted": false,
    "question_id": 4127581,
    "answer_id": 4127593
  },
  {
    "theorem": "World&#39;s smallest (Exception-based) proof-checking language (in Python). In what way is Coq, Lean, Isabelle &quot;better&quot;?",
    "context": "As you may already know, the codebases written for Coq, Lean, etc. are humungous.  Thus a goal in a new proof assistant might be to simplify things.  Suppose we restrict our attention not to general proof theory but to categorical contexts.   The closest concept I can think of would be a \"CAS for category theory (with proofs)\".\nC = Cat('C')\nX = C.ob('X')\ni = Id(X, 'i')\nj = Id(X, 'j')\n\nwith Proof(goal=Eq(i,j), given=[C, X, i, j]) as p:\n    eq1 = p.equals(i, j(i))\n    eq2 = p.equals(j(i), j)\n    p.QED(p.transitivity(eq1, eq2))\n    \nprint(p.proposition_text())\nprint(p.proof_text())\n\nThe idea is that print will present precisely the following text to the user:\n\nProp. Let $C$ be a category, $X$ an object in $C$, and $i,j$ identity\nmorphisms on $X$.  Then $i = j$.\nProof.\n$$i = i\\circ j \\tag{1}$$\n$$i\\circ j = j \\tag{2}$$\nTherefore, $i = j$ by transitivity of equality and (1), (2)  $\\blacksquare$\n\nThus the most basic proof in Category can be given in English using classes Ob, Cat, Arrow, Id, Proof, and Eq.    The methods of the proof instance p encode how to work with and return equalities.\nThus a proof of a statement is valid if and only if we can construct the statement's goal data.\nQuestions.  Would you consider using such a python library as a student of category theory?  How would Coq, Lean, or Isabelle be superior on this particular proof?  The last question is so that I can make changes to the design, so that it is on par with the other proof assistants.\n\nHow does proof checking work?  Each method call on the p object will perform a run-time assert of the proposed equality against the given equalities.  Inclusion of i,j into the givens automatically includes their definitional equalities.  Should any assertion fail, the mathematician (or coder / you) will be informed in their IDE of the AssertionFailure.  So that is the proof-checking  mechanism - it makes use of the exception system within a programming language, which is in this case Python's.\nIf the with block is exited without a successful call to p.QED then a ProofError will be thrown (another exception).  The data of the argument to QED must equal p.goal.\nSimply doing p.equals(i,j) from the start will result in a ProofError, since the reference i does not equal the reference j.  That is to say, a real proof environment is indeed formed - a.k.a. you can't cheat on a proof.\n",
    "proof": "This thing would be possible. I really struggle to believe it would be remotely as ergonomic as e.g. Lean, though. Phrasing your proofs as types in a strong type system gives the compiler perfect knowledge, and allows it to be extremely helpful (if you haven't experienced Agda or Idris's hole-filling, it is actually miraculous). The extremely dynamic nature of Python means you are crippled from the start. If the user promises they will stick only to using your built-ins, then sure, you could get an experience like Lean's or Coq's where the interactive proof assistant tells you all the information available, but you're embedded in Python where anything goes, so it's quite a bit of trust you're putting on the user.\nAdditionally, Python is based around mutable state. Your proof terms are inherently dynamic, and the fact that your language is running inside Python means you are inviting the user to knit together proof terms using mutability. I do not believe this will lead to readable or maintainable proofs, and it also makes the job of the kernel a lot harder: if I construct a proof term in function f, then I use it in proof h, then I mutate it in function g, you're going to have to track this somehow so that you can re-evaluate the construction of h. This sounds like a pretty big undertaking to me, not that I'm deeply familiar with Python's reflection APIs. An immutable language rules out this entire can of worms.\n",
    "tags": [
      "proof-writing",
      "category-theory",
      "computer-science",
      "math-software",
      "proof-theory"
    ],
    "score": 5,
    "answer_score": 3,
    "is_accepted": true,
    "question_id": 4032305,
    "answer_id": 4032404
  },
  {
    "theorem": "Prove that the least upper bound of $\\mathcal F$ is $\\bigcup\\mathcal F$ and the greatest lower bound of $\\mathcal F$ is $\\bigcap\\mathcal F$.",
    "context": "Not a duplicate of this or this.\nThis is exercise $4.4.23$ from the book How to Prove it by Velleman $($$2^{nd}$ edition$)$:\nProve theorem $4.4.11.$\nTheorem $4.4.11.$ Suppose $A$ is a set, $\\mathcal F\\subseteq \\mathscr P(A)$, and $\\mathcal F\\neq \\emptyset$. Then the least upper bound of $\\mathcal F$ $($in the subset partial order$)$ is $\\bigcup\\mathcal F$ and the greatest lower bound of $\\mathcal F$ is $\\bigcap\\mathcal F$.\nHere is my proof:\nLet $F$ be an arbitrary element of $\\mathcal F$. Let $x$ be an arbitrary element of $F$. Ergo clearly $x\\in\\bigcup\\mathcal F$. Since $x$ is arbitrary, $F\\subseteq\\bigcup\\mathcal F$. Therefore if $F\\in\\mathcal F$ then $F\\subseteq\\bigcup\\mathcal F$. Since $F$ is arbitrary, $\\bigcup\\mathcal F$ is an upper bound for $\\mathcal F$. Let $U$ be the set of all upper bounds for $\\mathcal F$ and let $X$ be an arbitrary element of $U$. Let $y$ be an arbitrary element of $\\bigcup\\mathcal F$. So we can choose some $G_0\\in\\mathcal F$ such that $y\\in G_0$. Since $X$ is an upper bound for $\\mathcal F$ then $G_0\\subseteq X$. Since $y\\in G_0$, $y\\in X$. Since $y$ is arbitrary, $\\bigcup\\mathcal F\\subseteq X$. Thus if $X\\in U$ then $\\bigcup\\mathcal F\\subseteq X$. Since $X$ is arbitrary, $\\bigcup\\mathcal F$ is the smallest element of $U$ and hence the least upper bound for $\\mathcal F$.\nLet $F$ be an arbitrary element of $\\mathcal F$. Let $x$ be an arbitrary element of $\\bigcap\\mathcal F$. Ergo clearly $x\\in F$. Therefore if $F\\in\\mathcal F$ then $\\bigcap\\mathcal F\\subseteq F$. Since $F$ is arbitrary, $\\bigcap\\mathcal F$ is a lower bound for $\\mathcal F$. Let $L$ be the set of all lower bounds for $\\mathcal F$ and let $Y$ be an arbitrary element of $L$. Let $y$ be an arbitrary element of $Y$. Since $Y$ is a lower bound for $\\mathcal F$, $Y\\subseteq F$. Since $y\\in Y$, $y\\in F$. Since $F$ is arbitrary, $y\\in\\bigcap\\mathcal F$. Since $y$ is arbitrary, $Y\\subseteq \\bigcap\\mathcal F$. Thus if $Y\\in L$ then $Y\\subseteq \\bigcap\\mathcal F$. Since $Y$ is arbitrary, $\\bigcap\\mathcal F$ is the biggest element of $L$ and hence the greatest lower bound for $\\mathcal F$.\n$Q.E.D.$\nIs my proof valid$?$\nThanks for your attention.\n",
    "proof": "Your proves look good. You could gain in clarity by explaining upfront what you're doing.\nFor example for least upper bond.\nLet's first prove that $\\bigcup \\mathcal F$ is an upper bound.\nProof...\nAnd now let's prove that $\\bigcup \\mathcal F$ is less than any upper bound $U$.\nProof...\nThis allows to conclude that $\\bigcup \\mathcal F$ is the least upper bound.\n",
    "tags": [
      "discrete-mathematics",
      "proof-writing",
      "solution-verification",
      "order-theory"
    ],
    "score": 5,
    "answer_score": 3,
    "is_accepted": true,
    "question_id": 3809290,
    "answer_id": 3809301
  },
  {
    "theorem": "Countability of a sequence of natural numbers",
    "context": "I am trying to determine if that the set, $T$, of all eventually constant sequences of natural numbers is countable. \nMy intuition: The set $T$ is countable. . Let $T_j$ denote the set of sequences of natural numbers so that $t_i=t_j$ for all $i>j$.\nIf we can show that $\\mathbb{N}^k$ for all $k\\in\\mathbb{Z}^+$ is countable, then the union $$T=\\bigcup_{j\\in\\mathbb{N}}T_j$$ is countable as a countable union of countable sets is also countable.\nIs my intuition correct?\n",
    "proof": "First let's prove that $|\\mathbb{N}^2| = |\\mathbb{N}|$. An easy way to do this is by using Schröder-Cantor-Bernstein. We easily see that $|\\mathbb{N}| \\leq |\\mathbb{N}^2|$ by sending $n$ to $(n, 0)$. We also find an injection $|\\mathbb{N}^2| \\leq |\\mathbb{N}|$ by sending $(a, b)$ to $2^a 3^b$. By induction we thus have that $|\\mathbb{N}^n| = |\\mathbb{N}|$ for each natural $n \\geq 1$.\nNow we can complete the argument based on your sketch.\nA finite sequence of length $n$ is just an element of $\\mathbb{N}^n$. An eventually constant sequence is a finite sequence with an infinite tail that has constant value $k \\in \\mathbb{N}$. So for each $n,k \\in \\mathbb{N}$ there is a set $S_{n,k}$ of sequences that after $n$ values just become the constant sequence with value $k$. Then\n$$\n|S_{n,k}| = |\\mathbb{N}^n| = |\\mathbb{N}|.\n$$\nWe have\n$$\nT = \\bigcup_{n,k \\in \\mathbb{N}} S_{n,k},\n$$\nwhich is a countable union of countable sets and hence countable.\n\nAs pointed out in the comments below your question, we generally need the axiom of choice (or a weak version of it) to prove that the countable union of countable sets is countable. This is because for each of the sets in the union we would have to choose a bijection. However, we can get around choice by explicitly constructing bijections $\\mathbb{N}^n \\to \\mathbb{N}$. See for example the Cantor pairing function.\nEdit: or see the excellent answer of String, describing an explicit bijection directly, showing that we do not need choice.\n",
    "tags": [
      "real-analysis",
      "elementary-set-theory",
      "proof-writing"
    ],
    "score": 5,
    "answer_score": 2,
    "is_accepted": true,
    "question_id": 3712017,
    "answer_id": 3712315
  },
  {
    "theorem": "Prove that $A \\cup C \\subseteq B \\cup C$ iff $A \\setminus C \\subseteq B \\setminus C$",
    "context": "This is an exercise from Velleman's \"How To Prove It\":\n\n\nSuppose $A$, $B$, and $C$ are sets. Prove that $A \\cup C \\subseteq B \\cup C$ iff $A \\setminus C \\subseteq B \\setminus C$.\n\n\nProof: Suppose that $A \\cup C \\subseteq B \\cup C$. Let $x \\in A \\setminus C$ be arbitrary. Then $x \\in A$ and $x \\notin C$. Suppose $x \\notin B$. Since $x \\notin B$ and $x \\notin C$, $x \\notin B \\cup C$. Since $x \\notin B \\cup C$ and $A \\cup C \\subseteq B \\cup C$, $x \\notin A \\cup C$. But this contradicts the fact that $x \\in A$. Thus, $x \\in B$. Since $x \\in B$ and $x \\notin C$, $x \\in B \\setminus C$. Since $x$ was arbitrary, it follows that $A \\setminus C \\subseteq B \\setminus C$.\nNow suppose that $A \\setminus C \\subseteq B \\setminus C$. Let $x \\in A \\cup C$ be arbitrary. Then either $x \\in A$ or $x \\in C$. Suppose $ x \\notin B \\cup C$. Since $x \\notin C$, it follows that $x \\in A$, so $x \\in A \\setminus C$. Then since $A \\setminus C \\subseteq B \\setminus C$, $x \\in B \\setminus C$. But this contradicts the fact that $x \\notin B$. Thus, $x \\in B \\cup C$. $\\square$\nMy first approach for the $\\rightarrow$ direction was to use a proof by cases on whether $x \\in B$ or $x \\notin B$. If $x \\in B$, then clearly $x \\in B \\setminus C$, but the other case leads to a contradiction (as shown in the proof above). In a proof by cases, is it valid to eliminate some cases by showing that they lead to a contradiction? I would appreciate any other comments on the proof as well. Thanks!\n",
    "proof": "Yes you can eliminate cases using contradiction,your proof writing is totally ok..\n",
    "tags": [
      "elementary-set-theory",
      "proof-writing",
      "solution-verification"
    ],
    "score": 5,
    "answer_score": 2,
    "is_accepted": true,
    "question_id": 3643654,
    "answer_id": 3643661
  },
  {
    "theorem": "Necessary &amp; sufficient conditions on the parameters in a recursive sequence",
    "context": "\nFor $a,b\\in\\mathbb R$ the sequence $(x_n)_{n\\in\\mathbb N}$ is defined\n  recursively: $$x_1:=b,\\;\\;x_{n+1}=x_n^2+(1-2a)x_n+a^2$$ What is the\n  necessary condition on $a,b$ and what the sufficient condition for the\n  sequence to converge? Then evaluate $\\displaystyle\\lim_{n\\to\\infty}x_n$.\n\nMy attempt:\nAs far as I'm concerned, I'm required to prove the sequence $(x_n)_{n\\in\\mathbb N}$ is a Cauchy sequence,i.e., \n\nthe elements become arbitrarily close to each other as the sequence\n  progresses.\n\n$$f(x):=x_n$$\nSince the sequence can be represented by a parabola, I chose the vertex as the candidate for the limit $L\\in\\mathbb R$, and therefore:\n$$\\forall x_n\\in [L, b]$$\nor more precisely by the definition,\n$$(\\forall\\varepsilon>0)(\\exists n_{\\varepsilon}\\in\\mathbb N)(\\forall n\\in\\mathbb N)((n>n_{\\varepsilon})\\implies(|x_n-L|<\\varepsilon))$$\n$$\\forall x_n\\in[L,b]\\;x_n\\in\\mathcal R_f$$\nUsing the usual method (and the fact the limit is a fixed point of $f$):\n$$L=L^2-(1-2a)+a^2\\iff (L-a)^2=0\\implies L=a$$\n$$b\\in\\Gamma_f\\implies b\\geq 0$$\nBut, I don't know the answer what the necessary and what the sufficient condition on the parameters $a,b$ is. May I ask for advice on how to end the problem and correction for possible mistakes?\n",
    "proof": "From the recurrence formula, if $x_n$ converges, it's limit is $a$.\nThis becomes more intuitive if you look at $x_n - a$ instead of $x_n$ and you write the recurrence formula as\n$$x_{n+1}-a = (x_n-a)^2+(x_n-a)$$\n$x_n$ converges when $y_n:=x_n-a$ converges and notice that $y_n$ is increasing. The problem now becomes casework on the initial term $y_1 := b-a$ with the recurrence $y_{n+1} =y_n^2+y_n$. It diverges when $y_1< -1$ and $y_1 >0$, and converges when $y_1 \\in [-1,0]$. So the condition is $b \\in [a-1,a]$ (both necessary and sufficient).\n",
    "tags": [
      "real-analysis",
      "calculus",
      "sequences-and-series",
      "proof-writing"
    ],
    "score": 5,
    "answer_score": 3,
    "is_accepted": true,
    "question_id": 3529602,
    "answer_id": 3529639
  },
  {
    "theorem": "How to prove $\\frac{a}{7a+b}+\\frac{b}{7b+c}+\\frac{c}{7c+a}\\le\\frac38$",
    "context": "Suppose that $a,b,c>0$. How to prove $$\\frac{a}{7a+b}+\\frac{b}{7b+c}+\\frac{c}{7c+a}\\le\\frac38$$\n? \nMy first idea: By AM-GM, $$7a+b\\geq \\sqrt{7ab}$$ so $$\\sum_{cyc} \\frac{a}{7a+b}\\le\\sum_{cyc}\\sqrt{\\frac{a}{7b}}$$ but I am not sure if we can continue from here. \nAlso I try Cauchy-Schwarz: $$\\sum_{cyc} \\frac{a}{7a+b}\\le\\sqrt{a^2+b^2+c^2}\\sqrt{\\sum_{cyc} \\frac{1}{(7a+b)^2}}.$$\nNow what?\n",
    "proof": "By C-S\n$$\\sum_{cyc}\\frac{a}{7a+b}=\\frac{3}{7}+\\sum_{cyc}\\left(\\frac{a}{7a+b}-\\frac{1}{7}\\right)=\\frac{3}{7}-\\frac{1}{7}\\sum_{cyc}\\frac{b}{7a+b}=$$\n$$=\\frac{3}{7}-\\frac{1}{7}\\sum_{cyc}\\frac{b^2}{7ab+b^2}\\leq\\frac{3}{7}-\\frac{1}{7}\\cdot\\frac{(a+b+c)^2}{\\sum\\limits_{cyc}(7ab+b^2)}.$$\nId est, it's enough to prove that\n$$\\frac{3}{7}-\\frac{1}{7}\\cdot\\frac{(a+b+c)^2}{\\sum\\limits_{cyc}(7ab+b^2)}\\leq\\frac{3}{8}$$ or\n$$8(a+b+c)^2\\geq3\\sum\\limits_{cyc}(7ab+a^2)$$ or $$\\sum_{cyc}(a-b)^2\\geq0$$ and we are done!\n",
    "tags": [
      "inequality",
      "proof-writing",
      "a.m.-g.m.-inequality",
      "cauchy-schwarz-inequality"
    ],
    "score": 5,
    "answer_score": 2,
    "is_accepted": true,
    "question_id": 3483200,
    "answer_id": 3483363
  },
  {
    "theorem": "Internal direct sum of kernel of surjective homomorphism and cyclic subgroup",
    "context": "I'm studying for a qualifying exam in algebra, and my abstract algebra skills are quite rusty. I'm attempting to solve the following problem:\n\nSuppose that $\\Phi:G\\rightarrow\\mathbb{Z}$ is a surjective group homomorphism from the abelian group $(G,+)$ to the group of integers under addition. Let $K$ be the kernel of $\\Phi$, and let $g$ be an element of $G$ for which $\\Phi(g)=1$. Prove that $G$ is the (internal) direct sum of $K$ and the subgroup of $G$ generated by $g$.\n\nI was initially thrown off by the fact that $\\Phi(g)=1$, thinking this meant $g\\in K$ as $1$ is typically used to denote the identity. However, the codomain here is $(\\mathbb{Z},+)$, so the identity is actually the additive identity of the integers, $0$. Thus $g\\notin K$. Which I believe makes it clear that $K\\cap\\langle g\\rangle=0$ by a simple homomorphism argument (something to the effect of $\\Phi(ng)=\\Phi(g)+\\Phi(g)+\\cdots+\\Phi(g)=1+1+\\cdots+1=n\\neq0$). What I'm stuck on is how to argue that $G=K+\\langle g\\rangle$. I can use the first isomorphism theorem to claim that $\\mathbb{Z}\\cong G/K$ and $K\\triangleleft G$, but I'm not sure how this helps.\nEdit: I must be thinking of something wrong, as it seems to me that $\\Phi(\\langle g\\rangle)=\\mathbb{Z}_{\\geq0}$. But this doesn't allow the desired result, as $\\Phi$ is surjective, meaning everything in $\\mathbb{Z}$ gets mapped to, so specifically, $\\exists h\\in G$ s.t. $\\Phi(h)=-1$. There's no way to write $h=k+g'$ where $k\\in K$ and $g'\\in\\langle g\\rangle$.\n",
    "proof": "This is exactly the reason why I prefer to denote the identity of a general group by $e$. You can't confuse it with anything. In most books it is denoted by $e$ by the way. \nAnyway, you want to show that $G=K+\\langle g\\rangle$. First of all it is clear that $K+\\langle g\\rangle\\subseteq G$, because a sum of an element in $K$ and an element in $\\langle g\\rangle$ is a sum of two elements in $G$, hence it belongs to $G$. Now we want to show the other direction. Let $h\\in G$. Denote $n=\\Phi(h)$. Let's suppose $n>0$. Then: \n$\\Phi(h)=n=1+...+1=\\Phi(g)+...+\\Phi(g)=\\Phi(ng)$. \nNow denote $k=h-ng$. Then $\\Phi(k)=\\Phi(h)-\\Phi(ng)=0$. So $h=k+ng$ where $k\\in K,ng\\in\\langle g\\rangle$. \nNote that we assumed that $\\Phi(h)>0$. Now if $\\Phi(h)<0$ use the fact that $\\Phi(-h)>0$, so $-h$ has the required representation. And if $\\Phi(h)=0$ then $h\\in K$ and then $h=h+0\\in K+\\langle g\\rangle$. \n",
    "tags": [
      "group-theory",
      "proof-writing",
      "abelian-groups",
      "group-homomorphism",
      "direct-sum"
    ],
    "score": 5,
    "answer_score": 3,
    "is_accepted": true,
    "question_id": 3237172,
    "answer_id": 3237231
  },
  {
    "theorem": "Proof that $\\sin {x}$ is infinitely continuously differentiable over $[m,n]$",
    "context": "I am trying to prove that $\\sin {x}$ is infinitely continuously differentiable over $[m,n]$ where $m$ and $n$ are real numbers. Here is my attempt at doing so. Is my proof complete? If not, what can I do to improve it? Thank you in advance. \nSince,\n$\\frac{d}{dx}\\sin{x} = \\cos{x}$,\n$\\frac{d^2}{dx^2}\\sin{x} = -\\sin{x}$,\n$\\frac{d^3}{dx^3}\\sin{x} = -\\cos{x}$,\nand \n$\\frac{d^4}{dx^4}\\sin{x} = \\sin{x}$, \nthe derivatives of $\\sin{x}$, are periodic. Since the first four derivatives of $\\sin{x}$ are continuous over $[m,n]$ where $m$ and $n$ are real numbers, $\\sin{x}$ must be differentiable an infinite amount of times over $[m,n]$. \n",
    "proof": "A more formal way to show this is by induction.  We know that $f(x) = \\sin(x)$ is continuous.  Also, $f'(x) = \\cos(x)$ is continuous.  Now, assume that $f^{(2n-1)}(x) = (-1)^{n+1}\\cos(x)$ for all $n = 1,2,...$.  Then, $f^{(2(n+1)-1)}(x) = (-1)^{n+1}*(-\\cos(x)) = (-1)^{(n+1)+1}\\cos(x)$, which is continuous.  This proves that all odd derivatives are continuous.  For even derivatives, we just take any odd derivative and differentiate it once: $\\frac{d}{dx}f^{(2n-1)}(x) = (-1)^{n+1}*(-\\sin(x)) = (-1)^{n+2}\\sin(x)$, which is also continuous.  So, for all $n \\geq 0$, $f^{(n)}(x)$ is continuous.\n",
    "tags": [
      "calculus",
      "trigonometry",
      "proof-verification",
      "proof-writing"
    ],
    "score": 5,
    "answer_score": 3,
    "is_accepted": true,
    "question_id": 3065484,
    "answer_id": 3065508
  },
  {
    "theorem": "The application $\\mu^*$ is an outer measure. A proof without the Fubin&#39;s Theorem.",
    "context": "\nTheorem. Let $\\mathcal{K}\\subseteq 2^{X}$ a set family such that $\\emptyset \\in\\mathcal{K}$ and be given an application $\\nu\\colon\\mathcal{K}\\to[0,+\\infty]$ such that $\\nu(\\emptyset)=0$, we define $$\\mu^*(E)=\\inf\\left\\{\\sum_{n\\in\\mathbb{N}}\\nu(I_n)\\;\\middle|\\;E\\subseteq\\bigcup_{n\\in\\mathbb{N}}I_n\\;\\text{and}\\;\\{I_n\\}_{n\\in\\mathbb{N}}\\subseteq\\mathcal{K}\\right\\},$$\n  then $\\mu^*$ is an outer measure.\n\nProof.(With Fubini's Theorem) We prove that $\\mu^*$ is $\\sigma$-subadditive, the other properties are trivial. Let $\\{E_n\\}_{n\\in\\mathbb{N}}\\subseteq 2^{X}$ a countable set family. We suppose that $\\mu^*(E_n)<+\\infty$ for all $n\\in\\mathbb{N}$, that is for all $n\\in\\mathbb{N}$ exists $\\{I_{n,k}\\}_{k\\in\\mathbb{N}}\\subseteq \\mathcal{K}$ such that $E_n\\subseteq\\bigcup_{k\\in\\mathbb{N}}I_{n,k}$.\nBe fixed $\\varepsilon> 0$. For what has been said above and for the properties of the infimum, for all $n\\in\\mathbb{N}$ exists $\\{I_{n,k}\\}\\subseteq\\mathcal{K}$ such that $$E_n\\subseteq\\bigcup_{k\\in\\mathbb{N}}I_{n,k}\\quad\\text{and}\\quad\\mu^*(E_n)+\\frac{\\varepsilon}{2^n}>\\sum_{k\\in\\mathbb{N}}\\nu(I_{n,k}).$$\nWe observe that $$\\bigcup_{n\\in\\mathbb{N}}E_n\\subseteq\\bigcup_{n\\in\\mathbb{N}}\\bigcup_{k\\in\\mathbb{N}}I_{n,k}=\\bigcup_{(n,k)\\in\\mathbb{N}\\times\\mathbb{N}}I_{n,k}\\quad\\text{and}\\quad\\{I_{n,k}\\}_{n,k\\in\\mathbb{N}}\\subseteq\\mathcal{K}.$$ At this point, in order to apply the definition of $\\mu^*$, it is necessary to specify that it is equivalent to $$\\mu^*(E)=\\inf\\left\\{\\sum_{(n,k)\\in\\mathbb{N}\\times\\mathbb{N}}\\nu(I_{n,k})\\;\\middle|\\;E\\subseteq\\bigcup_{(n,k)\\in\\mathbb{N}\\times\\mathbb{N}}I_{n,k}\\;\\text{and}\\;\\{I_{n,k}\\}_{(n,k)\\in\\mathbb{N}\\times\\mathbb{N}}\\subseteq\\mathcal{K}\\right\\}.$$ In general, what is important is that the set $E$ can be covered by a family $\\{I_\\gamma\\}_{\\gamma\\in\\Gamma}\\subseteq\\mathcal{K}$, where $\\Gamma$ is a countable set of indices. By definition of $\\mu^*$ we have\n\\begin{equation}\n\\mu^*\\bigg(\\bigcup_{n\\in\\mathbb{N}}E_n\\bigg)\\le\\sum_{(n,k)\\in\\mathbb{N}\\times\\mathbb{N}}\\nu(I_{n,k})\\color{BLUE}{=}\\sum_{n\\in\\mathbb{N}}\\sum_{k\\in\\mathbb{N}}\\nu(I_{n,k})<\\sum_{n\\in\\mathbb{N}}\\bigg[\\nu(I_{n,k})+\\frac{\\varepsilon}{2^n}\\bigg]=\\sum_{n\\in\\mathbb{N}}\\mu^*(E_n)+\\varepsilon,\n\\end{equation}\nwhere the blue equal is the Fubini's Theorem.$\\hspace{9cm}\\square$\nProof.(Without Fubini's Theorem) Let $\\{E_n\\}_{n\\in\\mathbb{N}}\\subseteq 2^{X}$ a countable set family. We suppose that $\\mu^*(E_n)<+\\infty$ for all $n\\in\\mathbb{N}$, that is for all $n\\in\\mathbb{N}$ exists $\\{I_{nk}\\}_{k\\in\\mathbb{N}}\\subseteq \\mathcal{K}$ such that $E_n\\subseteq\\bigcup_{k\\in\\mathbb{N}}I_{nk}$.\nBe fixed $\\varepsilon> 0$. For what has been said above and for the properties of the infimum, for all $n\\in\\mathbb{N}$ exists $\\{I_{nk}\\}\\subseteq\\mathcal{K}$ such that $$E_n\\subseteq\\bigcup_{k\\in\\mathbb{N}}I_{nk}\\quad\\text{and}\\quad\\mu^*(E_n)+\\frac{\\varepsilon}{2^n}>\\sum_{k\\in\\mathbb{N}}\\nu(I_{nk}).$$ Let $f\\colon\\mathbb{N}\\to\\mathbb{N}\\times\\mathbb{N}$ be a bijection and we place $I_{n,k}:=I_{nk}$ for all $(n,k)\\in\\mathbb{N}\\times\\mathbb{N}$. We consider the sequence $\\{I_{f(r)}\\}_{r\\in\\mathbb{N}}$. We prove that $\\{I_{f(r)}\\}_{r\\in\\mathbb{N}}\\subseteq\\mathcal{K}$ and that is a covering of $\\bigcup_{n\\in\\mathbb{N}} E_n$.We observe that $$\\bigcup_{r\\in\\mathbb{N}}I_{f(r)}= \\bigcup_{(n,k)\\in\\mathbb{N}\\times\\mathbb{N}}I_{nk}=\\bigcup_{n\\in\\mathbb{N}}\\bigg[\\bigcup_{k\\in\\mathbb{N}}I_{nk}\\bigg]\\supseteq\\bigcup_{n\\in\\mathbb{N}}E_n.$$ By definition of $\\mu^*$ we have $$\\mu^*\\bigg(\\bigcup_{n\\in\\mathbb{N}}E_n\\bigg)\\le\\sum_{r\\in\\mathbb{N}}\\nu(I_{f(r)}).$$ Now we place $$f(r):=(n_r,k_r)\\quad\\text{and}\\quad K_r=\\max\\{k_1,\\dots, k_r\\},$$ and we consider the partial sum s-th. Therefore \n\\begin{equation}\n\\begin{split}\n\\sum_{r=1}^s \\nu(I_{f(r)})=&\\nu(I_{f(1)})+\\cdots+\\nu(I_{f(s)})\\\\\n=&\\nu(I_{n_1k_1})+\\cdots+\\nu(I_{n_sk_s})\\quad\\text{We remember that}\\quad I_{nk}:=I_{n,k}\\\\\n\\color{RED}{\\le}& \\sum_{n=1}^{K_r}\\color{BLUE}{\\sum_{k\\in\\mathbb{N}}\\nu(I_{nk})}\\\\\n<&\\sum_{n=1}^{K_r}\\bigg[\\mu^*(E_n)+\\frac{\\varepsilon}{2^n}\\bigg]\\\\\n<&\\sum_{n\\in\\mathbb{N}}\\bigg[\\mu^*(E_n)+\\frac{\\varepsilon}{2^n}\\bigg]\\\\\n=&\\sum_{n\\in\\mathbb{N}}\\mu^*(E_n)+\\varepsilon.\n\\end{split}\n\\end{equation}\nFor $\\varepsilon\\to 0$ we have $$\\sum_{r=1}^s \\nu(I_{f(r)})\\le\\sum_{n\\in\\mathbb{N}}\\mu^*(E_n).$$\n\nQuestion 1. Why is the inequality in red true?\n\n$$$$\n\nQuestion 2. Could it be that some blue series is divergent?\n\n$$$$\nTherefore $$\\sum_{r\\in\\mathbb{N}}\\nu(I_{f(r)}):=\\lim_{s\\to+\\infty}\\sum_{r=1}^s \\nu(I_{f(r)})\\color{BLUE}{\\le}\\sum_{n\\in\\mathbb{N}}\\mu^*(E_n)$$ \n\nQuestion 3. Why is the inequality in blue true? In general if $\\{a_n\\}$ and $\\{b_n\\}$ are two real number sequence such that $\\lim a_n=a\\in\\mathbb{R}$ and $\\lim b_n=b\\in\\mathbb{R}$, then if $a_n\\le b_n$ for all $n\\in\\mathbb{N}$, then $a<b$. Is this the case? That is, the series $\\sum_{n\\in\\mathbb{N}}\\mu^*(E_n)$ is convergent?\n\nThanks for your patience!\n",
    "proof": "\nQuestion 1. Why is the inequality in red true?\n\nBecause if $a_n\\geq0$ for all $n $, then for any $k $ you have $a_k\\leq\\sum_na_n $.\n\nQuestion 2. Could it be that some blue series is divergent?\n\nNo, you assumed that it was bounded above by $\\mu^*(E_n)+\\varepsilon/2^n $ and that $\\mu^*(E_n)<\\infty $.\n\nQuestion 3. Why is the inequality in blue true? In general if $\\{a_n\\}$ and $\\{b_n\\}$ are two real number sequence such that $\\lim a_n=a\\in\\mathbb{R}$ and $\\lim b_n=b\\in\\mathbb{R}$, then if $a_n\\le b_n$ for all $n\\in\\mathbb{N}$, then $a<b$. Is this the case? That is, the series $\\sum_{n\\in\\mathbb{N}}\\mu^*(E_n)$ is convergent?\n\nAlmost. If $a_n\\leq b_n $ for all $n $, then $a\\leq b $. \nAnd no, bounding a series below tells you nothing about its convergence. And it could very well be the case that  $\\mu^*(\\bigcup_nE_n)=\\infty $ and $\\sum_r\\nu (I_{f (r)}=\\infty $.\n",
    "tags": [
      "measure-theory",
      "proof-verification",
      "proof-writing",
      "proof-explanation"
    ],
    "score": 5,
    "answer_score": 3,
    "is_accepted": true,
    "question_id": 3052899,
    "answer_id": 3052923
  },
  {
    "theorem": "Can any improvements be made to my proof that &quot;$\\sqrt{3} $ is irrational&quot;?",
    "context": "Suppose, for contradiction that $\\sqrt{3}$ is rational. Then there exists $a,b \\in \\mathbb{Z}$ such that $$\\frac{a}{b}= \\sqrt{3},$$\nwhere $a/b$ is in its simplest form. Then the above equation implies $$a^2=3b^2.$$ If $b$ is even, then $a$ is even, which is a contradiction since $a/b$ is therefore not in its simplest form. \nNow, consider $b$ to be odd, then a is odd. Then for $m,n \\in \\mathbb{Z}$, we have $$(2m+1)^2=3(2n+1)^2\\\\ 4m^2+4m+1=12n^2+12n+3\\\\\n2(2m^2+2m)=2(6n^2+6n+1)\\\\2(m^2+m)=2(3n^2+3n)+1.$$\nThe LHS is even since $m^2+m \\in \\mathbb{Z}$ and the RHS is odd since $ 3n^2+3n\\in \\mathbb{Z}$. This is a contradiction, and we therefore conclude that $\\sqrt{3}$ is irrational.\n",
    "proof": "Your proof looks correct to me. Instead of doing the algebra at the end, you could reduce the equation $a^2 = 3b^2$ modulo $4$. If $a$ and $b$ are both odd, then $$a^2 \\equiv 1 \\mod 4,$$\nand \n\\begin{align*}\nb^2 &\\equiv 1\\mod 4\\\\\n3b^2 &\\equiv 3\\mod 4,\n\\end{align*}\ncontradiction.\nAnother approach is to note that in the prime factorization of $a^2 = 3b^2$, the power of $3$ dividing the left hand side is even, while the power of $3$ dividing the right hand side is odd. \n",
    "tags": [
      "proof-verification",
      "proof-writing"
    ],
    "score": 5,
    "answer_score": 3,
    "is_accepted": true,
    "question_id": 3047840,
    "answer_id": 3047844
  },
  {
    "theorem": "Prove that $f(n) \\geq n$",
    "context": "Let $f$ be a function from $\\mathbb{N}$ to $ \\mathbb{N}$ such that $\\forall n \\in \\mathbb{N}$, $f(f(n)) <f(n+1)$.\nProve that $\\forall k \\geq n$, $f(k) \\geq n$.\nI've put much time and effort to solve this but unfortunately couldn't.\nI tried to prove a simpler version $\\forall n\\geq 0$, $f(n) \\geq n$.\nfor $n = 0$, $f(0) \\geq 0$ because it's absurd otherwise.\nWe can use the same idea to prove that $f(1) \\geq 1$ and so on, but as each time you have to find $n$ contredictions. I tried to use recursion, but, you know, I failed.\nCan I get some help/hints ? Thanks :D\n",
    "proof": "I think that the simpler version is actually harder to prove than the original question. We can prove the original statement with induction to $n$:\nBase case: For $n = 0$ it clearly holds, because $f(k) \\geq 0$ for all $k \\in \\mathbb{N}$.\nInductive step: Suppose that it holds for all $n = m$ with $m \\in \\mathbb{N}$. Let now $l \\geq m+1$, so $l-1 \\geq m$. Because the statement holds for $m$, we have $f(l-1) \\geq m$. Now we can apply the inductive hypothesis also for $k = f(l-1)$, and we get $f(f(l-1)) \\geq m$. Now it follows $f(l) > f(f(l-1)) \\geq m$. This is exactly the statement we wanted to prove for $n = m+1$.\n",
    "tags": [
      "proof-writing"
    ],
    "score": 5,
    "answer_score": 3,
    "is_accepted": true,
    "question_id": 2447754,
    "answer_id": 2447801
  },
  {
    "theorem": "Equivalent condition for mixing",
    "context": "Let $(X,\\mathscr{A},\\mu)$ be a measurable space and $T$ measure preserving map and define:\n$T$ is mixing iff $\\lim_{n\\to\\infty}\\mu(A\\cap T^{-n}B)=\\mu(A)\\mu(B)$ for all $A,B\\in\\mathscr{A}$.\nHow to prove that mixing property is equivalent with:\n$\\lim_{n\\to\\infty}\\langle U_T^n f,g\\rangle=\\langle f,1\\rangle\\langle 1,g\\rangle$ for all $f,g$ in dense subset of $L^2(\\mu)$?\nHere, $(U_Tf)(x)=f(Tx)$. \nOne implication is direct (take characteristic functions for $f$ and $g$), but I'm having trouble to prove the second one. \nAny help or hint is welcome. Thanks in advance. \n",
    "proof": "We denote the two statements we are interested in as follows, \n\nMixing: A measure space $(X,\\mathscr{A},\\mu)$ with measure preserving map $T:X\\rightarrow X$ is said to be mixing if for every $A,B\\in \\mathscr{A}$, $$\\lim_{n\\to\\infty}\\mu(A\\cap T^{-n}B)=\\mu(A)\\mu(B).$$\nAlternate Mixing: A measure space $(X,\\mathscr{A},\\mu)$ with measure preserving map $T:X\\rightarrow X$ is said to be alternate mixing if for every $f,g\\in D\\subseteq L^2(\\mu)$,\n  $$\\lim_{n\\to\\infty}\\langle U_T^n f,g\\rangle=\\lim_{n\\to\\infty}\\langle f(T^n),g\\rangle=\\langle f,1\\rangle\\langle 1,g\\rangle=\\mathbb{E}(f)\\mathbb{E}(g).$$ Where $D$ is a dense subset of $L^2(\\mu)$.\n\nAs stated by the OP, the implication (Alternate Mixing)$\\implies$(Mixing) follows easily by considering indicator functions.\nFor the other implication, we will need the following lemma that follows from Lemma $3.13$ of Rudin's Real and Complex \nAnalysis, $3$rd Ed, p. $69$. \n\nLemma Let $(X,\\mathscr{A},\\mu)$ be any measure space. Then the set $S$ of all simple functions with finite support are dense in $L^2(\\mu)$.\n\n(Mixing)$\\implies$(Alternate Mixing)\nAssume that the statement of the mixing definition holds true. Take any $r,t\\in S$. We define,\n $$ r(x)=\\sum^n_{i=1}a_i\\mathbf{1}_{A_i}(x)\\qquad t(x)=\\sum^m_{j=1}b_j\\mathbf{1}_{B_j}(x)$$\nWhere $\\bigcup_{i=1}^n{A_i}\\subseteq X$ and $\\bigcup_{j=1}^m{B_j}\\subseteq X$ and also $\\{a_i\\}_{i=1}^n\\cup\\{b_j\\}_{j=1}^m\\subseteq \\mathbb{R}$.\nConsider then, \n$$\\langle r(T^n),t\\rangle=\\int_X\t\\left(\\sum_{i}a_i\\mathbf{1}_{T^{-n}A_i}\\right)\t\\left(\\sum_{j}b_j\\mathbf{1}_{B_j}\\right)d\\mu$$\n$$=   \\int_X\t\\sum_{i,j}a_ib_j\\ \\mathbf{1}_{T^{-n}A_i\\cap B_j}\\ d\\mu.$$\nBy the linearity of the integral, \n$$\\langle r(T^n),t\\rangle=\\sum_{i,j}a_ib_j\t\\int_X\\mathbf{1}_{T^{-n}A_i\\cap B_j}\\ d\\mu=\\sum_{i,j}a_ib_j\\ \\mu(T^{-n}A_i\\cap B_j).$$\nTherefore, \n$$\\lim_{n\\rightarrow\\infty}\\langle r(T^n),t\\rangle=\\lim_{n\\rightarrow\\infty}\\sum_{i,j}a_ib_j\\ \\mu(T^{-n}A_i\\cap B_j)=\\sum_{i,j}a_ib_j\\ \\lim_{n\\rightarrow\\infty}\\mu(T^{-n}A_i\\cap B_j).$$\nBy our assumption, we have that,\n$$\\lim_{n\\to\\infty}\\langle U_T^n f,g\\rangle=\\lim_{n\\rightarrow\\infty}\\langle r(T^n),t\\rangle=\\sum_{i,j}a_ib_j\\mu(A_i)\\mu(B_j)=\\left(\\sum_{i}a_i\\mu(A_i)\\right)\\left(\\sum_{j}b_j\\mu(B_j)\\right)$$ \n$$=\\mathbb{E}(r)\\mathbb{E}(t)=\\langle r,1\\rangle\\langle 1,t\\rangle.$$\nAnd the required result follows. \nWe can use the result we have just proven to prove that the alternate mixing definition holds true on all of $L^2(\\mu)$. \nThis will follow from the fact that any $f,g\\in L^2(\\mu)$ can be approximated arbitrarily well by two sequences of simple measurable functions in $S$. Using these sequences, the above argument can be modified to prove the general result. \n",
    "tags": [
      "proof-writing",
      "operator-theory",
      "ergodic-theory",
      "mixing"
    ],
    "score": 5,
    "answer_score": 3,
    "is_accepted": true,
    "question_id": 2279246,
    "answer_id": 2292619
  },
  {
    "theorem": "Prove that every converging limit $\\lim_{n \\to \\infty} \\sum_{k=1}^{a(n)} f(k,n)$ is essentially a riemann sum.",
    "context": "Let $a(n)$ be a strictly increasing function of $n$.\nProof that every converging limit \n$$\\lim_{n \\to \\infty} \\sum_{k=1}^{a(n)} f(k,n)$$\nis essentially a Riemann sum.\n",
    "proof": "Providing a complete answer requires clarity on what is meant by \"essentially\" a Riemann sum and, most likely, narrowing the class of functions under consideration. Without making that attempt, I think examining a few cases might shed some light on whether this question can be answered in the most general form.\nI am speculating you mean that computation of the limit can be reduced to the evaluation of a definite Riemann integral, either (1) directly as the limit of a Riemann sum or (2) in a more general limiting process where one step involves the limit of a Riemann sum.\nAs an example of category (1), consider $a(n) = n$ and $f(k,n) = k/(n^2 + k^2)$ where we have a bona fide Riemann sum,\n$$\\lim_{n \\to \\infty}\\sum_{k=1}^n \\frac{k}{n^2 + k^2} = \\lim_{n \\to \\infty} \\frac{1}{n}\\sum_{k=1}^n \\frac{k/n}{1 + (k/n)^2} = \\int_0^1 \\frac{x}{1 + x^2} \\, dx = \\frac{\\log 2}{2} $$\nWith a slight modification, where $a(n) = n$ and $f(k,n) = k/(n^2 + k),$ we have an example from category (2), \n$$\\lim_{n \\to \\infty}\\sum_{k=1}^n \\frac{k}{n^2 + k} = \\lim_{n \\to \\infty} \\frac{1}{n}\\sum_{k=1}^n \\frac{k/n}{1 + k/n^2} $$\nIn this case, we no longer have a Riemann sum.  The limit can be shown to be $1/2\\,$ by various means. However, we can reintroduce Riemann sums by evaluating as an iterated  limit where the inner limit involves the sum,\n$$\\begin{align} \\lim_{n \\to \\infty}\\sum_{k=1}^n \\frac{k}{n^2 + k} &= \\lim_{m \\to \\infty} \\lim_{n \\to \\infty} \\frac{1}{n}\\sum_{k=1}^n \\frac{k/n}{1 + (k/n)(1/m)} \\\\ &=  \\lim_{m \\to \\infty} \\int_0^1 \\frac{x}{1 + x/m} \\, dx  \\\\ &= \\int_0^1 x \\, dx  \\\\ &= \\frac{1}{2} \\end{align}.$$\nThe steps of conversion to a double limit and passing the limit under the integral can be justified in this case using uniform and dominated convergence, respectively. \nThis raises the question of what general characteristics of $f$  where $\\sum f(k,n)$ converges would permit such steps.  I suspect that if $f$ is monotone, then this may work in many more cases. \nFinally, we should address the general form for the upper limit $a(n)$ which you specify as strictly increasing and, presumably, tending to $+\\infty$.  Here we find problems arising in terms of uniqueness.\nAn example is,\n$$\\lim_{n \\to \\infty} \\sum_{k=1}^{a(n)} \\frac{n}{n^2 + k^2} = \\lim_{n \\to \\infty} \\frac{1}{n}\\sum_{k=1}^{a(n)} \\frac{1}{1 + (k/n)^2} \\\\ = \\begin{cases} 0, \\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\, a(n) = \\sqrt{n} \\\\ \\pi/4, \\,\\,\\,\\,\\,a(n) = n\\\\ \\pi/2, \\,\\,\\,\\,\\, a(n) = n^2\\end{cases}.$$\nIn the case of $a(n) = n^2,$ this becomes an improper Riemann integral.\n",
    "tags": [
      "calculus",
      "limits",
      "summation",
      "proof-writing",
      "riemann-sum"
    ],
    "score": 5,
    "answer_score": 3,
    "is_accepted": true,
    "question_id": 2221977,
    "answer_id": 2225410
  },
  {
    "theorem": "Prove that $\\pm1\\pm2\\pm\\ldots\\pm(4n+1)$ yields all odd numbers up to $(2n+1)(4n+1)$",
    "context": "Problem: Prove that for different choices of signs $+$ and $-$ the expression $$\\pm1\\pm2\\pm3\\pm4\\pm5...\\pm(4n+1)$$ yields all odd positive integers less than or equal to $(2n+1)(4n+1).$\nMy Attempt: Let $n=1$, then we have the following expression to work with $$\\pm1\\pm2\\pm3\\pm4\\pm5.$$\nLet $+$ be represented by $0$ and $-$ be represented by $1.$ A binary string of length $5$ therefore represents the permutation of the operations. So $$+1+2+3+4+5\\text{ is equivalent to }00000$$ \n$$+1-2+3+4+5\\text{ is equivalent to }01000$$ and so on. With this representation we observe that:\n$$15=00000$$\n$$13=10000$$\n$$11=01000$$\n$$9=00100$$\n$$7=00010$$\n$$5=00001$$\n$$3=10001$$\n$$1=01001.$$ \nNotice how $1$ traverses in each number. Ignoring the trivial case when the odd number is equal to $(2n+1)(4n+1)$ (which in this case is $15$) we observe that there are $4*1+1$ numbers that can be written with only one $1$ since the numeral $1$ has $4*1+1$ places to move on. The remaining places $$\\frac{1*(1+3)}{2}=2$$ require a two $1$s. \nThis observation motivates the following Proof: Given any $n\\geq 1$ we find $4n+1.$ Then we have $4n^2+3n$ odd numbers that are strictly less than $(2n+1)(4n+1).$ Let $a_i$ denote these odd numbers where $1\\leq i\\leq 4n^2+3n$ and $a_1<a_2<....<a_{4n^2+3n}.$ We begin by covering the last $4n+1$ odd numbers by writing \n$$a_{4n^2+3n}=\\underbrace{1000000...0}_{4n+1}=8n^2+6n-1$$\n$$..$$\n$$a_{4n^2+3n-(4n)}=00000...1=8n^2+2n+1$$\n$$a_{4n^2+3n-(4n+1)}=\\underbrace{10000...1}_{4n+1}=8n^2+2n-1$$\n$$..$$\nand so on. Essentially $4n^2+3n=(4n+1)+(4n)+(4n-1)+(4n-2)+...+(4n-(n-1))+\\underbrace{\\frac{n(3+n)}{2}}_{\\text{need }n+1\\text{ 1s}}=4n^2+3n.$\nNow I don't know how to compile these observations into a formal proof. What proof technique would be best suited in this scenario, maybe induction? Moreover, should I inclucde Lemmas regarding the zero and one arrangments?\n",
    "proof": "Note that $1+2+3+...+(4n+1)=(2n+1)(4n+1)$ $(1)$ and that is a odd number. So if we sum $-2$ at both sides we get:\n$$-1+2+3+4+5+...+(4n+1)= (2n+1)(4n+1)-2$$\nIf we sum $-4$ at both sides of $(1)$ we get:\n$$1-2+3+4+5+...+(4n+1)= (2n+1)(4n+1)-4$$\nIf we sum $-6$ at both sides of $(1)$ we get:\n$$1+2-3+4+5+...+(4n+1)= (2n+1)(4n+1)-6$$\nAnd we just follow that algorithm until we have to sum $-2(4n+1)$ and get\n$$1+2+3+4+5+...-(4n+1)= (2n+1)(4n+1)-2(4n+1)$$\nNow we have to sum $-2-2(4n+1)$ in order to get the next odd number:\n$$-1+2+3+4+5+...-(4n+1)= (2n+1)(4n+1)-2(4n+1)-2$$\nWe can see now what is the algorithm that we have to follow in order to get all odd numbers from $1$ to $(2n+1)(4n+1)$. \n",
    "tags": [
      "number-theory",
      "proof-writing"
    ],
    "score": 5,
    "answer_score": 3,
    "is_accepted": true,
    "question_id": 2027405,
    "answer_id": 2027462
  },
  {
    "theorem": "how to prove $G$ is an abelian group under $*$ (called the real numbers mod 1)",
    "context": "Let $G = \\{x \\in \\mathbb{R}~|~0\\leq x < 1\\}$ and for $x,y \\in G$ let $x*y$ be the fractional part of $x+y$ i.e $x*y = x + y - [x + y]$ where $[a]$ is the greatest integer less than or equal to $a$. I need help proving $*$ is a well defined binary operation on $G$ and that $G$ is an abelian group under $*$.\nTo prove that $*$ is well defined, I rely on the assumption that +,- is well defined in the set of real numbers. evaluating the brackets [] is also well-defined in the set of real numbers. (is this an okay assumption?)\nTo prove $*$ is associative, (I imagine because +,- is associative, so will *?) I show that $(x*y)*z = x*(y*z)$ $$(x*y)*z =(x+y-[x+y])*z = x+y-[x+y]+z - [x+y-[x+y]+z]$$ $$x*(y*z) = x*(y+z - [y+z]) = x + y + z - [y+z] - [x+y+z - [y+z]]$$ rearranging $$(x*y)*z = x+y+z-[x+y]-[x+y+z-[x+y]]$$ $$x*(y*z) = x+y+z-[y+z]-[x+y+z-[y+z]]$$ I am not so sure that these two are equal. I don't see how distributing the $-$ to remove the terms $[x+y]$ and $[y+z]$ is fair (ex: $x = y = \\frac{1}{2}$ so $x*y = .5+.5-[.5]-[.5]$ yields a different answer than $x*y = .5+.5-[.5+.5]$\nThe identity element would be $0$, the inverse would have to be $-x$ which isn't in $G$. What am I doing wrong?\nedit: commutativity would be proven by showing $x*y = y*x$ which is easy to show $x+y - [x+y] = y+x - [y+x]$ right?\n",
    "proof": "You can think of your group $(G, *)$ as $(\\mathbb{R}, +)/\\mathbb{Z}$: That is, the reals under addition modded out by the integers. \nThis clearly works because we are essentially saying that is $x, y \\in \\mathbb{R}$, $x - y \\in \\mathbb{Z}$, then $x = \\mathbb{Z} + y$ in $\\mathbb{R} /\\mathbb{Z}$.\nThis has the same effect as “truncation”, but is now a nice subgroup\nHence, \n$$\n(G, *) \\simeq (\\mathbb{R}/\\mathbb{Z}, +)\n$$\nTo imagine this, realize that $\\mathbb{R}$ just contains shifted copies of $[0, 1)$ all laid down one next to the other. So, we simply “collapse” all these copies together for this construction to work out.\nNow, for this to be a group, we need the kernel of the map $$\\phi: (\\mathbb{R}, +) \\to (\\mathbb{R}, +) /\\mathbb{Z}$$\nto be a normal subgroup. Since the kernel of a quotient map is the quotienting subgroup itself, we simply need to\nshow that $\\mathbb{Z}$ is a normal subgroup of $\\mathbb{R}$. This is obvious in abelian groups.\n",
    "tags": [
      "proof-writing",
      "binary-operations",
      "associativity"
    ],
    "score": 5,
    "answer_score": 0,
    "is_accepted": true,
    "question_id": 1784169,
    "answer_id": 1784180
  },
  {
    "theorem": "Theorem 2.27 (a) in Baby Rudin: Is his proof complete enough?",
    "context": "Here's Theorem 2.27 (a) in the book Principles of Mathematical Analysis by Walter Rudin, 3rd edition: \n\nIf $X$ is a metric space and $E \\subset X$, then $\\overline{E}$ is closed. \n\nNow here's Rudin's proof: \n\nIf $p \\in X$ and $p \\not\\in \\overline{E}$ then $p$ is neither a point of $E$ nor a limit point of $E$. Hence $p$ has a neighborhood which does not intersect $E$. The complement of $\\overline{E}$ is therefore open. Hence $\\overline{E}$ is closed. \n\nIs the above proof good enough, especially at the level Rudin is intended for?\nNow here's the proof I propose: \n\nIf $p \\in X$ and $p \\not\\in \\overline{E}$ then $p$ is neither a point of $E$ nor a limit point of $E$. Hence $p$ has a neighborhood which does not intersect $E$. Let $N_\\epsilon (p)$ be this neighborhood. \nNow we show that no point of $N_\\epsilon (p)$ can be in $\\overline{E}$. Let $q \\in N_\\epsilon (p)$. Then $d(q,p) < \\epsilon$, where $d$ denotes the metric on $X$. \nLet $\\delta \\colon= \\epsilon - d(p,q)$. Then $0 < \\delta < \\epsilon$. Now if $a \\in N_\\delta (q)$, then $d(a,q) < \\delta = \\epsilon - d(q,p)$, which implies that $$d(a,p) \\leq d(a,q) + d(q,p) < \\epsilon,$$\n  and so $a \\in N_\\epsilon (p)$. \nThus we have shown that $N_\\delta (q) \\subset N_\\epsilon (p)$. Since \n  $ N_\\epsilon (p) \\cap E = \\emptyset$, we have $N_\\delta (q) \\cap E = \\emptyset$ as well. That is, the point  $q$ has a neighborhood --- namely  $N_\\delta (q)$ --- which does not intersect $E$ at all. So $q \\not\\in \\overline{E}$. \nBut $q$ was an arbitrary point in $N_\\epsilon (p)$. So $N_\\epsilon (p) \\subset \\left( \\overline{E} \\right)^c$. \nBut $p$ was an arbitrary point in $\\left( \\overline{E} \\right)^c$. Thus, we can conclude that every point of $\\left( \\overline{E} \\right)^c$ is an interior point. Hence $\\left( \\overline{E} \\right)^c$ is open. \n\nNow is my proof any better than Rudin's? Are there any extra advantages to be had from inclusion or exclusion of extra details?\n",
    "proof": "I would say that your proof is better. I would assume that he missed a detail, and left out a proof that if it holds for $E$ then it holds for $\\overline{E}$. Certainly, if I were grading a course I would mark his proof as incomplete - even in a course not for first or second years. Especially since his book is a standard introductory text for first and second years, I think this oversight is problematic.\nAs a commenter notes, authors have a habit of increasing the details they omit as time goes on, out of a combination of laziness, a desire to save space, and a desire to write less, but at chapter 2 of an intro book rigor should be the standard.\n",
    "tags": [
      "real-analysis",
      "analysis",
      "metric-spaces",
      "proof-writing",
      "education"
    ],
    "score": 5,
    "answer_score": 2,
    "is_accepted": false,
    "question_id": 1632147,
    "answer_id": 1632164
  },
  {
    "theorem": "Proof of existing degree $n$ binomial",
    "context": "\nLet $P(x)$ be a polynomial with real coefficients such that $P(x) > 0$ for all $x \\ge 0$. Prove that there exists a positive integer $n$ such that $(x + 1)^n P(x)$ is a polynomial with nonnegative coefficients.\n\nHINTS ONLY.\nI tried the binomial theorem with a quadratic case with:\n$f(x) = (x+1)^n (x^2 + 2x +1) = \\sum_{k=0}^{n} \\binom{n}{k} (x^{k+2} - x^{k+1} + 80x^k)$\nBut nothing really past this.\n",
    "proof": "Let ${\\cal S}$ be the set of  polynomials $P$ such that there exists a positive integer $n$ such that the coefficients of $(1+X)^nP$ are all positive. \n\n${\\cal S}$ is closed under polynomial multiplication, since  the set of polynomials with positive coefficients is closed under polynomial multiplication.\n${\\cal S}$ contains constant positive polynomials.\n${\\cal S}$ contains all polynomials of the form $X+\\lambda$ with $\\lambda>0$.\n${\\cal S}$ contains all polynomials of the form $X^2-2\\alpha X+\\beta$ with $\\alpha^2<\\beta$. Indeed, there is nothing to be proved \nif $\\alpha\\leq0$ so we may suppose that $\\alpha>0$. Now, let us consider\n$$\n(X+1)^n(X^2-2\\alpha X+\\beta)=X^{n+2}+\\sum_{k=0}^{n}c_kX^{k+1}+\\beta\n$$\nwith \n$$ c_k=\\binom{n}{k-1}-2\\alpha \\binom{n}{k}+\\beta\\binom{n}{k+1}\n=\\binom{n}{k}\\left(\\frac{k}{n-k+1}+\\beta\\frac{n-k}{k+1}-2\\alpha\n\\right)$$\nthat is $c_k=\\binom{n}{k}F\\left(\\frac{k}{n}\\right)$ where\n$$F(x)=\\frac{x}{1+\\frac{1}{n}-x}+\\beta \\frac{1-x}{\\frac{1}{n}+x}-2\\alpha,\\quad\\hbox{for $0\\leq x\\leq 1$.} $$\nNow, suppose that $n>\\max\\left(\\sqrt{\\beta},\\sqrt{1/\\beta}\\right) $, so that $F$ attains its minimum on the interval $[0,1]$ at\n$$x_0=\\frac{\\sqrt{\\beta}(1+n)-1}{ (1+\\sqrt{\\beta})n}$$ and the minimum of $F$ on $[0,1]$ is\n$$F(x_0)=2(\\sqrt{\\beta}-\\alpha)-\\frac{(1+\\sqrt{\\beta})^2}{n+2}$$\nIt follows that if\n$$n> n_0= \\max\\left(\\frac{(1+\\sqrt{\\beta})^2}{2(\\sqrt{\\beta}-\\alpha)},\\sqrt{\\beta },\\frac{1}{\\sqrt{\\beta}} \\right)$$\nthen $F(x)$ is positive for every $x\\in[0,1]$. That is $c_k>0$ for $0\\leq k\\leq n$. \nSo, we have shown that $(1+X)^n(X^2-2\\alpha X+\\beta)$ has \npositive coefficients if $n$ is large enough and the proof of this point is complete.\n${\\cal S}$ contains all polynomials having no nonnegative zeros. Indeed, such a polynomial is \nthe product of polynomials each one of them has one of the  forms  discussed earlier. The desired conclusion follows.\n\n",
    "tags": [
      "real-analysis",
      "algebra-precalculus",
      "analysis",
      "proof-writing",
      "contest-math"
    ],
    "score": 5,
    "answer_score": 3,
    "is_accepted": true,
    "question_id": 1513086,
    "answer_id": 1513196
  },
  {
    "theorem": "Proof that the $\\lim\\limits_{x \\to 2}\\dfrac{1}{x} = \\dfrac{1}{2}$ using the $\\epsilon$-$\\delta$ definition of limits (verification).",
    "context": "Prove that the $\\lim\\limits_{x \\to 2}\\dfrac{1}{x} = \\dfrac{1}{2}$ using the $\\epsilon-\\delta$ definition of limits.\n$$\n\\\\ \\begin{align}\n\\\\ &\\textrm{Let } \\forall \\epsilon > 0\n\\\\ &\\textrm{Choose } \\delta = \\min{\\{1, 2\\epsilon \\}}\n\\\\ &\\textrm{Assume } 0 < |x - 2| < \\delta :\n\\\\ \\end{align}\n$$\n$$\n\\\\ \\begin{align}\n\\\\ \\left|\\frac{1}{x} - \\frac{1}{2}\\right| &< \\epsilon\n\\\\ \\frac{|2 - x|}{|2x|} &< \\epsilon\n\\\\ |-1(x - 2)| &< \\epsilon|2x|\n\\\\ |x - 2| &< \\epsilon|2x|\n\\\\ \\end{align}\n$$\n$$\n\\\\ \\begin{align}\n\\\\ |x - 2| &< 1\n\\\\ -1 < x - 2 &< 1\n\\\\ 1 < x &< 3\n\\\\ \\end{align}\n$$\n$$\n\\\\ \\begin{align}\n\\\\ |x - 2| &< \\epsilon|2(1)|\n\\\\ |x - 2| &< 2\\epsilon\n\\\\ \\end{align}\n$$\n$$\n\\\\\\therefore \\delta \\leq 2\\epsilon\n$$\nRight, so I start by taking $|f(x)−L|<ϵ$. I then isolate $|x−2|$ to the left. I then limit $|x−2|$ to be less than one and then find a range of $x$ values which satisfy the inequality. Then I plug in the smallest $x$ value to minimise the value of $ϵ$. I make the conclusion that $δ≤2ϵ$. Am I excluding or misplacing steps? I'm fairly new to this whole thing.\n",
    "proof": "Here you have to show that for each $ \\epsilon >0$, there exists $ \\delta >0 $ such that for each $ x\\in Domn(\\frac{1}{x}) $ if $ 0<|x-2|<\\delta $ then $ \\left | \\frac{1}{x}-\\frac{1}{2}\\right |<\\epsilon $.\nSo begin with arbitrary $ \\epsilon >0 $. \nNotice that if $ 0<|x-2|<1 $ then $ 1<|x|<3 $ and hence $\\frac{1}{3}<\\frac{1}{|x|}<1$.\nNow choose $ \\delta =\\min\\{1,2\\epsilon\\} $. Then clearly $ \\delta >0 $.\nNow suppose $ 0<|x-2|<\\delta $.\nThen $ \\left | \\frac{1}{x}-\\frac{1}{2}\\right |=\\frac{|x-2|}{2|x|}<\\frac{|x-2|}{2}<\\frac{2\\epsilon}{2}=\\epsilon $.\nTherefore $$ \\lim_{x\\rightarrow 2}\\frac{1}{x}=\\frac{1}{2} .$$\n",
    "tags": [
      "real-analysis",
      "limits",
      "proof-verification",
      "proof-writing",
      "epsilon-delta"
    ],
    "score": 5,
    "answer_score": 2,
    "is_accepted": true,
    "question_id": 1147648,
    "answer_id": 1147679
  },
  {
    "theorem": "Proving that if $f$ is Riemann integrable and $1/f$ is bounded then $1/f$ is Riemann integrable",
    "context": "I have to prove the following \nSuppose  $f$ is Riemann integrable on $[a,b]$ and $1/f$ is bounded on $[a,b]$. Prove that  $1/f$  is Riemann integrable on $[a,b]$.\nMy attempt:\nSince $1/f$ is bounded we have that: $p \\ge1/f \\ge M $ therefore  $p \\le f \\le M $. Then we know that:\n$$|1/f(a) -1/f(b)|=|\\frac{f(a)-f(b)}{f(a)f(b)}|\\leq|\\frac{f(a)-f(b)}{p^{2}}| $$\nthen we have that \n$$U(P,1/f)-L(P,1/f) \\leq 1/p^{2}U(P,f)-L(P,f) \\leq \\epsilon/p^{2} \\leq \\epsilon $$\nAm I right? and if not, Can you help me to fix the mistakes? I will go to sleep because here in my country is very late, but I promest that tomorrow I'll check the answers and I tell you if I get stuck :), I will appreciate the help  you give thank you :) \n",
    "proof": "Everything looks good.  Except we could have $p<1$ and so $\\epsilon/p^{2}<\\epsilon$ is not necessarily true, but irrelevant for the conclusion.\n\nLet $P=\\{x_{0},x_{1},\\ldots,x_{n}\\}$ be a partition of $[a,b]$ such that $|U(P,f)-L(P,f)|<\\epsilon$ and put $g:=1/f$.  Since $$M^{g}_{j}=\\frac{1}{m^{f}_{j}}$$ and $$m^{g}_{j}=\\frac{1}{M^{f}_{j}},$$\nand all of these quantities are finite by hypothesis, we have\n$$|U(P,g)-L(P,g)|\\leq\\sum_{j=1}^{n}|M^{g}_{j}-m^{g}_{j}|\\Delta x_{j}=\\sum_{j=1}^{n}\\left|\\frac{M^{f}_{j}-m_{j}^{f}}{m^{f}_{j}M^{f}_{j}}\\right|\\Delta x_{j}\\leq (m_{f})^{-2}|U(P,f)-L(P,f)|\\leq (m_{f})^{-2}\\epsilon.$$\n",
    "tags": [
      "real-analysis",
      "proof-verification",
      "proof-writing"
    ],
    "score": 5,
    "answer_score": 3,
    "is_accepted": true,
    "question_id": 991251,
    "answer_id": 991285
  },
  {
    "theorem": "Prove that $\\lim_{\\Delta x\\to 0} \\frac{\\Delta ^{n}f(x)}{\\Delta x^{n}} = f^{(n)}(x).$",
    "context": "If $\\Delta f(x)=f(x+\\Delta x)-f(x)$, $(a)$ prove that $$\\Delta\\{\\Delta f(x)\\}=\\Delta^2f(x)=f(x+2\\Delta x)-2f(x+\\Delta x)+f(x);$$ $(b)$ derive an expression for $\\Delta^n f(x)$ where $n$ is any positive integer; and $(c)$ show that $$\\lim\\limits_{\\Delta x\\to0}\\dfrac{\\Delta^n f(x)}{(\\Delta x)^n}=f^{(n)}(x)$$ if this limit exists.\nI was able to prove $(a)$, and this is the expression I derived for $(b)$\n$$\\Delta ^{n}f(x)=\\sum_{i=0}^{n}(-1)^{n-i}\\binom{n}{i}f(x+i\\Delta x)$$\nI an fairly sure that the above is correct.\nHowever, I am not sure how to prove,\n$$\\lim_{\\Delta x\\rightarrow 0} \\frac{\\Delta ^{n}f(x)}{\\Delta x^{n}} = \\lim_{\\Delta x\\rightarrow 0} \\frac{\\sum_{i=0}^{n}(-1)^{n-i}\\binom{n}{i}f(x+i\\Delta x)}{\\Delta x^{n}} = f^{(n)}(x)$$\n",
    "proof": "To elaborate on the my comment, I am posting an answer. Let's us take the case of $n = 2$ then we know that $$\\Delta^{2} f(x) = f(x + 2\\Delta x) - 2f(x + \\Delta x) + f(x)$$ and hence\n$\\displaystyle \\begin{aligned}\\lim_{\\Delta x \\to 0}\\frac{\\Delta^{2}f(x)}{\\Delta x^{2}} &= \\lim_{\\Delta x \\to 0}\\frac{f(x + 2\\Delta x) - 2f(x + \\Delta x) + f(x)}{\\Delta x^{2}}\\\\\n&\\text{(apply L'Hospital Rule, differentiation wrt } \\Delta x, x \\text{ is constant)}\\\\\n&= \\lim_{\\Delta x \\to 0}\\frac{2f'(x + 2\\Delta x) - 2f'(x + \\Delta x)}{2\\Delta x}\\\\\n&= \\lim_{\\Delta x \\to 0}\\frac{f'(x + 2\\Delta x) - f'(x + \\Delta x)}{\\Delta x}\\end{aligned}$\nNow we need the assumption of existence of $f^{(n)}(x) = f''(x)$ here. We have\n$$ f''(x) = \\lim_{\\Delta x \\to 0}\\frac{f'(x + \\Delta x) - f'(x)}{\\Delta x}$$ hence we can see that $f'(x + \\Delta x) = f'(x) + \\Delta x\\{f''(x) + \\rho\\}$ where $\\rho$ denotes an expression which tends to $0$ with $\\Delta x$. Replacing $\\Delta x$ by $2\\Delta x$ we see that $f'(x + 2\\Delta x) = f'(x) + 2\\Delta x\\{f''(x) + \\rho'\\}$ where $\\rho'$ is another expression which tends to $0$ with $\\Delta x$. It now follows that $$f'(x + 2\\Delta x) - f'(x + \\Delta x) = \\Delta x\\{f''(x) + 2\\rho' - \\rho\\}$$ and therefore $$\\frac{f'(x + 2\\Delta x) - f'(x + \\Delta x)}{\\Delta x} = f''(x) + 2\\rho' - \\rho$$ Taking limits when $\\Delta x \\to 0$ we get $$\\lim_{\\Delta x \\to 0}\\frac{f'(x + 2\\Delta x) - f'(x + \\Delta x)}{\\Delta x} = f''(x)$$ and therefore we can see that $$\\lim_{\\Delta x \\to 0}\\frac{\\Delta^{2}f(x)}{\\Delta x^{2}} = f''(x)$$ The above proof can be carried in the same manner for any value of $n$ by repeated application of L'Hospital's rule, but at the last step when the denonominator $\\Delta x^{n}$ reduces to $\\Delta x$ we must make use of the existence of $f^{(n)}(x)$ the way explained above. In this connection please observe that we can't use L'Hospital rule at the last step to change $\\Delta x $ in denominator to $1$ because L'Hospital Rule assumes that both numerator and denominator are differentiable in a neighborhood of the point under consideration (except possibly at that point). Hence we have to make use of the existence of $f^{(n)}(x)$ at a single point $x$ in the manner I have indicated above.\nNote: The above solution would look better and more understandable if we replace $x$ by $a$ and $\\Delta x$ by $h$, but I have tried to keep the notation aligned with that of OP.\n",
    "tags": [
      "calculus",
      "derivatives",
      "proof-writing"
    ],
    "score": 5,
    "answer_score": 3,
    "is_accepted": false,
    "question_id": 543105,
    "answer_id": 543772
  },
  {
    "theorem": "Is this a valid proof of the contrapositive?",
    "context": "The question is the following: if $a$ and $b$ are distinct group elements, then either $a^2 \\neq b^2$ or $a^3 \\neq b^3$. I find this difficult to prove directly, so I formulated the contrapositive to be: if both $a^2 = b^2$ and $a^3 = b^3$, then $a = b$. \nThe proof is simple: Suppose that $a^2 = b^2$ and $a^3 = b^3$. Then $a^3 = b^3 \\Rightarrow a^2 = a^{-1}b^3 \\Rightarrow a^{-1}b^3 = b^2$ which then implies that   $\\Rightarrow a^{-1}b^2 = b \\Rightarrow a^{-1}b = e \\Rightarrow b = a$.\nIs this correct? I feel like there's a hole somewhere.\n",
    "proof": "As the comments have pointed out, you've done your proof correctly. Good job! Here's how you can organize the steps into a single chain of equalities:\n$$\na = a^3a^{-2} = (a^3)(a^2)^{-1} = (b^3)(b^2)^{-1} = b^3b^{-2}=b\n$$\n",
    "tags": [
      "group-theory",
      "proof-writing"
    ],
    "score": 5,
    "answer_score": 3,
    "is_accepted": true,
    "question_id": 504317,
    "answer_id": 504324
  },
  {
    "theorem": "Irrationality of numbers that are the sum of reciprocal factorials, like $e.$",
    "context": "I wish to prove that for any infinite $A\\subset \\mathbb{N},\\ \\displaystyle\\sum_{k\\in A} \\frac{1}{k!}$ is irrational, in the same manner that Rudin proves $e$ is irrational. I should mention the trivial case where $\\vert\\mathbb{N}\\setminus A\\vert$ is finite, for then $\\displaystyle\\sum_{k\\in A} \\frac{1}{k!} = \\left(e-\\frac{1}{0!}\\right) - \\sum_{k\\in (\\mathbb{N}\\setminus A)} \\frac{1}{k!}, $ which is $e$ subtract a rational number and is therefore irrational.\nRudin's proof that $e:= \\displaystyle\\sum_{k=0}^{\\infty} \\frac{1}{k!}$ is irrational goes as follows.\n\nDefine $ \\displaystyle s_n:= \\sum_{k=0}^{n} \\frac{1}{k!}.\\ $ Then, $e - s_n = \\displaystyle\\sum_{k=n+1}^{\\infty} \\frac{1}{k!} < \\frac{1}{(n+1)!}\\left\\{ 1 + \\frac{1}{n+1} + \\frac{1}{\\left(n+1\\right)^2} + \\ldots \\right\\} = \\frac{1}{n!n},\\ $\nso that\n$$ 0 < e-s_n < \\frac{1}{n!n}. \\tag{1}\\label{eq1} $$\nNow suppose $e$ is rational, that is, $e=p/q,$ where $p,q$ are positive integers. By $\\eqref{eq1},$\n$$ 0 < q!(e-s_q) < \\frac{1}{q}. \\tag{2}\\label{eq2} $$\nBy our assumption, $q!e$ is an integer. Since\n$$ q!s_q = q!\\left( 1 + 1 + \\frac{1}{2!} + \\ldots + \\frac{1}{q!} \\right) $$\nis an integer, we see that $q!(e-s_q)$ is an integer. Since $q\\geq 1,\\ \\eqref{eq2}\\ $ implies the existence of an integer between $0$ and $1.$ We have thus reached a contradiction.\n\nNow onto my attempted proof that for an infinite $A\\subset \\mathbb{N},\\ f:= \\displaystyle\\sum_{k\\in A} \\frac{1}{k!}$ is irrational. First, write $A$ as an increasing sequence of integers: $A = \\left( a_k \\right)_{k=1}^{\\infty}.$ Define $ \\displaystyle t_n:= \\sum_{k=1}^{n} \\frac{1}{a_k!}.\\ $ Then,\n$$f - t_n = \\sum_{k=n+1}^{\\infty} \\frac{1}{a_k!}< \\frac{1}{a_{n+1}!}\\left\\{ 1 + \\frac{1}{(a_{n+1}+1)(a_{n+1}+2)\\cdots a_{n+2} } + \\frac{1}{(a_{n+1}+1)(a_{n+1}+2)\\cdots a_{n+2}(a_{n+2}+1)\\cdots a_{n+3} } +  \\ldots \\right\\}.$$\nSince $a_n \\geq n$ and $a_{n+1}>a_n,$ we can certainly say that $0 < f - t_n < \\frac{1}{n!n}$ by comparison with the reasons given for $\\eqref{eq1},$ although this inequality is weak and could be improved.\nBut then if we continue the proof similar to irrationality of $e,$ that is, suppose $f=p/q,\\ p,q$ are integers. Then $0 < q!(f-t_q) < \\frac{1}{q}$ is true, but there are problems in the next statement: we cannot say $q!t_q = q!\\left( \\frac{1}{a_1!} + \\frac{1}{a_2!} + \\ldots + \\frac{1}{a_q!} \\right) $ is an integer.\nAlternatively, $a_q!t_q$ is an integer, although I don't see how to use this to make an equation like equation $\\eqref{eq1}.$\nHave I made a mistake somewhere? If not, is there a way to make some progress here and prove my desired statement? I do feel like I'm missing something relatively obvious.\n",
    "proof": "It is correct that $0 < f - t_n < \\frac{1}{n!n}$, but the upper bound can be improved:\n$$\n\\begin{align}\n 0 < f - t_n &= \\sum_{k=0}^\\infty \\frac{1}{a_{n+1+k}!} \\\\\n &\\le \\sum_{k=0}^\\infty \\frac{1}{(a_{n}+1+k)!} \\\\\n & < \\frac{1}{(a_{n}+1)!} \\sum_{k=0}^\\infty \\frac{1}{(a_n+1)^k} \\\\\n&= \\frac{1}{a_n! \\cdot a_n} \\, .\n\\end{align}\n$$\nAnd then assuming that $f=p/q$ is rational gives a contradiction:\n$$\n 0 < a_q! (f -t_q) < \\frac {1}{a_q}\n$$\nsince $a_q! (f -t_q)$ is an integer.\n",
    "tags": [
      "sequences-and-series",
      "proof-writing",
      "factorial",
      "irrational-numbers"
    ],
    "score": 5,
    "answer_score": 2,
    "is_accepted": true,
    "question_id": 4906516,
    "answer_id": 4906561
  },
  {
    "theorem": "Y.A. Rozanov &#39;Probability Theory A Concise Course&#39; Problem 2.17",
    "context": "Problem Statement:\nGiven any $n$ events $A_1,A_2, ...,A_n$, prove that the probability of exactly $m$ ($m \\le n$) of them happening is\n$$P_m - \\binom{m+1}{m}P_{m+1} + \\binom{m+2}{m}P_{m+2} - \\cdots \\pm \\binom{n}{m}P_n$$\nwhere $P_k = \\sum_{1\\le i_1<i_2 \\cdots <i_k \\le n}\\Pr(\\bigcap_{r=1}^k A_{i_r})$\nMy thoughts:\nThis looks quite similar to the inclusion-exclusion principle where we had:\n$$\\sum_{i=1}^n(-1)^{i+1}P_i$$\nbut here we have\n$$\\sum_{i=m}^n(-1)^{i+m}\\binom{i}{m}P_i$$\nI understand that $P_k$ implies you should sum the probabilities of all the combinations of $A$ of length $k$. Since this already includes the combinations, I don't understand why we need a binomial coefficient before each term. \nTo find the required probability, one can add all possible probabilities of a combination of $m$ events happening, then subtract those of $m+1$ events happening (since the first one includes this).\nThe same question has been asked before here and here. But in the first one, the answerer uses the Indicator function (I'm aware of its basic properties, but the answerer does something which is not clear to me), and in the second, the OP themselves provide an answer (but it has a flaw (?) - they take $\\Pr(M \\cap N^C) = \\Pr(M) - \\Pr(N)$). So, I'm looking for an answer that at least clearly states how can I proceed with the proof, the reason for the binomial coefficients and uses the Indicator function as less as possible (since till now the author has not discussed it in the book).\n",
    "proof": "There would be no binomial coefficients, and we would be able to use the ordinary principle of inclusion-exclusion, if we worked with the following quantities:\n\n$P_I$, the probability that for all $i \\in I$, $A_i$ happens;\n$Q_I$, the probability that for all $i \\in I$, $A_i$ happens while for all $i \\notin I$, $A_i$ does not happen.\n\nTo find $Q_I$ in terms of the probabilities $\\{P_J : J \\supseteq I\\}$, we use inclusion-exclusion on all the events $A_i$ with $i \\notin I$. We:\n\nStart by adding in $P_I$.\nSubtract off $P_{I \\cup \\{i\\}}$ for each $i \\notin I$.\nAdd back in $P_{I \\cup \\{i_1, i_2\\}}$ for every pair $\\{i_1, i_2\\}$ with $i_1, i_2 \\notin I$.\nSubtract off $P_{I \\cup \\{i_1, i_2, i_3\\}}$ and so forth.\n\nThis can be summarized as a sum over all sets $J$ containing $I$ as:\n$$\n    Q_I = \\sum_{J \\supseteq I} (-1)^{|J|-|I|} P_J.\n$$\nNow, we want to go from here to expressing $Q_m$ (the probability that exactly $m$ events happen, but we don't care which ones) in terms of $P_m$. The relationship here is that $Q_m$ is a sum of $Q_I$ over all $I$ with $|I|=m$, while $P_m$ (as defined in the question) is a sum of $P_I$ over all $I$ with $|I|=m$. From our previous equation, we have\n$$\n   Q_m = \\sum_{|I| = m} \\sum_{J \\supseteq I} (-1)^{|J|-|I|} P_J.\n$$\nThis is a double sum over all pairs $(I,J)$ with $|I|=m$ and $I \\subseteq J$. Let's write the sum in a different order. First, make it a sum over all $k$ from $0$ to $n-m$; second, for each $k$, sum over all $J$ with $|J|=m+k$; third, for each $k$ and $J$, sum over all $I$ with $|I|=m$ and $I \\subseteq J$. We get\n$$\n   Q_m = \\sum_{k=0}^{n-m} \\sum_{|J|=m+k} \\sum_{I \\subseteq J, |I|=m} (-1)^k P_J.\n$$\nThe expresion $(-1)^k P_J$ does not actually depend on $I$ at this point. So instead of summing over all $I$, we can just multiply by the number of terms in that inner sum. That number is $\\binom{m+k}{m}$ (the number of $m$-subsets of $J$) which is where the binomial coefficients appear. We get\n$$\n   Q_m = \\sum_{k=0}^{n-m} \\sum_{|J|=m+k} (-1)^k \\binom{m+k}{m} P_J.\n$$\nFinally, the sum of $P_J$ over all $J$ with $|J|=m+k$ is just the definition of $P_{m+k}$, so we can write the last equation more simply as\n$$\n   Q_m = \\sum_{k=0}^{n-m} (-1)^k \\binom{m+k}{m} P_{m+k}\n$$\nwhich was what we wanted.\n",
    "tags": [
      "probability",
      "probability-theory",
      "proof-writing"
    ],
    "score": 5,
    "answer_score": 2,
    "is_accepted": true,
    "question_id": 4634495,
    "answer_id": 4634515
  },
  {
    "theorem": "Spivak Chapter 1, Problem 19(a)",
    "context": "Per Spivak\n\nThe great granddaddy of all inequalities is the Schwarz inequality $${x}_{1}{y}_{1}+{x}_{2}{y}_{2} \\le \\sqrt{{x}_{1}^{2} + {x}{_2}^{2}}\\sqrt{{y}_{1}^{2} + {y}{_2}^{2}}$$\n\n\nProve that if ${x}_{1} = \\lambda {y}_{1}$ and ${x}_{2} = \\lambda {y}_{2}$ for some number $\\lambda \\ge 0$, then equality holds in the Schwarz inequality.\n\nThere is an answer here, but it seems to follow the pattern elsewhere of simply substituting on both sides of the given inequality, and changing the $\\le$ to an $=$, unless I am misunderstanding it. Amusingly, having invested in Spivak's answer book, his answer is \"The proofs for .... are straightforward\" :-)\nHere is my attempt.\nGiven ${x}_{1} = \\lambda y_{1}$ and ${x}_{2} = \\lambda y_{2}$ then\n$$\\sqrt{{x}_{1}^{2} + {x}_{2}^{2}}\\sqrt{{y}_{1}^{2} + {y}_{2}^{2}} = \\sqrt{(\\lambda {y}_{1})^{2} + (\\lambda {y}_{2})^{2}}\\sqrt{{y}_{1}^{2} + {y}_{2}^{2}}$$\n$$= \\sqrt{\\lambda^{2} {y}_{1}^{2} + \\lambda^{2} {y}_{2}^{2}}\\sqrt{{y}_{1}^{2} + {y}_{2}^{2}}$$\n$$=\\sqrt{\\lambda^{2}( {y}_{1}^{2} +  {y}_{2}^{2})}\\sqrt{{y}_{1}^{2} + {y}_{2}^{2}}$$\n$$=\\sqrt{\\lambda^{2}}\\sqrt{( {y}_{1}^{2} +  {y}_{2}^{2})}\\sqrt{{y}_{1}^{2} + {y}_{2}^{2}}$$  Note: $\\sqrt {\\lambda^{2}} = \\lambda \\iff \\lambda \\ge 0 $ as asserted in the problem.\n$$=\\lambda ({y}_{1}^{2} +  {y}_{2}^{2})$$\n$$=\\lambda {y}_{1}^{2} +  \\lambda{y}_{2}^{2}$$\n$$=\\lambda {y}_{1}{y}_{1} +  \\lambda{y}_{2}{y}_{2}$$\n$$=(\\lambda {y}_{1}){y}_{1} +  (\\lambda{y}_{2}){y}_{2}$$\n$$=x_{1}{y}_{1} +  x_{2}{y}_{2}$$\n",
    "proof": "I think the exact problem should be like this,\n$$|x_{1}y_{1}+x_{2}y_{2}|\\le \\sqrt{x_{1}^{2}+x_{2}^{2}}\\sqrt{y_{1}^{2}+y_{2}^{2}}$$\nIf $x_{1}=\\lambda y_1$ and $x_2=\\lambda y_2$\nNow it's obvious that,\n$$|x_{1}y_{1}+x_{2}y_{2}|\\le |x_{1}y_{1}|+|x_{2}y_{2}|$$\n... After following what op done in the description of the problem, we are left with,\n$$|x_{1}y_{1}+x_{2}y_{2}|\\le |x_{1}y_{1}|+|x_{2}y_{2}|\\le \\sqrt{x_{1}^{2}+x_{2}^{2}}\\sqrt{y_{1}^{2}+y_{2}^{2}}$$\nWhich implies that,\n$$|x_{1}y_{1}+x_{2}y_{2}|\\le \\sqrt{x_{1}^{2}+x_{2}^{2}}\\sqrt{y_{1}^{2}+y_{2}^{2}}$$\n",
    "tags": [
      "calculus",
      "algebra-precalculus",
      "inequality",
      "proof-writing"
    ],
    "score": 5,
    "answer_score": 2,
    "is_accepted": false,
    "question_id": 4292381,
    "answer_id": 4292654
  },
  {
    "theorem": "$G$ is bipartite iff $\\chi(G)=2$",
    "context": "If $G$ is a loop-free undirected graph with at least one edge, prove that $G$ is bipartite if and only if $\\chi(G)=2.$\nDefinition 11.18 A graph $G=(V,E)$ is called bipartite if $V=V_1 \\cup V_2$ with $V_1 \\cap V_2 = \\emptyset$, and every edge of $G$ is of the form $\\{a,b\\}$ with $a\\in V_1$ and $b\\in V_2$.\nI start by supposing $G$ is bipartite, coloring the vertices in $V_1$ $\\color{blue}{blue}$ and the vertices in $V_2$ $\\color{red}{red}$, then by definition of bipartite, every edge $\\{a,b\\}$ has vertices of different colors. $\\chi(G)=2$.\nNext, if $\\chi(G)=2$, then if $\\{a,b\\}\\in G$, then $a$ and $b$ are colored with different colors, and since $\\chi(G)=2$ there are only two colors. There is no edge where both vertices are of the same color. You can write $V=V_1 \\cup V_2$ where $V_1$ is all the vertices of color $1$, $V_2$ of color $2$. $V_1 \\cap V_2 = \\emptyset$ because no vertex is colored twice, and every edge is of the form $\\{a,b\\}$ where $a \\in V_1$ and $b \\in V_2$. Thus $G$ is bipartite.\nApparently I was supposed to make use of the fact that there was at least one edge?\n",
    "proof": "If there are no edges, then we have the empty graph on $n$ vertices (which only requires one color) or the null graph (the graph on no vertices, which has chromatic number $0$).\nA graph $G$ with at least one edge is bipartite iff $\\chi(G) = 2$. In general, a graph $G$ is bipartite iff $\\chi(G) \\leq 2$.\nNote that in the definition of a bipartite graph, there is no stipulation that $V_{1}, V_{2}$ are non-empty.\n",
    "tags": [
      "graph-theory",
      "proof-writing",
      "solution-verification",
      "coloring",
      "bipartite-graphs"
    ],
    "score": 5,
    "answer_score": 2,
    "is_accepted": true,
    "question_id": 3950626,
    "answer_id": 4022671
  },
  {
    "theorem": "Prove: If two finite-dimensional vector spaces are isomorphic, then they have the same dimension.",
    "context": "Please comment on the validity of my proof. Any tips and suggestions are appreciated.\nProve: If two finite-dimensional vector spaces are isomorphic, then they have the same dimension.\nLet $B=${$b_1,...,b_n$} be a basis for a finite-dimensional vector space $V$. Let $W$ be a finite-dimensional vector space, and define $T: V→W$ as an isomorphism between them.\nLet $x∈V$, then $T(x)=T(c_1b_1+...+c_nb_n)$ for some real scalars $c_1...c_n$ as $B$ is a basis for $V$. Also, because $T$ is an isomorphism, $T$ is a linear transformation, onto, and one-to-one.\nBecause $T$ is onto, for any $y∈W$, $y=T(x)=T(c_1b_1+...+c_nb_n)=c_1T(b_1)+...+c_nT(b_n)$. Thereofore, $span${$T(b_1),...T(b_n)$}=$W$.\nLet $c_1T(b_1)+...+c_nT(b_n)=0$. Because $T$ is one-to-one and linear $c_1T(b_1)+...+c_nT(b_n)=T(c_1b_1+...+c_nb_n)=0$ implies that $c_1b_1+...+c_nb_n=0$. But because $b_1...b_n$ is a basis, it is linearly independent and so $c_1=...=c_n=0$. Therefore {$T(b_1),...T(b_n)$} is a linearly independent set.\nSo, {$T(b_1),...T(b_n)$} is a basis for $W$ and is clearly $n$ dimensional. So, $dim(V)=dim(W)$.\n",
    "proof": "Your proof is correct. It is also written down very nicely and formally clean. I like it and do not have anything to add. I honestly think there isn't much to add.\nOnly suggestion I have is to replace onto and one to one with surjective and injective, as I think those are more widely understood. But then again, I'm not from a english speaking country so maybe I'm wrong about that and it's probably personal taste.\n",
    "tags": [
      "linear-algebra",
      "vector-spaces",
      "proof-writing",
      "solution-verification"
    ],
    "score": 5,
    "answer_score": 2,
    "is_accepted": true,
    "question_id": 3932274,
    "answer_id": 3948920
  },
  {
    "theorem": "Prove that $\\lim\\limits_{t \\to \\infty} x(t)$ exists and is an integer.",
    "context": "\nLet $f : \\Bbb R \\longrightarrow \\Bbb R$ be a $\\operatorname {C}^{\\infty}$-function such that $f(x) = 0$ if and only if $x \\in \\Bbb Z.$ Suppose the function $x : \\Bbb R \\longrightarrow \\Bbb R$ satisfies $x'(t) = f(x(t)),$ for all $t \\in \\Bbb R.$ If $\\Bbb Z \\cap \\{x(t)\\ |\\ t \\in \\Bbb R \\} = \\varnothing,$ then show that $\\lim\\limits_{t \\to \\infty}  x(t)$ exists and is an integer.\n\nWhat I have seen is that $x$ is also a $\\operatorname {C}^{\\infty}$- function and since image of $x$ doesn't contain any integer so by IVP it follows that $$\\{x(t)\\ |\\ t \\in \\Bbb R \\} \\subseteq (a,b)$$ where $a, b \\in \\Bbb R$ with $b-a \\leq 1$ such that $(a, b) \\cap \\Bbb Z = \\varnothing.$ In other words the image of $x$ is strictly lying between two consecutive integers. Hence $x$ is bounded and also by the definition of $x'$ and $f$ it follows that $x'(t) \\neq 0,$ for all $t \\in \\Bbb R.$\nNow how do I proceed? Any help will be highly appreciated.\nThanks in advance.\n",
    "proof": "As noted $f$ has a sign in the domain of interest  let's say $f$ is positive here (using the $a$ from the OP, we are saying without loss of generality that $f$ is positive on $(\\lfloor a \\rfloor, \\lfloor a \\rfloor,+1)$), then $x$ is increasing and therefore (by the assumed boundedness of $x$) $l=\\lim_{t\\to\\infty} x(t)$ exists as a real number (*).\nSuppose $l\\not\\in \\mathbb Z$. Thus $f(l)\\neq 0$. There is some $\\epsilon$ such that if $x\\in\\mathbb R$ is such that $l\\ge x>l-\\epsilon$, then $|f(x)-f(l)|<f(l)/2$, and hence $|f(x)|=f(x) \\ge f(l)/2$.\nFor all sufficiently large times, we have $l\\ge x(t) > l - \\epsilon_1 $, and therefore  $$x'(t) = f(x(t)) >\\frac{f(l)}2$$\nso $x(t)\\ge f(l)t/2 + C$ for some $C\\in\\mathbb R$, and $x(t)\\to\\infty$, contrary to (*).\n",
    "tags": [
      "real-analysis",
      "ordinary-differential-equations",
      "limits",
      "proof-writing"
    ],
    "score": 5,
    "answer_score": 1,
    "is_accepted": true,
    "question_id": 3813767,
    "answer_id": 3813819
  },
  {
    "theorem": "Would my proof of induction be accepted in an intro Abstract Algebra Course. Self-studying and New to proofs.",
    "context": "Hello I'm self studying and I'm also new to proofs and would like to know whether my proof is rigorous enough for a first course in Abstract Algebra.\nI'm asked to proof Induction of the second kind which states that:\nSuppose P(n) is a statement about the positive integers and c is some fixed positive integer. Assume\ni) P(c) is true\nii) for every $m > c $, if P(k) is true for all k such that $c \\leq k < m $ , then P(m) is true\nThen P(n) is true for all $n \\geq c $\nThe proof also has to use the well-ordering principle which states that:\nEvery nonempty subset of the positive integers has a smallest element.\nMy proof:\nFirst let $M = \\{x\\mid x \\in N \\land x>c \\land P(x) \\text{ is false} \\}$\nNow we assume M is nonempty. Then by the well-ordering principle there exists a smallest element of M which we'll call $y.$ We know that $P(n)$ for all $c \\leq n < y $ is true thus $P(y)$ is true by ii. This is a contradiction thus $M$ is the empty set which means that $P(n)$ is true for all $n \\geq c $\nI'm self-studying and do not know whether my proof would be acceptable for a intro abstract algebra class. So my questions are:\n\nIs my proof correct?\nand if it's not correct, why not and how would I prove it then?\nand if it's correcct, is there anything you would change?\n\nThanks in advance.\n",
    "proof": "As said in the other answer, the proof is almost correct, however I would \"fix\" it differently.\nNamely, nothing is wrong with your set $M$ the way it is, but this only lets you conclude that $P(n)$ is true for $n\\gt c$. Now (and that is the missing step, however trivial), you use (i), which says $P(c)$ is true too, to conclude $P(n)$ is true for $n\\ge c$.\n",
    "tags": [
      "elementary-set-theory",
      "proof-writing",
      "induction",
      "solution-verification"
    ],
    "score": 5,
    "answer_score": 1,
    "is_accepted": true,
    "question_id": 3780329,
    "answer_id": 3780354
  },
  {
    "theorem": "Children with torches puzzle",
    "context": "\nIt's almost nighttime and some children are trying to light up their backyard. They have $n$ torches ($n \\in  \\mathbb{N}$) and want to distribute themselves on the $n \\text{ } \\text{x} \\text{ }n$-Meter backyard in such a way, that no two torches illuminate each other. Each torch sends light in 8 different directions, as in the following picture:\n\nThere are horizontal beams (marked in red), vertical beams (marked in green) and diagonal beams (marked in yellow).\nWe suppose that $n \\geq 5$ and that $n$ is not divisible by $2$ nor $3$. Prove that the following positioning of $n$ children with torches $T_0, T_1, ..., T_{n-1}$ works, i.e no two torches light the same position in the backyard:\nFor $0 \\leq i \\leq n-1$ we position the torch $T_i$ on the field ($i, 2i \\text{ } \\text{mod } n).$\nHere, we use the ($x$-coordinate, $y$-coordinate) coordinate system, where $x$ describes the horizontal position, and $y$ the vertical. For example: The three torches in the picture are placed on the fields $(3, 1), (2, n-3)$ and $(n-2, n-2).$\n\nMy idea was to prove by contradiction and break up each case on how the torches light up their path (horizontally, vertically and diagonally), but I can't see what follows. Can someone offer their thoughts or point me in the right direction?\n",
    "proof": "You need to show that...\n\nNo two torches have the same $x$-coordinate. This is obvious, as the $x$-coordinate of the $i^{th}$ torch is $i$.\nNo two torches have have the same $y$-coordinate. This is not obvious. If the $i^{th}$ and $j^{th}$ torches had the same $y$ coordinate, it would mean $$2i\\equiv 2j\\pmod n$$Now, you can use some modular arithmetic to deduce this is only possible when $i\\equiv j\\pmod n$. (Hint: the equation above is equivalent to $2(i-j)\\equiv 0\\pmod n$. What is the definition of $\\equiv\\pmod n$?)\n\n\n\nNo two torches are on he same upward sloping diagonal. You need to think about how the $(x,y)$ coordinates of a torch relate to the upward diagonal it is on. For example, the main diagonal is the one where $x=y$, and for the one above it, $y=x+1$. In other words, the diagonals consist of squares $(x,y)$ for which $y-x$ is constant. If the $i^{th}$ and $j^{th}$ torches were on the same diagonal, then it would then be the case that $$ 2i-i\\equiv 2j-j\\pmod n$$which quickly leads to a contradiction.\n\n\n\nFor the downward sloping diagonals, two squares are now on the same diagonal if and only if the sum of their $x$ and $y$ coordinates is the same, so you instead would get $$i+2i\\equiv j+2j\\pmod n$$which leads to a contradiction in a similar manner to the second bullet point.\n\n",
    "tags": [
      "combinatorics",
      "proof-writing"
    ],
    "score": 5,
    "answer_score": 2,
    "is_accepted": true,
    "question_id": 3712642,
    "answer_id": 3712753
  },
  {
    "theorem": "How do I go about writing a proof?",
    "context": "I've decided to self-study math to conquer some old fears--I'm pretty new to the idea of proving overall, and I am stuck on this:  *Explain why there are no real numbers that satisfy the equation: $$|x^2 + 4x| = -12.$$\nI've tried to do a proof by induction; but I'm wondering whether this is the right way to approach this, and I frankly do not know how to word it at all. What should I do?\n",
    "proof": "The absolute value of a real number cannot be negative, hence the equation cannot be solvable.\n",
    "tags": [
      "algebra-precalculus",
      "proof-writing",
      "proof-explanation"
    ],
    "score": 5,
    "answer_score": 2,
    "is_accepted": true,
    "question_id": 3580004,
    "answer_id": 3580008
  },
  {
    "theorem": "Find all numbers $n$ that consist of three digits, so that $n^2$ satisfies two specified conditions",
    "context": "The following was a question in the final of the Flanders Mathematics Olympiad 2018:\n\nFind all numbers $n$ that consist of three digits, so that $n^2$ consists of six digits and the sum of the number formed by the three first digits and the number formed by the three last digits of $n^2$, equals $n$.\n\nPoints are assigned not only for finding the right answer, but also for formulating a rigorous and mathematically sound proof. To solve this question, I used the following reasoning:\nCall $x, y$ the number formed by the first and the last three digits of $n^2$, respectively. Then, we find:\n$$\n\\begin{cases}\nn^2 = 1000 x + y \\iff y = n^2 - 1000x\\\\\nn = x + y \\iff y = n - x\n\\end{cases}\n\\Rightarrow\nn^2 - n = n (n - 1) = 999 x\n$$\nIn order for $n (n - 1)$ to be a multiple of $999 = 3^3 \\cdot 37$, either:\n\n$n = 27 \\cdot 37 = 999, n - 1 = 998, x = 998$\n$n = 27 k, n - 1 = 37 l, x = k \\cdot l, k, l \\in \\mathbb{N}$\n$n = 37 k, n - 1 = 27 l, x = k \\cdot l, k, l \\in \\mathbb{N}$\n$n = 1000, n - 1 = 27 \\cdot 37 = 999, x = 1000$\n\nThe first case corresponds to a valid solution, while the last one does not. Solving the Diophantine equations using the extended Euclidean algorithm (details not presented), we find:\n$$27 k = 37l + 1, k < 37 \\iff k = 11, l = 8, x = 88$$\n$$37 k = 27l + 1, k < 27 \\iff k = 19, l = 26, x = 494$$\nSince $x$ must consist of three digits, only the last equation results in a valid solution. We thus find two solutions to the problem: $n = 703$ and $n = 999$.\nIt seems to me that this approach is quite tedious, and that there might be a more straightforward way to tackle this problem. Especially the use of Diophantine equations worries me, since this is not usually taught at high school level. Are there any alternative approaches to solve this question?\n",
    "proof": "The approach you've shown seems very feasible for a high school olympiad, and is likely the intended method. An alternative method, though quite similar, would be to solve\n$$1000x+y=n^2=(x+y)^2=x^2+2xy+y^2.$$\nThis is a quadratic in $x$, which immediately shows that\n$$x=500-y\\pm\\sqrt{500^2-999y},\\tag{1}$$\nand for this to be an integer you must have\n$$500^2-999y=z^2\\qquad\\text{ or equivalently }\\qquad 999y=(500+z)(500-z),$$\nfor some integers $y$ and $z$ with $0\\leq z<500$. \nThis yields four cases, as in your proof:\n\nEither $3^3\\cdot37$ divides $500+z$; then $z=499$.\nOr $3^3$ divides $500+z$ and $37$ divides $500-z$, which is impossible.\nOr $37$ divides $500+z$ and $3^3$ divides $500-z$, in which case $z=203$\nOr $3^3\\cdot37$ divides $500-z$, which is impossible.\n\nPluggin these two values of $z$ back in shows that either $y=1$ or $y=209$, and correspondingly $x=998$ or $x=494$, where in both cases we have the $+$-sign in equation $(1)$ as $x$ must have three digits. It follows that either $n=999$ or $n=703$.\n",
    "tags": [
      "proof-writing",
      "contest-math",
      "diophantine-equations",
      "solution-verification",
      "alternative-proof"
    ],
    "score": 5,
    "answer_score": 2,
    "is_accepted": true,
    "question_id": 3563294,
    "answer_id": 3564824
  },
  {
    "theorem": "$ \\pi(n) &gt; \\frac{n}{22 \\ln(n)} $ simple proof?",
    "context": "What is the easiest way to show that\n$$ n > 2 $$\n$$ \\pi(n) > \\frac{n}{2 \\ln(n)} $$\nWhere $\\pi(n)$ is the prime counting function. \nI read a proof of the PNT with the zeta function but this statement is much weaker !!\nWhat is the shortest proof ? \nThe simplest ?\nThe most elementary ?\nDo we use results from Mertens ? ( $\\Pi ( 1 - 1/p) $ or $ \\sum 1/p $ )\nDo we need to use results of Mertens ?\nDo we need to estimate $\\sum \\ln(p) $ ?\nHow about the even weaker\n$$ \\pi(n) > \\frac{n}{22 \\ln(n)} $$\nIs that even easier ? Or not ? \n",
    "proof": "Too long for a comment:\nIn Section 4.5 of Apostol's Introduction to Analytic Number Theory (page 82-84), we have\n\nTheorem 4.6 For every integer $n\\ge2$, we have\n$${1\\over6}{n\\over\\log n}\\lt\\pi(n)\\lt6{n\\over\\log n}$$\n\nThe proof is elementary. This takes care of the OP's question with $22$ in the denominator, but leaves open the question whether there's a simple proof if you replace Apostol's $6$ with a $2$. Apostol does, however, introduce the theorem saying, \"Although better inequalities can be obtained with greater effort (see [60]) the following theorem is of interest because of the elementary nature of its proof.\" Reference [60] is:\n\nRosser, J. Barkley, and Schoenfeld, Lowell (1962) Approximate formulas\n  for some functions of prime number theory. Illinois J. Math.,\n  6:69-94; MR 25, #1139.\n\n",
    "tags": [
      "number-theory",
      "elementary-number-theory",
      "proof-writing",
      "prime-numbers",
      "alternative-proof"
    ],
    "score": 5,
    "answer_score": 2,
    "is_accepted": false,
    "question_id": 3463302,
    "answer_id": 3463424
  },
  {
    "theorem": "Alternately Multiplying and Dividing Within Pascal&#39;s Triangle",
    "context": "I recently noticed that, if you take the $(2n)$th row of Pascal's triangle, and alternately multiply and divide the numbers that crop up, almost everything cancels. For example, the 4th row (1 4 6 4 1) becomes $1/4 \\times 6/4 \\times 1$, which simplifies to $3/8$. I performed the same procedure for some later rows, and got $5/16$, $35/128$, $63/256$, $(3 \\times 7 \\times 11)/(2^{10})$, and $(3 \\times 11 \\times 13)/(2^{11})$.\nAll of the denominators seem to be powers of 2, which I don't know how to prove. Also, the fractions seem to me to be (very) slowly approaching 0, which I also don't know how to prove (if it's even true). Any assistance with either question would be greatly appreciated- I'm not very well versed in methods of proof.\n",
    "proof": "We can simplify each individual ratio:\n$$ \\frac{\\displaystyle \\binom{2n}{0}}{\\displaystyle \\binom{2n}{1}}\\frac{\\displaystyle \\binom{2n}{2}}{\\displaystyle \\binom{2n}{3}}\\cdots\\frac{\\displaystyle\\binom{2n}{2n-2}}{\\displaystyle \\binom{2n}{2n-1}} = \\frac{1}{2n}\\frac{3}{2n-2}\\cdots\\frac{2n-1}{2} $$\nReorder the factors in the denominator, then square them while interlacing them up top:\n$$ \\frac{1\\cdot2\\cdot3\\cdot4\\cdots(2n-1)(2n)}{2^2\\cdot 4^2\\cdot6^2\\cdots (2n)^2} =\\frac{(2n)!}{\\big[2^n n!\\big]^2}=\\frac{1}{4^n}\\binom{2n}{n}. $$\n",
    "tags": [
      "elementary-number-theory",
      "proof-writing",
      "binomial-coefficients"
    ],
    "score": 5,
    "answer_score": 2,
    "is_accepted": true,
    "question_id": 3260419,
    "answer_id": 3260443
  },
  {
    "theorem": "A set as a graph of a function",
    "context": "The unit sphere $n$ dimensional is the set $$\\mathbb{S}^n=\\bigg\\{(x_1,x_2,\\dots, x_{n+1})\\in\\mathbb{R}^{n+1}\\;|\\;\\big(x_1^2+x_2^2+\\cdots+x_{n+1}^2\\big)^{1/2}=1\\bigg\\}.$$\nFor all $i=1,\\dots, n+1$ denote with $U_i^+$ the subset of $\\mathbb{R}^{n+1}$ so defined $$U_i^+=\\bigg\\{\\big(x_1,x_2,\\dots, x_{n+1}\\big)\\in\\mathbb{R}^{n+1}\\;|\\; x_i>0\\big)\\bigg\\}.$$\nWe consider the continous function $f\\colon\\mathbb{B}^n\\to \\mathbb{R}$ so defined $u\\mapsto\\sqrt{1-\\lvert u\\rvert^2}$. Then for all $i=1,\\dots, n+1$, $U_i^+\\cap \\mathbb{S}^n$ is the graph of the function  $$x_i=f(x_1, x_2,\\dots, x_{i-1},x_{i+i},\\dots, x_{n+1}).$$ \n\nLet's prove explicitly what has just been said.\n\nIn symbols, pointing with $\\hat{x}=(x_1,\\dots, x_{i-1}, x_{i+1},\\dots, x_{n+1})$, for all $i=1,\\dots, n+1$ we have\n$$U_i^+\\cap \\mathbb{S}^n=\\bigg\\{x=(\\hat{x}, x_i)\\in\\mathbb{R}^n\\times\\mathbb{R}\\;|\\; \\hat{x}\\in\\mathbb{B}^n,x_i=f(\\hat{x})\\bigg\\}.$$\nIndeed, if $x\\in U_i^+\\cap \\mathbb{S}^n$, then $x_i>0$ and $\\lvert x \\rvert =1$ On $U_i^+$ can resolve the equation $\\lvert x \\rvert=1$ respect to $x_i$ then we have $$x_i=\\sqrt{1-(x_1)^2- \\cdots -(x_{i-1})^2-(x_{i+1})^2- \\cdots - (x_{n+1})^2}.$$ If we consider the vector $\\hat{x}=(x_1, x_2,\\dots, x_{i+1}, x_{i+1}, \\dots, x_{n+1})$ we have $\\lvert \\hat{x} \\rvert <1$, then $\\hat{x}\\in\\mathbb{B}^n$, therefore $x=(\\hat{x}, x_i)\\in U_i^+\\cap\\mathbb{S}^n$.\nVice versa, if $x=(\\hat{x}, x_i)\\in U_i^+\\cap \\mathbb{S}^n$, then $\\hat{x}\\in\\mathbb{B}^n$ and $x_i=\\sqrt{1-\\lvert \\hat{x} \\rvert}>0$, moreover $\\lvert x \\rvert=1$, therefore $x\\in\\mathbb{S}^n$\n\nQuestion. Is my reasoning formally correct?\n\n",
    "proof": "I think your book uses strange notation, and is possibly wrong.\nThe graph $\\Gamma_f$ of $f: A \\longrightarrow B$ is defined as the subset of $A \\times B$ given by $\\Gamma_f = \\{(a,b) \\in A\\times B\\mid b = f(a)\\}$.\nThe equation $x_i = f(x_1,x_2,\\dots,\\hat{x_i},\\dots,x_n, x_{n+1})$ is not a function and hence does not have a graph, but it implicitly defines a subset $S_i$ of $\\Bbb R^{n+1}$ via \n$$S_i = \\left\\{(x_1,\\dots,x_{n+1})\\in\\Bbb R^{n+1}\\,\\middle|\\, x_i = f(x_1,x_2,\\dots,\\hat{x_i},\\dots,x_n, x_{n+1})\\right\\}.$$\nMoreover, as I pointed out in the comments, the book appears to be wrong.\nThere are points in $S_i$ whose $i$-th coordinates is $0$.\nIndeed, if $H_i\\subset R^{n+1}$ is the hyperplane $\\left\\{(x_1,\\dots,x_{n+1})\\in\\Bbb R^{n+1}\\,\\middle|\\, x_i =0\\right\\}$, then any $p\\in H_i \\cap \\Bbb S^n$ satisfies the equation $x_i = f(x_1,x_2,\\dots,\\hat{x_i},\\dots,x_n, x_{n+1})$ and hence lies in $S_i$.\nNo such $p$ however lies in $U_i^+$, so the claim made by the book cannot be right.\n",
    "tags": [
      "proof-writing",
      "manifolds",
      "smooth-manifolds"
    ],
    "score": 5,
    "answer_score": 2,
    "is_accepted": true,
    "question_id": 3214798,
    "answer_id": 3217973
  },
  {
    "theorem": "Proving the area of the circle using sticks.",
    "context": "I was just trying to prove the area of the circle but couldn't reach any conclusion.so here i went----- I know \nFor 2 identical sticks making a regular polygon we have the regular polygon as square i.e with 4 sides. For 3 identical sticks which are able to make aregular polygon we have hexagon similarly for n identical sticks we have 2n sided polygon. Now if n tends to infinity we get a circle. Now i am stuck in connecting the length of the stick and the equal angle between them with the side of the regular polygon. I cant generalise. Please help. Please note-  all the sticks intersect each other at a single point i.e the centre of the geometrical figure. We get the polygons by joining the terminal end points of the sticks.\n",
    "proof": "I suppose the diameter is the length of the stick, let it be $d$, then we need to prove that the area of the circle with infinite sticks is $\\pi\\, \\frac{d^2}{4}$.\nThe angle of the internal triangle formed with $n$ sticks is $\\frac{n-1}{n}\\times \\frac{\\pi}{2}$\nHence,\n\\begin{align*}\n  \\text{base} &= d \\, \\cos{\\left(\\frac{n-1}{n}\\cdot \\frac{\\pi}{2}\\right)}\\\\\n  \\text{height} &= \\frac{d}{2}\\, \\sin{\\left(\\frac{n-1}{n}\\cdot \\frac{\\pi}{2}\\right)}\\\\\n\\end{align*}\nHence, the total area of the polygon is\n\\begin{align*}\n  A &= \\frac{1}{2}\\cdot d \\, \\cos{\\left(\\frac{n-1}{n}\\cdot \\frac{\\pi}{2}\\right)} \\cdot  \\frac{d}{2}\\, \\sin{\\left(\\frac{n-1}{n}\\cdot  \\frac{\\pi}{2}\\right)} \\times 2\\, n\\\\\n\\end{align*}\nThen, take the limit\n\\begin{align*}\n  A &= \\lim_{n\\to\\infty} \\frac{d^2}{4} \\sin{\\left(\\frac{n-1}{n}\\cdot \\pi\\right)} \\times n\\\\\n  &= \\frac{d^2}{4}\\cdot \\pi\n\\end{align*}\n",
    "tags": [
      "proof-verification",
      "proof-writing",
      "circles"
    ],
    "score": 5,
    "answer_score": 2,
    "is_accepted": true,
    "question_id": 2625144,
    "answer_id": 2625353
  },
  {
    "theorem": "$\\inf A = -\\sup (-A)$",
    "context": "This is another probably heavily searched for problem from Rudin (Proof that $\\inf A = -\\sup(-A)$) But, my proof differs and I'd just like some verification and critique of proof writing and style. \n\nLet $A$ be a nonempty subset of $\\mathbb{R}$. Prove that $\\inf A = - \\sup(-A)$\n\nLet $\\alpha = \\inf A$ and $\\beta = - \\sup B$ where $B = \\{-x | x\\in A\\}$\nWLOG assume $\\alpha \\neq \\beta$ and $\\alpha > \\beta$. \n$\\exists q \\in\\mathbb{Q}$ such that $\\alpha > q > \\beta$. \nSo $-q < -\\beta = \\sup B$ Therefore $ \\exists b \\in B$ where $b > -q$ by definition of $\\sup$. \nSince $\\exists a \\in A$ where $-a = b$ implies $a < q$. But that leads to $a < q < \\alpha$. \nThis is a contradiction since $\\alpha$ is the $\\inf$.\nThus $\\inf A = - \\sup(-A)$ \n",
    "proof": "Globally, your proof is correct. I have only two minor remarks:\n\nWhy do you introduce $\\mathbb Q$? It is not wrong, but it sounds a bit arbitrary.\nWriting $\\sup-A$ looks strange. It would be beter to write $\\sup(-A)$.\n\n",
    "tags": [
      "real-analysis",
      "proof-verification",
      "proof-writing",
      "supremum-and-infimum"
    ],
    "score": 5,
    "answer_score": 2,
    "is_accepted": true,
    "question_id": 2604601,
    "answer_id": 2604605
  },
  {
    "theorem": "Product rule proof. Derivatives.",
    "context": "I've been asked to proof the product rule. \nI am fine with that, my proof was accepted completely.\nNevertheless, for me personally the very last step seems to be not totally incorrect, but, let say, \"intuitive\" or \"inductive\" to some extend. \nNot to confuse people, will mention few fateful points of my reasoning:\nAt some moment I've proved that:\n$$f(x + \\delta) = A + B \\times \\delta + o(E(\\delta))$$\nwhere: \n\n$o(E(\\delta))$ means \"of less order than $\\delta$\", i.e.\n$\\lim_{\\delta \\to 0}\\frac{E(\\delta)}{\\delta} = 0$;\n$A = f(x)$;\n$B = f'(x)$.\n\nAfter that I took both $f(x + \\delta)$ and $g(x + \\delta)$ assumed to be differentiable at the point $x$. Hence:\n$$f(x + \\delta) \\times g(x + \\delta) = [f(x) + f'(x)\\delta + o(E(\\delta))] \\times [g(x) + g'(x)\\delta + o(E(\\delta))]$$\n$$= f(x)g(x) + [f(x)g'(x) + g(x)f'(x)] \\times \\delta + \\biggl( [f(x) + f'(x)\\delta]o(E(\\delta)) + [g(x) + g'(x)\\delta]o(E(\\delta)) + [o(E(\\delta))]^2\\biggr)$$\n\nHERE GOES MY QUESTION:\nI've noticed that the last equation could be reformulated as:\n$$A = f(x)g(x)$$\n$$B = f(x)g'(x) + g(x)f'(x)$$\n$$o(E(\\delta)) = \\biggl( [f(x) + f'(x)\\delta]o(E(\\delta)) + [g(x) + g'(x)\\delta]o(E(\\delta)) + [o(E(\\delta))]^2\\biggr)$$\nThe question is: shouldn't I proof somehow (have no idea how, actually) that the pattern above is applicable here? Last step seems to be more of blinded guessing rather than an undeniable logical deduction...\n",
    "proof": "I prefer to write\n$$\nf(x+\\delta)=f(x)+\\delta f'(x)+\\delta\\varphi(\\delta)\n$$\nwhere the condition $\\lim_{\\delta\\to0}\\varphi(\\delta)=0$ is equivalent to $f$ being differentiable at $x$, with derivative $f'(x)$. More precisely: write, for $\\delta\\ne0$,\n$$\n\\varphi(\\delta)=\\frac{f(x+\\delta)-f(x)}{\\delta}-a\n$$\nThen $f$ is differentiable at $x$, with derivative $a$ if and only if $\\lim_{\\delta\\to0}\\varphi(\\delta)=0$.\nIt's not so different from your notation, actually, but less confusing, in my opinion.\nSuppose the same for $g$, so\n$$\ng(x+\\delta)=g(x)+\\delta g'(x)+\\delta\\psi(\\delta)\n$$\nand $\\lim_{\\delta\\to0}\\psi(\\delta)=0$.\nNow we have\n\\begin{align}\nf(x+\\delta)g(x+\\delta)\n&=\\bigl(f(x)+\\delta f'(x)+\\delta\\varphi(\\delta)\\bigr)\n  \\bigl(g(x)+\\delta g'(x)+\\delta\\psi(\\delta)\\bigr)\n\\\\\n&=f(x)g(x)+\\delta\\bigl(f'(x)g(x)+f(x)g'(x)\\bigr) \\\\\n&+\\delta\\bigl(\n  \\begin{aligned}[t]\n  &\\delta f'(x)g'(x)+\n  \\varphi(\\delta)g(x)+\n  \\delta f'(x)\\psi(\\delta)+{}\\\\\n  &\\delta\\varphi(\\delta)g'(x)+\n  f(x)\\psi(\\delta)+\\delta\\varphi(\\delta)\\psi(\\delta)\\bigr)\n  \\end{aligned}\n\\\\\n&=f(x)g(x)+\\delta\\bigl(f'(x)g(x)+f(x)g'(x)\\bigr) + \\delta\\eta(\\delta)\n\\end{align}\nand $\\lim_{\\delta\\to0}\\eta(\\delta)=0$, proving the statement.\n",
    "tags": [
      "calculus",
      "derivatives",
      "proof-verification",
      "proof-writing"
    ],
    "score": 5,
    "answer_score": 2,
    "is_accepted": true,
    "question_id": 2579257,
    "answer_id": 2579306
  },
  {
    "theorem": "If, as part of proving an implication, you suppose the antecedent - but the antecedent is false in a specific case, can you ignore that case?",
    "context": "For example. Say I have a conditional\n$$(\\frac{x}{x-2} \\leq 3 \\ \\land \\ x \\geq 2) \\implies x \\geq 3$$\nthen clearly, when $x=2$, I have a problem since then my fraction is undefined. \nNow, suppose I wish to prove the above implication strictly by means of a direct proof. That means assuming the antecedent, and from there on trying to prove that it logically leads to the consequent.\nHowever, if I assume the above antecedent $(\\frac{x}{x-2} \\leq 3 \\ \\land \\ x \\geq 2)$, then what do I do about the $\\geq$? After all, this means $= \\, or \\, >$, and since I'm assuming this to be true (as part of the overarching conjunction being true), how do I deal with the conflict of both my antecedent being correct, but $x=2$ being impossible and incorrect?\nIs it valid for me to say \"Because in the case of x=2, the fraction is undefined, our assumption reduces to $(\\frac{x}{x-2} \\leq 3 \\ \\land \\ x > 2)$\" and then go on to prove $(\\frac{x}{x-2} \\leq 3 \\ \\land \\ x > 2)$? \nThat somehow feels like cheating, or at the least like giving some sort of vacuous/irrelevant proof because I've reduced my assumption (and, by extension, my implication) to something it strictly wasn't. Can somebody shine some light on this?\n",
    "proof": "I think that, since the quantification on $x$ is implicit, you can assume that the $x$ concerned are those for which the assertions $\\frac{x}{x-2}\\leq 3$ and $x\\geq 2$ make sense simultaneously, and what you are trying to prove is more precisely\n$$\\forall x\\in\\mathbb{R}\\setminus\\{2\\},\\frac{x}{x-2}\\leq 3\\,\\wedge x\\geq 2\\implies x\\geq 3.$$ \nIt reminds me a question I asked of the same nature (here), with a nice answer from John M. Lee.\n",
    "tags": [
      "proof-verification",
      "proof-writing",
      "proof-explanation",
      "propositional-calculus"
    ],
    "score": 5,
    "answer_score": 2,
    "is_accepted": false,
    "question_id": 2489531,
    "answer_id": 2489554
  },
  {
    "theorem": "Proofs involving a set of infinite dimensional vectors",
    "context": "Given: \nA = $\\{  \\vec{v} = (v_{1}, ....v_{k}, ...) \\in \\mathbb{R}^{\\infty} | \\sum_{i=1}^\\infty v_i^2 \\text{converges} \\}$\nProve that the set is a subspace of $\\mathbb{R}^\\infty$ and:\n< $\\vec{v}, \\vec{u} > = \\sum_{i=1}^\\infty v_iu_i$ defines an inner product on A.\nI am really struggling with this problem, particularly showing closure under addition to show the set is a subspace, and proving positive definiteness of the inner product. I have been given the hint by a professor \"use the Cauchy-Schwarz inequality\" but I lack confidence in this advice, it is my understanding that a well defined inner product is a pre-condition for using Cauchy-Schwarz inequality. \nThis is associated with an intro Linear Algebra take home exam, I have found a lot of information that leads me to believe this is regarding an $\\mathscr{l}^2$ - Hilbert space, a concept that is not actually in my textbook.\nHow to go about this proof? Ideally in a way that someone with 1 semester of linear algebra could understand. \n",
    "proof": "Let  $x,y \\in A$ where $$x=(x_1,x_2....)$$ $$y=(y_1,y_2....)$$\nThen $x+y=(x_1+y_1,x_2+y_2.....)$ and \n$\\sum_{n=1}^{\\infty} (x_n+y_n)^2=\\sum_{n=1}^{\\infty}x_n^2+2x_ny_n+y_n^2 \\leqslant \n\\sum_{n=1}^{\\infty}2x_n^2+2y_n^2=2\\sum_{n=1}^{\\infty}x_n^2+2\\sum_{n=1}^{\\infty}y_n^2 < \\infty$  thus $x+y \\in A$(i used the fact that $x^2+y^2 \\geqslant 2xy \\Longleftrightarrow(x-y)^2 \\geqslant 0$)\nIf  $a \\in \\mathbb{R}$ and $x=(x_1,x_2...) \\in A$ then $\\sum_{n=1}^{\\infty}a^2x_n^2=a^2\\sum_{n=1}^{\\infty}x_n^2 < \\infty$ thus $ax \\in A$.\nWe proved that $A$ is a subbase of $\\mathbb{R}^{\\infty}$\nNow $<x,x> $ where $x \\in A$ is non negative because it is a sum of squares.\nIt is easy to prove the other axioms of inner product\n",
    "tags": [
      "linear-algebra",
      "proof-writing",
      "hilbert-spaces"
    ],
    "score": 5,
    "answer_score": 2,
    "is_accepted": true,
    "question_id": 2281328,
    "answer_id": 2281345
  },
  {
    "theorem": "(Beginner) Proving arguments with resolution with detailed steps",
    "context": "I am very new to resolution and would like to use it to prove the following two  arguments. I also would like to indicate the laws of equivalence used to obtain Skolem Normal Form of the premises and the negated conclusion:\n\n1) \n$Premise ~ 1: ∃x ~ toad(x) \\\\\nPremise ~ 2: ∀x ~ (toad(x) → slimy(x)) \\\\\nPremise ~ 3: ∀x ~ (slimy(x) → muddy(x)) \\\\\nConclusion: ∃x ~ (muddy(x) ∧ toad(x))$\nAttempt:\nAside for turning negated conclusion into clausal normal form:\n1. ∃x.(muddy(x) ∧ toad(x))        (Initial Conclusion)\n2. ¬∃x.(muddy(x) ∧ toad(x))       (Negated Conclusion)\n3. ∀x.¬(muddy(x) ∧ toad(x))       (DeMorgan Quantifer)\n4. ∀x.(¬muddy(x) ∨ ¬toad(x))      (DeMorgan)\n5. (¬muddy(x1) ∨ ¬toad(x1))       (∀-elim)\n\nAside for converting Premise 1:\n1. ∃x.toad(x)                     (Initial Premise 1)\n2. toad(c)                        (Skolemization)\n\nAside for converting Premise 2:\n1. ∀x.(toad(x) → slimy(x))        (Initial Premise 2)\n2. ∀x.(¬toad(x) ∨ slimy(x))       (Def. →)\n3. (¬toad(x2) ∨ slimy(x2))        (∀-elim)\n\nAside for converting Premise 3:\n1. ∀x.(slimy(x) → muddy(x))       (Initial Premise 3)\n2. ∀x.(¬slimy(x) ∨ muddy(x))      (Def. →)\n3. (¬slimy(x3) ∨ muddy(x3))       (∀-elim)\n\nActual Resolution Proof:\n[ 1] {¬muddy(x1), ¬toad(x1)}   Negated Conclusion\n[ 2] {toad(c)}                 Premise 1\n[ 3] {¬toad(x2), slimy(x2)}    Premise 2\n[ 4] {¬slimy(x3), muddy(x3)}   Premise 3\n[ 5] {¬muddy(c)}               Res: 1, 2; θ = [x1/c]\n[ 6] {¬slimy(x3), ¬toad(x3)}   Res: 1, 4; θ = [x1/x3]\n[ 7] {slimy(c)}                Res: 2, 3; θ = [x2/c]\n[ 8] {¬slimy(c)}               Res: 2, 6; θ = [x3/c]\n[ 9] {}                        Res: 7, 8\n\n\n2)\n$Premise 1: ∀x ~ (big(x) → (muddy(x) ∨ slimy(x))) \\\\\nPremise 2: ∀x ~ ((slimy(x) ∧ ∃y ~ greater(x, y)) → gobbles(x, x)) \\\\\nPremise 3: ∀x ~ ((big(x) ∧ muddy(x)) → ∃y ~ (toad(y) ∧ gobbles(x, y))) \\\\\nConclusion: ∀x ~ ((big(x) ∧ greater(x, shrek)) → ∃z ~ gobbles(x, z))$\nAttempt:\nAside for turning negated conclusion into clausal normal form:\n1. ∀x.((big(x) ∧ greater(x,shrek)) → ∃z.gobbles(x,z))    (Initial Conclusion)\n2. ¬∀x.((big(x) ∧ greater(x,shrek)) → ∃z.gobbles(x,z))   (Negation)\n3. ∃x.¬((big(x) ∧ greater(x,shrek)) → ∃z.gobbles(x,z))   (DeMorgan)\n4. ∃x.¬(¬(big(x) ∧ greater(x,shrek)) ∨ ∃z.gobbles(x,z))  (Def. →)\n5. ∃x.(¬¬(big(x) ∧ greater(x,shrek)) ∧ ¬∃z.gobbles(x,z)) (DeMorgan)\n6. ∃x.(big(x) ∧ greater(x,shrek) ∧ ∀z.¬gobbles(x,z))     (Double Neg., DeMorgan)\n7. big(c) ∧ greater(c,shrek) ∧ ∀z.¬gobbles(c,z)          (Skolemization)\n\nAside for converting Premise 1:\n1. ∀x(big(x) → (muddy(x) ∨ slimy(x)))        (Initial Premise 1)\n2. ∀x(¬big(x) ∨ (muddy(x) ∨ slimy(x)))       (Def. →)\n3. ¬big(x2) ∨ (muddy(x2) ∨ slimy(x2))        (∀-elim)\n\nAside for converting Premise 2:\n1. ∀x.((slimy(x) ∧ ∃y.greater(x,y)) → gobbles(x,x))         (Initial Premise 2)\n2. ∀x.(¬(slimy(x) ∧ ∃y.greater(x,y)) ∨ gobbles(x,x))        (Def. →)\n3. ∀x.((¬slimy(x) ∧ ¬∃y.greater(x,y)) ∨ gobbles(x,x))       (DeMorgan)\n4. ∀x.((¬slimy(x) ∧ ∀y.¬greater(x,y)) ∨ gobbles(x,x))       (DeMorgan)\n5. ((¬slimy(x3) ∧ ¬greater(x3,y1)) ∨ gobbles(x3,x3))        (∀-elim)\n\nAside for converting Premise 3:\n1. ∀x.((big(x) ∧ muddy(x)) → ∃y.(toad(y) ∧ gobbles(x,y)))         (Initial Premise 3)\n2. ∀x.(¬(big(x) ∧ muddy(x)) ∨ ∃y.(toad(y) ∧ gobbles(x,y)))        (Def. →)\n3. ∀x.((¬big(x) ∨ ¬muddy(x)) ∨ ∃y.(toad(y) ∧ gobbles(x,y)))        (DeMorgan)\n4. ∀x.((¬big(x) ∨ ¬muddy(x)) ∨ (toad(f(y)) ∧ gobbles(x,f(y))))     (Skolemization)\n5. ((¬big(x4) ∨ ¬muddy(x4)) ∨ (toad(f(y)) ∧ gobbles(x4,f(y))))     (∀-elim)\n6. ((¬big(x4) ∨ ¬muddy(x4)) ∨ toad(f(y))) ∧ ((¬big(x4) ∨ ¬muddy(x4)) ∨ gobbles(x4,f(y))) (Distribution)\n\nActual Resolution Proof:\n[ 1] {big(c)}                                             ∧-elim-1 Negated Conclusion, then ∧-elim-1 again \n[ 2] {greater(c,shrek)}                                   ∧-elim-1 Negated Conclusion, then ∧-elim-2 \n[ 3] {¬gobbles(c,x1)}                                     ∧-elim-2 Negated Conclusion, ∀-elim\n[ 4] {¬big(x2), muddy(x2), slimy(x2)}                     Premise 1\n[ 5] {¬slimy(x3), ¬greater(x3,y1), gobbles(x3,x3)}        Premise 2\n[ 6] {¬big(x4), ¬muddy(x4), toad(f(y))}                   Premise 3a ∧-elim-1\n[ 7] {¬big(x4), ¬muddy(x4), gobbles(x4,f(y))}             Premise 3b ∧-elim-2                                     \nHow to proceed?                                                            \n\n",
    "proof": "For 1), the clause you get for $\\exists x toad(x)$ should be $\\{ toad(c) \\}$, i.e. you need to use a constant here, not a variable. When you remove an existential quantifier and substitute a constant for the variable that was quantified by that existential quantifier, it is called Skolemization. \nThis constant should then be used (substituted for variables in order to unify) during subsequent resolution as well, e.g. on line 5 you would get $\\{ \\neg muddy(c) \\}$. In fact, on line 7 you would get $\\{ slimy(c) \\}$, and on line 8 $\\{ \\neg slimy(c) $, the two of which resolve to the empty clause $\\{ \\}$, and you are done.\nFor 2), you need to negate the whole conclusion, which gets you $\\exists x (big(x) \\land greater(x,shrek) \\land \\forall z \\neg gobbles(x,z))$, and by Skolemization you thus get $big(c) \\land greater(c,shrek) \\land \\forall z \\neg gobbles(c,z)$, and thus clauses $\\{ big(c) \\}$, $\\{ greater(c,shrek) \\}$, and $\\{ \\neg gobbles(c,x1) \\}$\n",
    "tags": [
      "logic",
      "proof-verification",
      "proof-writing",
      "predicate-logic"
    ],
    "score": 5,
    "answer_score": 2,
    "is_accepted": true,
    "question_id": 2236349,
    "answer_id": 2236711
  },
  {
    "theorem": "Which books on proof-writing are stuitable for someone engaged in self-study?",
    "context": "I want to learn how to read and write proofs. I only have basic pre-calculus skills. As I am preparing for my upcoming calculus 1 course, I wanted to understand questions like\n\nProve $\\left|ab\\right|=\\left|a\\right|\\left|b\\right|$ for any numbers $a$ and $b$.\n\nThese questions pop up in various calculus books and I need to understand them. \nAny book recommendations?\n",
    "proof": "If you're just about to move into Calculus I, you don't need to worry much about the formal logic behind the mathematics. Proofs typically pop up first in a linear algebra or abstract algebra course sophomore or junior year at a university. Real analysis is where you need to worry about proving both integral and differential Calculus, but that's a class that you take after you finish the Calculus sequence anyway. Any facts in a basic single variable Calculus class should be easy to prove with information from your textbook, if you are even required to prove anything at all.\n",
    "tags": [
      "calculus",
      "reference-request",
      "proof-writing",
      "self-learning",
      "book-recommendation"
    ],
    "score": 5,
    "answer_score": 2,
    "is_accepted": true,
    "question_id": 2079911,
    "answer_id": 2080036
  },
  {
    "theorem": "Proof of determinant formula",
    "context": "I have just started to learn how to construct proofs. That is, I am not really good at it (yet). In this thread I will work through a problem from my Linear Algebra textbook. First i will give you my \"solution\" and then, hopefully, you can tell me where I went wrong. If my proof strategy for this case is wrong I would love to hear why it's wrong (if it is possible) since I think that is how I will become better :)\n\nMy textbook says an often good proof strategy of different determinant formulas is by Mathematical induction and I think it also works in this case, but as i said earlier, I am not too good at constructing proofs yet.\n\nProblem:\n\n\nLet $X, Y$ be column-vectors. Show that $det(I+XY^T)=1+Y^TX$, where the last product is interpreted as a number.\n\nOk so here is my attempt to solve the problem:\nProof strategy: Induction\n1. Base case:\nThe statement is true when n=2, since:\n$$I=\\left( \\begin{array}{ccc}\n1 & 0 \\\\\n0 & 1 \\end{array} \\right), XY^T=\\left( \\begin{array}{ccc}\nx_1y_1 & x_1y_2 \\\\\nx_2y_1 & x_2y_2 \\end{array} \\right)$$\nand\n$|I+XY^T|=\\begin{vmatrix}\nx_1y_1+1 & x_1y_2\\\\ x_2y_1 & x_2y_2+1\n\\end{vmatrix}$\nWhen we expand the determinant, we get:\n$(x_1y_1+1)(x_2y_2+1)-x_1y_2x_2y_1= 1+(x_1y_1x_2y_2+x_1y_1+x_2y_2-x_1y_2x_2y_1)=1+(x_1y_1+x_2y_2)=1+Y^TX$\n\n2. induction hypothesis:\nSuppose it's true for the value $n-1$ and now I want to prove it's true for n.  \n\n3. The inductive step: \n$det(I+XY^T)=x_1y_1+x_2y_2+...+x_{n-1}y_{n-1}+x_ny_n + 1$\nAnd here is where i pretty much get stuck. I don't know where to go from here. It's kinda hard for me to grasp the idea behind mathematical induction. I don't really know what to do when I come to this step. What can I do to finish the proof? (well, if what I have done so far is correct, that is).\n",
    "proof": "The \"holes-digging\" method might be interesting to prove this.\nOn one hand, dig a hole at the lower-left corner of $A$,\n$$A := \\begin{bmatrix}I & X \\\\\n-Y^T & 1 \\end{bmatrix} = \\begin{bmatrix} I & 0 \\\\\nY^T & 1\\end{bmatrix}\\begin{bmatrix}I & X \\\\\n0 & 1 + Y^TX\\end{bmatrix}$$\nTake determinants on both sides to have $\\det(A) = \\det(I)\\det(I + Y^TX) = \\det(1 + Y^TX)$.\nOn the other hand, dig a hole at the upper-right corner of $A$,\n$$A = \\begin{bmatrix}I & X \\\\\n-Y^T & 1 \\end{bmatrix} = \\begin{bmatrix} I & X \\\\\n0 & 1\\end{bmatrix}\\begin{bmatrix}I + XY^T & 0 \\\\\n-Y^T & 1 \\end{bmatrix}$$\nTake determinants on both sides to have $\\det(A) = \\det(I + XY^T)\\det(1) = \\det(I + XY^T)$.\nTherefore, $\\det(1 + Y^TX) = \\det(I + XY^T)$.\n",
    "tags": [
      "linear-algebra",
      "proof-writing",
      "determinant"
    ],
    "score": 5,
    "answer_score": 2,
    "is_accepted": true,
    "question_id": 1342466,
    "answer_id": 1342525
  },
  {
    "theorem": "Proof check, showing pointwise convergence",
    "context": "My problem is this:\nFor $x \\in [0,\\frac{\\pi}{2}]$, $f_n(x) = \\frac{nx}{1 + n\\sin(x)}$\nFind the pointwise limit of $(f_n)$ for all $x \\in [0, \\frac{\\pi}{2}]$\n\nI am not sure if the way I constructed my \"proof\" is correct, but split it into cases, as follows:\nCase 1: $x \\in (0, \\frac{\\pi}{2}]$\n$$\\lim_{n \\to \\infty} f_n(x) = \\lim_{n \\to \\infty} \\frac{nx}{n \\sin(x) + 1} = \\frac{x}{\\sin(x)}$$\nCase 2: $x=0$\n$$\\lim_{n \\to \\infty} f_n(x) = \\lim_{n \\to \\infty} \\frac{n(0)}{1 + 0} = 0$$\nTherefore, \n$$\\lim_{n \\to \\infty} f_n(x) = f(x)$$\nwhere $$f(x) = 0 , x= 0$$ and $$ f(x) = \\frac{x}{\\sin(x)}, x \\in (0, \\frac{\\pi}{2}]$$\nIs there any way to improve how I have constructed this ?\nI was a little unsure with the choice of doing a \"case 1/case 2\" for this question.\nAny tips are appreciated\n",
    "proof": "Your proof is fine, but I added some points and shortened it.\nYou can say that $f_n \\rightarrow f$ pointwise, where \n$f = \\begin{cases} 0 & \\text{if } x = 0 \\\\ \\frac{x}{\\sin(x)} & \\text{if } x \\in (0, \\frac{\\pi}{2}] \\end{cases}$. \nFor $x = 0$, $\\lim_{n \\rightarrow \\infty}f_n(0) = 0$ and \nFor $x \\in (0,\\frac{\\pi}{2}]$, $\\lim_{n \\rightarrow \\infty}f_n(x) = \\lim_{n\\rightarrow \\infty}\\frac{x}{\\sin(x) + \\frac{1}{n}} = \\frac{x}{\\sin(x)}$ $\\square$\n",
    "tags": [
      "real-analysis",
      "convergence-divergence",
      "proof-writing"
    ],
    "score": 5,
    "answer_score": 2,
    "is_accepted": true,
    "question_id": 1302092,
    "answer_id": 1302117
  },
  {
    "theorem": "Let $G$ denote an arbitrary group. Prove: The center of any group $G$ is a normal subgroup of $G$",
    "context": "\nLet $G$ denote an arbitrary group. Prove: The center of any group $G$ is a normal subgroup of $G$\n\nLet $G$ be a group and $C$ the center, i.e., for any $a \\in C$ and any $x \\in G$, $xa=ax$.\nSo, $a = xax^{-1}$. Thus $xax^{-1} \\in C$. So, $C$ is normal.\nIs this proof correct? It seemed too trivial for me to think so.\nSomething tells me I need to show that for any $y \\in G$, $xax^{-1}y=yxax^{-1}$. Would this be correct? But this would really be the same as above.\n",
    "proof": "Let $G$ be a group and let $Z(G)$ be the center of $G$. \nIf $x \\in Z(G)$, then $xy=yx$ $\\ \\forall y \\in G$. Thus $yxy^{-1}=x \\in Z(G)$.\nSo, $Z(G) \\vartriangleleft G$.\n",
    "tags": [
      "abstract-algebra",
      "group-theory",
      "proof-verification",
      "proof-writing"
    ],
    "score": 5,
    "answer_score": 2,
    "is_accepted": true,
    "question_id": 1011451,
    "answer_id": 1228142
  },
  {
    "theorem": "Proving $f(x)$ attains $\\max$ or $\\min$ when $f(x)\\to0$ as $|x|\\to\\infty$.",
    "context": "\nSuppose $f:\\Bbb R\\to\\Bbb R$ is continuous such that $f(x)\\to0$ as $|x|\\to\\infty$. Prove that $f$ attains either a maximum or a minimum.\n\nMy attempt at the question :\nGiven $\\epsilon > 0 \\ \\ \\exists \\ \\alpha > 0$ such that\n$\\ |f(x)| < \\epsilon\\ \\ \\ \\forall \\ \\ \\  \\ |x| > \\alpha$  \nNow, when  $\\ \\ x \\in [-\\alpha,\\ \\alpha] $, by the Extreme value theorem, since $f(x)$ is a continuous function on a closed interval, it attains a maximum (say $ M$) and a minimum (say $m$).  \nI am stuck at this part. I think the way to move forward is by using the fact that we can make $ \\epsilon $ as small as possible. But I don't know how to prove that it will surely attain a maximum or a minimum or both.\n",
    "proof": "You are basically done. Here are 2 proofs:\nWLOG assume f is not a constant. For every ε we can find an α such that for all x outside of (-α,α), |f(x)| is ε-small. \nSince f is not constant it has a supremum S and infimum I over the whole space with S≠I. Since f is bounded, S and I are not infinite. Since S≠I, at least one is not zero. Choose ε small enough so that max(|S|,|I|)>ε.\nwe then know that there is α such that |f(x)| < ε for all |x|>α, so we only have to maximise/minimise the function on the compact set [-α,α] which we can do by EVT. Either the max is S or the min is I; this proves the result. \nAlternatively, choose $\\alpha$ minimally (as an infimum) and assume without loss $f$ is not constant. Suppose for a contradiction that $f$ does not achieve its sup and also does not achieve its inf. If $M\\geq\\varepsilon$ then M is a global maximum and if $m\\leq-\\varepsilon$ then m is a global minimum. So the only way for this to happen is if $m$ and $M$ are in $(-ε,ε)$; this leads to a contradiction as ε→0. (see chat/comments for details)\n",
    "tags": [
      "real-analysis",
      "proof-writing"
    ],
    "score": 5,
    "answer_score": 2,
    "is_accepted": true,
    "question_id": 970324,
    "answer_id": 970357
  },
  {
    "theorem": "Is this a proof by contradiction?",
    "context": "Below is a proof that any group of order $p^2$ is abelian $(p$ prime of course).\n\nLet $Z \\left({G}\\right)$ be the center of $G$. We know $|Z(G)|>1$. $\\color{blue}{\\text{Suppose}} \\left\\vert{Z \\left({G}\\right)}\\right\\vert = p$.\nThen $|G/ Z(G)|=p\\Rightarrow G/Z(G)$ is cyclic. It is not hard to then show that $G$ is abelian, $\\color{blue}{\\text{therefore}}$ $Z(G)=p^2$. This is a contradiction.\n\nI fully understand the proof, but the logic seems strange to me; mainly, I can't decide whether this is an actual proof by contradiction or whether it is disguised as one. What is actually happening here, in terms of logic?\n",
    "proof": "The proof as written is a proof by contradiction. Logically, the proof is fundamentally a proof by cases: because $Z(G)$ is a subgroup, we have that $|Z(G)|$ is $1$, $p$, or $p^2$. The case $|Z(G)| = 1$ is eliminated by a separate result, and $|Z(G)| = p^2$ is the desired result, so the proof proceeds by contradiction: assume $|Z(G)| = p$ (which is what we don't want to happen) and derive a contradiction.\nThis is not to say that every proof of this result must proceed by contradiction. At the cost of using some slightly more advanced facts, it is possible to write a proof that is not (directly) a proof by contradiction: \n\nWe know (as before) that $Z(G)$ is nontrivial. Let $h$ be any nonidentity element in the $Z(G)$. If $\\langle h \\rangle = G$ we are done. Otherwise, $|h| = p$. Then $\\langle h \\rangle$ is normal, since $h \\in Z(G)$, and $|G / \\langle h \\rangle | = p$. Thus $G / \\langle h\\rangle$ is cyclic. Then, because $\\langle h \\rangle \\subseteq Z(G)$, $G / Z(G)$ is a quotient group of $G / \\langle h \\rangle$, and so $G/Z(G)$ is cyclic. But that implies (by another theorem) that $G$ is abelian, as desired. \n\nIn this proof, we did not assume $|Z(G)| = p$, so there is no direct contradiction. \n",
    "tags": [
      "group-theory",
      "logic",
      "finite-groups",
      "proof-writing",
      "abelian-groups"
    ],
    "score": 5,
    "answer_score": 2,
    "is_accepted": true,
    "question_id": 799152,
    "answer_id": 799191
  },
  {
    "theorem": "Best proof of some theorems in calculus",
    "context": "I would like to choose (among the miriads of proofs) a well-structured, elegant, neat, clear proof of\n\nthe first fundamental theorem of calculus;\nthe second fundamental theorem of calculus;\nthe mean value theorem;\nthe intermediate value theorem;\nWeierstrass theorem;\nRolles' theorem.\n\nWhich book (I can choose among Spivak, Stewart, Apostol and a few other ones) offers the well-structured, elegant, neat, clear proof of them (NB you can pick a different book for every theorem if it is the case)? \nAdded: Do you think that Apostol's proofs are elegant and rigorous?\n",
    "proof": "In my humble opinion, Rudin's \"Principles of Mathematical Analysis\" is what you are seeking.\n",
    "tags": [
      "calculus",
      "soft-question",
      "proof-writing"
    ],
    "score": 5,
    "answer_score": 2,
    "is_accepted": false,
    "question_id": 738825,
    "answer_id": 739026
  },
  {
    "theorem": "Prove that one of the following sets is a subspace and the other isn&#39;t?",
    "context": "OK, here goes another. \nProve that $ W_1 = ${$(a_1, a_2, \\ldots, a_n) \\in F^n : a_1 + a_2 + \\cdots + a_n = 0$} is a subspace of $F^n$ but $ W_2 = ${$(a_1, a_2, \\ldots, a_n) \\in F^n : a_1 + a_2 + \\cdots + a_n = 1$} is not. \nOK. Any subspace has to contain the zero vector, be closed under addition and scalar multiplication by definition. So to prove this we first see whether the set $W_1$ meet those criteria. Plugging in 0 for $a_i$ obviously works, so the first condition is met. \nIs it closed under addition? let $b_i$ be the components of an arbitrary vector in $W_1$. So ($b_1, b_2, \\ldots, b_n) \\in W_1$ and if we add it to $(a_1, a_2, \\ldots, a_n)$ we get $(a_1 + b_1) + (a_2 + b_2) + \\cdots +(a_n + b_n) = 0$. That's pretty clearly part of $W_1$ and thus closed under addition.  \nNext we see if it is closed under multiplication by a scalar. We pick an arbitrary scalar $c$ and multiply it by $(a_1, a_2, \\ldots, a_n)$ to get $(ca_1, ca_2, \\ldots, ca_n)$ and plugging that into the original condition we find that it doesn't matter what c is, because $ca_1 + ca_2 + \\cdots + ca_n = 0$ and that's still in $W_1.$ Therefore $W_1$ is a subspace of $F^n$. \nIf we do the same procedure with $W_2$, though, we find that $0$ vector is not in the set. Because $a_1 + a_2 + \\cdots + a_n = 1$ is a contradiction. \nFurther, we can see that it isn't closed under multiplication either. $ca_1 + ca_2 + \\cdots + ca_n = c$ and that will only equal 1 if c=1, so the equation does not hold with an arbitrary $c$. \nTherefore $W_2$ is NOT a subspace of $F^n$. \nAny holes in this proof? \n(Yeah, I have been bothering folks here a lot but I finally feel that I am getting the hang of this and I have an exam tomorrow night).\n",
    "proof": "Your proof is fine.  Also, to prove that a subset $S$ is not subspace, it suffices to show that at least one of the conditions ($0 \\in S$, closure under addition and scalar multiplication) fails.  So as soon as you show that $0 \\notin S$, your proof is complete.  \n",
    "tags": [
      "linear-algebra",
      "vector-spaces",
      "proof-writing"
    ],
    "score": 5,
    "answer_score": 2,
    "is_accepted": true,
    "question_id": 422486,
    "answer_id": 422491
  },
  {
    "theorem": "If F→G is a consequence of F, then so is &#172;G→&#172;F. A direct proof?",
    "context": "Homework question (introduction to logic):\n\"If $F \\to G$ is a consequence of $\\mathcal F$, then so is $\\lnot G \\to \\lnot F$. We refer to this rule as $\\to$-contrapositive. Verify this rule by giving formal proof.\"\n\nMy attempt at proving it (by contradiction):\n$H: \\lnot G \\to \\lnot F$\n$(1)\\quad\\mathcal F\\vdash F \\to G \\quad$ {assumption}\n$(2)\\quad\\mathcal F \\cup \\{ \\lnot H \\} \\vdash \\lnot ( \\lnot G \\to \\lnot F) \\quad$ {assumption for contradiction}\n$(3)\\quad\\mathcal F \\cup \\{ \\lnot H \\} \\vdash \\lnot(\\lnot F \\lor G)\\quad$ {from $\\vdash \\lnot (\\phi \\to \\psi) \\equiv \\psi \\lor \\lnot \\phi$, and line: 2}\n$(4)\\quad\\mathcal F \\cup \\{ \\lnot H \\} \\vdash F \\land \\lnot G\\quad \\quad${from deMorgan's Law and line: 3}\n$(5)\\quad\\mathcal F \\cup \\{ \\lnot H \\} \\vdash F \\quad \\quad$ {$\\land$ - elimination, line: 4}\n$(6)\\quad\\mathcal F \\cup \\{ \\lnot H \\} \\vdash \\lnot G \\quad \\quad$ {$\\land$ - elimination, line: 4}\n$(7)\\quad\\mathcal F \\cup \\{ \\lnot H \\} \\vdash G \\quad \\quad$ {$\\to$ - elimination, lines: 1, 5}\n$(8)\\quad\\mathcal F \\cup \\{ \\lnot H \\} \\vdash G \\land \\lnot G\\quad \\quad$ {$\\land$ - introduction, lines: 6, 7}\n$\\quad \\quad\\mathcal F \\vdash H  .\\square$\nquestion 1: Can you verify if it's OK/ give hints what's wrong if it's not?question 2: Is there a direct proof?\n[edited]\n",
    "proof": "A direct proof has the outline:\n$H: \\lnot G \\to \\lnot F$\n$(1)\\quad\\mathcal F\\vdash F \\to G \\quad$ {assumption}\n$(2)\\quad\\mathcal F \\vdash \\lnot G$ {assumption for direct proof}\n...\n$(n)\\quad\\mathcal F \\vdash \\lnot F$ {reasons...}\nThe steps are very simlar to your proof by contradiction. I leave the details to be filled in by the reader.\n",
    "tags": [
      "logic",
      "proof-writing"
    ],
    "score": 5,
    "answer_score": 2,
    "is_accepted": true,
    "question_id": 341865,
    "answer_id": 341927
  },
  {
    "theorem": "Integral calculus proof",
    "context": "If $f(x)$ is continuous in $[a,b]$, prove that $ \\displaystyle \\lim_{n \\to \\infty} \\dfrac{b-a}{n} \\displaystyle \\sum^n _{k=1} f\\left( a + \\dfrac{k(b-a)}{n} \\right) = \\displaystyle \\int_a ^ b f(x)dx$\nThis is my first time I'm exposed to these type of math problems (being a high schooler), so I don't really know how to tackle this. Can anyone point me in the right direction?\nWhat I tried:\nI just tried to see the logic behind the RHS. I see that $\\dfrac{b-a}{n}$ divides the interval a,b into n rectangles. What I totally don't understand is why the height of these rectangles is given by the summation of $ f\\left(a + \\dfrac{k (b-a)}{n}\\right)$\n",
    "proof": "You will need to use the fact that $f$ is uniformly continuous on $[a,b]$.   \n",
    "tags": [
      "integration",
      "proof-writing"
    ],
    "score": 5,
    "answer_score": 2,
    "is_accepted": true,
    "question_id": 314020,
    "answer_id": 314028
  },
  {
    "theorem": "Is this proof that $\\Delta$ is countable correct?",
    "context": "I have to prove the following \n\nTHEOREM Let $f:[a,b]\\to\\Bbb R$ be such that  $\\lim\\limits_{y\\to x}f(y)$ exists for every $x\\in[a,b]$. Then the set $\\Delta\\subset[a,b]$ where $f$  is discontinuous is countable. \n\nFirst \n\nPROPOSITION Let $f:[a,b]\\to\\Bbb R$ be such that  $\\lim\\limits_{y\\to x}f(y)$ exists for every $x\\in[a,b]$. Define $g:[a,b]\\to\\Bbb R$ as $$g(x)=\\lim_{y\\to x}f(y)$$ Then $g$ is continuous.\n\nPROOF\nLet $\\alpha\\in[a,b]$. Then $g(\\alpha)=\\lim_{y\\to\\alpha}f(y)$. Thus, for $\\epsilon >0$ given there exists a $\\delta>0$ such that $0<|y-\\alpha|<\\delta$ implies $|g(\\alpha)-f(y)|<\\epsilon/2$. This is $g(\\alpha)-\\epsilon/2<f(y)<g(\\alpha)+\\epsilon/2$. Let $x$ such that $0<|x-\\alpha|<\\delta$. Then\n$$g(\\alpha)-\\epsilon/2\\leq \\lim_{y\\to x}f(y)\\leq g(\\alpha)+\\epsilon/2$$\nBut this means $|g(x)-g(\\alpha)|\\leq \\epsilon/2<\\epsilon$.$\\;\\blacktriangle$\nNow we can move on:\nPROOF Let $\\epsilon >0$ and consider the set $$\\Lambda(\\epsilon)=\\left\\{x\\in[a,b]:\\left|\\lim\\limits_{y\\to x}f(y)-f(x)\\right|>\\epsilon\\right\\}$$\nThe claim is that $\\Lambda(\\epsilon)$ is finite for every choice of $\\epsilon$. Indeed, suppose $\\Lambda(\\epsilon)\\subset[a,b]$ was not finite. Then, it has an accumulation point, $\\lambda$, in $[a,b]$. By hypothesis $\\lim\\limits_{y\\to \\lambda}f(y)$ exists. Thus for this $\\epsilon >0$ there exists a $\\delta >0$ such that \n$$\\tag 1 \\left|f(y)-\\lim_{x\\to \\lambda}f(x)\\right|<\\epsilon /2$$\nwhenever $0<|y-\\lambda |<\\delta$. Since $\\lambda$ is an accumulation point, for each $\\delta >0$ there exists a $w\\in\\Lambda(\\epsilon)$ such that $0<|w-\\lambda|<\\delta$ and \n$$\\tag 2 \\left|f(w)-\\lim_{x\\to w}f(x)\\right|>\\epsilon$$\nAnd because of the previous proposition, for this $\\epsilon>0$ there exists $\\delta' >0$ such that for each $y$ with $0<|y-\\lambda|<\\delta'$ we have \n$$\\tag 3 \\left|\\lim_{x\\to y}f(x)-\\lim_{x\\to \\lambda}f(x)\\right|<\\epsilon/2$$\nLet $\\delta''=\\min(\\delta,\\delta')$. We thus obtain from $(1)$,$(2)$ and $(3)$ a $w\\in \\Lambda(\\epsilon)$ such that $0<|w-\\lambda|<\\delta''$ with \n$$ \\left|f(w)-\\lim_{x\\to \\lambda}f(x)\\right|<\\epsilon /2$$\n$$ \\left|f(w)-\\lim_{x\\to w}f(x)\\right|>\\epsilon$$\n$$ \\left|\\lim_{x\\to w}f(x)-\\lim_{x\\to \\lambda}f(x)\\right|<\\epsilon/2$$\nwhich, by the triangle inequiality, is absurd. Then $\\Lambda(\\epsilon)$ is finite for each $\\epsilon>0$. In particular, for each $n\\in\\Bbb N$,  $\\Lambda(1/n)$ is finite. But\n$$\\bigcup_{n\\in\\Bbb N}\\Lambda(1/n)=\\Delta$$ which is the countable union of finite sets, whence, it is at most countable.$\\; \\blacktriangle$\n\nADD I cannot understand Spivak's proof of this. He considers $[0,1]$ in his exercise (I had forgotten) \n\nIf the set of such $a$ was infinite, it would have an accumulation point in $[0,1]$. Call it $x$. For every $\\delta >0$ there would be an $a$ such that $0<|x-a|<\\delta/2$ and $$\\tag 1 \\lim_{y\\to a}f(y)-f(a)|>\\epsilon$$ Thus, there would exist an $a'$ such that $|a'-a|<\\delta/2$ (from where $0<|x-a'|<\\delta$) such that $$\\tag 2 |f(a')-f(a)|>\\epsilon$$ But since $\\ell=\\lim\\limits_{y\\to x }f(y)$ for some $\\ell$, there exists a $\\delta >0$ such that $$|f(y)-\\ell|<\\epsilon/2$$ whenever $0<|y-x|<\\delta$. In particular, if $0<|x-a|<\\delta$ and $0<|x-a'|<\\delta$ then $\\epsilon < |f(a')-f(a)|<|f(a')-\\ell|+|f(a)-\\ell|<\\epsilon$\n  which is absurd.\n\nMy question is: where does he get $$\\epsilon < |f(a')-f(a)|$$ from?\n",
    "proof": "In Spivak’s proof let $$L=\\lim_{y\\to a}f(a)\\;,$$ and let \n$$d=|L-f(a)|=\\left|\\lim_{y\\to a}f(y)-f(a)\\right|>\\epsilon\\;.$$\nThen there is an $\\eta>0$ such that $|L-f(y)|<\\frac12(d-\\epsilon)$ whenever $0<|y-a|<\\eta$ and $y\\in[0,1]$, and for all such $y$ we have \n$$|f(y)-f(a)|>d-\\frac12(d-\\epsilon)=\\frac12(d+\\epsilon)>\\epsilon\\;.$$\nNow just pick $a'$ so that $0<|a'-a|<\\min\\left\\{\\frac{\\delta}2,\\eta\\right\\}$.\n",
    "tags": [
      "calculus",
      "real-analysis",
      "proof-writing"
    ],
    "score": 5,
    "answer_score": 1,
    "is_accepted": true,
    "question_id": 240843,
    "answer_id": 240875
  },
  {
    "theorem": "&quot;Algorithmic&quot; proofs in linear algebra",
    "context": "Although I am new to linear algebra, I want to study it with as much rigor as possible. After searching around, I picked up Halmos' Finite Dimensional Vector Spaces and Axler's Linear Algebra Done Right. \nI've noticed that they state theorems which they prove by a method which I would describe as \"algorithmic\". For example, verbatim from Axler (although Halmos is very similar):\n\nTheorem: In a finite-dimensional vector space, the length of every lin. ind. tuple is $\\leq$ to the length of every spanning tuple\n  of vectors. \nProof: Suppose that ($u_1$, ... $u_m$) is lin. ind. in $\\mathcal{V}$ and ($w_1$, ... $w_n$) spans $\\mathcal{V}$. We need to prove $m \\leq n$. We do so through\n  the multi-step process described below....\nStep 1: The tuple $(w_1, ... w_n)$ spans $\\mathcal{V}$, and thus adjoining any vector produces a linearly dependent tuple. In particular, the tuple $(u_1, w_1, ... w_n)$ is linearly independent. Thus, by the linear dependence lemma, we can remove one of the $w$'s so that the n-tuple B consisting of $u_1$ and the remaining $w$'s spans $\\mathcal{V}$. \nStep j: The n-tuple B from step $j-1$ spans $\\mathcal{V}$, and thus adjoining any vector to it produces a linearly dependent tuple. In particular, the $(n+1)$-tuple obtained by adjoining $u_j$ to B, placing it just after $u_1,...u_{j-1},$ is linearly dependent. By the linear dependence lemma (2.4), one of the vectors in this tuple is in the span of the previous ones.... We can remove that $w$ from $B$ so that the new $n$-tuple $B$ consisting of $u_1, ... u_j$ and the remaining $w$'s spans $\\mathcal{V}$. \nAfter step $m$, we have added all the $u$'s and the process stops. If at any step we added a $u$ and had no more $w$'s to remove, then we would have a contradiction. Thus there must be at least as many $w$'s as $u$'s.  \n\nI take issue with the level of rigor of \"algorithmic\" proof. Although I think these proofs might be amenable to treatment by induction, I'm not sure how to carry it out. As they stand, although I get the intuition, they don't really convince me. \nIf I had to be precise about what bothers me, I'd say that the actual sets resulting from each step of the operation aren't stated explicitly, and I'm not sure how these sets are being ordered/indexed (they play fast and loose there). \n(Disclosure: I am generally not thrilled with ...'s, unless I can see a clear way to come up with an argument which doesn't rely on imagining \"what's going on in there\", so dealing with about ten of these arguments at one sitting is irritating for me.)\nIs there a way to make these arguments - in particular, this one - more precise?\n",
    "proof": "Prove by induction $k$ that for $0\\le k\\le m$ there is a tuple $(v_1,\\ldots, v_n)$ such that \n\n$(v_1,\\ldots, v_n)$ spans $\\mathcal V$\n$k\\le n$\n$v_i=u_i$ for $1\\le i \\le k$\n$v_i\\in\\{w_1,\\ldots,w_n\\}$ for $k<i\\le m$.\n\nFor $k=0$ this is given. For $k=m$ it especially says $m\\le n$.\nThe induction step from $k\\to k+1$ works just as described in the book.\n",
    "tags": [
      "linear-algebra",
      "vector-spaces",
      "proof-writing"
    ],
    "score": 5,
    "answer_score": 2,
    "is_accepted": false,
    "question_id": 214454,
    "answer_id": 214463
  },
  {
    "theorem": "Proof of max product of partitions of n",
    "context": "For $n \\in \\mathbb{Z} : n \\geq 1$\n$\r\nf(n) = \\displaystyle\\max_{\\substack{\tx_1+\\dotsm+x_k = n\\\\ x_i\\in\\mathbb{Z}^{+} }}\r\n          x_1 x_2 \\dotsm x_k\r\n$\n$$\nf(n) = \\begin{cases}\n    1 & \\text{if $n = 1$}, \\\\\n    3^{\\left\\lfloor\\frac{n}{3}\\right\\rfloor}& \\text{if $n \\mod 3 = 0$},\\\\\n    3^{\\left\\lfloor\\frac{n}{3}\\right\\rfloor-1} \\cdot 2^2& \\text{if $n \\mod 3 = 1$},\\\\\n    3^{\\left\\lfloor\\frac{n}{3}\\right\\rfloor} \\cdot 2& \\text{if $n \\mod 3 = 2$},\n\\end{cases} $$\nProof:\nFirst observe that for any $x_i = 1$ in our product we do not increase the value of the final product. Therefore we want $x_i > 1$. However, for any set of three $x_i = 2$ we want to refactor this set into two $x_i = 3$ since $3\\cdot3 > 2\\cdot2\\cdot2$. Now, for any $x_i > 4$ observe that $2(x_i-2) = 2x_i - 4 > x_i$. Thus to maximize our product we have at most two 2's and as many 3's as possible.\nWhat is my proof possibly missing?\nHere's an updated proof.\nProof:\nFirst observe that for any $x_i = 1$ in our product we do not increase\nthe value of the final product. Therefore we want $x_i > 1$. However,\nfor any set of three $x_i = 2$ we want to refactor this set into two\n$x_i = 3$ since $3*3 > 2*2*2$. Now, for any $x_i > 4$ observe that\nsubtracting 2 from our term increases our total product. That is\n$2(x_i-2) = 2x_i - 4 > x_i$ for $x_i > 4$. We can repeat this process\nuntil $x_i \\leq 4$ and this limits our individual factors to being no\ngreater than 4. For any $x_i = 4$ where there are no existing factors\nequal to 2 we do not necessarily need to factor this into $2 \\cdot 2$\nsince this product is equal to $4$ and will not increase our total\nproduct. Thus to maximize our product we have at most two 2's, or a single 4, and\nas many 3's as possible. This leads to our piecewise defined function.\nFor any n divisible by 3 we will have a product comprised of 3 raised to\nthe power $\\frac{n}{3}$. If we have a remainder of 1 when dividing n by\n3 we want one less factor of 3 than $\\frac{n}{3}$ provides so that we\ncan create a factor of $2^2$ or $4$. Finally, if n divided by 3 has a\nremainder of 2 we will just include the remainder as a factor in our\nfinal product.\n",
    "proof": "Your overall proof strategy looks good. But I found two problems with the proof.\nFirst a nitpick. Look at your sentence:\n\nNow, for any $x_i > 4$, observe that $2(x_i - 2) = 2x_i-4 > x_i$. \n\nI do not immediately see what to conclude from this observation, or why I must conclude that. Using this argument, I presume you are convincing me that none of the factors can be $> 4$, but this was not explicitly stated anywhere. Neither is the justification immediately clear. I would therefore rewrite this to:\n\nNow, if any $x_i > 4$, then splitting $x_i$ into a pair of factors, namely $2$ and $x_i - 2$, would strictly increase the product, since $2(x_i - 4) = 2x_i - 4 > x_i$. Hence none of the factors can exceed $4$. \n\n\nNow, a gap in the proof. Your argument leaves out the possibility that some of the factors are $4$. Now, it is wrong to claim that $4$ cannot appear as a factor in the largest product; for example, if $n = 7$, then the partitions $\\{ 3, 2, 2 \\}$ and $\\{ 3,4 \\}$ have the same product of $12$, which is also the best possible.  \nSo what is the correct way to handle the factors of $4$? Taking a clue from the $x_i > 4$ case, I would think that splitting $4$ as $2+2$ would be of some help. Can you complete the argument for this case? \n",
    "tags": [
      "number-theory",
      "proof-writing"
    ],
    "score": 5,
    "answer_score": 2,
    "is_accepted": false,
    "question_id": 62201,
    "answer_id": 62211
  },
  {
    "theorem": "In a group, the number of non-self-inverse elements is even",
    "context": "The problem I am attempting to prove is the following: In any finite group $G$, the number of elements not equal to their own inverse is an even number.\nCaveat: I have had very limited experience with proofs. Any comments would be appreciated.\nLet $a_{1}, a_{2}, ..., a_{n}$ be elements of $G$. Since $G$ is a group, every element in $G$ must have an inverse in $G$. Let the inverse of each element be represented by $a_{1}^{-1}, a_{2}^{-1}, ..., a_{n}^{-1}$. An element $a_{k}$ in $G$ that is not equal to its own inverse can be written as $a_{k} \\neq a_{k}^{-1}$. For every element $a_{k}$, there is another element $a_{k}^{-1}$ that is not equal to its own inverse denoted by $a_{k}^{-1} \\neq a_{k}$. So for every element $a_{k}$ in $G$ whose inverse is not equal to their own inverse, there will be 2 elements in $G$ namely $a_{k}$ and $a_{k}^{-1}$. Let the integer $q$ denote the number of elements ranging from $a_{1}, a_{2}, ..., a_{n}$ who are not equal to their own inverse. In addition for every value $q$, there must be another element in $a_{1}^{-1}, a_{2}^{-1}, ..., a_{n}^{-1}$ not equal to their own inverse. So the total number of elements not equal to their own inverse can be denoted by 2$q$, which is an even number.\n",
    "proof": "Yes. The map $\\rm n \\mapsto n^{-1}$ is a permutation of order 2 so it decomposes into cycles of length 1 or 2. So non-fixed-points are paired with their inverse. This leads to a proof of Wilson's theorem for groups.\nAn analogous pairing arises for any nontrivial self-inverse map. Such involutions are ubiquitous. For example, the involution $\\rm\\: d \\to n/d\\:$ on the divisors of $\\rm\\:n\\:$ implies that nonsquare integers have an even number of divisors. Here are some further examples proofs using  involutions.\n",
    "tags": [
      "abstract-algebra",
      "group-theory",
      "proof-writing"
    ],
    "score": 5,
    "answer_score": 2,
    "is_accepted": false,
    "question_id": 52999,
    "answer_id": 53003
  },
  {
    "theorem": "Question About Proving The Countable Subadditivity of $\\mu^*:\\mathcal{P}(\\mathbb{R})\\to[0,+\\infty]$",
    "context": "Let $F:\\mathbb{R}\\to\\mathbb{R}$ be a bounded, nondecreasing, and right-continuous function that satisfies $\\lim_{x\\to-\\infty}F(x)=0$. Define a function $\\mu^*:\\mathcal{P}(\\mathbb{R})\\to[0,+\\infty]$ by letting $\\mu^*(A)$ be the infimum of the set of sums $\\sum_{n=1}^{\\infty}(F(b_n)-F(a_n))$, where $\\{(a_n,b_n]\\}$ rangers over the set of sequences of half-open intervals that cover $A$, in the sense that $A \\subseteq \\bigcup_{n=1}^{\\infty}(a_n,b_n]$.\nI need to prove the following:\n\nIf $\\{A_n\\}$ is an infinite sequence of subsets of $\\mathbb{R}$, then $\\mu^*(\\bigcup_nA_n)\\leq\\sum_n\\mu^*(A_n)$.\n\nI would like to precede in the following way:\n\nLet $\\{A_n\\}_{n=1}^{\\infty}$ be an arbitrary sequence of subsets of $\\mathbb{R}$. If $\\sum_n\\mu^*(A_n)=+\\infty$, then $\\mu^*(\\bigcup_nA_n) \\leq \\sum_n\\mu^*(a_n)$ certainly holds. So suppose that $\\sum_n\\mu^*(A_n)<+\\infty$, and let $\\epsilon$ be an arbitrary positive number. For each $n$ choose a sequence $\\{(a_{n,i},b_{n,i}]\\}_{i=1}^{\\infty}$ that covers $A_n$ and satisfies\n\\begin{align*}\n    \\sum_{i=1}^{\\infty}(F(b_{n,i}) - F(a_{n,i})) < \\mu^*(A_n) + \\frac{\\epsilon}{2^n}.\n\\end{align*}\nIf we combine these sequences into one sequence $\\{(a_j,b_j]\\}$, then the combined sequence satisfies\n\\begin{align*}\n    \\bigcup_nA_n \\subseteq \\bigcup_j(a_j,b_j)\n\\end{align*}\nand\n\\begin{align*}\n    \\sum_j(F(b_j) - F(a_j)) < \\sum_n\\left(\\mu^*(A_n) + \\frac{\\epsilon}{2^n}\\right) = \\sum_n\\mu^*(A_n) + \\epsilon.\n\\end{align*}\nThese relations, together with the fact that $\\epsilon$ is arbitrary, imply that $\\mu^*(\\bigcup_nA_n) \\leq \\sum_n\\mu^*(A_n)$.\n\nHowever, I feel that my description is not rigorous enough. Especially, I claimed the existence of a sequence $\\{(a_{n,i},b_{n,i}]\\}_{i=1}^{\\infty}$ that covers $A_n$ and satisfies $\\sum_{i=1}^{\\infty}(F(b_{n,i}) - F(a_{n,i})) < \\mu^*(A_n) + \\frac{\\epsilon}{2^n}$. However, I have difficulties construct a concrete sequence to show that this is indeed true. Here is some of my thought:\n\nDefine a sequence $\\{(c_{n,k},d_{n,k}]\\}$ of half-open intervals such that $\\bigcup_k(c_{n,k},d_{n,k}] = \\bigcup_i(a_{n,i},b_{n,i}]\\backslash A_n$ and that $d_{n,k} = c_{n,k}+p_{n,k}$, where $\\{p_{n,k}\\}_{k=1}^{\\infty}$ is a sequence of positive number converging to $0$ such that for every $\\epsilon>0$ we have $F(d_{n,k}) - F(c_{n,k}) = F(c_{n,k}+p_{n,k}) - F(c_{n,k}) < \\frac{\\epsilon}{2^k}\\cdot\\frac{1}{2^n}$. Note that such a sequence $\\{p_{n,k}\\}_{k=1}^{\\infty}$ exists due to the right-continuity of $F$. Then\n\\begin{align*}\n\\sum_{i=1}^{\\infty}(F(b_{n,i}) - F(a_{n,i})) &= \\sum_{k=1}^{\\infty}(F(d_{n,k}) - F(c_{n,k})) + \\inf\\left(\\sum_{i=1}^{\\infty}(F(b_{n,i}) - F(a_{n,i}))\\right)\\\\\n&< \\frac{\\epsilon}{2^n} + \\mu^*(A_n).\n\\end{align*}\n\nI seriously doubt that my attempt is entirely correct and rigorous. But this is really the best I can get. I would really appreciate it if someone could help me check my construction and correct my proof if there is any flaws!\nReference: Measure Theory 2nd Edition by Donald L. Cohn Proposition 1.3.10.\n",
    "proof": "$\\mu^{*}$ by definition is the infimum of that set, so you can take an element from that set which is smaller than $\\mu^{*}+\\epsilon$\n",
    "tags": [
      "real-analysis",
      "sequences-and-series",
      "measure-theory",
      "solution-verification",
      "proof-writing"
    ],
    "score": 5,
    "answer_score": 1,
    "is_accepted": true,
    "question_id": 4885821,
    "answer_id": 4885861
  },
  {
    "theorem": "Proving $\\lim_{n\\to\\infty}f_n(x)$ doesn&#39;t exist for any $x\\in[0,1],$ where $(f_n)_{n\\in\\Bbb N}$ is the Typewriter sequence",
    "context": "This problem has been discussed before, but I tried to write a more detailed proof based on this answer.\n\nProblem: prove that $\\displaystyle\\lim_{n\\to\\infty}f_n(x)$ does not exist for any $x\\in [0,1]$ if the sequence $(f_n)_{n\\in\\Bbb N}$ of functions $f_n:\\Bbb R\\to\\Bbb R$ is given by $$f_n=1_{\\left[\\frac{n-2^k}{2^k},\\frac{n-2^k+1}{2^k}\\right]},\\text{ where }k=\\left\\lfloor\\log_2 n\\right\\rfloor.$$\n\nMy attempt:\nLet's write $n\\in\\Bbb N$ as $n=2^m+k,$ where $m\\in\\Bbb N\\cup\\{0\\}$ and $k\\in\\{0,\\ldots,2^m-1\\}.$ Then, \\begin{aligned}\\frac{n}{2^k}-1&=\\frac{2^{\\log_2(2^m+k)}}{2^{\\left\\lfloor\\log_2\\left(2^m+k\\right)\\right\\rfloor}}-1\\\\&=2^{\\log_2\\left(2^m+k\\right)-\\left\\lfloor\\log_2\\left(2^m+k\\right)\\right\\rfloor}-1\\\\&=2^{m+\\log_2\\left(1+\\frac{k}{2^m}\\right)-m}-1\\\\&=2^{\\log_2\\left(1+\\frac{k}{2^m}\\right)}-1\\\\&=\\frac{k}{2^m}.\\end{aligned}\nWe can now find an increasing sequence $\\left(\\frac{p_n}{q_n}\\right)_{n\\in\\Bbb N}$ of fractions with $q_n$ being powers of $2$ s. t. $\\displaystyle\\lim_{n\\to\\infty}\\frac{p_n}{q_n}=x,$ which shows that for each $x\\in[0,1],$ there is a subsequence $\\left(f_{p_n}\\right)_{n\\in\\Bbb N}$ s. t. $f_{p_n}(x)=1,\\forall n\\in\\Bbb N.$\nOn the other hand, if $a_n:=\\frac{n-2^{k_n}}{2^{k_n}},k_n=\\left\\lfloor\\log_2(n)\\right\\rfloor,$ and we consider the following subsequence: \\begin{aligned}b_m:=a_{2^m-1}&=\\frac{2^m-1}{2^{k_{2^m-1}}}-1\\\\&=\\frac{2^m-1}{2^{\\left\\lfloor\\log_2\\left(2^m-1\\right)\\right\\rfloor}}-1\\\\&=\\frac{2^m-1}{2^{m-1}}-1\\\\&=\\frac{2^{m-1}}{2^{m-1}}\\cdot\\left(2-2^{1-m}\\right)-1\\\\&=1-2^{1-m},\\\\\\lim_{m\\to\\infty}b_m&=1\\end{aligned} we see that, for any given $x\\in[0,1],$ there is a subsequence $\\left(f_{q_n}\\right)_{n\\in\\Bbb N}$ s. t. $f_{q_n}(x)=0,\\forall n\\in\\Bbb N.$\nTherefore, the limit $\\displaystyle\\lim_{n\\to\\infty}f_n(x)$ doesn't exist for any $x\\in[0,1].$\nIs there anything wrong with my deduction and how can I improve my answer?\n",
    "proof": "Since $k\\neq\\lfloor\\log_2(2^m+k)\\rfloor$ in general, your first line of working: $$\\frac{n}{2^k}-1=\\frac{2^{\\log_2(2^m+k)}}{2^{\\lfloor\\log_2(2^m+k)\\rfloor}}-1$$Is wrong. I assume you meant, $\\frac{n}{2^m}-1=\\cdots=\\frac{k}{2^m}$. But that is a trivial calculation that does not need to involve (dubious?) manipulations with logarithms: $$\\frac{n}{2^m}-1=\\frac{n-2^m}{2^m}=\\frac{k}{2^m}$$Your stated equation $\\frac{n}{2^{\\color{red}{k}}}-1=\\frac{k}{2^m}$ is wrong. Note that if $n=2^m+k$ with $0\\le k<2^m$, $\\lfloor\\log_2(n)\\rfloor=m$, not $k$, so there is a confusion of notation with the $k$s appearing in the definition of the typewriter sequence.\nI would say something like, \"fix $x\\in[0,1]$\", to preface the next section. It's not clear to me at all why you expect $f_{p_n}(x)=1$, e.g. it is not given that $q_n=2^{\\lfloor\\log_2(p_n)\\rfloor}$ or anything like that. You've shown that $b_m\\to1$ but why should that imply the existence of $(q_n)_n$ with $f_{q_n}(x)\\to0$? What if $x=1$?\nIf $x<1$ then you have shown there is a sequence along which $f_{b_m}(x)\\to0$, but the case $x=1$ remains to be handled. However, the idea of showing there is a sequence along which $f_{b_m}(x)\\to1$ is not, in any way clear to me, handled yet.\n\nFix $x\\in(0,1)$ and $n\\in\\Bbb N$. Take $m:=1+\\max(n,\\lceil x^{-1}\\rceil,\\lceil(1-x)^{-1}\\rceil)\\in\\Bbb N$. Using $2^m>m$, by choice of $m$ I know $x\\in(2^{-m},1-2^{-m})$ so that there exists a unique integer $1\\le k\\le 2^m-1$ satisfying: $$\\frac{k}{2^m}\\le x\\le\\frac{k+1}{2^m}$$I define $n_1:=2^m$ and $n_2:=2^m+k$ for this choice of $k$ and $m$ (my $m$ is actually a lot larger than it needs to be, but I don't want to clutter the notation with lots of logs).\n\nFor any $n\\in\\Bbb N$, such $n_1,n_2\\in\\Bbb N$ exist with $n_1,n_2$ both greater than $n$\nFor these $n_{1,2}$, $f_{n_1}(x)=0$ and $f_{n_2}(x)=1$.\n\nHence $(f_n(x))_{n\\in\\Bbb N}$ is not convergent.\nI invite you to handle the cases $x=0,1$ yourself.\n",
    "tags": [
      "real-analysis",
      "solution-verification",
      "proof-writing"
    ],
    "score": 5,
    "answer_score": 1,
    "is_accepted": true,
    "question_id": 4702990,
    "answer_id": 4703520
  },
  {
    "theorem": "Verify: A differentiable function has a continuous approximation function",
    "context": "A function $s_a$ is defined as a secant function of $f$ at $a$ if $f(x) - f(a) = s_a(x)(x-a)$.  Prove that $f$ is differentiable at $a$ if and only if there exists a secant function $s_a$ continuous at $a$, in which case $s_a(a) = f'(a)$.  Use this to prove the Chain Rule.\nNote: Proofs are available.  This question asks for verification and critique of this proof.\nProof: For a fixed $f$ and $a$, each value at $a$ uniquely determines exactly one secant function $s_a$.  Thus, a secant function continuous at $a$ exists if and only if there exists a value at $a$ which makes the corresponding $s_a$ continuous at $a$.  This, in turn, is met if and only if $s_a(x)$ has a limit as $x \\to a$.  But\n$$\\lim_{x \\to a}s_a(x) = \\lim_{x \\to a}\\frac{f(x) - f(a)}{x-a} = f'(a).$$\nThus, if and only if $f'(a)$ exists, then a $s_a$ continuous at $a$ exists with $s_a(a) = f'(a)$.\nChain Rule: Given $f$ differentiable at $a$ and $g$ differentiable at $f(a)$, then $g \\circ f$ is differentiable at $a$ with derivative $g'(f(a)) \\cdot f'(a)$.\nProof: By the theorem above, we have\n$$\\begin{align*}f(x) - f(a) &= s_a(x)(x-a) \\\\\ng(x) - g(f(a)) &= r_{fa}(x)(x - f(a))\\end{align*}$$\nwith $s_a$ continuous at $a$ and $r_{fa}$ at ${f(a)}$. Consequently, $$q(x) := g'(f(x)) \\cdot f'(x) = r_{fa}(f(x)) \\cdot s_a(x)$$ is continuous at $x=a$.  Therefore\n$$\\begin{align*}q(x)(x-a) &= r_{fa}(f(x)) \\cdot s_a(x)(x - a) \\\\\n&= r_{fa}(f(x)) \\cdot (f(x) - f(a))) \\\\\n&= g(f(x)) - g({f(a)}) \\\\\n&= (g \\circ f)(x) - (g \\circ f)(a)\\\\\n\\end{align*}$$\nand $g \\circ f$ is differentiable at $a$ with derivative $q(a)$, completing the proof.\nQuestions: Is the proof correct and rigorous? How could the writing be made more clear?\n\nUpdate: I haven't received any responses.  Could someone comment if:\n\nThe proof is correct (and so no comment because no obvious errors)\nThe proof is incorrect, unclear, or otherwise jumbled (and so no comment because too jumbled to verify pr discuss)\n\n",
    "proof": "Your proofs are correct and rigorous basically, as I have checked. Here are some comments.\nOn the proof for the equivalence of the differentiability of $f$ and the existence of a continuous \"secant function\"\nIt is clearer to prove the \"if\" direction and \"only if\" direction separately in general. (There are exceptions such as transformations of equalities, which only support the general rule.) Nevertherless, your proof shows the equivalence very clearly.\nOn the proof for $g\\circ f$ is differentiable\nThe definition, \"$q(x) := g'(f(x)) \\cdot f'(x)$\" is wrong. $q(x)= g'(f(x)) \\cdot f'(x)$ is only true when $x=a$ in general. What you wanted to say is simply \"$q(x):=r_{fa}(f(x)) \\cdot s_a(x)$\".  Taking $x=a$, we know $q(a)=r_{fa}(f(a)) \\cdot s_a(a)=g'(f(a))\\cdot f'(a)$.\n\nThe following part is not about your proofs.\nOn the definition of \"secant function\"\n\nA function $s_a$ is defined as a secant function of $f$ at $a$ if $f(x) - f(a) = s_a(x)(x-a)$.\n\nThere are multiple issues with this definition.\n\nA \"secant function\" has been universally defined as the reciprocal of the cosine function basically. It is confusing to assign a different meaning.\n The function maps an $x\\not=a$ to the slope of the secant that cuts the graph of $f$ at point $(a, f(a))$ and $(x, f(x))$. I would call it \"a slope function of $f$ relative to $a$\" or \"a slope function of $f$ about $a$\".\n\nThe domain and codomain of the function is not defined explicitly. This might be good for faster understanding so as to concentrate on the main content. However, it is not ideal for correctness and rigor.\n\n$f$ should be defined on a neighborhood of $a$.\nAlso, $a$ should in the domain of $f$.\nThe domain of $s_a$ can be defined the same as that of $f$.\nThe codomain of $s_a$ can be the same as that of $f$, either real numbers or complex numbers.\n\n\n\n",
    "tags": [
      "real-analysis",
      "solution-verification",
      "continuity",
      "proof-writing"
    ],
    "score": 5,
    "answer_score": 1,
    "is_accepted": true,
    "question_id": 4601024,
    "answer_id": 4606737
  },
  {
    "theorem": "Where is my mistake in proving $\\frac{a}{b+2c}+\\frac{b}{c+2a}+\\frac{c}{a+2b} \\geq 1$?",
    "context": "\nLet $a,b,c$ be positive real numbers. Prove that $$\\frac{a}{b+2c}+\\frac{b}{c+2a}+\\frac{c}{a+2b} \\geq 1$$\n\nI was trying to solve the problem and I found some difficulties in my solution. Here is the solution:\nFrom Cauchy-Schwarz, we have $$\\frac{(\\sqrt a)^2}{b+2c}+\\frac{(\\sqrt b)^2}{c+2a}+\\frac{(\\sqrt c)^2}{a+2b} \\geq \\frac{(\\sqrt a +\\sqrt b +\\sqrt c)^2}{3(a+b+c)}.$$\nThen we have to prove that $$(\\sqrt a +\\sqrt b +\\sqrt c)^2 \\geq 3(a+b+c)\\\\ \\implies a+b+c+2(\\sqrt{ab}+\\sqrt{bc}+\\sqrt{ca})\\geq 3(a+b+c)\\\\ \\implies \\sqrt{ab}+\\sqrt{bc}+\\sqrt{ca}\\geq a+b+c$$\nwhich is not true because $$(a+b+c)(b+c+a) \\geq (\\sqrt{ab}+\\sqrt{bc}+\\sqrt{ca})^2\\\\ \\implies a+b+c \\geq\\sqrt{ab}+\\sqrt{bc}+\\sqrt{ca}$$ by Cauchy-Schwarz.\nI'm quite sure that the problem statement is correct. So, I made a mistake somewhere in my solution. But I am unable to find that. So, I basically want to know where my mistake is and I don't want alternative solution to the problem.\nThere is a similar problem here but that doesn't answer my question because I don't want the solution of the problem rather I want to know where the mistake is in my solution.\n\nAs @Macavity wrote, I used an inequality which is not tight enough. Then, my question is how to know if an inequality I'm using is tight enough or not.\n(This issue is bothering me a lot. Today I was proving that $$\\frac{a^2}{(a+b)(a+c)}+\\frac{b^2}{(b+c)(b+a)}+\\frac{c^2}{(c+a)(c+b)} \\geq \\frac 3 4$$ for all positive reals $a,b,c$ and the same thing happened as above. I really want to know how to avoid this.)\n",
    "proof": "\nHere is my soluation.\nI used a cauchy bunyakovsky instead of cauchy schwarz.\n",
    "tags": [
      "inequality",
      "proof-writing",
      "solution-verification",
      "contest-math",
      "cauchy-schwarz-inequality"
    ],
    "score": 5,
    "answer_score": 1,
    "is_accepted": false,
    "question_id": 4192518,
    "answer_id": 4611331
  },
  {
    "theorem": "prove that $S$ is a subgroup of $G$",
    "context": "QUESTION: Let $S$ be a subset of $G$ such that the identity element $1 \\in S$. Assume that the subsets $aS := \\{as \\mid s \\in\nS\\} \\subseteq G$ for $a \\in G$ form a partition of $G$. Prove that $S$ is a subgroup of $G$.\n\nThis is my solution thus far. Can someone check if it is remotely correct and where I can improve?\n",
    "proof": "Your equation involving $\\{1\\}$ is unnecessary and seems wrong. (Did you mean $\\{x\\}\\cup\\dots$ instead of $\\{1\\}\\cup\\dots$ ?) For that step, I'd just write \"we have $x=x1\\in xS$\". Also, you might want to note that $S=1S$ is of the form $aS$ in order to apply the fact about the partition. Besides that, LGTM.\n",
    "tags": [
      "abstract-algebra",
      "proof-writing",
      "solution-verification",
      "set-partition"
    ],
    "score": 5,
    "answer_score": 1,
    "is_accepted": false,
    "question_id": 4037607,
    "answer_id": 4037660
  },
  {
    "theorem": "Find the $x^2$ coefficient by induction of $\\frac{\\sin x}{x}$",
    "context": "In one of Wikipedia's proofs of the solution to the Basel problem, they state that the $x^2$ coefficient of $$\\begin{align*}\n\\frac{\\sin(x)}{x} &=\n\\left(1 - \\frac{x^2}{\\pi^2}\\right)\\left(1 - \\frac{x^2}{4\\pi^2}\\right)\\left(1 - \\frac{x^2}{9\\pi^2}\\right)\\cdots\n\\end{align*}$$\ncan be proven inductively to be\n$$-\\left(\\frac{1}{\\pi^2}+\\frac{1}{4\\pi^2}+\\frac{1}{9\\pi^2}\\cdots\\right)$$\nI do not see any way of proving this by induction. I haven't ever come across this use of induction before actually. Could someone point me in the right direction of how to prove this using induction? I don't even know what the inductive hypothesis would be.\nI can see this will certainly be the case though as the only way to have $x^2$ terms is by having each $x^2$ term in each bracket only multiplied together by the $1$'s inside the other brackets.\nThanks for your help.\n",
    "proof": "There's a footnote in the Wikipedia page, did you read it?\nIt's a formula for the coefficient of $x^2$ of the partial product. That's what can be proved by induction\n",
    "tags": [
      "trigonometry",
      "polynomials",
      "proof-writing",
      "induction",
      "proof-explanation"
    ],
    "score": 5,
    "answer_score": 1,
    "is_accepted": true,
    "question_id": 3968865,
    "answer_id": 3968887
  },
  {
    "theorem": "Show that a metric space $M$ is compact iff for every continuous function $f:M\\rightarrow \\mathbb{R}$, such that $f(x)&gt;0$ for all $x\\in M$",
    "context": "everybody! Could you give me feedback if my statements below $ \\Leftrightarrow $ are correct or I need to change them?\n$\\quad$Show that a metric space $M$ is compact iff for every continuous function $f:M\\rightarrow \\mathbb{R}$, such that $f(x)>0$ for all $x\\in M$, we have $\\inf\\limits_{x\\in M} f(x) >0$.\n($\\Rightarrow$)\n$\\quad$If $A$ is a bounded subset of $\\Bbb R$, and if $s=\\sup A$ then for each $\\epsilon >0$, $s−\\epsilon$ is not an upper bound of $A$ (since s is the least upper bound of $A$) and therefore, there is some $a\\in A$ such that $a>s−\\epsilon$. But, since $s$ is an upper bound of $A$, $a\\leq s$. So, $|s−a|<\\epsilon$. Since this occurs for every $\\epsilon>0$, $s\\in \\overline{A}$. For a similar reason, $\\textbf{inf}$ $A \\in \\overline{A}$. And, since $\\inf f(M)\\in f(M)\\subset(0,\\infty)$, there is some $m\\in M$ such that $\\inf f(M)>0$ and, since $f(M)>0$, this proves that $\\inf f(M)>0$.\n($\\Leftarrow$)\n$\\quad$ If there is a sequence $\\{x_n\\}$ with no convergent subsequence the $E=\\{x_1,x_2,⋯\\}$ is a closed set. Define $f:E\\rightarrow (0,1)$ by $f(x_n)=\\frac 1 n$. Then $f$ is continuous. By (one form of) Tietze Extension Theorem there exists a continuous function $F:X\\rightarrow (0,1)$ such that $F=f$ on $E$. This continuous function does not have a positive infimum. $\\textbf{(why isn't there a infimum postive?)}$\n$\\square$\n",
    "proof": "The second part is correct but the first one looks complicated and vague. It is not even clear as to where compactness comes in.\nIf $M$ is compact and $f$ is continuous then $f(M)$ is compact. The infimum of any compact subset of $\\mathbb R$ is attained. So there exists $t \\in f(M)$ such that $t \\leq f(x)$ for all $x \\in M$. Since $t \\in f(M)$ we can write $t=f(x_0)$ for some $x_0$. Now $\\inf \\{f(x): x \\in N\\}=f(x_0)>0$.\n",
    "tags": [
      "real-analysis",
      "general-topology",
      "proof-writing",
      "solution-verification",
      "proof-explanation"
    ],
    "score": 5,
    "answer_score": 1,
    "is_accepted": true,
    "question_id": 3954552,
    "answer_id": 3954668
  },
  {
    "theorem": "$n$ black squares on a grid become disappear in $n$ moves.",
    "context": "Found this fun puzzle:\nOn an infinite sheet of white graph paper (a paper with a square grid), $n$ squares are colored black. At moments $t = 1, 2, . . .$, squares are recolored according to the following rule: each square gets the color occurring at least twice in the triple formed by this square, its top neighbor, and its right neighbor.\nProve that after the moment $t = n$, all squares are white!\nMy solution:\nI'm not sure how to make it rigorous though if anyone can help me out and it's not quite correct.\nComplete induction on $n$: for all $k < n$ squares, it will be converted after $k$ steps. Let $R$ be smallest rectangle containing all black squares. Let $r$ be the bottom-row and $c$ be left-most column. By IH, $R - r$ takes $<n$ steps and $R - c$ takes $<n$ steps. Then the sum of steps is $<2n$. The last square is in bottom left at $r \\cap c$, which will go with $1$ step. In total, we have $2n + 1$ steps, but I'm doubling counting because $R - c \\cap R - r$. So, i'm not sure how to proceed, since we're trying to prove it will take at most $n$ steps.\nI'd appreciate if someone could help with this!\n",
    "proof": "I found that the problem was included in An Invitation to Discrete Mathematics, and that they give a hint which is basically a full solution. They have a similar idea to you, considering the bottom-most row and left-most column. However, you need to do a better estimate; since $R-r$ and $R-c$ are gone in at most $n-1$ steps, it follows that $(R-r)\\cup (R-c)$ is also gone in at most $n-1$ steps. You do not need to add the numbers of steps, since the processes are happening at the same time. Can you take it from here?\n",
    "tags": [
      "proof-writing",
      "induction",
      "puzzle"
    ],
    "score": 5,
    "answer_score": 1,
    "is_accepted": false,
    "question_id": 3851720,
    "answer_id": 3852787
  },
  {
    "theorem": "Spivak&#39;s Calculus: Chapter 3 Problem 24b",
    "context": "24b) Suppose that $f$ is a function such that every number $b$ can be written $b = f(a)$ for some real number $a$. Prove that there is a function $g$ such that $f \\circ g = I$\nI think I do understand this question and how to solve it, but I'm struggling to find a way to express my solution in a mathematically rigorous way, particularly when $f$ is not injective. Here's my idea:\nFirst of all, if $f$ is injective, then it's trivial.\nLet $g(x) = a$, where $x = f(a)$ for any $a \\in \\text{domain}(f)$\nSince $f$ is injective, by definition there is only one value of $a$ that satisfies $x = f(a)$ for each $x$, which means $g$ is well defined. And $\\text{domain}(g) = \\text{image}(f)$ (by definition of $g$), which from the supposition in the question is $\\mathbb{R}$. Also, $\\text{domain}(f) = \\text{image}(g)$, since $f$ and $g$ are injective (but that fact is not important). So $f(g(x))$ is defined for all $x ∈ \\mathbb{R}$. Finally, $f(g(x))$ = $f(a)$, where $x = f(a)$ for $x ∈ \\mathbb{R} \\to f(g(x)) = I(x)$.\nBut now if $f$ is not injective, it gets more complicated. If I keep my original definition of $g$, being \"$g(x) = a$, where $x = f(a)$ for any $a \\in \\text{domain}(f)$\", then that doesn't work because $g$ is no longer a function. Because since $f$ is not injective, there exists atleast 2 numbers $z$ and $w$ such that $z \\neq w$ but $f(z) = f(w)$, which means there exists $x$ such that: $g(x) = z = w$.\nI think the idea is to simply redefine $g$ to simply \"choose\" either $z$ or $w$, and assign it to $x$. For example it could choose the smaller of the two. The only difference this would make is now $\\text{domain}(f) \\subset \\text{image}(g)$, instead of $\\text{domain}(f) = \\text{image}(g)$. But since that fact wasn't important before, the conclusion in the question still holds.\nHere's my question. How do I explicitly write down a definition of $g$ that \"chooses\" the smaller of $z$ or $w$? Furthermore, recall there exists at least 2 numbers z and w. There could be arbitrarily more numbers such that $f(z) = f(w) = f(m) = f(n)$ and so on. And that's just one of the arbitrary branches the common values $f$ could take. There could be a different set of numbers $f(z_2)  = f(w_2) = f(m_2)$ and so on, that are not equal to $f(z)$, etc.\nThis is starting to get very messy. How can I express $g$ mathematically?\n",
    "proof": "The fallacy you noticed is real, well done for spotting it! What you are asked to show is basically the axiom of choice for the real numbers. It is an axiom because you can’t prove (the general version) from the other axioms of set theory, even though it seems kind of sensible.\nSo you have two options:\n\nYou can gloss over the fact that your definition has this problem and basically say: “Well, just choose any of the options, nothing strange to see here.”\nYou can invoke the axiom of choice. It says (straight from the Wikipedia article): For any indexed family $(S_i)_{i\\in I}$ of non-empty sets (where $I$ is some indexing set) there is a family $(x_i)_{i\\in I}$ such that $x_i \\in S_i$ for every $i\\in I$. I leave it to you to figure out how to get to Spivak’s claim. (Actually, my favorite formulation of the axiom of choice is basically what you have to prove, but not restricted to numbers.)\n\n",
    "tags": [
      "functions",
      "proof-writing",
      "notation"
    ],
    "score": 5,
    "answer_score": 1,
    "is_accepted": true,
    "question_id": 3792543,
    "answer_id": 3792656
  },
  {
    "theorem": "Prove that for any positive integer $a$, $a^{561} \\equiv a \\pmod{561}$.",
    "context": "Prove that for any positive integer $a$, $a^{561} \\equiv a \\pmod{561}$.\n\nWhat I have :\nUsing FLT\n$$a^{561} = a^{3 \\cdot187} \\equiv a^{187} = a^{3\\cdot62}a^1 \\equiv a^{62}a^1 = a^{3\\cdot21} \\equiv a^{21}=a^{3\\cdot7}\\equiv a^7 = a^{2\\cdot3}a^1 \\equiv a^2a^1 = a^3 \\equiv a \\pmod 3.$$\nSimilar calculations for $\\mod {11}$ and $\\mod {17}$ show that $a^{561} \\equiv a \\pmod{11, 17}$. By the Chinese Remainder Theorem, we have $a^{561} \\equiv a \\pmod {561}$, as desired. $\\square$\nCase 2: $\\gcd(a, 561) > 1$. This implies that $a$ is divisible by at least one of $3, 11,$ or $17$.\n\nI Don't know what to do for case 2.\n",
    "proof": "Hint:\n$a^p\\equiv a\\pmod p$ whether or not $(a,p)=1$.\nShow by induction that $a^{(p-1)n+1}\\equiv a\\pmod p$,\nusing $a^{(p-1)n+1}=a^{(p-1)(n-1)+1}a^{p-1}=a^{(p-1)(n-1)}a^p\\equiv a^{(p-1)(n-1)+1}\\pmod p$.\n",
    "tags": [
      "elementary-number-theory",
      "proof-writing"
    ],
    "score": 5,
    "answer_score": 1,
    "is_accepted": false,
    "question_id": 3780372,
    "answer_id": 3780383
  },
  {
    "theorem": "Prove that the reduced row echelon form (rref) of an $n$ by $n$ matrix either is the identity matrix &#119816; or contains at least one row of zeroes.",
    "context": "I'm trying to prove the following proposition\n\nProve that the reduced row echelon form (rref) of an $n$ by $n$ matrix either is the identity matrix $\\bf I$ or contains at least one row of zeroes.\n\nFirstly, I will quote the definition of the rref from the same book where the proposition was given:\n\nA matrix is in reduced row echelon form, normally abbreviated to rref,\n  if it satisfies all the following conditions:\n\nIf there are any rows containing only zero entries then they are located in the bottom part of the matrix. \nIf a row contains non-zero entries then the first non-zero entry is a 1. This 1 is called a leading 1. \nThe leading 1’s of two consecutive non-zero rows go strictly from top left to bottom right of the matrix. \nThe only non-zero entry in a column containing a leading 1 is the leading 1.\n\n\nNow, my attempt:\nAssume $\\bf A$ is $n$ x $n$ matrix, where $\\bf R$ is rref of $\\bf A$.\nSuppose $\\bf R ≠ I$. Then $\\bf R$ must have a leading $1$ (call it $x_{i,j}$) which is located in ith row and jth column and $j > i$. Since $\\bf R$ is in rref, then all leading $1$ must go strictly to the bottom right of the matrix. We are left with the $n - j$ columns and $n - i$ rows. because $j > i$, then $n - i > n - j$ and thus there must be at least $j-i$ zero rows.\nNow suppose $\\bf R$ doesn't have row of zeros. In this case, if $x_{i,j}$ = 1, then $i = j$, because we've shown that if $j > i$ then $\\bf R$ will have row of zeros. And by definition of the identity matrix, we can conclude that $\\bf R = I$ $\\Box$. \nAlthough I have a lot of doubts, but I will ask: is it correct?\n\nThe proposition is kind of intuitive, however, it was a struggle for me to formalize my thoughts. If you have any remarks/suggestions about the proof above, I'd be glad to hear them!\n",
    "proof": "It is correct but there are some nitpicks to make. For instance:\nYou write 'Suppose $\\mathbf{R} \\neq \\mathbf{I}$. Then $\\mathbf{R}$ must have a leading 1 (call it $x_{i,j}$) which is located in $i$th row and $j$th column and $j>i$.'\nStrictly speaking there is a second possibility and that is when $\\mathbf{R}$ doesn't contain any non-zero elements at all. Of course this case is easy to handle, but perhaps you should mention it. \nAlso, depending on your audience, it might not really be clear that (when $x_{i, j}$ exists) we have $j > i$. The fact that $\\mathbf{R} \\neq \\mathbf{I}$ only implies that $j \\neq i$ so you might want to elaborate a bit on why $j < i$ is ruled out.\nIn the converse direction there are two typos: 'if $x_{i,i} = 1$, then $i = j$' should probably read 'if $x_{i, j} = 1$ then $i = j$'. Just following that you write 'because we've shown that if $i > j$ then $\\mathbf{R}$ will have row of zeros', but what we have shown is something else, namely that if $i < j$ then $\\mathbf{R}$ will have row of zeros. \nOf course, also at this point the reader might wonder what happens in the case that $i > j$ (i.e. the case I wrote about earlier). I recommend you include it. \nThere is something else: you write two proofs now one that not being the identity implies having a row of zeroes and one that not having a row of zeroes implies being the identity matrix. This is nice towards the reader, but since both are logically equivalent (to each other and to the statement of the proposition) strictly speaking you only need one. (This last issue is not a problem with your proof but something that it is good to be aware of)\n",
    "tags": [
      "linear-algebra",
      "proof-verification",
      "proof-writing"
    ],
    "score": 5,
    "answer_score": 1,
    "is_accepted": true,
    "question_id": 3345158,
    "answer_id": 3346061
  },
  {
    "theorem": "Prove $9 \\mathrel| (4^n+6n-1)$ by induction",
    "context": "I know that this question was already answered, but I would like to know if the second step of induction its okay the way I did it.\nThis question is different from Induction proof for $n\\in\\mathbb N$, $9 \\mathrel| (4^n+6n-1)$ and Let $n ∈ N, n ≥ 1$. Prove that $4^n + 6n - 1$ is divisible by $9$. since it is just about to check a part from a proof that I did (if the way I thought is correct or not).\nSuppose that   $9 \\mathrel| (4^k+6k-1)$ is true for $k\\in \\mathbb{N},  k\\geq1$. I want to prove $P(k+1)$: $ 9 \\mathrel| 4^{(k+1)} +6(k+1) - 1$\nSince $9 \\mathrel| (4^k+6k-1)$, I can rewrite as:\n$4^k+6k-1 = 9q$,  where $q \\in \\mathbb{Z} $. By the definion of Divisibility.\n\nMultiplying the equation on both sides by 4, I'll get:\n\n$4^{(k+1)} +24k-4 = 36q$\n\nAdding 3 on both sides\n\n$4^{(k+1)} +24k - 1 = 36q + 3$\n\nSubtracting $18k$ on both sides\n\n$4^{(k+1)} +6k - 1 = 36q + 3 -18k$\n\nAdding 6 in order to have $6(k+1)$ as a factor\n\n$4^{(k+1)} +6k + 6 - 1 = 36q + 3 -18k + 6$\n$4^{(k+1)} +6(k+1) - 1 = 36q + 9 -18k $\nNote that $36q + 9 -18k$ can be written like this: $9(4q + 1 - 2k)$. Thus $9Q$, $Q=(4q + 1 - 2k), Q \\in \\mathbb{Z}$.\nTherefore:\n\n$4^{(k+1)} +6(k+1) - 1 = 9Q$. Which means that $4^{(k+1)} +6(k+1) - 1$ is also divided by 9.\n\nDoes my proof look fine? If there is a problem, please tell me. All help is appreciated.\n",
    "proof": "It's good.  I like it a lot.\nBut in induction proofs it's often easier to start at the destination and drive backwards through one way streets to the start, rather than starting from the start and driving through a bunch of intersections without a map in order to reach your destination.\n$4^{k+1} + 6(k+1) - 1=$\n$4*4^{k} + 6k + 5 = $\n$4(4^k + 6k -1) - 24k +4 +6k + 5=$\n$4(4^k + 6k -1) - 18k +9 = 4(4^k+6k-1) -9(2k-1)$.\nAnd $9|4^k + 6k -1\\implies 9|4(4^k+ 6-1) - 9(2k-1)$$\n",
    "tags": [
      "elementary-number-theory",
      "proof-verification",
      "proof-writing",
      "induction",
      "divisibility"
    ],
    "score": 5,
    "answer_score": 0,
    "is_accepted": true,
    "question_id": 3326477,
    "answer_id": 3326527
  },
  {
    "theorem": "A small help needed in proving a smart part of a result - to show that we need at least 2r-3 vertices to construct a particular type of graph.",
    "context": "I am given a graph $G$ with diameter two. I have to prove that I have to add minimum $2r-3$ vertices to $G$ to form a graph $H$ such that $H$ contains exactly two vertices with eccentricity $r+1$ (obviously peripheral vertices) and rest with eccentricity $r$ (central vertices), where $r\\geq 4$. Also, $G$ is induced in $H$.\nI am stuck at a particular part of the proof. Since diameter of $G$ is two, $G$ can at the most contain one diametral vertex or at the most one vertex with eccentricity $r+1$. I divided the proof in two cases. In first part, $G$ contains a vertex with eccentricity $r+1$. This part has been proved by me. In second case, $G$ does not contain a vertex with eccentricity $r+1$ and I am stuck in this case. However, I tried to prove it, but I am feeling that it is not surely true. \nIn the following attempt I consider a $z-w$ walk of length $r$ but I feel it is wrong as we already get a $z-w$ path of length $r$ lying on $x-y$ path since $z$ is adjacent to $x$. \nMY ATTEMPT:\nLet $P$ be a diametral path in $H$ of length $r+1$ with end vertices $x$ and $y$ ($x,y\\notin G$). \nSince $diam(G) =2$, at the most 3 vertices of $G$ can lie on $P$ and thus \n$P$ contains at least $r-1$ new vertices. Since $r\\geq 4$, $r+1\\geq 5$\nand $P$ contains at least six vertices. If $x$ and $y$ are  end vertices of $P$ which are not in $G$\nthen there exists $z\\notin V(G)$ and $x\\sim z$ or $y\\sim z$.\nWithout loss of generality, let $x\\sim z$. Since $e(z)=r$, there exists a $z-w$ path $P_2$\nsuch that $l(P_2)=r =d(z,w)$. Now, the path $P$ can not be extended, otherwise $d(x,y)>r+1$,\nand $P_2$ contains at most three vertices from $G$. This follows that at least $r-2$ vertices in $P_2$ are not in $G$.\nThis proves that we need to add at least $(r-1) +(r-2) = 2r-3$ new vertices.\nIs this proof correct? I feel this proof is still incomplete. Somewhere I feel that I am missing some part. Kindly help me. Thanks a lot for your time and help. \nP.S. It might be possible that $P$ and $P_2$ intersect at many vertices. In that case how to prove the result.\n----------------------------------------------------\nAdded one more point in the proof (edited proof)\n$G$ does not contain any diametral vertex.\nSince $diam(G) =2$, here also $P$ contains at least $r-1$ new vertices. Since $r\\geq 4$ and $r+1\\geq 5$,\n$P$ contains at least six vertices. If $x$ and $y$ are  end vertices of $P$ which are not in $G$\nthen there exists $z\\notin V(G)$ and $x\\sim z$ or $y\\sim z$.\nWithout loss of generality, let $x\\sim z$. Since $e(z)=r$, there exists a $z$--$w$ path $P_2: z,w_1,w_2,\\ldots,w_r=w$\nsuch that $l(P_2)=r =d(z,w)$.\nNow, the path $P$ can not be extended, otherwise $d(x,y)>r+1$.\nNow, the $P_2$ contains at most three vertices from $G$ since $diam(G) =2$.\nFurther, if $P_2$ intersect vertices of the $P$ then $d(z,w)<r$, which\nis a contradiction because $e(z) = r$. This follows that at least $r-2$ vertices in $P_2$ are not in $G$.\n",
    "proof": "Let $v$ be a vertex of $G$ with at least one neighbor among the new vertices.\nFor $i \\in \\{1,\\ldots,r\\}$, let $S_i$ be the set of vertices at distance exactly $i$ from $v$. Note that $S_1$ and $S_2$ have at least one new vertex each. Suppose we show that all but one $S_i$ for $i \\geq 3$ have at least two vertices. Then the number of new vertices is at least $2(n-3)+1+2=2n-3$.\nLet $S_i=\\{x\\}$ for some $i$. Then $x$ is a cut-vertex and it must be the case that its eccentricity is $r$, say component $C$ of $H \\setminus v$ has $y$ such that $d(x,y)=r$, and $V(H) \\setminus (C \\cup \\{v\\})$ consists of one vertex of eccentricity $r+1$ which is a neighbor of $x$. This shows that there can be at most two cut-vertices in $H$, but hopefully this idea is sufficient to complete the proof with a more careful analysis.\n",
    "tags": [
      "combinatorics",
      "discrete-mathematics",
      "graph-theory",
      "proof-writing"
    ],
    "score": 5,
    "answer_score": 1,
    "is_accepted": false,
    "question_id": 3270486,
    "answer_id": 3278484
  },
  {
    "theorem": "Symbol for &quot;by the definition of&quot;",
    "context": "I find that I am using the term \"by the definition of\" a lot in my proofs, especially after && in align* environments.  Is there a symbol that I can use instead for conciseness?\nExample:\nHaving established that $x$ is odd, I would say $x=2k+1$ for some $k\\in\\mathbb{Z}$, $\\textit{by the definition of odd}$.\n",
    "proof": "Usually, if something follows by definition in a (well-written) mathematical text you don't need to call that out.  Your example (and I appreciate it's probably contrived) is actually a good example of that: having established that $x$ is odd you can just write that $x=2k+1$ without saying \"by definition of odd\" (by the way, either oddness, or oddity if you want to be mildly amusing, but \"odd\" alone is wrong).\nIf you want to remind the reader that something was defined further up, before something interesting/complicated was discussed, then slipping \"(by definition)\" in is sufficient to indicate that if they don't remember it they should go back and check.\n",
    "tags": [
      "proof-writing"
    ],
    "score": 5,
    "answer_score": 1,
    "is_accepted": true,
    "question_id": 3251398,
    "answer_id": 3251531
  },
  {
    "theorem": "Is there an abbreviation for &quot;we want to prove that&quot;?",
    "context": "I think it's possible to abbreviate \"such that\" into \"s.t.\" but is there an abbreviation for \"we want to prove that\"? Because this statement keeps repeating in proving inequalities.\n",
    "proof": "In my undergraduate math classes, I have seen some faculty use \"WTS:\" as shorthand for \"Want to show:\". I found this to be quite convenient, but only after the meaning was explained: until then it was quite confusing, so do take @GiuseppeNegro's comment above into account.\nIf you feel you are going to be making repeated use of it in a document/exam paper then you could mention at the start what this abbreviation stands for exactly.\n",
    "tags": [
      "proof-writing",
      "notation"
    ],
    "score": 5,
    "answer_score": 1,
    "is_accepted": false,
    "question_id": 2822615,
    "answer_id": 2822637
  },
  {
    "theorem": "Combinatorics) Painting a cube - how to prove constructive counting method?",
    "context": "\nIn the classic counting problem where you have to give the number of possible ways to paint a cube with 6 different colors,\nA well-known approach is constructive counting: fixing the first color to one of the faces, assuming the cube is fixed in some position relative to that colored face, and then going about painting the rest of the faces with the remaining 5 colors.\nThis goes : $1 \\cdot 5 \\cdot 3! = 6/6 \\cdot 5 \\cdot 3!$\nObviously, \"fixing\" is equivalent to assuming that the 6 different starting options we have in the beginning for the first color are all going to end up with the same set of 6-color arrangement, with the cardinality (the number of elements in the set) being $30$. \nMore specifically, if we assume each element of the set is a permutation or an arrangement of the 6 colors on the cube, each of the 6 starting options enumerate the same set.\nSince in this case, the problem space is quite small and visualization of why \"fixing\" doesn't cause problem is obvious.\nBut, generally in these counting problems, when you use the constructive counting method,\n\nIf we have $\\{a,b,c,d,e\\}$ to arrange, how can we show that progressing in the order of $abcde$ and $cdeab$ (or any different order) indeed count the same set?\nHow can you prove that your method does not miss out anything? i.e. the set constructed by the method is actually surjective? I would also really appreciate it if you can explain what the domain of this relation\nHow can you show that your method does not over-count? i.e. the set constructed is indeed a set and does not contain any duplicate elements by choosing different options at one level of construction.\n\nRefer to this post I was blocked from commenting on this one and my questions are aimed at different targets.\n",
    "proof": "Maybe it was also tacitly assumed that $1+1=2$ during the counting.\nOn a more serious note: Missing out cases or overcounting do indeed occur when solving such problems, and hopefully they are detected by the reading community.\nNow the example at hand is so simple that nobody cares for a more formal proof. (The only fine point being that one has to distinguish mirror equivalent colorings.) In fact a \"formal proof\" of obvious claims tends to obscure clear issues and nourishes doubts about the  statements to be proven. – \"Formalizing\" for an intended computer proof is another matter. But computers don't have geometric insight.\n",
    "tags": [
      "probability",
      "combinatorics",
      "discrete-mathematics",
      "proof-writing"
    ],
    "score": 5,
    "answer_score": 1,
    "is_accepted": true,
    "question_id": 2822373,
    "answer_id": 2822461
  },
  {
    "theorem": "Prove that closed interval is not open",
    "context": "\"Prove that the set $A=[a,b)$ is not open.\" I am looking for a rigorous proof. \nMy approach:\nSuppose that $A$ is actually open, which, by definition, would mean that for any $x\\in A$ there would exist some $\\epsilon>0$ such that the neighbourhood of $x$: $N_\\epsilon(x)=(x-\\epsilon,x+\\epsilon)\\subseteq A$. \nChoose $x=a$, then we have $N_\\epsilon(a)=(a-\\epsilon,a+\\epsilon)$. Now, we find a point $\\xi \\in N_\\epsilon(a)$ such that $\\xi\\notin A$. For all $\\xi\\in N_\\epsilon(a)$ we have $a-\\epsilon<\\xi<a+\\epsilon$. We choose $\\xi=a-\\epsilon/2$ therefore we have $a-\\epsilon<\\xi<a<a+\\epsilon$. By definition of  interval, $A=\\{x|x\\geq a \\wedge x<b\\}$. That shows that $\\xi\\notin A$ therefore $N_\\epsilon(a)\\not\\subseteq A$ for any $\\epsilon>0$.\nMy question is: Is this proof valid and rigorous enough? Can it be made simpler?\n",
    "proof": "It looks very good. I would recommend a slight adjustment, though. You say the following:\n\nFor all $\\xi\\in N_\\epsilon(a)$ we have $a-\\epsilon<\\xi<a+\\epsilon$.\n\nThis is true, but not used in your proof. Rather, you use the converse of this statement, so I'd say the following, instead:\n\nFor all $\\xi$ such that $a-\\epsilon<\\xi<a+\\epsilon,$ we have $\\xi\\in N_\\epsilon(a).$\n\nThen, you have by $a-\\epsilon<\\xi<a<a+\\epsilon$ that $\\xi\\in N_\\epsilon(a)$ and $\\xi\\notin A,$ by reasoning as you did from there.\n\nAside from the observation above, there is one more issue that should be clarified for rigor: it isn't clear where the $\\epsilon$ comes from in \"...then we have $N_\\epsilon(a)=(a-\\epsilon,a+\\epsilon).$\" It seems that you were going for a proof by contradiction, so you probably meant something like this:\n\n...then since $a\\in A,$ we have $N_\\epsilon(a)\\subseteq A$ for some $\\epsilon>0$ by our assumption that $A$ is open.\n\nFrom there with those few tweaks, since you later prove that $N_\\epsilon(a)\\nsubseteq A,$ you'll have completed a proof by contradiction. However, you didn't actually need to proceed that way, as you're very close to a direct proof that $A$ is not open, by demonstrating that there is an $x\\in A$ such that for all $\\epsilon>0,$ we have $N_\\epsilon(x):=(x-\\epsilon,x+\\epsilon)\\nsubseteq A.$ This is because \"$A$ is not open\" is the formal negation of \"$A$ is open,\" which looks like any of the following equivalent formal statements: $$\\neg\\bigl(\\forall x\\in A, \\exists\\epsilon>0:N_\\epsilon(x)\\subseteq A\\bigr)\\\\\\exists x\\in A:\\forall \\epsilon>0, \\neg\\bigl(N_\\epsilon(x)\\subseteq A\\bigr)\\\\\\exists x\\in A:\\forall \\epsilon>0, N_\\epsilon(x)\\nsubseteq A.$$ If you go the direct route, then instead, you'd want to start something like this:\n\nTake any $\\epsilon>0$ and consider $N_\\epsilon(a):=(a-\\epsilon,a+\\epsilon).$ We show that $N_\\epsilon(a)\\nsubseteq A,$ which, since $a\\in A,$ will show that $A$ is not open. To do so, we will find a point $\\xi\\in N_\\epsilon(a)$ such that $\\xi\\notin A.$\n\nFrom there, you can proceed in much the same way, with the minor tweak I suggested above.\n",
    "tags": [
      "proof-verification",
      "proof-writing"
    ],
    "score": 5,
    "answer_score": 1,
    "is_accepted": true,
    "question_id": 2758765,
    "answer_id": 2758912
  },
  {
    "theorem": "does my proof on inverse functions make sense?",
    "context": "I am proving this question:\nlet $g : N \\rightarrow M$ and $ A \\subseteq M$. Prove that if $f$ is surjective then $g(g^{-1}(A)) = A$\nFor the proof this is what I have said: \n(forwards)Let $y \\in g(g^{-1}(A)) $, also lets say that $\\exists x \\in g^{-1}(A)$. Then by subjectivity $g(x) = y$. Then $g(x)=y \\in A$. Wich leads to the conclution that $ g(g^{-1}(A)) \\subseteq A$\n(reverse) let  $y \\in A$, also lets say that $\\exists x \\in N$, such that $g(x)=y\\in A$.  This leads to $x \\in g^{-1}(A)$ which is equivelent to $y = g(x) \\in g(g^{-1}(A))$. This leads to the conclution that $A \\subseteq g(g^{-1}(A))$\nHence   $g(g^{-1}(A)) = A$. \nThanks for the help, any improvements welcome\n",
    "proof": "In \"forward\" you don't need surjectivity. If $y\\in g(g^{-1}(A))$, that means that there exists $x\\in g^{-1}(A)$ with $g(x)=y$. And $x\\in g^{-1}(A)$ means that $g(x)\\in A$; so $y=g(x)\\in A$, and as $y$ was arbitrary we obtain that $g(g^{-1}(A))\\subseteq A$. The way you wrote it, you don't say what $x$ is, and you wrote $D$ where you should have written $A$. \nIt's in \"reverse\" where you need surjectivity. Given $y\\in A$, there exists $x\\in N$ with $g(x)=y$. So $x\\in g^{-1}(A)$, and $y\\in g(g^{-1}(A))$. Thus $A\\subseteq g(g^{-1}(A))$.  The way you wrote it, I assume you were trying to say something like I said, but I cannot really follow your sentences; and you don't seem to have used surjectivity, which would make it wrong. \nTo see that surjectivity is needed, one looks at the case where $g^{-1}(A)=\\varnothing$. For instance let $g:\\mathbb R\\to\\mathbb R$, $g(x)=1$. Take $A=[2,4]$. Then $g^{-1}(A)=\\varnothing$, so $g(g^{-1}(A))\\subsetneq A$. \n",
    "tags": [
      "real-analysis",
      "functions",
      "proof-verification",
      "proof-writing"
    ],
    "score": 5,
    "answer_score": 1,
    "is_accepted": true,
    "question_id": 2603202,
    "answer_id": 2603213
  },
  {
    "theorem": "Proof by induction in trigonometry.",
    "context": "\nProve that $\\cos x +\\cos 2x + \\cos 3x + ...+ \\cos nx =\\cos \\left(\\dfrac{n+1}{2}x\\right) \\sin \\left(\\dfrac{nx}{2}\\right)\\csc \\dfrac{x}{2}$\n\nAttempt: \nClearly, $P(1)$ is true. \nAssume $P(m)$ is true. \nThus,  $P(m+1) = (\\cos x +\\cos 2x + \\cos 3x + ...+ \\cos mx)+ \\cos((m+1)x)$\n$= \\cos \\left(\\dfrac{m+1}{2}x\\right) \\sin \\left(\\dfrac{mx}{2}\\right)\\csc \\dfrac{x}{2} + \\cos((m+1)x) \n\\\\= \\csc (\\dfrac x 2)\\left(\\cos \\left(\\dfrac{m+1}{2}x\\right) \\sin \\left(\\dfrac{mx}{2}\\right)+ (\\cos(m+1)x)\\sin (\\dfrac x 2)\\right)$\nWhat do I do next? \n",
    "proof": "Formula to be used: \n$\\sin A- \\sin B = \\cos\\left(\\dfrac{A+B}{2}\\right)\\sin \\left(\\dfrac{A-B}{2}\\right)$\nThus, \n$\\csc \\left(\\dfrac x 2 \\right)\\left(\\cos \\left(\\dfrac{m+1}{2}x\\right) \\sin \\left(\\dfrac{mx}{2}\\right)+ (\\cos(m+1)x)\\sin (\\dfrac x 2)\\right)$\n$= \\csc \\left(\\dfrac x 2 \\right)\\left(\\dfrac 1 2 \\left(\\sin \\dfrac{2mx+x}{2} - \\sin \\dfrac x 2 \\right)+ \\dfrac 1  2 \\left(\\sin \\dfrac{2mx+3x}{2} - \\sin \\dfrac{2mx + x}{2 } \\right)  \\right)$\nNow, again use the formula on the left out terms. \n$= \\csc \\left(\\dfrac x 2 \\right)\\left(\\dfrac 1 2 \\left(\\sin \\dfrac{2mx+3x}{2}   - \\sin \\dfrac x 2 \\right)  \\right)$\n$= \\cos \\left(\\dfrac{m+2}{2}x\\right) \\sin \\left(\\dfrac{(m+1)x}{2}\\right)\\csc \\dfrac{x}{2} $\nThus, $P(m+1)$ is also true. \nQ.E.D.\n",
    "tags": [
      "trigonometry",
      "proof-writing",
      "induction",
      "trigonometric-series"
    ],
    "score": 5,
    "answer_score": 1,
    "is_accepted": true,
    "question_id": 2568173,
    "answer_id": 2568249
  },
  {
    "theorem": "How to prove statements of the form $(A \\implies B) \\implies C$",
    "context": "This question grew out of this question, but my question is on the logic structure to a possible answer to that question.\nIn that question, the OP was trying to prove a statement of the form\n$$(A \\implies B) \\implies C$$\nThe way I've learnt this, is that you assume the antedecent $(A \\implies B)$ and prove the consequent $C$. My problem with this is that from the given antecedent, we cannot assume $A$, nor $B$, but only that $A$ implies $B$. Yet in the hint to the OP in that question, the answerer seems to reason like this: \n\"Assume $A$, and that $B$ follows from $A$, now prove C\"\nI can't justify to myself why I would be allowed to assume $A$. \nMore broadly, what exactly can I use when proving these statements? Can I assume $A$ is true and that $B$ follows?\nIf I may ask for an example of a proof of some trivial statement in the style of everyday language (\"All men are mortal\" or the like), I'm sure this will clear up.\nThanks in advance.\n",
    "proof": "To prove something in a logical theory $\\mathfrak{T}$ such as $A\\implies B$, one adjoins $A$ to the explicit axioms of $\\mathfrak{T}$ and proves $B$ within the newly gained theory. In short, what you stated was right. $A\\implies B$ is to be assumed, but nothing about $A$ or $B$.\nIf you would like to gain grounding in some basic logic, I recommend Bourbaki's Theory of Sets book. It is a bit old (some outdated terminology), but definitely a beautiful resource to learn from.\n",
    "tags": [
      "logic",
      "proof-writing"
    ],
    "score": 5,
    "answer_score": 1,
    "is_accepted": false,
    "question_id": 2484160,
    "answer_id": 2484167
  },
  {
    "theorem": "Uniform Convergence Preserves Continuity",
    "context": "Briefly, the definitions of point-wise convergence (PWC) and uniform convergence (UC) for a sequence of functions $f_n:[a,b]\\to\\mathbb{R}$ in my mind are recorded as\n\\begin{align*}\n&\\text{Point Wise Convergent on $[a,b]$} \\iff \\\\ \n&\\forall x\\in [a,b]\\,\\forall\\epsilon\\gt0\\,\\exists N=\\mathcal{N}(\\epsilon,x)\\gt0,\\,n\\ge N \\implies |f_n(x)-f(x)|<\\epsilon \\\\ \\\\\n&\\text{Uniformly Convergent on $[a,b]$}\\iff \\\\\n&\\forall x\\in [a,b]\\,\\forall\\epsilon\\gt0\\,\\exists N=\\mathcal{N}(\\epsilon)\\gt0,\\quad\\, n\\ge N \\implies |f_n(x)-f(x)|<\\epsilon.\n\\end{align*}\nSo the difference is that in PWC the number $N$ depends on $x$ while in UC it does not which means just one $N$ works for all $x$ in $[a,b]$.\nI want to prove the following theorem.\n\nTheorem. If the functions $f_n:[a,b]\\to\\mathbb{R}$ are continuous at $x_0\\in[a,b]$ and their sequence converges uniformly to the function $f:[a,b]\\to\\mathbb{R}$ on $[a,b]$ then $f$ is continuous at $x_0$.\n\nProof. According to the definition of continuity at $x_0$ for $f$, we want to show that\n\\begin{align*}\n\\forall\\epsilon\\gt0\\,\\exists \\delta=\\Delta(\\epsilon,x_0)\\gt0,\\,|x-x_0|<\\delta \\implies |f(x)-f(x_0)|<\\epsilon.\n\\end{align*}\nAccording to triangle inequality we have\n\\begin{align*}\n|f(x)-f(x_0)|\\le|f(x)-f_n(x)|+|f_n(x)-f_n(x_0)|+|f_n(x_0)-f(x_0)|.\n\\tag{1}\n\\end{align*}\nIf we could control each of the three terms on the RHS of $(1)$ such that they were less than $\\frac{\\epsilon}{3}$ then the theorem was proved. According to the assumptions we know that the following holds\n\\begin{align*}\n&\\forall\\epsilon_1\\gt0\\,\\exists \\delta_1=\\Delta_1(\\epsilon_1,x_0,n)\\gt0,\\,|x-x_0|<\\delta_1 \\implies |f_n(x)-f_n(x_0)|<\\epsilon_1 \\\\\n\\\\\n&\\forall x\\in [a,b]\\,\\forall\\epsilon_2\\gt0\\,\\exists N=\\mathcal{N}(\\epsilon_2)\\gt0, n\\ge N \\implies |f_n(x)-f(x)|<\\epsilon_2.\n\\end{align*}\nFinally, choosing any $\\epsilon_1$ and $\\epsilon_2$ such that $0<\\epsilon_1\\le\\frac{\\epsilon}{3}$ and $0<\\epsilon_2\\le\\frac{\\epsilon}{3}$ and setting any $\\delta$ such that $\\delta\\le\\delta_1$ will do the job. For simplicity, one can usually take the equality cases which means $\\epsilon_1=\\epsilon_2=\\frac{\\epsilon}{3}$ and $\\delta=\\delta_1$.\n$1$. Is my proof OK? Any suggestions for improvement is really appreciated.\n$2$. Are the notations $\\mathcal{N}(\\epsilon,x)$ or $\\Delta(\\epsilon,x,n)$ OK? I just employed them to emphasize the the dependence on $\\epsilon$ and $x$. Any better suggestion is welcomed.\n$3$. I was wondering which step would fail if we just had PWC? An example can be helpful.\n",
    "proof": "Your notations and proof seem great, and why the condition PWC is not sufficient is that under this you cannot choose your $\\mathcal{N}(\\epsilon_2)$ feasible for any $x$ in your domain. (Maybe for arbitrarily large $N$ there always exist some $x$ near $x_0$ making your argument fail.)\n",
    "tags": [
      "real-analysis",
      "proof-verification",
      "proof-writing"
    ],
    "score": 5,
    "answer_score": 1,
    "is_accepted": true,
    "question_id": 2425866,
    "answer_id": 2425879
  },
  {
    "theorem": "Which open sets $O$ satisfy $\\text{Int}[O^c] \\cup O = X$?",
    "context": "Let $(X,\\tau)$ be a topological space, let $O\\in\\tau$, let $O^c$ be the complement of $O$ in $X$, let $\\text{Int}[O]$ be the interior of $O$.\nWhich open sets $O$ satisfy $\\text{Int}[O^c] \\cup O = X$?\nMy guess is that it's precisely the clopen sets. I think the following is a proof. Is it correct?\nSuppose $O$ is clopen. Then, in particular, $O$ is closed. So, its complement $O^c$ is open. Since the interior of an open set is again that open set, we have that $\\text{Int}[O^c]$ is again $O^c$. Therefore, $\\text{Int}[O^c] \\cup O$ equals $O^c \\cup O$, which equals $X$ by the definion of complement. This proves one direction.\nNow suppose $\\text{Int}[O^c] \\cup O = X$. Since $O^c\\cup O = X$ by definition, we must have that $O^c \\subseteq \\text{Int}[O^c]$, because otherwise the union $\\text{Int}[O^c] \\cup O$ wouldn't be enough to cover all of $X$. But the interior of a set is always a subset of that set, meaning $\\text{Int}[O^c]\\subseteq O^c$. Now we have $\\text{Int}[O^c] = O^c$. This means $O^c$ is open, and so $O$ is closed. Also, $O$ is open by hypothesis.\n",
    "proof": "Your proof is correct, but can be drastically shortened.\nPoints of $X$ w.r.t. $E$, $E\\subseteq X$, can only be of one of this type, exclusively: interior, boundary, or exterior. $\\operatorname{Int}(E^c)$ are the exterior points of $E$. $\\operatorname{cl}(E) =\\operatorname{Int}(E^c)^c$\n$$\\operatorname{Int}(E^c)\\cup E = X \\iff E=\\operatorname{Int}(E^c)^c \\iff E \\textrm{ is closed}$$\nIf you want to add on a closed set the condition that it must be open, then it is clopen.\n\nI will repeat at a slower pace.\nThis $$\\operatorname{Int}(E^c)\\textrm{ are the exterior points of }E$$\ncomes from the definition of exterior points of a set. Follow this:\nPoints that are neither interior nor boundary points (that is points that are not adherence points) of a set have some neighborhoods disjont from the set, that is have some neighborhoods included in the complement of the set, that is are interior points of the complement of the set.\nThis $$\\operatorname{cl}(E) =\\operatorname{Int}(E^c)^c$$\ncome from what I have already written: a point can be one and only one among interior, boundary, and exterior of a set.\nNow the closure of a set is the union of the set of the interior points and the set of the boundary points that in turn for what just said is the complement of the set of the exterior points. But the exterior points of a set are the interior points of the complement of the set.\nThis $$\\operatorname{Int}(E^c)\\cup E = X \\iff E=\\operatorname{Int}(E^c)^c$$ comes from the fact that, given the sets $X$, $A\\subseteq X$, $B\\subseteq X$, with $A\\cap B = \\emptyset$, $A\\cup B = X \\iff B=A^c$ and the fact that $\\operatorname{Int}(E^c)\\cap E = \\emptyset$ (that is exterior points of $E$ does not belong to $E$)\nThis $$E=\\operatorname{Int}(E^c)^c \\iff E \\textrm{ is closed}$$ comes from the fact that $\\operatorname{cl}(E) =\\operatorname{Int}(E^c)^c$ and $E=\\operatorname{cl}(E) \\iff E \\textrm{ is closed}$\n",
    "tags": [
      "general-topology",
      "proof-verification",
      "proof-writing"
    ],
    "score": 5,
    "answer_score": 1,
    "is_accepted": true,
    "question_id": 2363855,
    "answer_id": 2364449
  },
  {
    "theorem": "Prove the following: If $m$ and $n$ are even integers, then so are $m+n$",
    "context": "I have been asked to prove the following and am having difficulty:\n\nIf $m$ and $n$ are even integers, then so are $m+n$ and $mn$.\n\nMy professor has hinted to us to use the definition of an even integer in our proof.\nThis is my proof for $m+n$ thus far:\n\nEven integers are defined as being divisible by 2.\nSo, if $m$, is even, using this definition we can rewrite this as $m=2j$.\nSimilarly, we can rewrite $n$ as $n=2k$.\nWe now have $m+n=2k+2j$.\nDistributivity then allows us to write $2j+2k=2(j+k)$\nWe now have that $m+n=2(j+k)$.\nI now use associativity to create $m+n=(j+k)2$\nNext, the definition of divisibility states that 'When $m$ and $n$ are integers, we say $m$ is divisible by $n$ if there exists $j∈ Z$ such that $m=jn$. \nThis allows me to conclude that, since all even numbers are divisible by 2, that $m+n$ must be an even number.\n\nI am unsure about whether or not my last two steps are correct. Our teacher has hinted to us to use the definition of divisibility, but I am having trouble wrapping my head around a. how to use it, and b. how it is accurate to do so?\nAny advice would be much appreciated!\n",
    "proof": "This looks mostly right, well done! As noted in the comments, there is a small error in 9. You wish to use the fact that all numbers that are divisible by $2$ are even to conclude that it's even. Instead, you evoke the fact that all even numbers are divisible by $2$.\n",
    "tags": [
      "elementary-number-theory",
      "proof-verification",
      "proof-writing",
      "divisibility",
      "parity"
    ],
    "score": 5,
    "answer_score": 1,
    "is_accepted": false,
    "question_id": 2111315,
    "answer_id": 2111323
  },
  {
    "theorem": "Proving interior of a metric space $E$ is open",
    "context": "\nIf $E^{\\circ}$ denotes the set of interior points of $E$, prove $E^{\\circ}$ is open.\n  \n\nMy attempt:\nIf $x\\in (E^{\\circ})^c$ then $x$ is not an interior point of $E$. Therefore, no neighborhood of $x$ is contained entirely in $E$ and thus all neighborhoods of $x$ contain a point $p\\in E^c \\subset (E^{\\circ})^c$, hence $x$ is a limit point of $(E^{\\circ})^c$ therefore $(E^{\\circ})^c$ is closed and $E^{\\circ}$ is open.\n\nIs there something wrong with this proof? I ask because all the proofs I have seen of this fact avoid using this argument, which somehow raises doubts about its validity, given it is just as simple as other arguments used. \n",
    "proof": "Your argument is perfectly fine but is more cumbersum. The openness of $E^\\circ$ can be proved directly without invoking closedness:\nSince $E^\\circ$ is a neighbourhood (contains a ball at) of all of its points, $E^\\circ$ is open.\n",
    "tags": [
      "real-analysis",
      "proof-verification",
      "proof-writing"
    ],
    "score": 5,
    "answer_score": 1,
    "is_accepted": true,
    "question_id": 2085432,
    "answer_id": 2085447
  },
  {
    "theorem": "How prove this inequality: $\\frac1{1-a}+\\frac1{1-b}+\\frac1{1-c}\\ge \\frac1{ab+bc+ac}+\\frac1{2(a^2+b^2+c^2)}$ for $a+b+c=1$?",
    "context": "Let $a,b,c>0$ and such $a+b+c=1$ show that \n$$\\dfrac{1}{1-a}+\\dfrac{1}{1-b}+\\dfrac{1}{1-c}\\ge \\dfrac{1}{ab+bc+ac}+\\dfrac{1}{2(a^2+b^2+c^2)}$$\nLet $p=a+b+c=1,ab+bc+ac=q,abc=r$\n$$\\Longleftrightarrow -4q^3+q^2-3qr+2r\\ge 0$$\nit seem hard to prove.\nwhy I say it hard prove:\nuse Schur inequality\n$$p^3-4pq+9r\\ge 0\\Longrightarrow r\\ge\\dfrac{4q-1}{9}$$\nit  remains to prove that\n$$\\dfrac{4q-1}{9}(2-3q)+q^2-4q^3\\ge 0$$\nIn fact, this inequality can't hold (4q-1)\n",
    "proof": "Let $a+b+c=3u$, $ab+ac+bc=3v^2$ and $abc=w^3$.\nHence, our inequality it's $\\frac{9u^2+3v^2}{9uv^2-w^3}\\geq3u\\left(\\frac{1}{3v^2}+\\frac{1}{18u^2-12v^2}\\right)$, \nwhich is equivalent to $f(w^3)\\geq0$, where $f$ is a linear function.\nBut the linear function gets a minimal value for an extremal value of $w^3$.\n$a$, $b$ and $c$ are positive roots of the equation $(x-a)(x-b)(x-c)=0$ or\n$w^3=x^3-3ux^2+3v^2x$ and we see that the line $y=w^3$ and graph of $y=x^3-3ux^2+3v^2x$ have three common points (draw it!). \nThus, an extremal value of $w^3$ we get for equality case of two variables\nand we need to check also the case $w^3\\rightarrow0^+$. \n\n$b=c$. After homogenization we can assume $b=c=1$, which gives $a(a-1)^2\\geq0$;\n$w^3\\rightarrow0^+$. \n\nLet $c\\rightarrow0^+$. After homogenization we can assume $b=1$, \nwhich gives $(a-1)^2\\geq0$. Done!\n",
    "tags": [
      "multivariable-calculus",
      "inequality",
      "proof-writing",
      "uvw"
    ],
    "score": 5,
    "answer_score": 1,
    "is_accepted": false,
    "question_id": 1842554,
    "answer_id": 1842842
  },
  {
    "theorem": "If a function is continuous and one-to-one then it&#39;s strictly monotonic",
    "context": "\nLet $f:(A,B)\\to \\mathbb{R}^1$ be continuous and one-to-one.\n  Prove that $f$ is strictly monotonic on $(A,B)$.\n\nProof: \nSuppose $f$ is not strictly monotonic on $(A,B)$. Then there exist $a<b<c$ such that $f(a)\\geqslant f(b)$ and $f(c)\\geqslant f(b)$. \nSuppose that $f(a)\\geqslant f(c)$. Since $[a,b]$ is connected and $f$ is continuous then $f([a,b])$ is connected. \nIt's easy to prove that $[f(b),f(a)]\\subset f([a,b])$.\nIf $f(b)=f(a)$, then $f$ is not one-to-one and we have contradiction.\nIf $f(b)<f(a)$, then $f(c)\\in f([a,b])$ and by the intermediate value theorem $\\exists c'\\in [a,b]$ such that $f(c')=f(c)$ and $f$ is not one-to-one.\nSorry if this topic repeated but I would like to know is my proof correct?\nThanks in advance.\n",
    "proof": "Like BrianO said, your reduction to studying how $f$ behaves with three points is a little quick. However, the reduction is elementary in the sense that it only uses ordered field axioms and not the LUB or other analytic theorems.\nLet me just give the details of the reduction:\n\nA map is monotone iff:\n$(i)$: it is monotone on every set of four points or less. \nThis is just figuring out the negation of being monotone: if $f$ is not monotone then there are points $a \\leq b$, $c \\leq d$ in its domain such that $f(a) > f(b)$ (non increasing) and $f(c) < f(d)$ (non decreasing) and $f$ is not monotone on $\\{a;b;c;d\\}$.\n$(ii)$: it is monotone on every set of three points or less. \nIndeed, assume $f$ satisfies $(ii)$. Let $a < b < c < d$ be four points with for instance $f(a) \\leq f(b) \\leq f(c)$.\nIf $f(a) = f(b) = f(c)$ then $f$ is monotone on $\\{a;b;c;d\\}$: increasing if $f(d) \\geq f(a)$ and decreasing otherwise. Else, if for instance $f(a) < f(c)$, then applying $(ii)$ to $\\{a;c;d\\}$ yields $f(a) < f(c) \\leq f(d)$ so $f$ is increasing on $\\{a;b;c;d\\}$. The three other cases work the same.\n",
    "tags": [
      "real-analysis",
      "proof-verification",
      "proof-writing"
    ],
    "score": 5,
    "answer_score": 1,
    "is_accepted": false,
    "question_id": 1668257,
    "answer_id": 1669797
  },
  {
    "theorem": "$G$ is abelian when any two non-identity $a$ , $b$ there is an automorphism $\\delta$ such that $\\delta(a)=b.$",
    "context": "$G$  is a  finite  group with identity  $\\mathcal e.$  Suppose  for any  two non-identity elements $a$ , $b$ of $G$ , there is an automorphism $\\delta$ such that  $\\delta(a)=b.$ Then prove that $G$ is abelian .\nSo what I was thinking to do is that $$a\\cdot b=b\\cdot a$$\nBy the given condition there is a $\\delta$ such that $$\\delta(a\\cdot b)=b\\cdot a\\\\or,\\ \\ \\ \\delta(a)\\cdot \\delta(b)=b\\cdot a.$$\nNow I don't know what to do next .\nAlso another thing that I observed was that for any non-identity element $a$, there is a automorphism $\\phi$ such that $$a=\\phi(a^2)\\\\={(\\phi(a))}^2$$  and  thus every non-identity element of $G$ is a square.  Then we have $$a=a_1^2=a_2^4=a_3^8=.............=a_n^{2^n}=..$$  But for finiteness  of  $G$  this  has  to  stop  somewhere.  I supposed that  might  be  helpful  in  some ways  but  could  not figure  out  what  conclusions  might  be  drawn  from  that either .\n",
    "proof": "Firstly, observe that all elements have the same order and this order must be a prime number $p$ (this follows from Cauchy's Theorem and the fact that $Aut(G)$ acts transitively on $G - \\{1\\}$). So $G$ is a $p$-group, and hence does have a non-trivial center. Pick $z \\in Z(G)$, $z \\neq 1$. Let $a,b \\in G$. Then there exists a $\\delta \\in Aut(G)$ with $\\delta(a)=z$. But then $\\delta(ab)=z\\delta(b)=\\delta(b)z=\\delta(ba)$. Since $\\delta$ is injective it follows that $ab=ba$, whence $G$ is abelian. It even follows that $G \\cong C_p \\times \\cdots \\times C_p$ for some prime $p$.\n",
    "tags": [
      "abstract-algebra",
      "finite-groups",
      "proof-writing"
    ],
    "score": 5,
    "answer_score": 1,
    "is_accepted": false,
    "question_id": 1593094,
    "answer_id": 1593097
  },
  {
    "theorem": "Whether a real number is a dyadic rational iff its binary expansion terminates?",
    "context": "In self-studying a textbook on computability theory, I found that many of the exercises depend on the following factlet:\n\nA dyadic rational is a rational number whose denominator is a power of two, i.e. a rational number of the form $\\frac{a}{2^b}$. A real number is a dyadic rational if and only if its binary expansion terminates.\n\nI have the following for the forward direction:\n\nThe binary expansion of a number between $0$ and $1$ is of the form\n    \\begin{equation*}\n    0.x_1x_2x_3\\cdots = \\sum_{k=1}^{\\infty}x_k2^{-k} = \\sum_{k=1}^{\\infty}\\frac{x_k}{2^k}\n  \\end{equation*}\n    Suppose a number $0 < x < 1$ has a terminating binary expansion.\n    Then its expansion is of the form $0.x_1\\cdots x_k$, where $x_k$ is the last $1$ digit.\n    Then\n    \\begin{equation*}\n    x = \\frac{x_1}{2^1} + \\frac{x_2}{2^2} + \\ldots + \\frac{x_k}{2^k} = \\frac{x_12^{k-1} + x_22^{k-2} + \\ldots + x_k}{2^k}\n  \\end{equation*}\n    Since this is base-$2$, each $x_i$ must be either $0$ or $1$, whence it follows that the denominator and numerator are integers, and the denominator is a power of two, which means $x$ is a dyadic rational.\n\nFor the converse, I have the following idea, but do not have the background to write up a rigorous proof (in particular, I cannot imagine how to deal with the ambiguity of infinite $1$s versus infinite $0$s at some point in the expansion):\n\nConversely, we must show that if $0 < \\frac{a}{2^b} < 1$ is a dyadic rational, then its binary expansion terminates.\n    Every dyadic rational can be represented as the finite sum/product $\\left(\\frac{1}{2} + \\ldots + \\frac{1}{2}\\right)\\frac{1}{2}\\cdot\\ldots\\cdot\\frac{1}{2}$.\n    The binary expansion of the sum of two numbers with terminating binary expansions terminates, same for the product; and it follows that the binary expansion of a dyadic rational terminates.\n\nI have not yet formally tackled (but have vague, possibly incorrect, intuition of) the construction of real numbers, Cauchy convergence and proof by induction (which I gather could be used somehow...), but I need to convince myself of the factlet and its possible pitfalls to continue with the material for the time being (namely, Cantor's diagonalization proofs). Any detailed hints or full-on proofs would be greatly appreciated.\n(I have found this question but the hint in the answer seems unhelpful given my lack of background.)\nEDIT:\n\nObserve that $a$ is a finite integer, and so can be written as the finite-term sum $x_12^{k-1} + x_22^{k-2} + \\ldots + x_k$, where $x_i \\in \\{0, 1\\}$.\n    Since $\\frac{a}{2^b} < 1$, it follows that $a < 2^b$.\n    By the definition of binary expansion, $x_1 = 1$, whence $2^{k-1} \\le a < 2^b$, and we have $k-1<b$.\n    But $1 \\le i \\le k$, whence $k-i<b$.\n    Then we can divide each term by $2^b$, obtaining:\n    \\begin{equation*}\n    \\frac{a}{2^b} = \\frac{x_1}{2^{b-k+1}} + \\frac{x_2}{2^{b-k+2}} + \\ldots + \\frac{x_k}{2^b},\n  \\end{equation*}\n    which gives us a finite binary expansion; this completes the proof.\n\n",
    "proof": "Hint: the binary representation of $\\frac{a}{2^k}$ is obtained by shifting the digits (or bits, if you prefer) in the binary representation of $a$ by $k$ places to the right.\n",
    "tags": [
      "analysis",
      "elementary-number-theory",
      "proof-verification",
      "proof-writing",
      "binary"
    ],
    "score": 5,
    "answer_score": 1,
    "is_accepted": true,
    "question_id": 1404291,
    "answer_id": 1404301
  },
  {
    "theorem": "$K^n \\cong K^m \\implies n = m$",
    "context": "Let $K$ be a field and let $E$ be a vector space over $K$. I want to prove that any two finite bases of $E$ are equinumerous. What I did was:\nLet $B = \\{u_1, \\cdots, u_n\\}$ be a finite basis of $E$. Consider:\n$$f: K^n \\longrightarrow E$$\nDefined by $f(\\alpha_1, \\cdots, \\alpha_n) = \\sum_{i=1}^n\\alpha_i u_i$\nIt can be easily shown that $f$ is an isomorphism. It follows that $K^n \\cong E$. \nLet $B_0$ be any other finite basis of $E$, say of cardinality $m$. In a similar reasoning to above, we can prove that $K^m \\cong E$.\nIt follows that $K^n \\cong K^m$. \nNow, how do I show that $n = m$?\nI know that an isomorphism preserves the algebraic structure, that's, if two objects are isomorphic, then what is true about a designated part of one object's structure is true about the other's. That's why one feels that $n = m$ should be true. But how do I prove that rigorously?\nThanks a lot.\n",
    "proof": "Although this question has essentially already been answered in the comments, I will present a solution anyway.  It's obvious that $K^n$ has a basis of cardinality $n$, namely $\\{(1,0,\\ldots,0), (0,1,0,\\ldots,0), \\ldots, (0,\\ldots,0,1)\\}$, so to prove that $K^n \\cong K^m$ implies $n = m$, it's enough to prove that any two finite bases of a vector space have the same cardinality.  The simplest argument to show this that I know of involves successively exchanging elements of one basis with the other and is given in the Wikipedia articles http://en.wikipedia.org/wiki/Dimension_theorem_for_vector_spaces#Alternative_Proof and http://en.wikipedia.org/wiki/Steinitz_exchange_lemma.  \nTo illustrate this proof by example, suppose that the vector space $V$ had a basis $B=\\{v,w,x\\}$ of cardinality $3$ and a basis $B'=\\{a,b,c,d,e\\}$ of the larger cardinality $5$.  Then, since $B$ is a basis, $a$ must be a linear combination  of $v$, $w$, and $x$: \n$$\\alpha v + \\beta w + \\gamma x-a=0.\\qquad (1)$$\n Also, since $B'$ is a basis, $\\{a\\}$ must be linearly independent, which means that $a\\ne 0$.  Therefore, at least one of $\\alpha$, $\\beta$, and $\\gamma$ must be nonzero; this means that we can rewrite $(1)$ to express one of the elements of $B$ as a linear combination of the other elements of $B$ together with $a$.  Assuming without loss of generality that the nonzero coefficient is $\\alpha$, we can express $v$ \n as a linear combination of $w$, $x$ and $a$:\n$$\nv=-\\alpha^{-1}\\beta w-\\alpha^{-1}\\gamma x+\\alpha^{-1} a.\n$$  Therefore, since $B=\\{v,w,x\\}$ spans $V$, $\\{w,x,a\\}$ must also span $V$.  \nWe can now perform the same step again: write $b$ as a linear combination of $w$, $x$ and $a$:\n$$\\epsilon w + \\zeta x + \\mu a-b=0.$$\nSince $\\{a,b\\}$ is a subset of the basis $B'$, it is linearly independent, so either $\\epsilon$ or $\\zeta$ must be nonzero.  This means that we can express either $w$ or $x$ as a linear combination of the other elements of $\\{w,x,a\\}$ together with $b$.  Assuming again without loss of generality that we can express $w$ in this way, $w$ is a linear combination of $x$, $a$ and $b$, and so $\\{x,a,b\\}$ must span $V$.  \nPerforming the same step one more time, in the same way, we find that $\\{a,b,c\\}$ must span $V$.  Therefore, $d$ is a linear combination of $a$, $b$, and $c$, which contradicts the linear independence of $B'$ and hence the assumption that $B'$ was a basis.  Since we can clearly perform a similar argument for any two finite bases of unequal cardinalities, we can conclude that any two finite bases of a vector space must have the same size.\n",
    "tags": [
      "abstract-algebra",
      "proof-verification",
      "proof-writing"
    ],
    "score": 5,
    "answer_score": 1,
    "is_accepted": false,
    "question_id": 1173855,
    "answer_id": 1177656
  },
  {
    "theorem": "Prove spatial velocity identity - screw theory",
    "context": "This question involves a proof regarding coordinate transformations of velocities of screw motions. This comes from \"A Mathematical Introduction to Robotic Manipulation\" (the text is available for free here: http://www.cds.caltech.edu/~murray/books/MLS/pdf/mls94-complete.pdf) \nThe identity is shown below, and is on pg. 59 of the above link. Here's the image:\n\nI know how to get $V_{a,c}^c$ (with respect to frame c). That comes directly from the chain rule, switching the order of $\\dot{g}$ and $g^{-1}$. But I don't know how to get $V_{a,c}^b$ (with respect to frame b), i.e. the left-hand side. \nAny help much appreciated.\n",
    "proof": "The proof of 2.15 is more or less the same as proof of 2.14 as stated.\nLet's start with definition of $V_{ac}^b$\n$$\nV^b_{ac} = g^{-1}_{ac}\\dot g_{ac}\n$$\nNow substitute $g_{ac} = g_{ab}g_{bc}$ and $g^{-1}_{ac}= g^{-1}_{bc}g^{-1}_{ab}$.\n\\begin{align}\nV^b_{ac} &= g^{-1}_{bc}g^{-1}_{ab} \\dot{\\overline{ g_{ab}g_{bc}}}  \\\\\n&= g^{-1}_{bc}g^{-1}_{ab} \\left( \\dot g_{ab}g_{bc} + g_{ab}\\dot g_{bc}\\right) \\\\\n&=  g^{-1}_{bc}\\left(g^{-1}_{ab} \\dot g_{ab}\\right)g_{bc} + g^{-1}_{bc} \\dot g_{bc} \\\\\n&= Ad_{g^{-1}_{bc}} V^b_{ab} + V^b_{bc}\n\\end{align}\nI hope it is clear. \n(Just in case: The overdot overline notation means $\\dot{\\overline{ g_{ab}g_{bc}}}= \\frac{d}{dt}\\left(g_{ab}g_{bc}\\right)$. The bar is useful to indicate what has to be differentiated, only the dot would be confusing.)\n\nIn the text there is no meaning given to the symbol $V^c$ but only to $V^b$(velocity in body frame) and $V^s$ (velocity in global frame).\n\nI'll try to explain how to think about screw motions and what is the diffference between . We can explain this quite easily with rotations. \nLet's have two coordinate frames $A,B \\in SO(3)$. The fastest way to transform $A$ to $B$ is by rotation $g_{ab}$ around some great circle. You might be familiar with slerp, that is the way how to calculate this rotation. Velocity is just vector perpendicular to that great circle. You can express it the spatial(world) frame or in the rotating(body) frame, which is rotating from frame $A$ to the frame $B$. What is quite peculiar is that when you express velocity in the body frame, which is moving, the coordinates of velocity stays the same. In fact only vectors parallel to the velocity vector has constant coordinates in body frame. All other vectors, not parallel to velocity, expressed in body frame change in time.\nRotations around great circles are the same to the $SO(3)$ as screw motions to the $SE(3)$. So screw motions are just generalization of above so except changing orientation you can change position too.\n",
    "tags": [
      "linear-algebra",
      "proof-writing",
      "transformation"
    ],
    "score": 5,
    "answer_score": 1,
    "is_accepted": true,
    "question_id": 1050895,
    "answer_id": 1065551
  },
  {
    "theorem": "With $N$ a constant $&gt;0$, show $\\prod_{p&lt;x}\\frac{1}{p^{N+1}-1}&gt;\\frac{0.2}{\\log^2 x}$.",
    "context": "Related.\nShow that if $x$ is large enough,$$\\prod_{\\substack{p<x \\\\ p \\ \\text{prime}}}\\frac{1}{p^{N+1}-1}>\\frac{0.2}{\\log^2 x}.$$\nSpeaking of which, Theorem 6.12, and maybe others, of this paper might be useful.\nIf $N$ cannot be arbitrarily large for the inequality to hold, any conditions for truthfulness regarding its value are welcome.\n",
    "proof": "Your inequality is equivalent to $$-\\underset{p\\leq x}{\\sum}\\log\\left(p^{N+1}-1\\right)>\\log\\left(0.2\\right)-2\\log\\left(\\log\\left(x\\right)\\right).$$\n Now we have, for partial summation and Prime Number Theorem, that exists $c_{1},c_{2}>0$\n  such that $$-\\log\\left(0.2\\right)-\\underset{p\\leq x}{\\sum}\\log\\left(p^{N+1}-1\\right)<-\\underset{p\\leq x}{\\sum}\\log\\left(p\\right)=-\\left(c_{1}x+c_{2}\\frac{x}{\\log\\left(x\\right)}+o\\left(\\frac{x}{\\log\\left(x\\right)}\\right)\\right)<-cx<-2\\log\\left(\\log\\left(x\\right)\\right)$$\n for some $c>0$\n  and $x$\n  large enough. So it is false.\n",
    "tags": [
      "real-analysis",
      "number-theory",
      "prime-numbers",
      "proof-writing",
      "products"
    ],
    "score": 5,
    "answer_score": 1,
    "is_accepted": true,
    "question_id": 1039037,
    "answer_id": 1039600
  },
  {
    "theorem": "Checking understanding on proving uniqueness of identity and inverse elements of a group.",
    "context": "Sorry for such a trivial question, but just wanted to check my understanding.\nWhen proving a statement, for example, that the inverse of a group element is unique (in elementary group theory) one starts by supposing that there exists two inverses $h$ and $k$ for a given element $g \\in G$, where $G$ is some group with binary operation $\\ast:G\\times G \\rightarrow G$, such that $h\\ast g = g\\ast h = e$  and $k\\ast g = g\\ast k = e$ where $e \\in G$ is the unique identity for the group $G$. From this, one can show that $h=k$ in the following manner: $$ h=h\\ast e =h\\ast\\left( g\\ast k\\right) = \\left(h\\ast g\\right)\\ast k = e\\ast k =k$$ and as such $h=k$. \nNow, is the reason we can from this state that the inverse of an element $g\\in G$ is unique because $h$ and $k$ were chosen arbitrarily, apart from the requirement that they are inverses of $g$, and as such, if we know the value of one of the inverses, say $h$, then we know that for any other value $k$ to be an inverse of $g$ it must be equivalent to the known inverse $h$, and thus $h$ is the unique inverse of $g$. Is this reasoning correct? \nSorry for the wordiness of this question, but just wanted to check my understanding explicitly. Also, although I understand that, logically, I should have proven this first (but I'd already written out the inverses part before thinking of this, so apologies for that), but is the reasoning the same for arguing that the identity element of a group is unique? (i.e. If we assume that there are two identity elements $e,g \\in G$ and subsequently show that $e=g$, then this implies that if we know the form of one of the identities, say $e$, then for any other value $g$ to be an identity of the group $G$ it must be equivalent to $e$ and thus the identity element of a group is unique). \n",
    "proof": "Group axioms tell you that an identity element must exist, and also that every element has an inverse. They don't tell you that there's only one identity element, and they don't tell you that an element can have only one inverse: these are things that you have to prove.\nSo, if for example you want to prove that there is only one identity element, suppose instead that there are more than one. If so, you can choose two of them that are not the same element: yet as the proof goes on you see that those elements are instead the same one, as you know. This means that supposing that there are lots of identity elements yields a contradiction: hence, since you can't have more than one of them, and since at least one has to exist because of the group axioms, you see that the only possible option is that there's exactly only one identity element.\nThe same happens when you're trying to show that every element of the group has only one inverse.\n",
    "tags": [
      "group-theory",
      "logic",
      "proof-writing"
    ],
    "score": 5,
    "answer_score": 2,
    "is_accepted": false,
    "question_id": 911451,
    "answer_id": 912005
  },
  {
    "theorem": "How to prove the inequality?",
    "context": "Given $0<x<1$, $0<a<b<1$, and $a+b<1$, how to prove $a^x(1-ax)<b^x(1-bx)$?\nI've tried using $f(x)=x^t(1-xt)$ to do some manipulations (including derivations), but failed.\n",
    "proof": "The function $f(x)=x^k(1-kx)=x^k-kx^{k+1}$ has derivative\n$$f'(x)=kx^{k-1}(1-(k+1)x).$$\nSo the only critical point of $f$ is at $1/(k+1)$, and the latter is greater than $1/2$.\nNow assume $0<x<y$ and $x+y<1$, and form the difference\n$$w(x,y)=f(y)-f(x).$$\nWe wish to show this is nonnegative under the conditions. The inequalities defining the restriction may be made inclusive, and then the region $R$ satisfying  them is bounded by a triangle with vertices $(0,0),\\ (1/2,1/2),\\ (0,1).$  When the partials of $w$ are each put to $0$ to get a possible critical point, the only one having positive $x,y$ in the containing square $[0,1]^2$ is $(1/(k+1),1/(k+1))$  but this lies outside $R$ since $1/(k+1)>1/2.$ [We do not even have both partials defined on the left vertical side of $R$, since in fact as $x \\to 0^+$ we have $f' \\to + \\infty.$] Therefore the minimum of $w$ must occur on one of the three edges of the triangle $R$.\nOn the left edge, where $x=0,\\ 0 \\le y \\le 1$ we have $w=f(y) \\ge 0.$ On the edge corresponding to $y=x$ we have $w=0.$ The remaining \"downwardly slanted\" edge may be parametrized by $(x,y)=(t,1-t)$ where $0 \\le t \\le 1/2.$ We thus turn to considering the function $h(t)=f(1-t)-f(t).$ The direct derivative of $h$ is involved, and I could not get its zeros. However we only need to show that $h'(t)<0$ for $0 \\le t \\le 1/2,$ for given that, since at $t=1/2$ we have $h(t)=0,$ we could conclude as desired that $h(t)\\ge 0$, i.e. that $w \\ge 0$ on the last edge.\nNow we take the derivative of $h(t)$ (the chain rule introduces two extra minus signs), and rewrite it in a convenient form:\n$$\\frac{1}{2k}h'(t)=(-1)\\cdot \\frac{(1-t)^{k-1}+t^{k-1}}{2}+(k+1)\\cdot \\frac{(1-t)^k+t^k}{2}. \\tag{1}$$\nWe can use the convexity/concavity of the functions $u^{k-1}$ and $u^k$ to obtain an upper bound for $h'(t)/(2k),$ and this upper bound turns out to be negative. First since $u^{k-1}$ is concave up, we have that the first fracion in $(1)$ (without the negative sign in front) is at least $(1/2)^{k-1}.$ Similarly since $u^k$ is concave down the second fraction in $(1)$ is at most $(1/2)^k.$  Putting these together we obtain the upper bound for $h'(t)/(2k)$ of\n$$-(1/2)^{k-1}+(k+1)(1/2)^k=(1/2)^{k-1}(-1+(k+1)(1/2))\n\\\\ =(1/2)^{k-1}(k-1)/2<0,$$\nthe last because of the assumption $0<k<1.$\n",
    "tags": [
      "inequality",
      "proof-writing"
    ],
    "score": 5,
    "answer_score": 1,
    "is_accepted": true,
    "question_id": 851210,
    "answer_id": 852433
  },
  {
    "theorem": "How to prove a very basic algorithm by induction",
    "context": "I just studied proofs by induction on a math book here and everything is neat and funny: the general strategy is to assume the LHS to be true, and use it to prove the RHS (for the inductive step). Now I'm trying to prove the correctness of this algorithm for incrementing numbers, i.e. $y \\rightarrow (y+1)$, but I have problems to bridge the gap between the two approaches.\nIncrement(y)\n    if y = 0 then return(1) else\n    if (y mod 2) = 1 then\n        return(2 * Increment(⌊y/2⌋))\n    else return(y + 1)\n\nThe beginning is easy:\n\nLet $P(n)$ be $Inc(y)$\nBase case: $P(0)$ is true because $Inc(y)$ returns 1\n\nAccording to the algo book, if the number is even we're good since $y+1$ is explicitly returned.\nQuestion 1: is this enough to build a complete proof? Or should I better lay out something like:\nFor the inductive step I assumed $P(n)$ to prove $P(n+1)$, i.e. $Inc(y) \\rightarrow Inc(y+1)$; then:\n$$\nInc(y) = y + 1 \\rightarrow Inc(y+1) = y+2\n$$\nand then say that adding 1 to both sides of the equation on the LHS gives the equation on the RHS therefore if the first one is true by assumption the second one has to be true as well? Is this completely trivial/redundant?\nQuestion 2: when y is odd, I thought I had to start with something like\n$$\nInc(y) \\rightarrow 2 \\times Inc(\\frac{y}{2})\n$$\nand find a correlation between the two sides, thus using the assumed LHS to prove the RHS. On the other hand, the algo book sketches the proof as\n\nWe assume that the general case holds for all $y \\leq n−1$\n$$\n>2 · Increment(⌊(2m + 1)/2⌋) = 2 · Increment(⌊m + 1/2⌋) \\\\\n>= 2 · Increment(m) \\\\\n>= 2 (m+1) \\\\\n>= 2m + 2 = y + 1\n>$$\n\nwhich I don't quite understand (what's up with the floor thing?) and I can't relate to my proof-template. For the matter, the algorithm is taken from the Algorithm Design Manual, page 16, freely available here.\nTL;DR: I see that the algo works but I can't write a complete proof for it.\n",
    "proof": "Your induction hypothesis is that $\\,I(n) = n+1.\\,$ The base case is true by the first line of the function. Assume it is true for all integers $ < n.\\,$ If $\\,n = 2k\\,$ then it is true by the last line of the function. Else $\\, n = 2k+1\\,$ so $\\,n+1 = 2(k+1),\\ k = \\lfloor n/2\\rfloor.\\,$ But, by induction, $\\,I(k) = k + 1,\\,$ so the middle line returns $\\, 2\\, I(k) = 2(k+1) = n+1,\\,$ which is correct. So the algorithm is correct both when $\\,n\\,$ is even or odd, which exhausts all possible cases for $\\,n,\\,$ completing the induction\n",
    "tags": [
      "algorithms",
      "proof-writing",
      "proof-verification",
      "induction",
      "recursive-algorithms"
    ],
    "score": 5,
    "answer_score": 1,
    "is_accepted": true,
    "question_id": 834048,
    "answer_id": 834135
  },
  {
    "theorem": "Span and Dimension: A subspace",
    "context": "If $A$ is finite set of linearly independent vectors then the dimension of the subspace spanned by $A$  is equal to the number of vectors in $A$.\nThis is obviously true. Since $A$ is a finite set of linearly independent vectors and spans a subspace, $A$ is a basis for that subspace spanned by $A$ and thus by definition the dimension of a vector space is equal to the cardinality of any basis.\nI would help with writing the above argument in a concise, precise manner with mathematical notation and other shorthand\nSecondly in general what tips and/or advice you could give in general to make my arguments and proofs as efficient (time-wise) as possible. \n",
    "proof": "Here are some tips that I follow when writing proofs.\n\nWrite in complete sentences including punctuation.  (This seems contradictory since there are often so many symbols in math proofs.  But symbols have exact meanings in words.  For example, $\\exists$ means \"there exists\".  Anywhere you see $\\exists$, in your mind you can replace that symbol with \"there exists\".  In this way, math proofs should be paragraphs of complete sentences with punctuation.)\nWrite down the relevant definitions first.  Often, the proof is just showing that the circumstances match the definitions.\n\nI think you're trying to prove the statement: if $A$ is a finite set of linearly independent vectors then the dimension of the subspace spanned by $A$ is equal to the number of vectors in $A$.\nHere is one proof:  The dimension of a vector subspace is the size of any of its bases.  (Recall the theorem: all bases of a vector subspace have the same size.)  A basis for a vector subspace $V$ is a set of linearly independent vectors that spans the subspace.  We are given that $A$ is a set of linearly independent vectors.  Therefore $\\text{Span}(A)$ is a subspace, and its dimension is $|A|$ (the number of elements in $A$).\n",
    "tags": [
      "linear-algebra",
      "vector-spaces",
      "proof-writing"
    ],
    "score": 5,
    "answer_score": 1,
    "is_accepted": true,
    "question_id": 692721,
    "answer_id": 895456
  },
  {
    "theorem": "If $\\Omega = \\{1,2,3,\\ldots,\\}$, then $S_{\\Omega}$ is an infinite group.",
    "context": "I would like to know if my proof below is correct. I do not have issues proving that $S_{\\Omega}$ is a group; what I am not sure is whether my proof that $\\vert S_{\\Omega} \\vert = \\infty$ is correct.\n\n\nProblem Prove that if $\\Omega = \\{1,2,3,\\ldots\\}$, then $S_{\\Omega}$ is an infinite group.\n\nSolution: First it is easy to show that it is a group by checking the definition. The only slightly non-trivial part is $\\vert S_{\\Omega} \\vert = \\infty$. Given any $n$, we have the element $(1,2,\\ldots,n)$ is of order $n$, i.e., the group generated by $(1,2,\\ldots,n)$ has $n$ distinct elements. We also have that the order of the group is greater than or equal to the order of the group generated by any element, i.e., all these elements are also elements of $S_{\\Omega}$. Hence, we have $\\vert S_{\\Omega} \\vert \\geq n$ for all $n \\in \\mathbb{Z}^+$. Hence, $\\vert S_{\\Omega} \\vert = \\infty$.\n\nThanks\n",
    "proof": "Yes, your argument looks fine.\n\nFor a different, perhaps easier way to proceed, just think about\n$$(12), (13), (14), ...$$\nas elements of the group. How many such elements are there?\n",
    "tags": [
      "abstract-algebra",
      "group-theory",
      "proof-writing",
      "proof-verification"
    ],
    "score": 5,
    "answer_score": 1,
    "is_accepted": false,
    "question_id": 618969,
    "answer_id": 618977
  },
  {
    "theorem": "For bijection $f:A \\rightarrow B$, prove that $f^{-1} \\circ f = {\\text {id}}_{A}$",
    "context": "I have to prove that for a bijection $f:A \\rightarrow B$, $f^{-1} \\circ  f = {\\text {id}}_{A}$, where ${\\text {id}_A}$ is the identity function of $A$, and we define $f^{-1}: B \\rightarrow A$ by $f^{-1}(b) = a$ if $f(a)=b$. This definition is given by the homework prompt, so I think it's safe to assume it.\nHere is my work:\n\nFor $a \\in A$ and $b \\in B$, we have defined that if $f(a)=b$, then $f^{-1}(b)=a$, so $f^{-1}(f(a))=a$, so $f^{-1} \\circ f:A\\rightarrow A$. We know ${\\text{id}}_A:A\\rightarrow A$ is the bijective function s.t. $\\forall a \\in A,  {\\text{id}}(a)=a$. As the domains of the functions are equal and $\\forall a \\in A, (f^{-1}\\circ f)(a) = {\\text {id}}_{A}(a)$, the two functions are equal.\n\nCan someone please confirm whether it is sufficiently rigorous, or if not, suggest any improvements?\n",
    "proof": "Under the given definition of $f^{-1}$, your proof is correct.\nDepending on what you're allowed to assume, it may be necessary (or in any case, a good exercise) to prove that $f^{-1}:B \\to A$, i.e., that the domain of $f^{-1}$ as given is $B$, and its codomain is $A$.\n",
    "tags": [
      "elementary-set-theory",
      "functions",
      "proof-writing",
      "proof-verification"
    ],
    "score": 5,
    "answer_score": 1,
    "is_accepted": true,
    "question_id": 531137,
    "answer_id": 537929
  },
  {
    "theorem": "Proof to sequences in real analysis",
    "context": "I need some verification for my proof in part a) and help to get me started on part b)\na) Prove that the sequence $a_n = (2n+1)/(3n+5)$ converges to $2/3$ directly from the definition of convergence of a sequence.\nSolution: Some rough work first: Given $\\epsilon>0$, we want $|a_n-(2/3)|< \\epsilon$. Need $|(2n+1)/(3n+5) - (2/3)|< \\epsilon$ which after simplifying gives $7/(9n+15)< \\epsilon$. Solving for $n$ gives $n> (7-15\\epsilon)/9\\epsilon$.\nThen we write our formal proof: Given $\\epsilon>0$, set $N= (7-15\\epsilon)/9\\epsilon$. Let $n>N$. Then $|a_n-(2/3)| = 7/(9n+15) < 7/(9((7-15\\epsilon)/9\\epsilon)+15) =\\epsilon$. \nThus by definition the sequence converges to 2/3.\nb) Suppose that the sequence $\\{a_n\\}$ is not bounded above (that is, for any real number $x$, there is some $n$ so that $a_n>x$). Prove that $\\{a_n\\}$ does not converge to $42$.\nTo solve this I am assuming that we have to suppose that $\\{a_n\\}$ does converge to $42$, and then we will get a contradiction, that is the sequence will be bounded above. So can anyone help me get started on this? \n",
    "proof": "Your answer to part (a) is well thought through.  However your post for part (b) has confused the assumption you need for a proof.\nFor part (b): If $\\{a_n\\}$ is not bounded above, suppose for a contradiction that $\\{a_n\\}$ converges to $42$.  Let us fix $\\varepsilon=1$ and choose $N\\in\\mathbb{N}$ such that for every $n>N$, $|a_n-42|<1$.  (All we have done is use the definition of convergence.) Since\n$$|a_n-42|<1 \\iff -1<a_n-42<1,$$\nwhenever $n>N$ we have that $a_n<43$.  Now let us concentrate on a global upper bound, one that includes the finite number of terms $a_1, a_2, \\ldots ,a_{N-1}, a_{N}$ (using the same value of $N$ as before).\nThe finite number of terms are bounded above by $\\max_{1\\le i\\le N} a_i$.  It follows that for every $n\\in\\mathbb{N}$ we have the following bound:\n$$a_n\\le \\max\\left\\{ \\max_{1\\le i\\le N}a_i, 43\\right\\}.$$ Clearly $\\{a_n\\}$ is bounded above, a contradiction.\n",
    "tags": [
      "real-analysis",
      "proof-writing",
      "proof-verification"
    ],
    "score": 5,
    "answer_score": 0,
    "is_accepted": true,
    "question_id": 495784,
    "answer_id": 498391
  },
  {
    "theorem": "A statement about an element $a$ in semigroup S, such that $aS$ containts idempotent and $a=axa$ implies $x=xax$",
    "context": "I have been currently studying some characteristics of completely regular and completely simple semigroups and I have came across a lemma, which seems simple, but I'm struggling\nwith it's proof, so I was wondering if anyone could help me.\nThe lemma states:\nLet $S$ be an arbitrary semigroup and let $a\\in{}S$ be such element, that\n$aS$ contains an idempotent and for every $x\\in{}S$, $a=axa$ implies that $x=xax$.\nThen $a\\in{}a^2S$ and for every $x\\in{}S$, $a=a^2x$ implies that $x=x^2a$.\nI would appreciate any help :-)\n",
    "proof": "Here is a counterexample. Let $S = \\{a, 0\\}$ with $s^2 = 0$ and $a0 = 0 = 00 =\n0a$. Then $aS$ contains the idempotent $0$ and for every $x \\in S$, the\ncondition $a=axa$ implies that $x=xax$, since the premise is never fulfilled.\nHowever $a^2S = \\{0\\}$ and thus $a \\notin a^2S$.\nBut in fact, you did not reproduce the exercise correctly.  The original statement was the following:\n\nThe following conditions are equivalent:\n\nFor all $a \\in S$, the set $aS$ contains an idempotent and for all $a, x \\in S$, the condition $axa = a$ implies $x = xax$.\nFor all $a \\in S$, $a \\in a^2S$ and for all $a, x \\in S$, the condition $a = a^2x$ implies $x = x^2a$.\n\n\nYou asked for a proof that (1) implies (2). Here is one:\nLet $a \\in S$. Then $aS$ contains an idempotent, say $e = ax$. Thus $e = e(ax)e$, whence $ea = (ea)x(ea)$ and $x = x(ea)x$ by the second part of (1). It follows $x = xeax = xee = xe$. Therefore $x = xax$ and $a = axa$. \nLet now $y \\in S$ be such that $a^2y$ is idempotent. By the previous argument, $y = ya^2y$ and thus $ay = aya^2y = (ay)a(ay)$, whence $a = a(ay)a = a^2(ya)$. Thus $a \\in a^2S$.\nSimilarly, $ya = (ya)a(ya)$, whence $a = a(ya)a$ and $a \\in Sa^2$. It follows in particular that $a\\ \\mathcal{H}\\ a^2$. Thus by Green's lemma, the $\\mathcal{H}$-class of each element of $S$ is a group. \nSuppose now that $a = a^2x$ for some $x \\in S$. Let $e$ be the idempotent of the $\\mathcal{H}$-class $G$ of $a$ and let $f$ be the idempotent of the $\\mathcal{H}$-class $H$ of $x$. Let $\\bar a$ be the inverse of $a$ in $G$. Then $e = \\bar aa = \\bar a(a^2x) = ax$ and thus $axa = a$ and $xax = x = xe$. Thus $e \\ \\mathcal{L}\\ x \\ \\mathcal{L}\\ f$ and thus $ef = e$ and $fe = f$. By Green's lemma, the map $u \\to fu$ defines a bijection from $G$ onto $H$ and thus there exists $b \\in G$ such that $x = fb$. Therefore, since $a = ae$, we get $a = a^2x = a^2fb = a(ae)fb = aaeb = aab$ and thus $b = \\bar a$. Consequently, since $xf = x$, we get $x^2a = xf\\bar aa = x\\bar aa = xe = x$ .\n",
    "tags": [
      "proof-writing",
      "semigroups"
    ],
    "score": 5,
    "answer_score": 1,
    "is_accepted": true,
    "question_id": 486575,
    "answer_id": 487670
  },
  {
    "theorem": "Proof of the sine rule",
    "context": "So I made my first attempt at a proof. I think it turned out well. Maybe not. But I was wondering if someone could take a look at it and tell me what they think. I'd be glad to hear some criticism on it. So here it goes:\nThe Sine Rule is as follows: $$\\frac{\\sin(\\alpha)}{a}=\\frac{\\sin(\\beta)}{b}=\\frac{\\sin(\\gamma)}{c}$$ $\\mathbf{Proof}$: We are given an acute triangle $\\triangle OPQ$ with sides $a$,$b$, and $c$. Opposite of the sides are the angles $\\alpha$,$\\beta$, and $\\gamma$, respectively. We will divide $\\triangle OPQ$ with $2$ line segments, $h_1$ and $h_2$. The line segment $h_1$ will have an endpoint at angle $\\gamma$ and extend to side $c$ to make $h_1$ perpendicular to side $c$. The line segment $h_2$ will have an endpoint at angle $\\alpha$ and will extend to side $a$ to make $h_2$ perpendicular to side $a$. We know that $\\sin(\\alpha)=\\frac{h_1}{b}$ and $\\sin(\\beta)=\\frac{h_1}{a}$. We can deduce that $h_1=a\\sin(\\beta)$ and $h_1=b\\sin(\\alpha)$. From these two equations, we can produce the following result: $$\\begin{align}a\\sin(\\beta)=b\\sin(\\alpha)\\\\ \\frac{\\sin(\\beta)}{b}=\\frac{\\sin(\\alpha)}{a}\\end{align}$$ We now turn our attention to see that $\\sin(\\gamma)=\\frac{h_2}{b}$ and $\\sin(\\beta)=\\frac{h_2}{c}$. As before we can deduce that $h_2=b\\sin(\\gamma)$ and $h_2=c\\sin(\\beta)$. From this system of equations we can simplify it in the following way: $$\\begin{align}b\\sin(\\gamma)=c\\sin(\\beta)\\\\ \\frac{\\sin(\\gamma)}{c}=\\frac{\\sin(\\beta)}{b}\\end{align}$$ It follows that if $\\frac{\\sin(\\alpha)}{a}=\\frac{\\sin(\\beta)}{b}$, and $\\frac{\\sin(\\beta)}{b}=\\frac{\\sin(\\gamma)}{c}$, then $\\frac{\\sin(\\alpha)}{a}=\\frac{\\sin(\\gamma)}{c}$. Thus proving that $$\\frac{\\sin(\\alpha)}{a}=\\frac{\\sin(\\beta)}{b}=\\frac{\\sin(\\gamma)}{c}$$ Which concludes my proof. I know it's a pretty basic concept learned in high school trig but I just wanted to start off  on something easy. ANY input would be great.\n",
    "proof": "Proof:\n$$\\sin\\left(\\alpha\\right)=\\frac{H_c}{b}$$\n$$\\sin\\left(\\beta\\right)=\\frac{H_c}{a}$$\n$$H_c=b\\sin(\\alpha)$$\n$$H_c=a\\sin(\\beta)$$\n$$a\\sin(\\beta)=b\\sin(\\alpha)$$\n$$\\frac{a}{\\sin(\\alpha)}=\\frac{b}{\\sin(\\beta)}$$\n",
    "tags": [
      "geometry",
      "trigonometry",
      "proof-writing"
    ],
    "score": 5,
    "answer_score": 1,
    "is_accepted": false,
    "question_id": 282970,
    "answer_id": 1266139
  },
  {
    "theorem": "Extension of Fatou&#39;s lemma",
    "context": "let $X$ be a finite measure space and $\\{f_n\\}$ be a sequence of integrable functions, $f_n \\rightarrow f\\text{ a.e.}$ on $ X$.\nI want to show if (1) holds, then (2) holds too.\n$$\\lim_{n \\rightarrow \\infty}\\int_X |f_n| \\, d\\mu=\\int_X |f| \\, d\\mu,\\tag{1}$$\n$$\\lim_{n \\rightarrow \\infty}\\int_X |f_n-f| \\, d\\mu=0.\\tag{2}$$\nMy attempt:\nI have proven that (2) holds for nonnegative $f$.\nThen for the general case, I split the set to $E^+=\\{x: f \\geq 0\\}$ and $E^-=\\{x: f \\leq 0\\}$:\n$$\\lim_{n \\rightarrow \\infty}\\int_{E^+} f_n \\, d\\mu-\\int_{E^+} f \\, d\\mu -\\lim_{n \\rightarrow \\infty}\\int_{E^-} f_n \\, d\\mu+\\int_{E^-} f \\, d\\mu=0$$\nBut I don't know how to proceed from here!\n",
    "proof": "As Lukas Geyer points out, the result isn't true unless $f$ is integrable. To see why, consider $f_n(x) = n$. Clearly, $f(x) = \\infty$. Also:\n$$\n\\int_X f_n \\, d\\mu = n \\mu(X)\n$$\nThus:\n$$\n\\lim_{n \\to \\infty} \\int_X f_n \\, d\\mu = \\int_X f \\, d\\mu = \\infty\n$$\nYet:\n$$\n\\lim_{n \\to \\infty} \\int_X |f_n - f| \\, d\\mu = \\infty \\neq 0\n$$\nNow, assuming $f$ is integrable, we have:\n$$\n\\left||f_n - f| - |f_n|\\right| \\le |f|\n$$\nHence, by the dominated convergence theorem:\n$$\n\\lim_{n \\to \\infty} \\int_X \\left(|f_n - f| - |f_n|\\right) \\, d\\mu = - \\int_X |f| \\, d\\mu\n$$\nRearrange to get the required result.\n",
    "tags": [
      "real-analysis",
      "measure-theory",
      "integration",
      "proof-writing",
      "lebesgue-integral"
    ],
    "score": 5,
    "answer_score": 1,
    "is_accepted": false,
    "question_id": 225834,
    "answer_id": 226344
  },
  {
    "theorem": "limit superior of a sequence proof",
    "context": "Let $(x_{n})\\in\\mathbb{R}^{+}$ be bounded and let $x_{0}=\\lim\\sup_{n\\rightarrow\\infty}x_{n}$. $\\forall\\epsilon>0$, prove that there are infinitely many elements less than $x_{0}+\\epsilon$ and finitely many terms greater than $x_{0}+\\epsilon$. \nMy attempt:\nBy definition of limit superior, $\\forall\\epsilon>0$, $\\exists N_{\\epsilon}\\in\\mathbb{N}$  s.t. $\\forall n>N_{\\epsilon}$, $x_{n}<r+\\epsilon$ . Since $\\{x_{n}\\}$ is a bounded sequence, there are only $N_{\\epsilon}$  values of $\\{x_{n}\\}$  s.t. $x_{n}>r+\\epsilon$.  However, since for all $n>N_{\\epsilon}$,  $x_{n}<r+\\epsilon$,  there are infinitely such $x_{n}<r+\\epsilon$.\nI think my proof is probably incomplete/too informal. What do you think? \n",
    "proof": "As Paul said in the comments, your argument is basically just fine, apart from using $r$ where the question has $x_0$. There are a few minor problems with this sentence in addition to the use of $r$ for $x_0$:\n\nSince $\\{x_n\\}$ is a bounded sequence, there are only $N_\\epsilon$ values of $\\{x_n\\}$ s.t. $x_n>r+\\epsilon$.\n\nFirst, you should omit the opening clause, since you’re not actually using the boundedness of the sequence here: what you’re using is simply the definition of $N_\\epsilon$. (The boundedness of the sequence is needed only to ensure that the limit superior is finite, so that the question makes sense.) Secondly, there may not be $N_\\epsilon$ terms that exceed $x_0+\\epsilon$: some of the first $N_\\epsilon$ terms might be $\\le x_0+\\epsilon$. However, there are definitely at most $N_\\epsilon$ such terms. Finally, ‘values of ${x_n}$’ reads a bit oddly: ‘terms of ${x_n}$’ or ‘values of $n$’ would be clearer. After making these changes, the sentence becomes:\n\nBy the choice of $N_\\epsilon$ there are at most $N_\\epsilon$ values of $n$ such that $x_n\\ge x_0+\\epsilon$ and therefore at most $N_\\epsilon$ values of $n$ such that $x_n>x_0+\\epsilon$\n\nAs a matter of taste I’d then make a small change in the wording of the last sentence to match:\n\nHowever, since for all $n>N_\\epsilon$, $x_n<x_0+\\epsilon$, there are infinitely many $n$ such that $x_n<x_0+\\epsilon$.\n\n",
    "tags": [
      "sequences-and-series",
      "limits",
      "proof-writing",
      "limsup-and-liminf"
    ],
    "score": 5,
    "answer_score": 1,
    "is_accepted": false,
    "question_id": 101507,
    "answer_id": 101584
  },
  {
    "theorem": "How to show that the limit of a sequence is not equal to some value?",
    "context": "In the second chapter of the book Understanding Analysis by Abbott, Example 2.2.6 proved that picking $N>\\frac{1}{\\varepsilon}$ suffices to prove the convergence of $\\frac{n+1}{n}$ to the number $1$. He then continues by giving a full outline of the proof with your usual \"Let $\\varepsilon>0$ be arbitrary and set $N>\\frac{1}{\\varepsilon}$,...\" At the end, he wrote a remark \"It is instructive to see what goes wrong in the previous example if we try to prove that our sequence converges to some other limit other than $1$.\"\nI have tried doing the same steps to show $\\lim \\frac{n+1}{n}=2$, hoping for a contradiction. I assumed that it was true, then for some $\\varepsilon>0$ there is some $N\\in\\mathbb{N}$ such that for all $n\\geq N$ we have $\\left|\\frac{n+1}{n}-2\\right|<\\varepsilon$ which reduces to the simpler expression $\\left|\\frac{1}{n}-1\\right|<\\varepsilon$. I proceeded by letting $n=N$, and with the reverse triangle inequality I obtained $N>\\frac{1}{\\varepsilon+1}$. However, I'm not seeing where I'm supposed to go wrong here, as Abbott claimed in his remark. Any help resolving this issue? Where am I supposed to go wrong here?\n",
    "proof": "I will clarify my comment. By definition, $l$ is the limit of the sequence $a_n$ if for every $\\varepsilon>0$ you can find $N$ such that $\\left|a_n-l\\right|<\\varepsilon$ for all $n>N$. Thus, in this case, $2$ would be the limit of $a_n=\\frac{n+1}{n}$ if for every $\\varepsilon>0$ you can find $N$ such that $\\left|\\frac{1}{n}-1\\right|<\\varepsilon$ for all $n>N$. To contradict this statement, it is sufficient to show that for at least one value of $\\varepsilon>0$ you cannot find such $N$. Firstly, observe that for all $n\\in\\mathbb{N} \\left|\\frac{1}{n}-1\\right|=1-\\frac{1}{n}$. Hence we want to check if for every $\\varepsilon>0$ (as small as you want) we can find $N$ such that $1-\\frac{1}{n}<\\varepsilon$ for all $n>N$.\nThe idea is that increasing $n$ makes $1-\\frac{1}{n}$ closer and closer to $1$ and not $0$. Take for example $\\varepsilon=\\frac{1}{2}$. Suppose that the claim is true. Firstly observe that for $N=3$ you would get $$\\frac{2}{3}=1-\\frac{1}{3}<\\varepsilon=\\frac{1}{2}$$ which is clearly false. You may want to verify if another $N^*$ works. But this is not the case. In fact, you should look for an $N^*>3$ such that $1-\\frac{1}{n}<\\frac{1}{2}$ for all $n>N^*$\n(taking $N^*=1$ or $N^*=2$ does not make sense because we have already checked that the condition is not satisfied for    $n=3$, so in particular it is not satisfied for all $n>N^*$). However, $$1-\\frac{1}{n} > 1-\\frac{1}{N^*}>1-\\frac{1}{3} = \\frac{2}{3} > \\frac{1}{2}$$\nwhich means the limit is not $2$.\n",
    "tags": [
      "real-analysis",
      "limits",
      "analysis",
      "convergence-divergence",
      "proof-writing"
    ],
    "score": 5,
    "answer_score": 0,
    "is_accepted": true,
    "question_id": 4947213,
    "answer_id": 4947230
  },
  {
    "theorem": "Can you critique my exposition of $\\int_{-\\infty}^{+\\infty}e^{-x^2}dx = \\sqrt \\pi$?",
    "context": "Prove\n$$\\int_{-\\infty}^{+\\infty}e^{-x^2}dx = \\sqrt \\pi$$\nusing Fubini's Theorem.\nMy solution is below. Proof is Correct.\nWhat I want to know is :\nIs it well written?\nHow could the writing be improved, made clearer, or more rigorous?\n\nSolution: We first show that $$\\int_{-\\infty}^{+\\infty}\\left [e^{-y^2}\\int_{-\\infty}^{+\\infty}e^{-x^2}dx \\right ]dy = \\pi.$$  By Fubini's Theorem, $$\\int_{-\\infty}^{+\\infty}\\left [e^{-y^2}\\int_{-\\infty}^{+\\infty}e^{-x^2}dx \\right ]dy = \\iint_{(x,y) \\in \\mathbb R^2}e^{-(x^2+y^2)}dxdy$$ which, by changing to spherical coordinates, equals $$\\begin{align*}\\int_{0}^{2\\pi}\\int_0^\\infty re^{-r^2}dr d \\theta &= \\int_{0}^{2\\pi} \\Biggr|_0^\\infty \\frac {-e^{-r^2}}{2} d\\theta\\\\ &=\\pi.\\end{align*}$$\nNow, by linearity of integrals, $$\\int_{-\\infty}^{+\\infty}\\left [e^{-y^2}\\int_{-\\infty}^{+\\infty}e^{-x^2}dx \\right ]dy = \\left [ \\int_{-\\infty}^{+\\infty}e^{-x^2}dx  \\right ] \\left [ \\int_{-\\infty}^{+\\infty}e^{-y^2}dy \\right ]$$ and since  $e^{-x^2} > 0$ for all $x$, $$\\int_{-\\infty}^{+\\infty}e^{-x^2} dx = \\sqrt \\pi$$ as desired.\n",
    "proof": "I started editing the Question to \"improve\" it & then realized that OP is actually asking what \"improvements\" to make & that should come in the Answer. Hence I left it as-is after the \"divider line\" & will make my suggestions here.\nProof is well-written & well-known & rigorous enough.\nIt could be improved to make it slightly clearer what the aim is & what the method is.\nThere are no tags , to refer to later.\nThere is a slight non-linearity [ \"roundaboutness\" ] in Presentation.\nThere is no variable to capture the Integral value , which we can use to refer later.\nHere is my way to write it out :\n\nLet\n$$ P = \\int_{-\\infty}^{+\\infty}e^{-x^2} dx \\tag{1.1} $$\n$$ P^2 = \\int_{-\\infty}^{+\\infty}e^{-x^2} dx \\times \\int_{-\\infty}^{+\\infty}e^{-y^2} dy \\tag{1.2} $$\nBy linearity of Integrals ,\n$$ P^2 = \\left [ \\int_{-\\infty}^{+\\infty}e^{-x^2}dx  \\right ] \\left [ \\int_{-\\infty}^{+\\infty}e^{-y^2}dy \\right ] = \\int_{-\\infty}^{+\\infty}\\left [e^{-y^2}\\int_{-\\infty}^{+\\infty}e^{-x^2}dx \\right ]dy \\tag{1.3} $$\nBy the Fubini Theorem ,\n$\\cdots\\text{ skipping the calculations given by OP , to be written with few tags where necessary }\\cdots$\n$$ P^2 = \\pi \\tag{2.1} $$\nSince the Exponential is Positive ,\n$$ P = \\sqrt{\\pi} \\tag{2.2} $$\nWe have the necessary Conclusion :\n$$ \\int_{-\\infty}^{+\\infty}e^{-x^2} dx = \\sqrt{\\pi} \\tag{2.3} $$\nQED\n\nWhen we have other further work involving that Integral , we can easily refer to $(2.3)$\nHere is the Image to high-light what changes I made to eliminate the \"roundaboutness\" :\n\n",
    "tags": [
      "real-analysis",
      "multivariable-calculus",
      "solution-verification",
      "proof-writing",
      "fubini-tonelli-theorems"
    ],
    "score": 5,
    "answer_score": 0,
    "is_accepted": true,
    "question_id": 4819783,
    "answer_id": 4820521
  },
  {
    "theorem": "how is my proof on equinumerous sets",
    "context": "Hello i am hoping that you can tell me how my proof is and the improvements i can do. If i am complexity wrong please do tell.\nProve that if $W$ is denumerable, then $W$ is equinumerous with a proper subset of itself.\n$S$ is denumerable. therefore the is a bijection between S and the natural number: $f : S \\rightarrow \\mathbb{N}$. Say that we have a infinite Set $ A \\subset \\mathbb{N}$. $A$  would also be denumerable as a infinite subset of a denumerable set is also denumerable. Then we do $f^{-1}(A) = B$. Then $ T \\subset S $ is a proper subset.  Lets take another function $g: \\mathbb{N} \\rightarrow A$. g wouuld also be a bijection. Now let $h = g\\circ f, h$ would be a function that went $S \\rightarrow B$ and would also be a bijection . \nTherefor you can say that S and T are equinumermous. Hence $S$ is equinumermous with a proper subset of itself.\nHow did i do?\n",
    "proof": "The idea is good. A few small issues:\n\nYou didn't assume that $A$ was proper, so $B$ doesn't have to be a proper subset of $S$, which I think you want. \nAlso where did $T$ come from? You just introduced a new variable $T$ without explanation.\nYou took a function $g$ from $\\Bbb{N}\\to A$. $g$ doesn't have to be a bijection. However since $A$ is denumerable, there is a bijection, which we can call $g$.\nI think $B$ and $T$ are supposed to be the same thing, but you keep switching back and forth between the two variables.\nNot so much a mathematical issue, but you keep typing \"equinumermous.\" (At least three times, though I fixed one earlier when I noticed it and before I noticed the others.) It's actually spelled \"equinumerous.\" \n\n",
    "tags": [
      "real-analysis",
      "elementary-set-theory",
      "proof-verification",
      "proof-writing"
    ],
    "score": 5,
    "answer_score": 0,
    "is_accepted": true,
    "question_id": 2603275,
    "answer_id": 2603288
  },
  {
    "theorem": "Why calculating XOR of consecutive values can be simplified?",
    "context": "I was trying to calculate integer xor of 0..n.\nI named the function xored(n).\nNote that in examples below ^ does not mean power but integer xor (like in C or Java language)\nSo, xored(0) = 0, xored(1) = 0 ^ 1, xored(2) = 0 ^ 1 ^ 2, xored(3) = 0 ^ 1 ^ 2 ^ 3, etc...\nOr, if you prefer recursive definition:\nxored(0) = 0\nxored(n) = n ^ xored(n-1)  \nFirst, I created a naive implementation that that loops through the values and xors them together.\nThen, I noticed a funny thing.\nI noticed a pattern with my results.\nAll you need to do is to calculate n modulo 4 (call it m) then:\nif m = 0, xored(n) = n\nif m = 1, xored(n) = 1\nif m = 2, xored(n) = n + 1\nif m = 3, xored(n) = 0  \nMy question is. Why does it work? How can this be proven mathematically?\nI am not good at proving theorems...\n",
    "proof": "I think some of the proofs are based on observations and generalizing the patterns. It helps at many places where you can't prove things using a theorem but things can be sorted out using observations.\nEuler's Polyhedral Formula $ V(vertices) + F(faces) - E(edges) = 2$ is such an example. Euler did not give a proper proof (though it was proved in many ways afterwards), this result inspired huge advances.\nEDIT: There are a lot of threads with the same question and everyone solved it using mere observation. One of the threads can be found here.\n",
    "tags": [
      "proof-writing",
      "integer-programming",
      "integers",
      "binary-operations"
    ],
    "score": 5,
    "answer_score": 0,
    "is_accepted": false,
    "question_id": 1409538,
    "answer_id": 1589162
  },
  {
    "theorem": "Prove that - for every positive $x \\in \\mathbb{Q}$, there exists positive $y \\in \\mathbb{Q}$ for which $y \\lt x$",
    "context": "First my apologies if this question has been asked before. \nExposition \nI'm new at learning how to prove theorems and among the given exercises from my reference material it is asked to prove the following: \nThe original question in words: \nFor every positive $x \\in \\mathbb{Q}$, there exists positive $y \\in \\mathbb{Q}$ for which $y \\lt x$. \nI translated it and got:\n$\\forall x \\in \\mathbb{Q}_{\\gt0} \\ \\exists y \\in \\mathbb{Q}_{\\gt0}, \\ y \\lt x$\nHere is my attempted proof.\n\nIf $x \\in \\mathbb{Q}_{\\gt0}$  then $\\exists y \\in \\mathbb{Q}_{\\gt0}$  such  that  $y \\lt x$.  Suppose  $\\forall y \\in \\mathbb{Q}_{\\gt0}$,  $y \\geq x$.\n  So if  $y \\in \\mathbb{Q}_{\\gt0}$  then $y \\geq x$.  By  contrapositive  if $y \\lt x$  then  $y \\notin \\mathbb{Q}_{\\gt0}$.\n  But  this  doesn't  make  sense.  Hence  we  were  wrong  to  assume  that $\\forall y \\in \\mathbb{Q}_{\\gt0}$.\n\nQuestion\nI'm having trouble with the part starting from this doesn't make sense. I looked at the $y \\notin \\mathbb{Q}_{\\gt0}%$ and made a somewhat educated guess regarding the fact that $y \\notin \\mathbb{Q}_{\\gt0}%$ doesn't logically follow from the premise that $y \\lt x$. This in the sense that the less than 'operator' can only be defined between two mathematical objects of the same kind. Is there something i got wrong? Does this make sense? Is the proof complete anyway? What would be the correct proof?\nIn clear and concise terms, I'm trying to understand if my proof is correct.\nThanks\nUPDATE\nI re-read the question again from the material and $y$ is supposed to be a positive rational too. Yet i think given replies at the original time of this update still apply.\nUPDATE 2\nWith regards to the answer provided by @crf i think i should provide the proof strategy. By this point if someone could see something wrong in the proof, i guess it came from me making something wrong in my strategy. So here is the proof strategy. All that follows of course is supposed to be part of draft work.\n1. First i get the statement into symbolic form in order to 'safely' transform the expression:\n$\\forall x \\in \\mathbb{Q}_{\\gt0} \\ \\exists y \\in \\mathbb{Q}_{\\gt0}, \\ y \\lt x$ \n2. Transform the obtained statement into conditional form:\nWe know that $\\forall x \\in S, \\ Q(x)$ is equivalent to $(x \\in S) \\Rightarrow Q(x)$. \nThen we get:\n$x \\in \\mathbb{Q}_{\\gt0} \\Rightarrow \\exists y \\in \\mathbb{Q}_{\\gt0}, \\ y \\lt x$ \nReference: Book of proof by Richard Hammack, pp 54, Fact 2.2 available online here\n3. Then we attempt a proof by contradiction for this conditional statement:\nAs such our hypotheses become: \n$$\nx \\in \\mathbb{Q}_{\\gt0} \\\\ \\forall y \\in \\mathbb{Q}_{\\gt0} \\ (y \\geq x)\n$$\nAnd this is equivalent to : \n$$\nx \\in \\mathbb{Q}_{\\gt0} \\\\ y \\in \\mathbb{Q}_{\\gt0} \\Rightarrow y \\geq x\n$$\nand conclusion: will be a contradiction\n4. Transform $\\forall y \\in \\mathbb{Q}_{\\gt0} \\ (y \\geq x)$ to get its contrapositive:\nWe get as new hypotheses: \n$$\nx \\in \\mathbb{Q}_{\\gt0} \\\\ y \\lt x \\Rightarrow y \\notin \\mathbb{Q}_{\\gt0}\n$$\nThat where i got stuck and i started guessing: how does $y \\notin \\mathbb{Q}_{\\gt0}$ follow from $y \\lt x$? I couldn't see a rigorous contradiction between that and premises and got got stuck!\nThanks for bearing all this!\n",
    "proof": "What you've done so far actually has nothing to do with proving your theorem. If you do want such a symbolic lead-in, OK-it's not false, though it's worth noting that such intricate manipulations are ugly compared to a direct and brief argument. In any case, to finish, you must stop playing with symbols and say something mathematical, such as \"But wait! $\\frac{x}{2}\\in Q_{>0}$ is less than $x,$ so by contradiction, my assumption $y<x \\rightarrow y\\notin Q_{>0}$ is false!\" That last sentence is the entire mathematical content of the proof. \n",
    "tags": [
      "proof-writing",
      "rational-numbers"
    ],
    "score": 4,
    "answer_score": 11,
    "is_accepted": true,
    "question_id": 182747,
    "answer_id": 182791
  },
  {
    "theorem": "What are the advantages of ending a proof with “QED”?",
    "context": "I know “QED” indicates the end of a proof. We also use $\\square$ or similar.\nHow did we start doing it? It’s a very old practice passed down from Greek mathematicians like Euclid over twenty centuries ago. \nBut what are the advantages of doing so? I’d like an answer that addresses several contexts, ranging from books to papers to even posts here.\nI can make guesses and persuade myself one way or another, but I’d like to hear from those who are more experienced.\n",
    "proof": "I'll offer a somewhat different answer that contends we sometimes have a net benefit from such a strategy, but sometimes don't. So when is it beneficial? Well, that's complicated.\nLet's contrast two very different ways of explaining how we know something. Example 1:\n\nPositive integers have prime factorisations (since a minimal counterexample couldn't be prime and hence would be a product of two smaller positive integers whose prime factorisations force a contradiction), and up to permuting prime factors such factorisations are unique (since if $n=\\prod_i p_i=\\prod_j q_j$ are distinct factorisations for a minimal counterexample $n$ each $p_i$ divides, and hence is equal to, some $q_j$, whence $n/p_1$ or $n/q_1$ is a smaller counterexample unless both products are empty and hence identical).\n\nThis one-sentence proof of the fundamental theorem of arithmetic would be easy enough for a reader who knows (1) how to present a proof by induction in terms of hypothetical minimal counterexamples and (2) that primes divide at least one factor of products they divide, which follows from Bézout's lemma. If you're confident your readers can manage this, you might think it'd be a waste of everyone's time to write\nTheorem, with name: blah blah blah\nProof: several sentences $\\square$\nI almost never write solutions on this website in such a format, partly because I fear verbosity can stop a reader seeing the forest for the trees. Even in my PhD thesis, when I could prove something succinctly I simply \"reasoned out loud\" in sentences that make it seem more like a string of casual observations, where $\\square$ doesn't belong (and, if it were used, you'd feel like it was a strange choice when a full stop would do). Sometimes, that's a good way to do it, if only because (I think) it mirrors the way people understand things. People are accustomed to thinking in sentences, not in a particular formatting style exclusive to text. And while this strategy very rarely uses words such as proof or theorem, in many cases it doesn't hurt the rigour and formality of the proof.\n(Mind you, I'll admit if I'm trying to defend such a writing style, the lengthy sentence in my example is \"pushing it\".)\nBut you asked why we would use such formatting, right? Well, let's look at a few things that can derail the above style:\n\nThe proof is long enough a new paragraph ought to start when it's finished; and, just in case the next paragraph might be expected to also be a part of the proof, you need to make clear it isn't. Proofs can be long because no shorter option exists, because you as author don't know of one, or because the sort of compression I used above asks too much of the reader. (I suspect my example above would be a bit much for most people if it was the first time they ever see the FTA proven.) Heck, even needing to make one or more equations display-line can break the eye's definition of a paragraph too much to get away with the \"conversational\" approach I described.\nThe proof needs to be stretched out to familiarise the reader with techniques it's meant to illustrate. Example 2 below shows how this looks when we're helping people learn induction (but after a while, you can literally just write the theorem and say it follows by induction, and the entire proof will instantly form in the reader's head):\n\n\nTheorem: for all integers $n\\ge 0$, $\\sum_{j=1}^n(2j-1)=n^2$.\nBase step of proof by weak induction: $\\sum_{j=1}^0(2j-1)$ is an empty sum, equal to $0=0^2$ as desired.\nInductive step: if $\\sum_{j=1}^k(2j-1)=k^2$ then $\\sum_{j=1}^{k+1}(2j-1)=k^2+2(k+1)-1=(k+1)^2$. $\\square$\n\n\nThe result is so important it has a name, and needs front-and-centre attention. It might give its name to the chapter you're reading, even if most of the chapter discusses its significance while the proof itself is half a page. If I had appended \"This is called the fundamental theorem of arithmetic\" to example 1, I would have buried the lede. (And the reader might wonder whether both parts or just the second one were the theorem; and if I'd said \"these two results\", some readers wouldn't have parsed it so as to know what two results I mean. So either way, I can't win.) You're welcome to invent your own examples of when the name needs to be up front, but the reasons why would vary. Formatting is of course an art, where any number of concerns can come up from time to time. But once you're committed to theorem-with-name followed by the proof... well, you've seen where that leads in terms of spacing.\n\nAs I said, I sometimes see a benefit in making a proof seem more like an obvious observation than something that needs formatting carved out of the rest of the document's flow. I said that not doing this can stop one seeing the forest for the trees; one wants to know the \"main point\" of the proof, the \"reason why\" a theorem is true. (Mathematicians rarely think of proofs as explanations, but they can be succinct enough to be comparable to the explanations we find elsewhere in life.) However, sometimes you really need to take the trees one at a time, or there are too many to take in the whole forest at once. Sometimes, a proof is even presented as:\n\nWe will prove the A theorem, which is B. We begin by proving C. We then prove D. We then show this implies A.\n\nNo wonder you need an ending signal after all that, before we move onto the next theorem.\nHaving said that, you could argue some long mathematical publications, be they PhD theses or Andrew Wiles's proof of Fermat's last theorem, are essentially one long proof with a lot of lemmata (which may or may not be explicitly highlighted as such). The end of that proof doesn't have the same rationale for such signposting, precisely because nothing is next.\n",
    "tags": [
      "proof-writing",
      "soft-question",
      "terminology"
    ],
    "score": 4,
    "answer_score": 3,
    "is_accepted": true,
    "question_id": 3246207,
    "answer_id": 3246773
  },
  {
    "theorem": "How are proofs normally constructed in a write up, in one line or split up into multiple lines?",
    "context": "Here is the same proof formatted two different ways. My questions is what is the standard for a writeup like a homework assignment? I think the first one looks better but I don't think that is the way that I normally see it. If someone could just say which one is better I would really appreciate it. \n\n\n",
    "proof": "The first one is way easier to read. I'm always telling off students who squash all their work up into a tiny space. Apart from reading it, it's also much easier to write comments on specific bits when it's spaced out. Paper is not expensive any more, so there's no reason to cram everything in. \nIf you must do the second, at least double-space the lines and put space between paragraphs.\nPeople often say that papers are written like the latter. This is true, but people who are writing papers hopefully know what they're doing, and have been in the business a lot longer than people writing homework. Whereas on homework, you are expected to get things wrong, or inaccurate, or miss things out: the whole point of having someone look at it is to correct these things. (Even PhD theses are usually set double-spaced.)\n",
    "tags": [
      "proof-writing"
    ],
    "score": 4,
    "answer_score": 11,
    "is_accepted": true,
    "question_id": 3370137,
    "answer_id": 3370147
  },
  {
    "theorem": "Simple proof; Show that $(4^n - 1)$ is divisible by 3 (Guided proof task)",
    "context": "\nFirst part of the task is just to show that $(4^n-1)$ actually is divisible by 3 for n=1,2,3,4. No problem.  \nSecond step: is to show that $(4^n -1) = (2^n-1)(2^n+1)$ No problem, just algebra. \nThird step is to explain that $(2^n-1)$,$2^n$,$(2^n+1)$ is three consecutive numbers. And that only one is divisible by three.\n$2^n$ has two as a factor and is not divisible by 3. It can't be even. That leaves $(2^n-1)$ and $(2^n+1)$. The one with 3 as a factor seems to be random dependent on n. \nFourth step, tie it all together.\nNow, I'm not sure if I'm suppose to dig deeper into which of them is actually divisible by 3. Or has 3 as a factor. But knowing that one of them at the time (dependent on n) has indeed 3 as a factor implies that, in respect to the second step, 3 is a factor of $(4^n -1)$.  \n\nIs this all there is to it? Keep in mind that this is a really beginners proof task, not very formal. It's slightly funny going back to high school math after being scared to death by logic and descrete math at university-level. I just expect everything thing to be super complex. \n",
    "proof": "Once you have show that $4^n-1=(2^n+1)(2^n-1)$, and one of the two factors is divisible by three, you have got that $3$ divides $4^n-1$.\nIf $2^n-1$ is divisible by three, write it as $3k$ then $4^n-1=3k(2^n+1)$ and therefore it is divisible by three (you should figure out how to write the argument in full).\nThe other case, where $2^n+1$ is the one divisible by three is symmetric, but you should write the details for that case as well, to practice your writing.\n",
    "tags": [
      "logic",
      "proof-writing"
    ],
    "score": 4,
    "answer_score": 10,
    "is_accepted": true,
    "question_id": 242646,
    "answer_id": 242653
  },
  {
    "theorem": "I don&#39;t understand the explanation of Proof by Contradiction",
    "context": "I was given this explanation in my notes to understand Proof by Contradiction:\n\nProof by Contradiction\nWe want to prove that $\\ P(n) \\to Q(n) $ is true. In\na proof by contradiction, we assume by contradiction that $\\ P(n) \\to Q(n) $\nis false, that is, that: $\\ \\neg (P(n) \\to Q(n)) $ is true.\nThe only way this might happen, is if $\\ P(n) $ is true and $\\ Q(n)$ is false. Thus we start with $\\ P(n)$ true and $\\ Q(n)$ false. If from there we deduce a contradiction,\nthat is a statement of the form $\\ C \\wedge \\neg C $, which is always false, what we\nhave proven is :\n$\\ \\neg (P(n) \\to Q(n)) \\to C \\wedge \\neg C$ , is true.\nThis is equivalent to $\\ P(n) \\to Q(n) $. To see that, set $\\ S(n) = \"P(n) \\to Q(n)\"$, and look at the truth table:\n\n\nWhat I don't understand is this line:\n\"$\\ \\neg (P(n) \\to Q(n)) \\to C \\wedge \\neg C$ , is true.\"\nHow is it true if previously stated that\n$\\ \\neg (P(n) \\to Q(n))$ is True\n$\\ C \\wedge \\neg C$ is False (a contradiction)\nBut we know that... $\\ P \\to Q $ is always False?\nHow am I interpreting this explanation wrongly? I am really confused right now... any help/explanation is very much appreciated, thanks!!!\n\nOriginal screenshot (in case I formatted the equations wrongly... I'm new to mathjax/latex thing):\n\n",
    "proof": "You're getting caught up on the word assume. You can make a pretend world by assuming a proposition is true when it's actually false. You're essentially breaking math. So, in this pretend world, ¬(P(n)→Q(n)), but also (C∧¬C). Since math has to be consistent, there was something wrong with our assumption, therefore the negation must be true, hence (P(n)→Q(n)).\n",
    "tags": [
      "discrete-mathematics",
      "proof-verification",
      "logic",
      "proof-writing"
    ],
    "score": 4,
    "answer_score": 11,
    "is_accepted": true,
    "question_id": 3360608,
    "answer_id": 3360685
  },
  {
    "theorem": "Proof by induction or contradiction that $(4k + 3) ^2 - (4k + 3)$ is not divisible by $4$?",
    "context": "I have to prove that $(4k + 3) ^2 - (4k + 3)$ is not divisible by $4$.\nWhat would be the best approach for this, proof by induction or contradiction?\nI've tried both and haven't got very far. Any hints would be appreciated, I'm not looking for a full answer..I wanna try it out myself but I need some help on where to begin.\n",
    "proof": "The polynomial is simple enough that it’s no problem simply to multiply it out:\n$$\\begin{align*}\n(4k+3)^2-(4k+3)&=16k^2+24k+9-4k-3\\\\\n&=16k^2+20k+6\\\\\n&=4\\left(4k^2+5k+1\\right)+2\\;,\n\\end{align*}$$\nwhich is clearly not a multiple of $4$. This is perhaps a little less elegant than njguliyev’s solution, but it works just fine.\n",
    "tags": [
      "discrete-mathematics",
      "proof-writing",
      "induction"
    ],
    "score": 4,
    "answer_score": 12,
    "is_accepted": true,
    "question_id": 510290,
    "answer_id": 510305
  },
  {
    "theorem": "Combinatorial Proof of falling factorial and binomial theorem",
    "context": "\nFor $n,m,k \\in \\mathbb{N}$, prove the equality\n  $$(n+m)^{\\underline{k}}=\\sum^{k}_{i=0}\\binom ki \\cdot n^{\\underline{k-i}} \\cdot m^{\\underline{i}}$$\n  Here, $x^{\\underline{j}}$ denotes a falling factorial, defined by $x^{\\underline{j}} = \\dfrac{x!}{\\left(x-j\\right)!} = x\\left(x-1\\right)\\cdots\\left(x-j+1\\right)$.\n\nI can prove the binomial theorem for itself combinatorically and also the falling factorial version of it, but combined I hit a wall. Any suggestions?\n",
    "proof": "How many ways can you pick $k$ balls from a set of $n$ different red balls and $m$ green different balls?\nAnswer\n$$(n+m)^{\\underline k}$$\nBut you can count them another way. First suppose that the $k$ balls are red, then $k-1$ are red and $1$ is green, etc.\nThis gives\n$$\\sum_{j=0}^k\\binom kj n^{\\underline j} m^{\\underline {k-j}}$$\n",
    "tags": [
      "proof-writing",
      "binomial-coefficients",
      "factorial",
      "binomial-theorem",
      "combinatorial-proofs"
    ],
    "score": 4,
    "answer_score": 8,
    "is_accepted": true,
    "question_id": 1271688,
    "answer_id": 1271700
  },
  {
    "theorem": "Prove that $ 1+2q+3q^2+...+nq^{n-1} = \\frac{1-(n+1)q^n+nq^{n+1}}{(1-q)^2} $",
    "context": "Prove:\n\n$$ 1+2q+3q^2+...+nq^{n-1} = \\frac{1-(n+1)q^n+nq^{n+1}}{(1-q)^2} $$\n\nHypothesis: \n$$ F(x) = 1+2q+3q^2+...+xq^{x-1} = \\frac{1-(x+1)q^x+xq^{x+1}}{(1-q)^2} $$\nProof:\n$$ P1 | F(x) = \\frac{1-(x+1)q^x+xq^{x+1}}{(1-q)^2} + (x+1)q^x = \\frac{1-(x+2)q^{x+1}+xq^{x+2}}{(1-q)^2} $$\n$$ P2 | \\frac{1-(x+1)q^x+xq^{x+1}+[(x+1)(1-q)^2]q^x}{(1-q)^2} = \\frac{1-(x+2)q^{x+1}+xq^{x+2}}{(1-q)^2} $$ \n$$ P3| \\frac{x\\color{red}{q^{x+1}}+[-(x+1)]\\color{red}{q^x}+1+[(x+1)(1-q)^2]\\color{red}{q^x}}{(1-q)^2} = \\frac{x\\color{red}{q^{x+2}}-(x+2)\\color{red}{q^{x+1}}+1}{(1-q)^2} | $$\nHere I just reorganize both sides of the equation, so LHS is explicity an expression with a degree of x+1, while the degree of RHS is x+2. Both LHS' $\\color{red}{q^x}$ are added next.\n$$P4| \\frac{xq^{x+1}+[-(x+1)+(x+1)(<1^2q^0+\\binom{2}{1}1q-1^0q^2>)]q^x+1}{(1-q)^2}=\\frac{xq^{x+2}-(x+2)q^{x+1}+1}{(1-q)^2} $$\n$$P5 | \\frac{xq^{x+1}+[2xq-xq^2+2q-q^2]q^x+1}{(1-q)^2} = \\frac{xq^{x+2}-(x+2)q^{x+1}+1}{(1-q)^2} $$\nI get stuck at this point. I don't know if i'm approaching  the problem the right way. So, any help would be appreciated.\nThanks in advance.\n",
    "proof": "You can also prove by induction. Assume true for $n$, and show for $n+1$.\n$$1+2q+3q^2+\\cdots+nq^{n-1} = \\frac{1-(n+1)q^n+nq^{n+1}}{(1-q)^2}.$$\nThen,\n$$1+2q+3q^2+\\cdots+nq^{n-1}+(n+1)q^n = \\frac{1-(n+1)q^n+nq^{n+1}+(n+1)q^n(1-q)^2}{(1-q)^2}=  \\frac{1-(n+2)q^{n+1}+(n+1)q^{n+2}}{(1-q)^2}.$$\nThus the result holds for $n+1$. Since the result holds for $n=1$, the proof is complete.\n",
    "tags": [
      "sequences-and-series",
      "algebra-precalculus",
      "proof-verification",
      "proof-writing"
    ],
    "score": 4,
    "answer_score": 3,
    "is_accepted": true,
    "question_id": 1825825,
    "answer_id": 1825835
  },
  {
    "theorem": "How to prove that $n^{\\frac{1}{3}}$ is not a polynomial?",
    "context": "I'm reading Barbeau's Polynomials, there's an exercise:\n\nHow to prove that $n^{\\frac{1}{3}}$ is not a polynomial?\n\nI've made this question and with the first answer as an example, I guess I should assume that:\n$$n^{\\frac{1}{3}}=a_pn^p+a_{p-1}n^{p-1}+\\cdots+ a_0n^0$$\nAnd then I should make some kind of operation in both sides, the resultant difference would be the proof. But I have no idea on what operation I should do in order to prove that. \n",
    "proof": "Cube both sides, then consider whether the difference could be identically $0$.\n",
    "tags": [
      "proof-writing"
    ],
    "score": 4,
    "answer_score": 6,
    "is_accepted": true,
    "question_id": 247800,
    "answer_id": 247804
  },
  {
    "theorem": "Within If and Only If Proofs, why can the proof for one direction be easier than the other?",
    "context": "For $ P \\iff Q$, my initial sentiment is that because P and Q are equivalent, the total of two proofs (one for each direction) should entail the equivalent level of \"difficulty\" or \"exertion\", as well as an analogous number of steps. Otherwise, they wouldn't be equivalent.\nYet, this is false and I can't perceive why? \nMoreover, a priori, how would one divine/previse:\n$2.$ if one direction is truly easier than the other  ?   \n$3.$ then which of the two is it, on the condition that $2$ is true?\n",
    "proof": "I don't think there is a reasonable general answer to this very genuine phenomenon: that's just the way the (mathematical) world is.\nIn a related area, most of modern cryptography (and thus of the modern economy: think banking) relies on the fact that it is very easy to multiply two huge prime numbers $p, q$ but extremely difficult, given just their product $n=p.q$, to retrieve the two factors $p,q$.\n",
    "tags": [
      "proof-writing",
      "intuition"
    ],
    "score": 4,
    "answer_score": 8,
    "is_accepted": false,
    "question_id": 701450,
    "answer_id": 701478
  },
  {
    "theorem": "Proving that if $a+b$ and $ab$ are of the same parity, then $a$ and $b$ are even.",
    "context": "Here is the proof:\n\nLet $a,b \\in \\mathbb{Z}$. Prove that if $a+b$ and $ab$ are of the same parity, then $a$ and $b$ are even.\n\nWhen working these problems, I do try to set them up logically. My scratch work usually looks something like this:\n\n$P: a+b$ and $ab$ are the same parity\n$Q: a$ and $b$ are even\n$P \\Rightarrow Q:$ If $a+b$ and $ab$ are the same parity, then $a$ and $b$ are even.\n$Q \\Rightarrow P$: If $a$ and $b$ are even, then $a+b$ are the same parity.\n\nFor some reason, I am getting caught up on writing the contrapositive of this statement when addressing the forwards ($P \\Rightarrow Q$) direction of this biconditional.\nHere is what I was thinking:\n\nIf $a$ or $b$ is not even (odd), then $a+b$ or $ab$ are of different parity.\n\nI believe I am negating too much, and this is where I struggle: what would the proper negation look like? How do I know what to negate (or switch) vs. what I should leave alone? I generally get that \"and\" becomes an \"or\", but then I feel like I start to negate too much (if that makes sense...).\nAny advice? I can usually get this far pretty easily, but I wanted to use contrapositive for $P \\Rightarrow Q$, and I don't believe I am writing the statement correctly. In addition to verifying this, I would appreciate any advice when it comes to what portion of my statement I should negate. Thanks!\n",
    "proof": "The contrapositive of the statement\n\nIf $\\overbrace{\\text{$ab$ and $a+b$ have the same parity}}^{\\large P}$, then $\\overbrace{\\text{$a$ is even and $b$ is even}}^{\\large Q}$.\n\nis\n\nIf $\\overbrace{\\text{$a$ is odd or $b$ is odd}}^{\\large\\lnot Q}$, then $\\overbrace{\\text{$ab$ and $a+b$ have different parities}}^{\\large\\lnot P}$.\n\nNote that $Q$ is the conjunction $S\\land T$, where $S$ means \"$a$ is even\" and $T$ means \"$b$ is even\". Thus, $\\lnot Q$ is the disjunction $\\lnot S\\lor\\lnot T$.\n\nHere is an alternate approach:\n$\\begin{array}{cr}\n\\text{$ab$ and $a+b$ have the same parity}\\\\\n\\Updownarrow&\\qquad\\text{(1)}\\\\\n\\text{$ab-(a+b)$ is even}\\\\\n\\Updownarrow&\\qquad\\text{(2)}\\\\\n\\text{$(a-1)(b-1)$ is odd}\\\\\n\\Updownarrow&\\qquad\\text{(3)}\\\\\n\\text{$a-1$ is odd and $b-1$ is odd}\\\\\n\\Updownarrow&\\qquad\\text{(4)}\\\\\n\\text{$a$ is even and $b$ is even}\\\\\n\\end{array}$\nComments:\n$(1)$: $m$ and $n$ have the same parity if and only if $m-n$ is even\n$(2)$: $ab-(a+b)=(a-1)(b-1)-1$\n$(3)$: $mn$ is odd if and only if both $m$ and $n$ are odd\n$(4)$: $a-1$ has the opposite parity from $a$\n",
    "tags": [
      "discrete-mathematics",
      "logic",
      "proof-writing"
    ],
    "score": 4,
    "answer_score": 14,
    "is_accepted": true,
    "question_id": 1376320,
    "answer_id": 1376351
  },
  {
    "theorem": "In a proof that is reliant on proven theorems, does one assume the reader&#39;s familiarity with said theorems, or explicitly include their logic?",
    "context": "In composing a proof that is reliant on proven theorems, does one simply assume the reader's familiarity with said theorems, or does one explicitly include their logic in the new logic? \n",
    "proof": "It depends on the context. My masters thesis, for example, began with saying that I expect the reader to be familiar with forcing. I am not expecting the reader to be familiar with other topics which are relatively common, though. These topics are fully explained in my thesis.\nWhen writing something one can usually foresee who is going to read the text, and what the readers are expecting to see written.\nOver-detailed writing is very hard to read; but under-detailed writing is very hard to understand. It takes time and experience to find the balance. Consult an advisor or referee regarding your actual text.\nThis may also be affected by where you are sending the text. When writing a very short article (say 3-4 pages) it's fine to just quote results, but when writing a book it's expected to prove them as well.\n",
    "tags": [
      "soft-question",
      "proof-writing"
    ],
    "score": 4,
    "answer_score": 8,
    "is_accepted": true,
    "question_id": 268308,
    "answer_id": 268311
  },
  {
    "theorem": "Understanding the proof $C(X)$ is complete",
    "context": "\nProve $(C_X,||.||)$ ,where $||.||$ is the maximum norm and X is compact, is complete.\n\nThe following proof was given. It is the one I am striving to understand:\n\nLet $(f_n)$ be a Cauchy sequence: $\\forall\\epsilon>0\\exists N\\in\\mathbb{N}:n,m\\geqslant N\\implies ||f_m-f_n||<\\epsilon$\n$\\forall t\\in X$ \n$0\\leqslant |f_n(t)-f_m(t)|\\leqslant \\max_{x\\in X}|f_n(x)-f_m(x)|\\to 0$ as $m,n\\to\\infty$\n$\\forall t\\in X\\:, (f_n(t))_{n\\in\\mathbb{N}}$ is a Cauchy sequence in $\\mathbb{R}$.\nThen $(f_n)_n\\to f$ uniformly then $f$ is continuous.\n\nThis is how the proof was handed to me. I think I can fill the gaps but I would need someone to back me on that.\nSo first the author considers a Cauchy sequence and assumes it converges in $C(X)$\nThen it arrives to the following inequality:\n$0\\leqslant |f_n(t)-f_m(t)|\\leqslant \\max_{x\\in X}|f_n(x)-f_m(x)|\\to 0$ as $m,n\\to\\infty$ since the it assumed $\\max_{x\\in X}|f_n(x)-f_m(x)|\\to 0$ then $|f_n(t)-f_m(t)|\\to 0$ \nSo the convergence in $C(X)$ verifies that the same Cauchy sequence converges in $\\mathbb{R}$ that is by assumption complete with the usual topology.\nSince $X$ is compact then $f_n$ converges uniformly in $\\mathbb{R}$ and it converges to a continuous function. Therefore it converges in $C(X)$ proving the latter is complete.\nQuestion:\nIs this the reasoning behind the proof?\nThanks in advance!\n",
    "proof": "Let $X$ be a compact space. Let $C_{X}$ be the space of functions mapping from $X$ to $\\mathbb{R}$.\nFor any function $f$ in $C_{X}$, define\n$$\n\\left\\Vert f\\right\\Vert \\equiv\\max_{x}\\left|f(x)\\right|.\n$$\nThis is called the maximum norm.\nWe would like to prove the following:\nTheorem. $(C_{X},\\Vert\\cdot\\Vert)$ is complete.\nDefinition. A space $(Y, |\\cdot|)$ is complete if any sequence $(y_n)_n$ of points in $Y$ which is Cauchy with respect to the norm $|\\cdot|$ converges to some point $y$ in $Y$.\nRemark. Don't be confused by the terminology \"points\" here.\nSince $C_{X}$ is a space of functions, its points are functions.\nProof. Start with an arbitrary Cauchy sequence $(f_{n})_{n}$ in $C_{X}$.\nLet $t$ be an arbitrary point in $X$.\nNote that\n$$\n\\left|f_{n}(t)-f_{m}(t)\\right|\\leq\\max_{t}\\left|f_{n}(t)-f_{m}(t)\\right|=\\left\\Vert f_{n}-f_{m}\\right\\Vert .\\tag{1}\n$$\nLet $\\epsilon$ be a positive constant and pick $N$ such that $\\Vert f_{n}-f_{m}\\Vert<\\epsilon$ for all $n,m\\geq N$.\nWe can do this because we assumed the sequence $(f_{n})_{n}$ was Cauchy. \nBy the above inequality, it follows that $|f_{n}(t)-f_{m}(t)|$ is also strictly less than $\\epsilon$.\nTherefore, the sequence $(f_{n}(t))_{n}$ is also Cauchy.\nRemark. Note that the sequences $(f_{n})_{n}$ and $(f_{n}(t))_{n}$ are different!\nOne is a sequence of functions and one is a sequence of numbers in $\\mathbb{R}$.\nNow, since $\\mathbb{R}$ is complete, it follows that $(f_{n}(t))_{n}$ converges to some real number.\nLet's call that real number $f(t)$.\nSince $t$ was arbitrary, we have essentially defined a new function, $f:X\\rightarrow\\mathbb{R}$.\nLastly, let's make sure that $f_{n}$ converges to this\nnew function $f$ with respect to the maximum norm.\nTaking limits with respect to $m$ in the inequality (1),\n$$\n\\left|f_{n}(t)-f(t)\\right|=\\lim_{m}\\left|f_{n}(t)-f_{m}(t)\\right|\\leq\\lim_{m}\\left\\Vert f_{n}-f_{m}\\right\\Vert =\\left\\Vert f_{n}-f\\right\\Vert .\n$$\nIn the above, we have used the fact that limits and continuous functions commute and norms are continuous.\nThis implies that $f$ is continuous since if a sequence of continuous functions converge \"uniformly\" (i.e., with respect to the maximum norm) to some function, that function must be continuous.\n",
    "tags": [
      "real-analysis",
      "functional-analysis",
      "analysis",
      "proof-writing",
      "proof-explanation"
    ],
    "score": 4,
    "answer_score": 10,
    "is_accepted": true,
    "question_id": 3114445,
    "answer_id": 3114471
  },
  {
    "theorem": "How do I prove or disprove this prime number conjecture?",
    "context": "How would I go about proving something like this? \nProve or disprove: If $p$ and $q$ are prime numbers for which $\\ p < q$, then $\\ 2p+q^2$ is odd. \nI'm assuming its definitely true because and even $+$ odd is always odd, and odd $\\times$ odd is always odd too. So I know it's true, but how do I prove it? \n",
    "proof": "Hint Since $p <q$ are prime numbers, then $q \\geq 3$ and hence $q$ is odd.\nSince $2p$ is even and $q$ is odd, $2p+q^2$ is ....\n",
    "tags": [
      "discrete-mathematics",
      "proof-writing",
      "prime-numbers"
    ],
    "score": 4,
    "answer_score": 14,
    "is_accepted": true,
    "question_id": 2331583,
    "answer_id": 2331584
  },
  {
    "theorem": "How to show that the set of all $(x,y)$ such that $3x^2 + 2y^2&lt;6$ is an open set.",
    "context": "How can it be proved that the set of all $(x,y)$ such that $3x^2 + 2y^2<6$ is an open set?\nI tried to prove directly the aforementioned statement. Without success I tried to prove that the image of a linear mapping applied in a open set is also an open set. I do not know if this last part is true, but I could not prove it. Can someone help me? Thank you.\n",
    "proof": "$f(x,y)=3x^2+2y^2$ is a continuous function from $R^2$ to $R$ and $(-\\infty,6)$ is an open subset of $R$, so $f^{-1}(-\\infty,6)$ is an open subset of $R^2$.\n",
    "tags": [
      "elementary-set-theory",
      "proof-verification",
      "proof-writing",
      "proof-explanation",
      "alternative-proof"
    ],
    "score": 4,
    "answer_score": 10,
    "is_accepted": false,
    "question_id": 1847065,
    "answer_id": 1847070
  },
  {
    "theorem": "If $A$ and $B$ are symmetric matrices, so is $A+B$",
    "context": "This question looks pretty easy and hard at the same time. Here's how it goes:\nLet A and B be symmmetric nxn matrices. Show that A + B is also symmetric.\nTo me this sounds like a pretty obvious fact, well, if its not obvious I still have a good idea of why this is true, but I'm new to university level maths and I don't really know how I would go about writing a formal proof for this. Any help would be much appreciated. =D\n",
    "proof": "This is how I would write a proof of this statement:\nLet $A=(a_{ij})_{i,j=1}^n$,$B=(b_{ij})_{i,j=1}^n$ be symmetric matrices, then it holds that $a_{ij}=a_{ji}$ and correspondingly for $B$. Then consider the sum $C=A+B$, then $c_{ij}=a_{ij}+b_{ij}$, and $c_{ji}=a_{ji}+b_{ji}$. Then since $A$, $B$ are both symmetric $a_{ji}+b_{ji}=a_{ij}+b_{ij}$ and thus $c_{ji}=c_{ij}$ and therefore $C$ must be symmetric.\n",
    "tags": [
      "linear-algebra",
      "proof-writing"
    ],
    "score": 4,
    "answer_score": 8,
    "is_accepted": false,
    "question_id": 921276,
    "answer_id": 921280
  },
  {
    "theorem": "Proving that $(A \\cup B) \\cap \\overline{(A \\cap B)} = (A \\cap \\overline{B}) \\cup (\\overline{A} \\cap B) $",
    "context": "I'm trying to teach myself set-theory. I have been unable to prove algebraically that:\n$(A \\cup B) \\cap \\overline{(A \\cap B)} = (A \\cap \\overline{B}) \\cup (\\overline{A} \\cap B) $\nI know it's elementary but I just want to see an example. (The book doesn't provide one.)\nCorrect me if I'm wrong...the first step is to use De Morgan's law to expand the left side. i.e. \n$ (A \\cup B) \\cap  \\overline{(A \\cap B)} = (A \\cup B) \\cap \\overline{A} \\cup \\overline{B}$\n$ (A \\cup B) \\cap  \\overline{(A \\cap B)} = A \\cup (B \\cap \\overline{A}) \\cup \\overline{B} $, by associative property.\n$ (A \\cup B) \\cap  \\overline{(A \\cap B)} = (A \\cup \\overline{A}) \\cap (B \\cup \\overline{B} )$, by commutative property.\nafter that I don't know what to do. \nI don't know whether I should rewrite these or if different notation would help. i.e. should I expand both sides to use the notations/definition of set-union? i.e. $ A \\cup B = \\{x \\mid x \\in A \\lor x \\in B \\}$\n",
    "proof": "$$(A \\cup B) \\cap \\overline{(A \\cap B)} = (A \\cap \\overline{B}) \\cup (\\overline{A} \\cap B)\\tag{1}$$\n\nStep (1) $ (A \\cup B) \\cap  \\overline{(A \\cap B)} = (A \\cup B) \\cap \\overline{A} \\cup \\overline{B}$\n\nYes, correct, but you should keep parentheses: $ (A \\cup B) \\cap  \\overline{(A \\cap B)} = (A \\cup B) \\cap (\\overline{A} \\cup \\overline{B})$\n\nStep (2) Not correct: $ (A \\cup B) \\cap  \\overline{(A \\cap B)}$ = $A \\cup (B \\cap \\overline{A}) \\cup \\overline{B} $, by associative property.\n\nNo: the associative property (like the commutative property) applies to a chain of unions, or a chain of intersections, but not a mixed chains of unions and intersections. This is why parentheses in the first step are needed. You need to use distribution: \n$ (A \\cup B) \\cap  \\overline{(A \\cap B)}$ = $(A \\cup B) \\cap (\\overline{A} \\cup \\overline{B}) = [(A\\cup B) \\cap \\overline{A}] \\cup [(A \\cup B) \\cap \\overline{B}]$\nCan you take it from here? You can use distributivity again...then use that fact that $A \\cap \\overline{A} = \\varnothing$. Likewise for $B\\cap \\overline{B} = \\varnothing$. Simplify.\n\nAnother approach is to unpack what proving your equality $(1)$ requires: \nIn general, we prove that for two sets $P, Q$, $$P = Q \\iff P \\subseteq Q \\text{ AND}\\;\\;Q\\subseteq P$$\nFor your equality $(1)$, that means you can prove the equality by proving: \n$$[(A \\cup B) \\cap \\overline{(A \\cap B)}] \\subseteq [(A \\cap \\overline{B}) \\cup (\\overline{A} \\cap B)]\\tag{2}$$\nand by proving $$[(A \\cap \\overline{B}) \\cup (\\overline{A} \\cap B)] \\subseteq [(A \\cup B) \\cap \\overline{(A \\cap B)}]\\tag{3}$$\nUnpack what this means in terms of \"chasing elements\", for which I'll give you a start: \n$(2)$ If $x \\in [(A \\cup B)\\cap \\overline{(A \\cap B)}]$ then $x \\in (A\\cup B)$ AND $x \\notin (A \\cap B)$, which means $(x \\in A$ OR $x \\in B)$ AND $(x \\notin A$ OR $x \\notin B)$...\n$(3)$ If $x \\in [(A\\cap \\overline{B})\\cup (\\overline{A} \\cap B)]$, then ...$(x \\in A \\cap \\overline{B})$ OR $(x \\in \\overline{A} \\cap B)$ ....\n",
    "tags": [
      "elementary-set-theory",
      "proof-writing"
    ],
    "score": 4,
    "answer_score": 8,
    "is_accepted": true,
    "question_id": 266199,
    "answer_id": 266205
  },
  {
    "theorem": "Structure of a proof by contradiction",
    "context": "I know this is a trivial question, but it has been bothering me for a while.\nMy textbook says \"Proof of contradiction exploits the fact that the statement \"if A holds, then B holds\" is equivalent to the statement \"if B does not hold, then A does not hold\". So far, okay. But how can we put the proof of $\\sqrt2$ being an irrational number into this framework?\nEvery relevant proof that I've seen derives a contradiction by assuming that $\\sqrt2$ is a rational number. In this case, what is the statement B? Is it \"$\\sqrt2$ is a irrational number\"? Then, what is the statement A?\n",
    "proof": "\nProve by contradiction: $\\sqrt2$ is irrational.\n\n\nStatement $A$ (the assumption that we want to falsify):\n $\\sqrt2$ is rational.\nStatement $P_1:$\n There exists coprime positive integers $m$ and $n$ such that $\\sqrt2=\\frac mn.$\nStatement $P_n:$\n There does not exist coprime positive integers $m$ and $n$ such that $\\sqrt2=\\frac mn.$\n\nThe proof-by-contradiction argument goes:\n\nWe assume that $\\mathbf A$ is true.\nConsequently, $P_1$ is true.\nConsequently, $P_2$ is true.\n$\\ldots$\nConsequently, $P_n$ is true.\nBut $P_n$ negates $P_1.$ As statements $P_1$ and $P_n$ conflict with each other, we have obtained the contradiction $\\mathbf{P_1\\land P_n}.$\nTherefore, $\\mathbf A$ must have been a false assumption.\n\n\nIn a proof by contradiction, it is sufficient to derive any contradiction.\nIn the above example, we derived two separate consequences that conflict with each other;\nin another example, we might derive a statement that the angles $\\alpha,\\beta,\\gamma$ of a flat-plane triangle add up to greater than $180^{\\circ},$ which conflicts with a well-known theorem.\n\nAny proof by contrapositive can be recast as a proof by contradiction; however, the converse is not true.\nYour book is actually describing a special case of Proof by Contradiction—proving a conditional statement using Proof by Contrapositive but under the guise of Proof by Contradiction—and is therefore being misleading and confusing.\n\nProving $\\forall x{\\in}S\\,\\big(P(x)\\Rightarrow Q(x)\\big)$ by Contradiction\n\n\n",
    "tags": [
      "proof-writing"
    ],
    "score": 4,
    "answer_score": 5,
    "is_accepted": false,
    "question_id": 4200952,
    "answer_id": 4200978
  },
  {
    "theorem": "Clean and clever proofs to show every homomorphism of two groups with coprime orders is trivial?",
    "context": "I'm practicing for a qualifying exam in algebra (the freebie attempt we get the week before my first semester of grad school). I don't have a lengthy or deep background with math so I'm especially interested in learning to write cleaner and more clever proofs or proofs that use different methods. Does anyone have feed back on my proof of the titular question or perhaps an alternative proof?\n\nuse the fact that $G$ and $H$ don’t have any subgroups of the same order besides the subgroup which is just the identity\nthen use that the kernel of a homomorphism is a normal subgroup and the kernel defines the homormorphism\nBut $G/\\ker(f)$ is isomorphic to H. Since the order of $G/\\ker(f)$ is a number composed of the product of primes that divide the order of G\nbecause the order of $G$ is coprime to the order of $H$ we know $|G/\\ker(f)| = 1$ and that therefore $f$ maps all of $G$ into the identity of $H$ hence $f$ is a trivial homomorphism\n\n",
    "proof": "Say $|G|=n$ and $|K|=m$, with $\\gcd(m,n)=1$. Find $r,s\\in\\mathbb{Z}$ such that $rm+sn=1$. Note that if $g\\in G$ and $k\\in K$, then $g^n=e$, $k^m=e$.\nLet $f\\colon G\\to K$ be a morphism, and let $g\\in G$. Then $g=g^{rm+sn} = g^{rm}(g^n)^s=g^{rm}$. So\n$$f(g) = f(g^{rm}) = f(g)^{rm} = (f(g)^m)^r = e^r = e.$$\nThus, $f$ is the trivial map.\n\nFeedback: your assertion in (3) that $G/\\mathrm{ker}(f)$ is $H$ is incorrect. You can say that $G/\\mathrm{ker}(f)$ is isomorphic to its image in $H$, but since you are not assuming $f$ is onto, you can’t assert it is isomorphic to $H$, just to a subgroup of $H$.\nThe rest of the argument is too convoluted: note that $|G/\\mathrm{ker}(f)|$ divides both $|G|$ (since it is equal to $|G|/|\\mathrm{ker}(f)|$, and $|H|$ (because it is isomorphic to a subgroup of $H$), hence divides their gcd, which is $1$.\n",
    "tags": [
      "abstract-algebra",
      "group-theory",
      "proof-writing",
      "finite-groups",
      "alternative-proof"
    ],
    "score": 4,
    "answer_score": 7,
    "is_accepted": true,
    "question_id": 3497959,
    "answer_id": 3497965
  },
  {
    "theorem": "Proof of $\\sin^2 x+\\cos^2 x=1$ using Euler&#39;s Formula",
    "context": "How would you prove $\\sin^2x + \\cos^2x = 1$ using Euler's formula?\n$$e^{ix} = \\cos(x) + i\\sin(x)$$\nThis is what I have so far:\n$$\\sin(x) = \\frac{1}{2i}(e^{ix}-e^{-ix})$$\n$$\\cos(x) = \\frac{1}{2} (e^{ix}+e^{-ix})$$\n",
    "proof": "Multiply $\\mathrm e^{\\mathrm ix}=\\cos(x)+\\mathrm i\\sin(x)$ by the conjugate identity $\\overline{\\mathrm e^{\\mathrm ix}}=\\cos(x)-\\mathrm i\\sin(x)$ and use that $\\overline{\\mathrm e^{\\mathrm ix}}=\\mathrm e^{-\\mathrm ix}$ hence $\\mathrm e^{\\mathrm ix}\\cdot\\overline{\\mathrm e^{\\mathrm ix}}=\\mathrm e^{\\mathrm ix-\\mathrm ix}=1$.\n",
    "tags": [
      "trigonometry",
      "complex-numbers",
      "proof-writing"
    ],
    "score": 4,
    "answer_score": 9,
    "is_accepted": true,
    "question_id": 316936,
    "answer_id": 316942
  },
  {
    "theorem": "How can I prove the existence of multiplicative inverses for the complex number system",
    "context": "If the complex number system is defined to be the set $\\mathbb C = \\mathbb R \\times \\mathbb R$ (called the complex numbers) together with the two functions from $\\mathbb C \\times \\mathbb C $ into $\\mathbb C$, denoted by $+$ and $\\times$ (multiplication), that are given by $(a,b) + (c,d) = (a+c, b+d)$ and $(a,b)\\times(c,d) = (ac-bd, ad+bc)$ for all $a,b,c,d\\in\\mathbb R$.\nIf I am trying to prove the existence of the multiplicative inverses for $\\mathbb C$, do I need to begin by laying out that $$(ac-bd, ad+bc)^{-1} \\times (ac-bd, ad+bc) = 1? $$\n",
    "proof": "You want to prove a statement of the form: \"For any $X$ there exists a $Y$ such that $P(X,Y)$ holds.\"\nA typical proof for such a statement starts as follows: \"Let $X$ be arbitrary. Then define $Y$ to be ...\". Then you only have to show that $P(X,Y)$ does indeed hold for the $Y$ you defined.\nHence, in your case you start: \"Let $(a,b) \\in \\mathbb{R} \\times \\mathbb{R}$  be an arbitrary complex number such that at least one of $a$ and $b$ is not zero. Then, we define the complex number $(c,d) \\in \\mathbb{R} \\times \\mathbb{R}$ by setting $c := \\dots$ and $d := \\dots$.\"\nThen, you conclude the proof by showing that you have $(a,b)*(c,d)=(1,0)$.\n",
    "tags": [
      "real-analysis",
      "analysis",
      "complex-numbers",
      "proof-writing",
      "inverse"
    ],
    "score": 4,
    "answer_score": 9,
    "is_accepted": true,
    "question_id": 4964765,
    "answer_id": 4964772
  },
  {
    "theorem": "Is this a valid proof that if 5 investors split a payout of \\$2,000,000, then at least 1 investor receives at least $400,000?",
    "context": "Question from Coursera's Introduction to Mathematical Thinking: \n\nProve that if 5 investors split a payout of \\$2,000,000 at least 1 investor receives at least $400,000.\n\nMy proof : \nlet i : investor\nlet e : equal amount\nlet p : payout\nlet s : split  \nSet of 5 investors : $s : \\{i1,i2,i3,i4,i5\\}$\np(e) : 2M / 5 = $400'000\nAs each investor receives an equal payment then at least 1 investor receives 400'000, in other words : $\\forall(s)[e(i)] => p(400'000)$\nIs this a valid proof?\n",
    "proof": "If each investor gets less than $\\$400,000$, then the sum of their payments is less than $\\$2,000,000$, a contradiction; thus at least one investor gets at least $\\$400,000$.\n",
    "tags": [
      "proof-verification",
      "proof-writing",
      "proof-explanation"
    ],
    "score": 4,
    "answer_score": 8,
    "is_accepted": true,
    "question_id": 2148450,
    "answer_id": 2148461
  },
  {
    "theorem": "Prove by induction that numbers of Fibonacci of the form $F_{3n}$ are even",
    "context": "I am not quite asking to solve this problem, but I am asking what they are asking me. This is the problem:\n\nThe Fibonacci numbers $F_n$ for $n \\in \\mathbb{N}$ are defined by $F_0\n = 0$, $F_1 = 1$, and $F_n = F_{n−2} + F_{n−1}$ for $n \\geq 2$. Prove (by induction) that the numbers $F_{3n}$ are even for any $n \\in\n \\mathbb{N}$.\n\nWe all know what the Fibonacci numbers are, and I also know in general how proofs by induction work: assume for $n$ case, prove by $n + 1$ case. Very nice!\nMy problem is: what the heck are they asking me? What do you think they mean by $F_{3n}$? Are they maybe asking us to prove for the cases $3n$? For example, instead of for $n=1$ we prove for $n=3$, instead of $2$ we have $6$, and so on?\nSo, if my reasonings are correct, for example, the base case would be $n = 0$ and $n = 1$ as usual, but we then use the new formula $F_{3n}$. \nThus we would have to show $F_{3*0}=F_{0} = 0$ and $F_{3*1} = F_{3} = (F_{2} = 1 + F_{1} = 1) = 2$.\nWell, we have show that they are even, so they are of the form $F_n = 2k$.\nWhat exactly am I suppose to do?\n",
    "proof": "A proof by induction could go more or less like this.\n$F_1=1$, $F_2=1$, and $F_3=2$. So, $F_{3\\cdot1}=2$ is even. \nAssume that $F_{3n}$ is even.\nNow, $F_{3n+3}=F_{3n+2}+F_{3n+1}=F_{3n+1}+F_{3n}+F_{3n+1}=2F_{3n+1}+F_{3n}$.\nSince $F_{3n}$ is even and $2F_{3n+1}$ is also even we get $F_{3(n+1)}=F_{3n+3}$ is even because it is the sum of these two even numbers.\n",
    "tags": [
      "elementary-number-theory",
      "proof-writing",
      "induction",
      "fibonacci-numbers",
      "parity"
    ],
    "score": 4,
    "answer_score": 16,
    "is_accepted": true,
    "question_id": 1209336,
    "answer_id": 1209338
  },
  {
    "theorem": "What is the relation A = B = C called in a proof?",
    "context": "When writing a proof if I have the relationship\n$$\nA = B = C\n$$\nAnd I want to use that to prove\n$$\nA = C\n$$\nI remember there being some term for it. What is that term, and what would be an appropriate intertext in this situation?\n",
    "proof": "What you're looking for is the transitive property of equality.\nhttp://www.mathwords.com/t/transitive_property.htm\n$$\n\\text{Given}\\\\\nA = B\\\\\nB = C\\\\\n\\text{Equality is transitive}\\\\\n\\therefore A = C\n$$\n",
    "tags": [
      "proof-writing"
    ],
    "score": 4,
    "answer_score": 9,
    "is_accepted": true,
    "question_id": 831258,
    "answer_id": 831262
  },
  {
    "theorem": "Prove that any group $G$ of order $p^2$ is abelian, where $p$ is a prime number",
    "context": "\nPossible Duplicate:\nShowing non-cyclic group with $p^2$ elements is Abelian \n\n\"Let $p$ be a prime number. Prove that any group $G$ of order $p^2$ is abelian. You may assume the fact that the centre of a $p$-group is non-trivial\".\nI understand from the question is that the group $G$ is a $p$-group, with $p^2$ number of elements in it (meaning it is finite). Also, the centre, i.e the set of elements where $g_1g_2 = g_2g_1$ for all $g_1, g_2 \\in G$, is non-trivial, meaning $\\neq e$.\nSo, the non-trivial bit shows that the center, $Z$, either generates the group, so it's cyclic and therefore abelian by definition. Or, it generates a subgroup of order p, and so $Z$ with some elements missing from group $G$ generate G, but as these groups commute (because they have a centre of $Z$), we can say the group is abelian.\nIs this correct? \n",
    "proof": "The center of a p-group can't be trivial. If $Z(G)=p^2$ then we are done. If $Z(G)=p$ then $G/Z(G)$ has order p and is cyclic. Thus, G is abelian using the G/Z theorem.\n",
    "tags": [
      "abstract-algebra",
      "group-theory",
      "proof-writing"
    ],
    "score": 4,
    "answer_score": 10,
    "is_accepted": true,
    "question_id": 253922,
    "answer_id": 253926
  },
  {
    "theorem": "Prove by delta/epsilon that $\\lim_{x \\to \\infty}{\\frac{x}{1+x^2}} = 0$",
    "context": "Here's what we tried:\nFor every $\\epsilon > 0$ there is a large number $K$ such that $|f(x)| < \\epsilon$ when $x>K$.\nKnowing that $K$ is a large positive number, take the positive absolute value of $f(x)$:\n$$\\displaystyle \\frac{x}{1+x^2} < \\epsilon$$\nSolve for\n$$x > \\displaystyle \\sqrt{\\frac{x - \\epsilon}{\\epsilon}}$$\nAnd thus $$K = \\displaystyle \\sqrt{\\frac{x - \\epsilon}{\\epsilon}}$$.\nIs it acceptable to have $K$ in terms of $x$?\n",
    "proof": "No it is not. Recall the definition of $f$ having limit $0$ at $+\\infty$:\n$$\r\n\\forall\\varepsilon>0,\\quad\\exists K_\\varepsilon\\in\\mathbb R,\\quad\\forall x\\in\\mathbb R,\\quad x\\geqslant K_\\varepsilon\\implies |f(x)|\\leqslant\\varepsilon.\r\n$$\nSince $\\varepsilon$ is the only object mentioned before the introduction of $K_\\varepsilon$ by $[\\exists K_\\varepsilon\\in\\mathbb R]$, $K_\\varepsilon$ depends on $\\varepsilon$ and on $\\varepsilon$ only.\nIn your case, $K_\\varepsilon=1/\\varepsilon$ is fine.\n",
    "tags": [
      "calculus",
      "limits",
      "proof-writing"
    ],
    "score": 4,
    "answer_score": 6,
    "is_accepted": false,
    "question_id": 72549,
    "answer_id": 72552
  },
  {
    "theorem": "Definition of Inverse in Linear and Abstract Algebra",
    "context": "In a linear algebra text, the following is the definition of the inverse of a matrix\n\nAn $n\\times n$ matrix $A$ is invertible when there exists an $n \\times n$ matrix $B$ such that $$AB = BA = I_n$$\n\nAnd likewise in an abstract algebra textbook, the definition of the inverse of a group is\n\nGiven that $G$ is a group with operation $*$, for each $a \\in G$, there exists an element $a^{-1}$ such that $$a*a^{-1} = a^{-1}*a = e,$$ where $e$ is the identity element in $G$. Such element is called the inverse of $a$ in $G$.\n\nUnfortunately, the second semester of abstract algebra didn't quite finalize due to low enrollment, so I'm doing independent self-study of topics I missed in Linear Algebra (as mind preparation). Here's my question:\nIs it sufficient to show that $AB = I_n \\;\\;\\implies \\;\\;B = A^{-1}$ and $A = B^{-1}$? Or must you check both that $AB = I_n$ and $BA = I_n$ to completely conclude that $A = B^{-1}$ and $B = A^{-1}$?\nI remember on an exam, I had to prove that for a group homomorphism $\\phi: G\\to H$, for any $a \\in G$, $\\phi(a^{-1}) = [\\phi(a)]^{-1}$ which I proved by asking the reader to observe that $\\phi(a)\\phi(a^{-1}) = \\phi(aa^{-1}) = \\phi(e_G) = e_H$ which because $\\phi(a)\\phi(a^{-1}) = e_H$, this can only mean that $\\phi(a)^{-1} = \\phi(a^{-1})$ by definition of inverse. And I got full points for it, but it leaves me wondering: am I supposed to check both arrangements to create the strongest possible argument?\n",
    "proof": "If you already know that $G$ is a group, then to prove that $a$ and $b$ are inverses, it is enough to check $ab = e$.\nIf $G$ is not a group or you don't yet know that $G$ is a group (for example, if you are trying to prove that $G$ is a group), then it is not enough to show $ab =e$. You also need to show that $ba = e$.\nFor matrices over a field, $AB = I$ automatically implies that $BA = I$ if $A$ and $B$ are square. Otherwise, if $A$ is $m \\times n$ and $B$ is $n \\times m$, where $m < n$, then $BA = I$ is impossible. This can be shown by an argument using ranks.\n",
    "tags": [
      "linear-algebra",
      "abstract-algebra",
      "proof-writing",
      "inverse"
    ],
    "score": 4,
    "answer_score": 6,
    "is_accepted": true,
    "question_id": 1645532,
    "answer_id": 1645596
  },
  {
    "theorem": "Proof of divisibility using modular arithmetic: $5\\mid 6^n - 5n + 4$",
    "context": "Prove that: \n$$6^n - 5n + 4 \\space \\text{is divisible by 5 for} \\space n\\ge1$$\nUsing Modular arithmetic. Please do not refer to other SE questions, there was one already posted but it was using induction,  I want to use this number theory method.\nObviously we have to take $\\pmod 5$\nSo:\n$$6^n - 5n + 4 \\equiv x \\pmod 5$$\nAll we need to do prove is prove $x = 0$\nHow do we do that? I just need a hint, I am not sure how to solve congruences. Some ideas will be helpful. \nThanks!\n",
    "proof": "Hint:-\n$6\\equiv1 \\pmod 5\\implies 6^n\\equiv1\\pmod 5\\tag{1}$\n$-5(n-1)\\equiv 0\\pmod 5\\tag{2}$\nSolution:-\n\n$(1)+(2)$ gives,$$6^n-5n+4\\equiv0\\pmod 5$$\n\n",
    "tags": [
      "elementary-number-theory",
      "proof-writing",
      "modular-arithmetic",
      "divisibility"
    ],
    "score": 4,
    "answer_score": 7,
    "is_accepted": true,
    "question_id": 1089430,
    "answer_id": 1089436
  },
  {
    "theorem": "Prove $\\lim _{x \\to 0} \\sin(\\frac{1}{x}) \\ne 0$",
    "context": "Prove $$\\lim _{x \\to 0} \\sin\\left(\\frac{1}{x}\\right) \\ne 0.$$\nI am unsure of how to prove this problem. I will ask questions if I have doubt on the proof. Thank you!\n",
    "proof": "HINT\nConsider the sequences $$x_n = \\dfrac1{2n \\pi + \\pi/2}$$ and $$y_n = \\dfrac1{2n \\pi + \\pi/4}$$ and look at what happens to your function along these two sequences. Note that both sequences tend to $0$ as $n \\to \\infty$.\n",
    "tags": [
      "real-analysis",
      "limits",
      "proof-writing"
    ],
    "score": 4,
    "answer_score": 5,
    "is_accepted": true,
    "question_id": 214010,
    "answer_id": 214012
  },
  {
    "theorem": "Almost uniform convergence implies convergence in measure",
    "context": "Let $(A,\\mathcal{F},\\mu)$ be finite measure space and $\\{f_n\\}$ a sequence of finite real measurable functions so that $f_n\\rightarrow f$ a.e. We say $f_n\\rightarrow f$ almost uniformly if $\\epsilon>0$, there is $E\\subseteq A$ such that $f_n \\rightarrow f$ uniformly on $E^c$ and $\\mu(E)<\\epsilon$. \nI want to show that $f_n\\rightarrow f$ almost uniformly implies convergence in $\\mu$. For this, suppose not. Then \n$$\\exists \\eta,\\epsilon>0:\\forall N\\in \\mathbb{N}:\\exists n>N:\\mu(\\mid f_n-f\\mid\\geq\\epsilon)\\geq \\eta, $$\n i.e., for infinitely many points $n\\in \\mathbb{N}$. From the definition of almost uniform convergence, $\\exists E:\\mu(E)<\\eta$ and $f_n\\rightarrow f$ uniformly on $E^c$. Contradiction.\nQuestion\nIt seems intuitive to me. But how to deduce this contradiction precisely?\nI know that if $x\\in E$, then it must satify the negation of uniform convergence which is \n$$\\exists \\epsilon>0:\\forall N\\in \\mathbb{N}:\\exists n>N:\\mid f_n-f\\mid\\geq\\epsilon.$$\nNow, $x$ may not be in $\\{f_n \\text{ does not converge in measure to } f \\}$ if $\\mu(\\mid f_n(x)-f(x)\\mid\\geq\\epsilon)<\\eta$. So I conclude that $$\\{f_n \\text{ does not converge in measure to } f \\}\\subseteq E$$ implying that $\\eta>\\mu(E)\\geq \\eta$; a contradiction.\nMy argument seems right but also very inefficient. How could you express this idea as clean as possible?\nThanks!\n",
    "proof": "Here's a way without going for contradiction: Let $\\epsilon >0$ be fixed and consider some $\\delta >0$. \nBy almost uniform convergence, there exists some $E$ with $\\mu(E)\\leq \\delta$ and some $N$ such that $n\\geq N\\implies \\forall x\\in E^c, |f_n(x)-f(x)|< \\epsilon$.\nFor $n\\geq N$, note the inclusion $(|f_n-f|\\geq \\epsilon) \\subset E$, hence $\\mu((|f_n-f|\\geq \\epsilon))\\leq \\mu(E)\\leq \\delta$.\nHence $\\forall \\delta>0, \\exists N, n\\geq N \\implies \\mu((|f_n-f|\\geq \\epsilon))\\leq \\delta$. Thus $ \\mu((|f_n-f|\\geq \\epsilon)) \\to 0$.\n",
    "tags": [
      "real-analysis",
      "measure-theory",
      "proof-writing"
    ],
    "score": 4,
    "answer_score": 10,
    "is_accepted": true,
    "question_id": 3332265,
    "answer_id": 3332277
  },
  {
    "theorem": "Prove $a^{2}(1+b^{2})+b^{2}(1+c^{2})+c^{2}(1+a^2)\\geq 6abc$",
    "context": "Prove $a^{2}(1+b^{2})+b^{2}(1+c^{2})+c^{2}(1+a^2)\\geq 6abc$ \nMy attempt: \n$a^{2}(1+b^{2})+b^{2}(1+c^{2})+c^{2}(1+a^2)-6abc\\geq 0$ \n$\\implies a^{2}+a^{2}b^{2}+b^{2}+b^{2}c^{2}+c^{2}+c^{2}a^{2}-2abc-2abc-2abc\\geq 0$ \n$\\implies (a-bc)^{2}+(b-ac)^{2}+(c-ab)^{2}\\geq 0$ \nEach of these terms must be non-negative, thus the sum is also non-negative. \nI'm new to writing proofs, so I don't know whether this proof is fine.\n",
    "proof": "The idea behind your proof is okay, except for the fact that your $\\implies$ should rather be $\\iff$ for it to be sufficient.\nWhat's more it is not (formally) correct to write $\\implies$ signs one after the other, since $\\implies$ is not associative ($(A\\implies B) \\implies C $ is different from $A\\implies (B \\implies C)$, and both are different from what you seem to mean by $A\\implies B \\implies C$).\nThis is one of the reasons why it is always better to use words rather than mathematical notations when it comes to reasonning, in order to avoid confusion, and simply because it (hopefully) requires less effort from the ones who read you since it makes your proof much smoother.\n",
    "tags": [
      "inequality",
      "proof-verification",
      "proof-writing"
    ],
    "score": 4,
    "answer_score": 3,
    "is_accepted": true,
    "question_id": 1991311,
    "answer_id": 1991315
  },
  {
    "theorem": "Proof of $a \\equiv b \\mod n$ implies $a^k \\equiv b^k \\mod n$",
    "context": "Prove that for $n$ in the set of natural numbers, with $n \\geq 2$:\nFor all $a, b \\in \\mathbb{N}$, $a \\equiv b \\mod n$ implies that $a^2 \\equiv b^2 \\mod n$.\nalso what about this \nProve by induction that for $n$ in the set of natural numbers, $n \\geq 2$\nFor all $a,b \\in \\mathbb{N}$, $a \\equiv b \\mod n$ implies that $a^k \\equiv b^k \\mod n$.\n",
    "proof": "$a\\equiv b\\pmod{n}\\iff n\\mid a-b$\n$\\implies n\\mid (a-b)(a+b)=a^2-b^2\\iff a^2\\equiv b^2\\pmod{n}$\n",
    "tags": [
      "elementary-number-theory",
      "discrete-mathematics",
      "proof-writing",
      "modular-arithmetic"
    ],
    "score": 4,
    "answer_score": 4,
    "is_accepted": true,
    "question_id": 1558910,
    "answer_id": 1558917
  },
  {
    "theorem": "Proof of a discovery involving the square of whole numbers",
    "context": "It was probably discovered by someone else but:\nWhen you take the square of a non-zero whole number the sum of the numbers digit is always equal to $1,4,7,9$\nHow can I write a mathematical proof of that?\n",
    "proof": "This follows quite easily from the fact that digit sum arithmetic is equivalent to arithmetic modulo 9. To see this we express a given integer $n$ as $n = k_1 + 10 k_2 + ... + 10^j k_j$ where the $k_i$s are integers (in fact the digits of $n$). Then $n = k_1 + k_2 + ... + k_j \\, (mod \\, 9)$.  \nThen the question is simply which numbers are square modulo 9 and the answer to that is precisely the numbers in your list.\n",
    "tags": [
      "proof-writing",
      "recreational-mathematics"
    ],
    "score": 4,
    "answer_score": 10,
    "is_accepted": false,
    "question_id": 1394669,
    "answer_id": 1394679
  },
  {
    "theorem": "Proving the inequality $|a-b| \\leq |a-c| + |c-b|$ for real $a,b,c$",
    "context": "\nLet $a,b,c$ real numbers. Prove the inequality $|a-b| \\leq |a-c| + |c-b|$. Prove that equality holds if and only if $a \\leq c \\leq b$ or $b \\leq c \\leq a$.\n\nI've tried starting with just $a \\leq c$ and using field properties to reconstruct the inequality, however I haven't been able to make it work. I also tried making the negatives positive and stripping the inequalities and making something happen but again I don't know if that's a proper rule and it didn't seem to get me anywhere.\n",
    "proof": "You could use the triangle inequality and the fact that you can write $|a-b|=|(a-c)+(c-b)|$.\n",
    "tags": [
      "real-analysis",
      "inequality",
      "proof-writing",
      "absolute-value"
    ],
    "score": 4,
    "answer_score": 7,
    "is_accepted": true,
    "question_id": 708431,
    "answer_id": 708440
  },
  {
    "theorem": "Proof that the Irrationals are Countable",
    "context": "Proof: Between any two irrationals lies a rational, by the Density of the rationals in the real number system. There are only countably many rationals; therefore, there are only countably many pairs of irrationals. Therefore the number of irrationals is countable since the cardinality of $2\\mathbf{N}$ is $\\mathbf{N}$.\nI don't know why I came across this logic since I know the irrationals are uncountably infinite, but I don't see the hole in my logic.\n",
    "proof": "The claim that between two rationals there is only one irrational is false. In fact between two rationals there are many irrationals, so you will have to map a lot of irrationals to the same pair (for most pairs too).\nTherefore your proof does not constitute of a bijection, or even a well-defined function. This is a common mishap with infinite objects, though. They tend to get very confusing! \n",
    "tags": [
      "elementary-set-theory",
      "proof-writing",
      "infinity"
    ],
    "score": 4,
    "answer_score": 12,
    "is_accepted": true,
    "question_id": 166794,
    "answer_id": 166795
  },
  {
    "theorem": "Proof of solutions to “24” operations game",
    "context": "Consider a deck of cards with values $1$ through $13$, each with multiplicity $4$, so that $$S = \\left( \\bigcup_{i=1}^{13} \\{ i \\} \\right) \\times \\{1,2,3,4\\}$$\nSupposedly, there exists a game in which four cards $(v,m)$ in $S$ are selected (here $m$ stands in for the suit), and participants try to devise a series of operations using:\n\naddition $+$;\nsubtraction $-$;\nmultiplication $\\times$;\ndivision $\\div$;\nexponentiation $\\wedge$;\nfactorial-ization $!$; and\nany number of parentheses $()$\n\non the $v$-values that yields $24$. For example,\n$$(2, 1) \\times (10,3) - (2,4) + (3,2)! = 24$$\n\n\nDo there exist any combinations of cards for which no series of operations yielding $24$ exists?\nWhat mathematical processes could one use to analyze other aspects of solutions, such as how many solutions exist?\n\nThis is a post-examination game we are playing in class to kill time. I haven’t the slightest idea of how one would approach any sort of ‘proof.’\n",
    "proof": "EDIT: there was a bug in my code so it missed some solutions. There are actually only 48 unsolvable hands, not 56.\nEDIT 2: due to rounding errors, my code thought [9,7,5,5] was solvable, when in fact it is not (see update below).\nI have played that game too, so when I saw your question I was pretty sure there are some hands that have no series reaching 24. Since there's only $\\binom{52}{4} = 270,725$ possible hands of 4 cards (even less since this game doesn't count suits), I figured it would be feasible to do a computer search to find all of these solutions (see the bottom for my code). \nMy program found solutions for all but 48 hands (ignoring different suits): \n\n[1,1,7,7]  [1,1,9,9]  [1,1,10,10]  [1,1,10,11]  [1,7,7,13]  [1,9,9,9]  [1,9,10,10]  [1,10,10,10]  [1,10,10,11]  [1,10,11,11]  [1,11,11,11]  [1,13,13,13]  [6,6,6,13]  [6,6,7,7]  [6,6,7,13]  [6,6,13,13]  [6,7,7,13]  [6,7,13,13]  [6,8,8,13]  [6,11,11,13]  [7,7,7,7]  [7,7,7,13]  [7,7,10,10]  [7,7,10,12]  [7,7,11,11]  [7,7,13,13]  [7,8,9,9]  [7,10,10,13]  [7,11,11,13]  [7,13,13,13]  [8,8,9,9]  [8,8,11,11]  [8,9,9,10]  [8,9,13,13]  [9,9,9,9]  [9,9,9,11]  [9,9,10,10]  [9,9,13,13]  [9,10,10,10]  [9,10,10,11]  [9,10,11,11],  [10,10,10,10]  [10,10,10,11]  [10,10,11,11]  [10,10,13,13]  [10,11,11,11]  [11,11,11,11]  [13,13,13,13]\n\nCounting suits, this works out to be 2413 possibilities out of the 270,725 total hands, or a probability of just a tad below 0.9%.\nSince my program did not use factorials of any numbers larger than 13, so it is possible that some of these 48 actually do have solutions, but I don't find that very likely. Among these combinations all have repeated cards. None of the combinations have a 2, 3, 4, or 5, and only one has a 12. Surprisingly, the most common number in the unsolvable hands is 10 (accounting for suits, it shows up in 1109 out of the 2413 unsolvable hands). As someone who has played this game a lot, I was expecting 11 or 13 to be the most likely to give an unsolvable hand, but they only showed up in 845 and 1073 unsolvable hands, respectively.\nAs for the number of ways to get to 24 from a given hand, my program didn't really look at that because I was trying not to make the program take up to much computational power, and it was much easier to simply remember a single boolean than a whole chain.\n\nRounding errors update:\n[9,7,5,5] is not a solution, but my code thinks it is due to rounding errors. For example, it couldn't distinguish $9^{5-7!}$ from $0$, and found this \"solution\":$$\n\\left(5 - \\left(9^{5-7!}\\right)!\\right)! \\approx 24\n$$\nIf you use the gamma function to interpolate the factorials, this is off by only about$5\\times 10^{-4804}$.\nRemoving exponentiation from the options, other than using $1^a = 1$, shows that this is the only solution that took advantage of this particular numerical error. Since I didn't ever take a factorial of a number bigger than 13, the factorials should not have introduced any other fake solutions.\n\nSome interesting example hands:\nAmong the hands that have solutions, I've chosen to highlight a few particularly interesting ones:\n\n[13,11,10,6], [13,6,1,1], [13,12,10,8], [11,11,5,5], [11,11,5,1], [9,7,7,7], and [5,1,1,1] \n\nIf you want to challenge yourself, see if you can solve these before looking at the solutions.\n[13,11,10,6] \n\n This is the only hand that requires a factorial of a number bigger than 10:\n $$24 = \\left(\\frac{13 + \\frac{11!}{10!}}{6}\\right)!$$\n\n[13,6,1,1]\n\n This and [9,8,1,1] are the only ones that require use of exponentiation:\n\\begin{eqnarray} 24&=&\\left(\\frac8{1+1^9}\\right)!\\\\24&=&\\left(6 - 1^{13} - 1\\right)! \\end{eqnarray}\n\n[9,7,7,7]\n\n This hand has only one possible solution, and is an interesting example of one which requires factorials but doesn't use $4!=24$.\n $$24 = \\frac{9!}{7!+7!+7!}$$\n\nIf you disallow factorials, there are fully 430 unsolvable hands (not counting suits). Some of my favorite solutions without factorials include [11,11,1,5], [13,12,10,8], [11,11,5,5] and [5,1,1,1], which have the following unique non-factorial solutions: \n[13,12,10,8]\n\n $$24=\\frac{10\\cdot 12}{13-8}$$\n\n[11,11,5,5]\n\n $$24=5\\cdot5-\\frac{11}{11}$$\n\n[11,11,5,1]\n\n $$24=\\frac{11\\cdot 11 - 1}{5}$$\n\n[5,1,1,1]\n\n $$5^{1+1} - 1$$\n\n\nBelow is my Haskell program. Programming is not my strong point, so excuse the messiness.\n\nunsolvable = p 4\n-- all hands that have no way to reach 24.\n\n-- derivation:\n\n-- returns True if an input four card hand is solvable.\ncheck :: (Num a, Fractional a, Eq a, Ord a, Floating a, Enum a) => [a] -> Bool\ncheck [a,b,c,d] = firstCheck a b c d\ncheck _ = False\n\n--check as a four-argument function\nfirstCheck :: (Num a, Fractional a, Eq a, Ord a, Floating a, Enum a) => a -> a -> a -> a -> Bool\nfirstCheck a b c d = or [fCheck a b c d, fCheck b c d a, fCheck c d a b, fCheck d a b c, fCheck d b a c, fCheck a c b d]\n\nfCheck :: (Num a, Fractional a, Eq a, Ord a, Floating a, Enum a) => a -> a -> a -> a -> Bool\nfCheck a b c d = or $ map (secondCheck a b) $ op c d\n\n{--finalCheck :: (Num a, Fractional a, Eq a, Ord a, Floating a, Enum a) => a -> a -> Bool\nfinalCheck a b = (24 `elem` s) \n    where s = op a b --}\n\n--check if 3 numbers can make 24\nsCheck :: (Num a, Fractional a, Eq a, Ord a, Floating a, Enum a) => a -> a -> a -> Bool\nsCheck a b c = or $ map (finalCheck a) $ op b c\n\nsecondCheck :: (Num a, Fractional a, Eq a, Ord a, Floating a, Enum a) => a -> a -> a -> Bool\nsecondCheck a b c = (sCheck a b c) || (sCheck b a c) || (sCheck c a b)\n\n-- checks if 2 numbers can combine to 24 or 4! = 24\nfinalCheck :: (Num a, Fractional a, Eq a, Ord a, Floating a, Enum a) => a -> a -> Bool\nfinalCheck a b = (24 `elem` s) || (4 `elem` s)\n    where s = op a b\n\n--all operations except factorial, with the assumption that a a -> a -> [a]\nopp 0 b = [0,1,b,-b, 1+b, b-1, 1-b]\nopp 1 b = [1+b,b,1/b,1-b,b-1,1]\nopp a b = [a+b,a*b,a/b,a-b,b-a,b/a,a**b,b**a]\n\n-- op includes operations from opp as well as manually including factorials up to 13!\nop ::  (Num a, Fractional a, Eq a, Ord a, Floating a, Enum a) => a -> a -> [a]\nop 3 b = opp (min 3 b) (max 3 b) ++ op b 6 ++ op 6 b\nop 4 b = opp (min 4 b) (max 4 b) ++ op b 24\nop 5 b = opp (min 5 b) (max 5 b) ++ op b 120 \nop 6 b = opp (min 6 b) (max 6 b) ++ op b 720\nop 7 b = opp (min 7 b) (max 7 b) ++ op b 5040 \nop 8 b = opp (min 8 b) (max 8 b) ++ op b 40320 \nop 9 b = opp (min 9 b) (max 9 b) ++ op b 362880 \nop 10 b = opp (min 10 b) (max 10 b) ++ op b 3628800 \nop 11 b = opp (min 11 b) (max 11 b) ++ op b 39916800\nop 12 b = opp (min 12 b) (max 12 b) ++ op b 479001600\nop 13 b = opp (min 13 b) (max 13 b) ++ op b 6227020800\nop a b = opp (min a b) (max a b) \n\n\nlcheck :: (Num a, Fractional a, Eq a, Ord a, Floating a, Enum a,Enum a) => [a] -> [Bool]\nlcheck [a,b,c,d] = [check [a,b,c,d]]\nlcheck s = map (and . lcheck . (\\n -> n:s)) [(head s)..13] \n-- s must be in order from largest to smallest.\n-- (lcheck s !! i) is True whenever there is a solvable hand sorted \n-- from largest to smallest that ends with (head s + i):s\n\nccheck :: (Num a, Fractional a, Eq a, Ord a, Floating a, Enum a,Enum a) => [a] -> Bool\nccheck = and . lcheck\n-- returns True if a sorted hand ending with s is solvable\n\np1 = filter (not . (\\k -> (ccheck [k])) . fromIntegral) [1..13]\n-- possible smallest cards for an unsolvable hand\n\npf :: (Integral a) => [a] -> [[a]]\npf x = map (\\s -> s:x) $ filter (not . (\\k -> (ccheck $ map fromIntegral (k:x))) . fromIntegral) [(head x)..13] \n-- if x is n cards, pf x lists possibilities of length n+1 endings for sorted hands that end in x\n\np :: Int -> [[Integer]]\np 1 = map (\\x -> [x]) p1\np n = foldr (++) [] $ map pf $ p (n-1)\n-- lists possible last n cards of a sorted unsolvable hand.\n\n",
    "tags": [
      "proof-writing",
      "combinations",
      "game-theory",
      "combinatorial-game-theory",
      "card-games"
    ],
    "score": 4,
    "answer_score": 10,
    "is_accepted": true,
    "question_id": 2785407,
    "answer_id": 2785838
  },
  {
    "theorem": "Show that no two of the spaces $(0, 1), (0, 1], [0, 1]$ are homeomorphic",
    "context": "\nShow that no two of the spaces $(0, 1),  (0, 1], [0, 1]$ are homeomorphic\n\nMy Attempted Proof\nWe have $(0, 1) \\subset (0, 1] \\subset [0, 1]$.\nWe also have $(0, 1] = (0, 1) \\cup \\{1\\}$ and thus $|(0, 1)| \\neq |(0, 1]|$ and thus no bijective mapping $f : (0, 1) \\to (0, 1]$ exists, hence $(0,1)$ and $(0, 1]$ are not homeomorphic regardless of the topology defined on them.\nThe proof for the other cases are similar. $\\ \\square$\n\nIs my proof correct? I've seen proofs which form a contradiction through connectedness. Is there any error in my arguments?\n",
    "proof": "\nWe also have $(0, 1] = (0, 1) \\cup \\{1\\}$ and thus $|(0, 1)| \\neq |(0, 1]|$ and thus no bijective mapping $f : (0, 1) \\to (0, 1]$ exists\n\nFalse. There exists a bijective mapping from $(0,1]$ to $(0,1)$. For example, one of them is\n$$f(x) = \\begin{cases}\\frac x2 & \\text{if } x=\\frac{1}{2^k}\\text{ for some }k\\in \\mathbb N\\cup\\{0\\}\\\\\nx & \\text{else}\\end{cases}$$\nwhich maps $1$ to $\\frac12$, $\\frac12$ to $\\frac14$, $\\frac14$ to $\\frac18$ and so on, while leaving all the other numbers in place.\nYour argument is flawed, because if a set is infinite, adding one more element does not increase its size. For example, the set of even integers is equipotent to the set of all integers.\n\nTo actually prove your statement, think about the open sets that contain $1$ in $(0,1]$.\n\nAlso, and please don't take this the wrong way, but you may be rushing in your studies a little. If you don't yet know that $(0,1)$ has just as many elements as $(0,1]$, then your knowledge of set theory is at quite a basic level (i.e., it's something you should learn in your first month or so of graduate level math), while the topic you are addressing is basic topology, which is slightly more advanced (i.e., typically second-year undergrad). I advise you to slow down and take it step by step (if you are self-learning) or grab some first-year books and go back to step one (if you are a student).\n",
    "tags": [
      "general-topology",
      "elementary-set-theory",
      "proof-verification",
      "proof-writing",
      "connectedness"
    ],
    "score": 4,
    "answer_score": 9,
    "is_accepted": true,
    "question_id": 2143658,
    "answer_id": 2143663
  },
  {
    "theorem": "Proof of $f(\\{x\\})=\\{f(x)\\}$",
    "context": "I thought, is this really that simple? Or am I missing a piece?\nThis is my proof:\n$f(\\{x\\})=\\{f(x)\\}$.\nLook at $f(\\{x\\})$. By definition, $f(\\{x\\})=\\{f(a)|a \\in \\{x\\}\\}$, and therefore $f(\\{x\\})=\\{f(x)\\}$.\n",
    "proof": "This is basically the same proof as yours, I just rewrote it more formally/at lower level.\nTo prove that $f(\\{x\\})=\\{f(x)\\}$ we need to show that:\n$$z\\in f(\\{x\\}) \\Leftrightarrow z\\in \\{f(x)\\}.$$\nThis can be shown as follows:\n$z\\in f(\\{x\\})$ $\\Leftrightarrow$ $(\\exists a\\in\\{x\\}) z=f(a)$ $\\Leftrightarrow$ $z=f(x)$ $\\Leftrightarrow$ $z\\in\\{f(x)\\}$\n(You might want to think for a bit why the equivalences I wrote above are true - in case you want to make a really detailed proof. I think that most of the details are explained nicely in Asaf's answer.)\n",
    "tags": [
      "elementary-set-theory",
      "proof-writing"
    ],
    "score": 4,
    "answer_score": 6,
    "is_accepted": true,
    "question_id": 65038,
    "answer_id": 65040
  },
  {
    "theorem": "How do I formally prove a universal implication?",
    "context": "A textbook I am reading (Discrete Mathematics and its Applications by Rosen) went from introducing formal propositional and predicate logic (including popular rules of inference like Modus Ponens, Modus Tollens, and Universal Generalization) to introducing direct methods of proof for theorems of the form ∀n(P(n)->Q(n)).\nApparently, most mathematical proofs of any kind of theorem are \"informal\" and omit many logical rules of inference and argumentative steps for the sake of conciseness. However, because the textbook doesn't provide even one example of a detailed \"tedious\" proof that expresses most or all rules of inference and axioms used in the proof, though I have a general idea of the connection between the two, I have been struggling to fully tie together the ideas of formal logic to the ideas of mathematically proving theorems of the form ∀n(P(n)->Q(n)). Can anyone provide an example of a detailed mathematical proof of a simple theorem that omits few (if any) logical steps in the argument? I have personally struggled with (as a personal exercise) meticulously proving the theorem \"for all integers, if n is odd then the square of n is odd\", but any logically detailed argument proving a simple theorem similar to that would be very useful.\n",
    "proof": "Yes, I noticed that about the Rosen book as well: he never provides a detailed example for Universal Generalization ... which is very frustrating, as that rule is actually a bit tricky because of the constraint put on the constant being used. That is, Rosen defines the rule as:\n\n$P(c)$ for an arbitrary $c$\n$\\therefore \\forall x \\ P(x)$\n\nBut doesn't really specify how the $x$ being 'arbitrary' is handled formally. His informal discussion says:\n\nUniversal generalization is used when we show that ∀xP(x) is true by taking an arbitrary element c from the domain and showing that P(c) is true. The element c that we select must be an arbitrary, and not a specific, element of the domain. That is, when we assert from ∀xP(x) the existence of an element c in the domain, we have no control over c and cannot make any other assumptions about c other than it comes from the domain.\n\nWell, unless you know what you're doing, that's still not very clear, I agree.\nIndeed, it is for this reason I always recommend against using the Rosen book to learn formal proofs ... it's a nice textbook on discrete mathematics, but on this particular section it is actually very poor (actually, another complaint I have about the book is that its treatment on Turing-machines is far too sparse: it does not give the reader any sense of how important Turing-machines are in the foundations of computer science).\nAlso, before going on, I think that Ethan Bolker makes an excellent point about formal proofs: most computer scientists (and even most mathematicians) don't really need to know how exactly formal proofs work ... you just need to know how to do proofs at somewhat more informal level ... and Rosen's book is much better on that score. That said, formal proofs do teach you to be precise and organized about your proofs, so I am not totally against doing formal proofs either, even if they are overkill for most situations.\nIndeed, you asked about formal proofs, and how to use in particular the Universal Generalization rule, so let me try and address that in the rest of my Answer.\nAgain, Rosen never really gives a precise formalization of the rule. But, fortunately, other textbooks (dedicated to just formal logic) have ways of formalizing this. Let me give some examples.\nSuppose you have premises:\n$\\forall x (P(x) \\to Q(x))$\n$\\forall x (Q(x) \\to R(x))$\nFrom this, we should be able to infer:\n$\\forall x (P(x) \\to R(x))$\nHere is a formal proof you could do:\n\\begin{array}{lll}\n1&\\forall x (P(x) \\to Q(x))&Premise\\\\\n2&\\forall x (Q(x) \\to R(x))&Premise\\\\\n3&P(c) \\to Q(c)&Universal \\ Instantiation \\ on \\ 1\\\\\n4&Q(c) \\to R(c)&Universal \\ Instantiation \\ on \\ 2\\\\\n5&P(c) \\to R(c)&Hypothetical \\ Syllogism \\ on \\ 3,4\\\\\n6&\\forall x (P(x) \\to R(x))&Universal \\ Generalization \\ on \\ 5\\\\\n\\end{array}\nNow, why is this a correct use of the rule Universal Generalization (and thus a proper proof?).  It is because when the $c$ was used for the first time, it came from a Universal Instantiation .. meaning that indeed we weren't making any additional assumptions about $c$ other than it being some arbitrary object from the domain. That is, on line 3 we effectively said: \"Let $c$ be an arbitrary object from the domain. Well, then because of line 1 we have $\\forall x (P(x) \\to Q(x))$, this should be true for all objects, and thus also for $c$, and hence we have $P(c) \\to Q(c)$.  More importantly, on line 6, we say: \"We have found (on line 5) that $P(c) \\to R(c)$.  OK, but since $c$ was an arbitrary object from the domain, then line 5 effectively states that $P(c) \\to R(c)$ is true for any $c$, and hence we can conclude that it is true for all objects from the domain, i.e. we have $\\forall x (P(x) \\to R(x))$\nSome formalizations actually make the step of saying \"Let $c$ be an arbitrary object from the domain.\" very explicit by giving it its own line. In those formalizations, the proof would look something like this:\n\\begin{array}{lll}\n1&\\forall x (P(x) \\to Q(x))&Premise\\\\\n2&\\forall x (Q(x) \\to R(x))&Premise\\\\\n3&c&Introduce \\ c\\\\\n4&P(c) \\to Q(c)&Universal \\ Instantiation \\ on \\ 1\\\\\n5&Q(c) \\to R(c)&Universal \\ Instantiation \\ on \\ 2\\\\\n6&P(c) \\to R(c)&Hypothetical \\ Syllogism \\ on \\ 3,4\\\\\n7&\\forall x (P(x) \\to R(x))&Universal \\ Generalization \\ on \\ 3-6\\\\\n\\end{array}\nThe single $c$ on line 3 is of course not a claim, but again it is a way of saying $\\exists x \\ P(x)$ \"Let $c$ be an arbitrary object from the domain.\" So then, on line 7, the proof notes that the $c$ used in line 6, was introduced on line 3, meaning that $c$ is arbitrary, and hence line 6 can be universally generalized.\nTo do formal proofs, I myself like to use a piece of software called Fitch, in which they use 'subproofs' to indicate assumptions. You can use those very subproofs to formalize the introduction of new constants as well:\n\nNotice how the rules are called a little differently here: Generalization is 'Introduction', while Instantiation is 'Elimination'. Indeed, in a Fitch system, you have Introduction and Elimination rules for all logical operators ... and no other rules.  In fact, the Hypothetical Syllogism (HS) in this proof is used as a Lemma: it refers to an earlier proof I already did that derives Hypothetical Syllogism from more basic Introduction and Elimination rules. If we expand this Lemma in the proof, we get:\n\nHere, $\\to$ Elim is of the same as Modus Ponens, and $\\to$ Intro formalizes the important proof technique of Conditional Proof.\nNow, notice that many universal claims have a conditional as the main connectives of their body. Indeed, you yourself explicitly asked about how to prove statements of the form $\\forall x (P(x) \\to Q(x))$. This example is no exception. Thus, you get a Conditional Proof inside a Universal Proof. This pattern is so common that these two often get combined into what is called a Universal Conditional Proof. For example, to prove that \"All $P$'s are $R$'s\", you typically start your proof with \"Let's consider any $P$ ...\" and try to show that this $P$ is an $R$. What's cool, is that Fitch can be used to formalize this proof technique as well:\n\nSee how that works?  OK, now let's look at a different example. Suppose we have:\n$\\forall x (P(x) \\to Q(x))$\n$\\exists x \\ P(x)$\nFrom this, we should be able to prove $\\exists x \\ Q(x)$, but not $\\forall x \\ Q(x)$\nOK, let's see how this fleshes out formally. First, a valid proof using Existential Generalization (this is actually very similar to example 13 from Rosen):\n\\begin{array}{lll}\n1&\\forall x (P(x) \\to Q(x))&Premise\\\\\n2&\\exists x \\ P(x)&Premise\\\\\n3&P(c) &Existential \\ Instantiation \\ on \\ 2\\\\\n4&P(c) \\to Q(c)&Universal \\ Instantiation \\ on \\ 1\\\\\n5&Q(c)&Modus \\ Ponens \\ on \\ 3,4\\\\\n6&\\exists x \\ Q(x) &Existential \\ Generalization \\ on \\ 5\\\\\n\\end{array}\nOK, so first note that Existential Generalization does not come with any constraints on the constant that is being used. Rosen says that you merely need $P(c)$ for some constant $c$ in order to conclude $\\exists x \\ P(x)$ ... so it doesn't matter where this $c$ comes from. And that makes sense: if you know that $P(c)$, then you can conclude $\\exists x \\ P(x)$, no matter how this $c$ is being used or how it was introduced.\nOK, but what goes wrong if we do:\n\\begin{array}{lll}\n1&\\forall x (P(x) \\to Q(x))&Premise\\\\\n2&\\exists x \\ P(x)&Premise\\\\\n3&P(c) &Existential \\ Instantiation \\ on \\ 2\\\\\n4&P(c) \\to Q(c)&Universal \\ Instantiation \\ on \\ 1\\\\\n5&Q(c)&Modus \\ Ponens \\ on \\ 3,4\\\\\n6&\\forall x \\ Q(x) &Universal \\ Generalization \\ on \\ 5\\\\\n\\end{array}\nOK, what goes wrong here is that the $c$ wasn't an arbitrary object, i.e. it wasn't just any object. Rather, it was one-of-those-objects that, given line $2$, we know to exist and that have property $P$. So, we can't universally generalize.\nUsing the method of having to explicitly introduce an arbitrary object, we would see how the proof fails as well:\n\\begin{array}{lll}\n1&\\forall x (P(x) \\to Q(x))&Premise\\\\\n2&\\exists x \\ P(x)&Premise\\\\\n3&c&Introduce \\ c\\\\\n4&P(c) &Existential \\ Instantiation \\ on \\ 2\\\\\n5&P(c) \\to Q(c)&Universal \\ Instantiation \\ on \\ 1\\\\\n6&Q(c)&Modus \\ Ponens \\ on \\ 3,4\\\\\n7&\\forall x \\ Q(x) &Universal \\ Generalization \\ on \\ 3-6\\\\\n\\end{array}\nNow, the problem is on line 4:  $c$ was already introduced (on line 3) as a completely arbitrary object. But that means that we cannot be for certain that $c$ has property $P$. Yes, sure, line 2 tells us that some objects have property $P$, but that does not mean that given any arbitrary object $c$, I can say for certain that it has property $P$. So, again, line 4 fails.\nIndeed, like Universal Generalization, the Existential Elimination rule comes with an important constraint:  The constant that you use to instantiate for the variable that is existentially quantified should not just be any constant, but a new constant, i.e. a constant that has not been used earlier in the proof. To see this, consider:\n\\begin{array}{lll}\n1&P(c)&Premise\\\\\n2&\\exists x \\ Q(x)&Premise\\\\\n3&Q(c) &Existential \\ Instantiation \\ on \\ 2\\\\\n4&P(c) \\land Q(c)&Conjunction \\ on \\ 1,3\\\\\n5&\\exists x (P(x) \\land Q(x))&Existential \\ Generalization \\ on \\ 4\\\\\n\\end{array}\nHere, line 3 is where things go wrong. Sure, from line 2 we know that there are some objects that have property $Q$, but of course this does not mean that specifically object $c$ (that line 1 already makes reference to, and thus has a certain use/role for) has property $Q$.\nUnfortunately, in his semi-formal way of describing the rule, Rosen fails to point out this important restriction, and instead merely says:\n\n$\\exists x \\ P(x)$\n$\\therefore P(c)$ for some element $c$\n\nNo! It needs to be for some new element $c$!\nIn his informal description of the rule, Rosen writes:\n\nExistential instantiation is the rule that allows us to conclude that there is an element c in the domain for which P(c) is true if we know that ∃xP(x) is true.We cannot select an arbitrary value of c here, but rather it must be a c for which P(c) is true. Usually we have no knowledge of what c is, only that it exists. Because it exists, we may give it a name (c) and continue our argument.\n\n... I suppose here he gets it right ... but it is not very clear: did you understand that $c$ had to be a new constant from this?\nOnce again, I like how Fitch does things.  Here is the formalization of the valid proof in Fitch:\n\nWhen the system verifies this proof, and gets to line 7, it checks to make sure that the $c$ that was used to eliminate the existential (and hence was introduced at the start of the subproof) is indeed a new constant, rather than a constant already used. So, in this case, it checks out. But, in the following 'proof' it does not:\n\nOK, finally, let's do an actual proof: let's prove that an odd number squares will be an odd number itself.  Of course, we'll need to decide on what axioms to use, i.e. what we can use as premises. Here, we could decide to use the Peano Axioms, but starting with those from scratch, the proof would become really long. So, instead, here is a proof in Fitch that does use a few of those axioms, but also assumes some basic properties of commutation, association, and distribution regarding addition and multiplication:\n\nSee how tedious this becomes?  ... this is why practicing computer scientists and mathematicians rarely do formal proofs.\n",
    "tags": [
      "discrete-mathematics",
      "logic",
      "proof-writing",
      "natural-deduction",
      "formal-proofs"
    ],
    "score": 4,
    "answer_score": 5,
    "is_accepted": true,
    "question_id": 3242640,
    "answer_id": 3242727
  },
  {
    "theorem": "Does the non-commutativity of quaternions follow directly from $\\rm i^2=j^2=k^2=ijk=-1$?",
    "context": "All of quaternions are, from what I understand, defined simply by\n$$\\newcommand{\\i}{\\mathrm{i}}\n\\newcommand{\\j}{\\mathrm{j}}\n\\newcommand{\\k}{\\mathrm{k}}\n\\i^2=\\j^2=\\k^2=\\i\\j\\k=-1$$\nIt is known that quaternion multiplication is not commutative. Does this follow directly from the definition? I suspect this might could be proven via contradiction.\nAdditionally, the quaternion multiplication (Hamilton product) formula is downright disgusting. This makes the proof all the more daunting.\nHow should I go about deriving the non-commutativity?\n",
    "proof": "Yes, it does (assuming that the algebra is not trivial, i.e. $1 \\ne 0$).\n$i - jk = - i^2(i-jk) = -i(i^2 - ijk) = -i(-1 - (-1)) = 0$ so  $i = jk$.\nSimilarly\n$ k - ij = - (k - ij)k^2 = - (k^2 - ijk) k = 0$ so $ij = k$.\nThen $ik = - i j^2 k = - (ij)(jk) = -ki$.  So either $ik = 0$ or $i$ and $k$ don't commute.  But if $ik = 0$, $ijk = -1 \\ne 0 = jik $, so $i$ and $j$ can't commute. \n",
    "tags": [
      "proof-writing",
      "quaternions"
    ],
    "score": 4,
    "answer_score": 7,
    "is_accepted": true,
    "question_id": 3223870,
    "answer_id": 3223940
  },
  {
    "theorem": "How to prove that triangle inscribed in another triangle (were both have one shared side) have lower perimeter?",
    "context": "This question looks really simple, but to my (and my co-workers) frustration we were not able to prove this in any way. I tried all triangle formulas known to me but I feel I'm missing the point, and proof will be or much simpler or much more complicated than what I tried.\nSo, the question:\nGiven a triangle ABC and point P inside that triangle, prove that for triangle APB the following inequality holds:\n|AB| + |BC| > |AP| + |PC|\n(Actually it doesn't matter for me if it's > or >=).\n",
    "proof": "There is really an elementary proof of a more general result: if one polygon fits within the other (boundaries may touch) then its perimeter is smaller than that of the bigger polygon\nhttp://www.cut-the-knot.org/m/Geometry/PerimetersOfTwoConvexPolygons.shtml\nThe idea is that the sides of the small polygon may be extended, trimming the bigger polygon until the sides of one are on the sides of the other so that side-by-side inequalities could be summed up to get an inequality for the perimeters\n",
    "tags": [
      "proof-writing",
      "triangles"
    ],
    "score": 4,
    "answer_score": 2,
    "is_accepted": true,
    "question_id": 613296,
    "answer_id": 995577
  },
  {
    "theorem": "Proof that there is only one homomorphism from $\\Bbb{Z}$ to $\\Bbb{Z}/n\\Bbb{Z}$",
    "context": "Could anyone help me (even just a start) to prove this ? Homomorphism (in rings) is a new notion for me and I have to confess that I am a bit lost, I don't know how to start. \nThanks in advance\n",
    "proof": "If you are talking about unital ringhomomorphisms then for every ring $R$ (also $\\mathbb{Z}/n\\mathbb{Z}$) there is exactly\none ringhomomorphims $\\phi:\\mathbb{Z}\\rightarrow R$. This because\nthe condition $\\phi\\left(1\\right)=1_{R}$ determines $\\phi$. Notice for instance \nthat $\\phi\\left(n\\right)=\\phi\\left(1+\\cdots+1\\right)=\\phi\\left(1\\right)+\\cdots+\\phi\\left(1\\right)=1_{R}+\\cdots+1_{R}$\nfor $n>0$.\n",
    "tags": [
      "abstract-algebra",
      "proof-writing"
    ],
    "score": 4,
    "answer_score": 12,
    "is_accepted": false,
    "question_id": 550520,
    "answer_id": 550539
  },
  {
    "theorem": "Given a rational number $x$ and $x^2 &lt; 2$, is there a general way to find another rational number $y$ that such that $x^2&lt;y^2&lt;2$?",
    "context": "Suppose I have a rational number $a$ and $a^2 < 2$. Can I find another rational number $B$ such that $a^2<B^2<2$?\nBased on the answer to this question, I thought of doing the following:\n\n$$ a^2 < 2 \\implies a < \\frac{2}{a}\\\\\n\\text{Let}\\hspace{1cm} B=\\frac{a+\\frac{2}{a}}{2}=\\frac{a^2+2}{2a}\n$$\n\n$B$ is greater than $a$ because:\n\n$$ \\begin{array} {aa} B>a & \\implies \\frac{a^2+2}{2a}>a \\\\\n& \\implies a^2 + 2 > 2a^2 \\\\\n& \\implies 2 > a^2 \\\\\n& \\implies a^2 < 2\n\\end{array}$$\n\nIf $B^2$ is less than $2$, then $B^2-2<0$, but:\n\n$$\\begin{array} {aa} B^2-2 < 0 & \\implies \\left( \\frac{a^2+2}{2a} \\right)^2 - 2 < 0 \\\\\n& \\implies \\frac{a^4+4a^2+4}{4a} - \\frac{8a^2}{4a^2} < 0 \\\\\n& \\implies \\frac{(a^2-2)^2}{(2a)^2} < 0\n\\end{array}$$\n\nWhich is a contradiction since the left hand side of the inequality will be positive for all values of $a$.\nBut I think we must be able to find such a $B$ since based on my understanding of this answer, we can find a another rational number whose distance from $a$ is less than the distance between $a$ and $\\sqrt{2}$\nTherefore, I have 2 questions to ask:\n\nWhy does this approach work in the case of $a^2>2$ but not when $a^2<2$?\nHow should I approach these kind of questions since it seems that there are a few ways to construct a $B$ that satisfies a given set of restrictions? For example, see here (the proof is immediately before the section \"13. The Completeness Axiom\".\n\n",
    "proof": "The substitution $$a \\leftarrow \\frac{a^2+2}{2a} $$ is exactly what you get from Newton's method for the function $f(a)=a^2-2$.  This will result in iterations that are greater than $\\sqrt{2}$ since $f$ is an increasing convex function on $(0, \\infty)$.  I will sketch two alternatives, both based on Newton's method, to get iterations less than $\\sqrt{2}$.\nFirst idea.  Use some other function than $f(a) = a^2-2$ with Newton's method.  For example, if $0 \\leq p < \\sqrt{2}$ then\n$$\n\\frac{1}{\\sqrt{2}+p} = \\frac{\\sqrt{2}-p}{2-p^2}\n$$\nand so $\\sqrt{2}$ is a root of the function $$f(a) = \\frac{2-p^2}{a+p}-a+p = \\frac{2-a^2}{a+p}.$$\nThis function is convex and decreasing on $(0, \\infty)$.  Applying Newton's method to this function results in the iteration\n$$\na \\leftarrow \\frac{p a^2+4 a+2 p}{a^2+2 p a+2}\n$$\nwhich produces an increasing sequence that converges to $\\sqrt{2}$ from below as required. For example for $p=0$ you get\n$$\na \\leftarrow \\frac{4a}{a^2 + 2}\n$$\nwhich is exactly the harmonic mean of $a$ and $\\frac{2}{a}$ as suggested by haruspex.  For $p=1$ and $p=\\frac{7}{5}$ you get\n$$\na \\leftarrow \\frac{a^2+4a+2}{a^2+2a+2} \\textrm{ and } a \\leftarrow \\frac{7 a^2+20 a+14}{5 a^2+14 a+10}\n$$\nrespectively, to give just two other examples.  Substituting $a=\\frac{7}{5}$ in the latter results in $\\frac{1393}{985}$ which is less than $4\\times 10^{-7}$ below $\\sqrt{2}$.\nSecond idea.  For this idea I will assume that $a\\geq 1$.  As you already found out we have $$a < \\sqrt{2} < \\frac{a^2+2}{2a}.$$\nTherefore we could try to take some weighted average of $a$ and $\\frac{a^2+2}{2a}$ that ends up below $\\sqrt{2}$.  So we're looking for some factor $\\lambda \\in (0,1)$ such that\n$$\n\\lambda a + (1-\\lambda)\\frac{a^2+2}{2a} < \\sqrt{2}\n$$\nfor all $a \\in [1, \\sqrt{2})$.  It is not difficult to show that this is the case exactly if $\\lambda \\in (3 - 2\\sqrt{2}, 1)$.  We can take $\\lambda = \\frac{1}{5}$ or $\\lambda = \\frac{5}{29}$ for example to get the iterations\n$$\na \\leftarrow \\frac{3 a^2+4}{5a} \\textrm{ and } a \\leftarrow \\frac{17 a^2 + 24}{29a}\n$$\nrespectively.\n",
    "tags": [
      "proof-writing",
      "rational-numbers"
    ],
    "score": 4,
    "answer_score": 1,
    "is_accepted": true,
    "question_id": 464009,
    "answer_id": 464325
  },
  {
    "theorem": "Prove that $A \\mathop \\triangle C \\subseteq (A \\mathop \\triangle B)\\cup (B \\mathop \\triangle C)$",
    "context": "Suppose A, B, and C are sets, prove that $A \\mathop \\triangle C \\subseteq (A \\mathop \\triangle B)\\cup (B \\mathop \\triangle C)$\nI'm just wondering if this proof is ok, or if I'm overlooking something, thanks!\nProof. Suppose $x \\in A \\mathop \\triangle C$. If $x \\in A \\mathop \\triangle B$, then clearly $x \\in (A \\mathop \\triangle B) \\cup (B \\mathop \\triangle C)$, so we will consider the case when $x \\notin A \\mathop \\triangle B$. Suppose $x \\notin A \\mathop \\triangle B$. This means that $x \\in A \\leftrightarrow x \\in B$. Since $x \\in A \\mathop \\triangle C$, either $x \\in A \\mathop \\backslash C$ or $x \\in C \\mathop \\backslash A$. We will consider both cases. \nCase 1: $x \\in A \\mathop \\backslash C$. So $x \\in A$ and $x \\notin C$. Since $x \\in A$ and $x \\in A \\leftrightarrow x \\in B$, it follows that $x \\in B$. Then since $x \\notin C$, $x \\in B \\mathop \\backslash C$, so $x \\in B \\mathop \\triangle C$. Hence, $x \\in (A \\mathop \\triangle B)\\cup (B \\mathop \\triangle C)$\nCase 2: $x \\in C \\mathop \\backslash A$. So $x \\in C$ and $x \\notin A$. Then since $x \\notin A$ and $x \\in A \\leftrightarrow x \\in B$, it follows that $x \\notin B$. Thus, $x \\in C \\mathop \\backslash B$, so $x \\in B \\mathop \\triangle C$, and hence, $x \\in (A \\mathop \\triangle B)\\cup (B \\mathop \\triangle C)$. \nEdit: Also, I would be interested to see any alternative methods of proving this.\n",
    "proof": "Here is an alternative:\n$$A\\triangle C=A\\triangle\\varnothing\\triangle C=A\\triangle(B\\triangle B)\\triangle C=(A\\triangle B)\\triangle(B\\triangle C)\\subseteq(A\\triangle B)\\cup(B\\triangle C)$$\n",
    "tags": [
      "elementary-set-theory",
      "proof-writing"
    ],
    "score": 4,
    "answer_score": 7,
    "is_accepted": true,
    "question_id": 350466,
    "answer_id": 350494
  },
  {
    "theorem": "Uniqueness of primitives",
    "context": "We all know that a primitive $F(x)$ of a function $f(x)$ is the function which derivative is $f(x)$. For example:\n$$\\int 2x dx = x^2 + C$$\nwhere $x^2 + C$ is the set of all primitives of the function. However, how can I be sure that $x^2 + C$ is the only possible solution? How can I be sure that a function $f(x) \\ne x^2 + C\\ ,\\ f'(x) = 2x$ does not exist?\nHow could I create a mathematical proof of this fact (in this specific case and in the general case)? Does it make sense to seek such a proof? If not, why?\n",
    "proof": "What you said is a consequence of the following:\nIf $f^{\\prime}=g^{\\prime}$ then $f=g+c$ for some constant $c\\in \\mathbb{R}$. \nTo prove this let $h=f-g$. Then $h$ is differentiable and $h^{\\prime}=f^{\\prime}-g^{\\prime}=0$. $h$ is therefore constant (*), that is $\\exists c\\in \\mathbb{R}:h(x)=c$. Then, $f(x)=g(x)+c$\nTherefore, a continuous function always has a primitive, unique up to additive constant.\n(*) This is an important consequence of the Mean Value Theorem.\nHere is a proof of that\nLet $f$ be continuous in $[a,b]$ and differentiable in $(a,b)$ so that $f^{\\prime}\\equiv 0$. Let $x,y\\in [a,b]$ and WLOG $x<y$. By the MVT in $[x,y]$,\n\\begin{equation}\\exists \\xi \\in (x,y):f^{\\prime}(\\xi)=\\frac{f(y)-f(x)}{y-x}\\implies\\frac{f(y)-f(x)}{y-x}=0\\implies f(y)=f(x)\\end{equation}\nand so $f$ is constant\n",
    "tags": [
      "calculus",
      "proof-writing"
    ],
    "score": 4,
    "answer_score": 8,
    "is_accepted": true,
    "question_id": 262763,
    "answer_id": 262766
  },
  {
    "theorem": "How to prevent ambiguity that arises from multiple conditional statements in one sentence?",
    "context": "Conditional statements are important when writing mathematical proofs/articles. However, I have found that it is sometimes important to be able to have multiple conditional statements in the same sentence when trying to prove something. Consider statements $a$, $b$, $c$ and $d$ and consider the following example sentence that is dependent on the result of some previous sentence (this is why the sentence starts with \"Hence\"):\n\n\"Hence, $a$ if $b$ and $c$ if $d$.\"\n\nThis can be interpreted as meaning ($a$ if $b$) and ($c$ if $d$), (which is usually my intention), or as (($b$ and $c$) if $d$) and ($a$ if ($b$ and $c$)). Another seemingly valid interpretation is that ($a$ if ($b$ and $c$)), which is only true if $d$.\nHow might one restructure the sentence so that the intended meaning is clear (($a$ if $b$) and ($c$ if $d$))? One could separate the conditional statements into their own sentences. But the sentence starts with \"Hence\" and it wouldn't make sense to separate out the conditional statements into separate sentences that each begin with \"Hence\". E.g. consider the modified version of the original sentence:\n\n\"Hence, $a$ if $b$. Hence, $c$ if $d$.\"\n\nThis version of the sentence does not make sense because the second sentence in this version is not dependent on the result of the first sentence in this version, it is dependent on the sentence before the original sentence.\nI have also considered the use of semi-colons, colons or additional commas. However, it is not completely clear to me if these methods make sense. Consider the following sentence:\n\n\"Hence, $a$ if $b$, and $c$ if $d$.\"\n\nIt seems like this extra comma removes some ambiguity. As does the following sentence:\n\n\"Hence: $a$ if $b$; $c$ if $d$.\"\n\nHowever, I'm unsure if this is a correct use of semi-colons and colons, and I'm unsure if the use of the comma completely removes any ambiguity. Could the comma be interpreted as simply an oxford comma in the list of hypotheses for the first conditional statement? I've also seen the suggestion that you can parenthesize the sentences. However, I never see this in any journals and articles in my field.\nAny ideas or strategies that can help remove ambiguity?\n",
    "proof": "\n\n\"Hence, for any $(a,b,c,d) \\in A$, $a=1$ if $b=1$ and $c=1$ if $d=1$.\"\n\n\nNon-ambiguous alternatives:\n\nHence, for every $(a,b,c,d) \\in A$, it is both the case that $a=1$ if $b=1$ and that $c=1$ if $d=1.$\n\nHence, for every $(a,b,c,d) \\in A$, it is both the case that $b=1$ implies $a=1$ and that $d=1$ implies $c=1.$\n\nEthan's suggestion in the other answer.\n\n\n\nThis can be interpreted as $(a=1 \\text{ if } b=1)$ and $(c=1 \\text{ if } d=1)$ (which is usually my intention),\n\n\nor as $((b=1 \\land c=1) \\text{ if } d=1)$ together with $(a=1 \\text{ if } (b=1 \\land c=1)).$\n\n\nAnother seemingly valid interpretation is: $(a=1 \\text{ if } (b=1 \\land c=1))$ is true if $d=1.$\n\nWhile the sentence $$P⟹Q⟹R$$ would be informally understood as \"$(P⟹Q)$ and $(Q⟹R)$\", the sentence $$P \\text{ if } Q \\text{ if } R$$ is simply ambiguous, and not typically understood as \"$(P$ if $Q)$ and $(Q$ if $R)$\".\nThe most charitable reading of $$a=1 \\text{ if } b=1 \\text{ and } c=1 \\text{ if } d=1$$ is your intended one, as opposed to (a=1 if (b=1 and c=1)) if d=1, ((a=1 if b=1) and c=1) if d=1, etc.\n",
    "tags": [
      "logic",
      "proof-writing",
      "article-writing"
    ],
    "score": 4,
    "answer_score": 3,
    "is_accepted": true,
    "question_id": 4957193,
    "answer_id": 4957228
  },
  {
    "theorem": "Proof that $i$ is not a real number",
    "context": "This will not be a very rigorous proof of that $i=\\sqrt{-1}$ is not a real number.\nUsing the properties of the $\\mathbf{R}$:\n\nThe real numbers are closed under multiplication\nThe real numbers can negative, zero or positive\n\nAssuming that $i\\in \\mathbf{R}$,\nTherefore working on three cases:\nCase 1: $i=0$\nIf $i=0 \\Rightarrow i\\cdot i=0 \\cdot i\\Rightarrow -1=0$ which is clearly wrong.\nHence $i\\not=0$\nCase 2: $i>0$\nIf $i>0 \\Rightarrow i\\cdot i>0 \\cdot i\\Rightarrow -1>0$ which is once again clearly wrong\nHence $i\\not>0$\nCase 3: $i<0$\nIf $i<0 \\Rightarrow i\\cdot i>0 \\cdot i\\Rightarrow -1>0$\nHence $i\\not<0$\nHenceforth $i$ is not positive, neither is it negative nor is it zero.\nThis can only be possible if $i\\notin \\mathbf{R}$\nI'd like to know if this is a good proof if it was backed up group theory.\n",
    "proof": "This is correct, but there is no need to consider $3$ different cases. You could just say that if $i\\in\\Bbb R$, then $i^2\\geqslant0$. But this is false, since $i^2=-1$.\n",
    "tags": [
      "complex-numbers",
      "proof-writing",
      "solution-verification"
    ],
    "score": 4,
    "answer_score": 5,
    "is_accepted": false,
    "question_id": 4089803,
    "answer_id": 4089804
  },
  {
    "theorem": "How many unique &quot;$\\phi$-nary&quot; expansions are there for $1$?",
    "context": "I was playing around the expansions of numbers in irrational bases, namely base $\\phi=\\frac{1+\\sqrt5}{2}$. Of course, I should immediately define what it means to symbolize digits in a non-integer base.\nAt least in my case, the expansions consist of $\\lceil\\phi\\rceil=2$ unique digits, (0 & 1). Hence, I've dubbed it \"phi-nary\".\nDue to the base being the golden ratio, it carries along several unique properties, such as\n$$1.1_\\phi=10_\\phi=\\phi$$\nWhich got me thinking: This base is able to express a number in multiple unique terminating expansions! Immediately, I was curious to see how many there were for 1.\nI found these 3:\n$$1_\\phi=0.11_\\phi=0.1011_\\phi$$\nUsing $\\phi^2=\\phi+1$ and $\\phi^{-1}=\\phi-1$, here's the proof for $0.11_\\phi$:\n$0.11_\\phi=\\phi^{-1}+\\phi^{-2}=(\\phi-1)+(\\phi^{-1})^2=(\\phi-1)+(\\phi-1)^2=(\\phi-1)+(\\phi^2-2\\phi+1)=-\\phi+(\\phi+1)=1$\nThe third expansion follows the same modes of deduction.\nI also found the non-terminating expansion $0.\\bar{10}_\\phi=1$\n\nMy intuition tells me there are a (countably) infinite amount, but I do not know how to go about proving that. Are those the only three terminating expansions?\n\n\nIn other words, in general for what $S\\subset\\mathbb{Z}$ does $$\\sum_{k\\in S}\\phi^k=1$$\n",
    "proof": "There are countably infinitely many finite expansions. For starting with $1$, we can replace the terminating $1$ in the $n$th phi-nimal place by $011$ in the $n$th, $n+1$th, and $n+2$th places respectively.\nNow suppose given an infinite binary sequence $b$ such that $\\sum b_n \\phi^{-n} = 1$. Consider the following possibilities:\n\n$b_0 = 1$. Then $b$ is a single $1$ followed by infinite zeroes.\n\n$b_0 = 0$ and $b_1 = 1$. Then we have $\\sum b_{n + 2} \\phi^{-n} = 1$.\n\n$b_0 = 0$ and $b_1 = 0$. Then we have $\\sum b_{n + 2} \\phi^{-n} \\leq \\frac{1}{1 - \\phi^{-1}} = \\phi^2$, and equality can only hold when every $b_i$ for $i \\geq 2$ is 1.\n\n\nThus, it is apparent that either\n\n$b$ is the alternating sequence $0, 1, 0, 1, ...$\n$b$ begins with a prefix of the sequence $0, 1, ...$ but eventually terminates with a $1$ in an evenly indexed position\nor\n$b$ begins with a prefix of the alternating sequence $0, 1, ...$ but eventually has a $0$ in an odd-indexed position, followed by an endless sequence of $1$s\n\nSo the set of all $\\phi$-nary representations of $1$ is countable.\n",
    "tags": [
      "proof-writing",
      "irrational-numbers"
    ],
    "score": 4,
    "answer_score": 5,
    "is_accepted": true,
    "question_id": 3777370,
    "answer_id": 3777400
  },
  {
    "theorem": "How can I prove mathematically the reflection matrix has only the eigenvalues 1 or -1?",
    "context": "Specifically where S is a subspace of $R^n$. $P_S$ is the orthogonal projection onto $S$.\nand the reflection matrix $ M = I - 2P_S$\nI understand a similar proof where the eigenvalues of the projection matrix is either 0 or 1. Now trying to get the intuition for the reflection matrix (M) case.\nThis is the proof for projection matrices that I have seen:\n\n$$Px = \n\\lambda x $$\n$$P^2 = P$$\n$$P^2x = \\lambda x$$\n$$P(Px) = \\lambda x$$\n$$\\lambda^2x = \\lambda x$$\n$$\\lambda(\\lambda -1)x = 0$$\n\n",
    "proof": "$M=I-2P$. Then $M^2=(I-2P)(I-2P)=I^2-4P+4P^2=I-4P+4P=I$ and $M^2x=λ^2x=Ix=x$. Then $λ^2=1$. Hence $λ=±1$\n",
    "tags": [
      "linear-algebra",
      "proof-writing",
      "eigenvalues-eigenvectors",
      "reflection"
    ],
    "score": 4,
    "answer_score": 5,
    "is_accepted": true,
    "question_id": 3384148,
    "answer_id": 3384172
  },
  {
    "theorem": "If $f \\circ f = 0 $, show that transformations $f + id_x$ and $f - id_x$ are isomorphisms of $X$",
    "context": "I have a problem with this task: \nThe linear transformation $f \\in L(X,X)$ has property $f \\circ f = 0 $\nShow that transformations $f + id_x$ and $f - id_x$  are isomorphisms of $X$ space with itself\n\nIf I need to be honest I have no idea how to prove this. I was tryinging something like that: \nIf $f + id_x$ is isomorphism it must be both injective and surjective.\nOk, but $id_x$ is injective and surjective. \n I thought to show that f is surjective but unfortunately sum of two surjectives can give me something what is not surjective... \nMaybe the key is in $f \\circ f = 0 $?\nThanks for your time!\n",
    "proof": "$f - id_X$ is an isomorphism iff $-(f - id_X) = id_X - f$ is an isomorphism.\nWe have\n$$[(f + id_X) \\circ (id_X - f)](x) = ((f +id_X)((id_X - f)(x))) = (f +id_X)(x - f(x)) = (f + id_X)(x) - (f + id_X)(f(x)) = f(x) + x -f(f(x)) - f(x) = x,$$\ni.e. $(f + id_X) \\circ (id_X - f) = id_X$. Similarly$(id_X - f) \\circ (f  + id_X  = id_X$.\nThis shows that $f + id_X, id_X - f$ are inverse isomorphisms.\n",
    "tags": [
      "linear-algebra",
      "proof-writing"
    ],
    "score": 4,
    "answer_score": 3,
    "is_accepted": true,
    "question_id": 3062226,
    "answer_id": 3062240
  },
  {
    "theorem": "Every group with order prime power $p^n$ then it has subgroup of order $p^k$ for $0\\leq k \\leq n$",
    "context": "\nEvery group with order prime power $p^n$ then it has subgroup of order $p^k$ for $0\\leq k \\leq n$\n\nMy Attempt:\nBy Mathematical Induction (Strong),\n\nAs Every subgroup of order $p$ , it is cyclic and it is obvious .\nFor group of order $p^2$ \nCase 1: If cyclic , done\n\nCase 2: If not then by $G/Z(G)$  theorem , its $Z(G)$ has order $p$.\nSo done \nAssume for any group of order $p^n$ it is true. And assume it also true for any $k\\leq n$\nTO prove for group of order $p^{n+1}$\nAS from application of conjugacy class, its centre is nontrivial SO its order must be $p^a$ And by strong mathematical induction we have a subgroup of order $p^t$ for $0\\leq t \\leq a$\nNow I can not able to show this for $n+1\\geq k>a$\nI thought to use fact that $Z(G)$ is normal so $G/Z(G)$ is the group but I did not get much .\nAny Help will be appreciated\n",
    "proof": "Assume the statement is true for any group of order $p^{n-1}$ when $n\\geq 2$ and let $G$ be a group of order $p^n$. As you said the center is not trivial. So by Cauchy's theorem we know there is an element $x\\in Z(G)$ of order $p$. $x$ is in the center so $g\\langle x\\rangle g^{-1}=\\langle x \\rangle$ for all $g\\in G$. Hence $\\langle x \\rangle \\triangleleft G$ and we can look on the quotient group $G/\\langle x \\rangle$ which has order $p^{n-1}$. By induction $G/\\langle x \\rangle$ has subgroups $H_0, H_1,...H_{n-1}$ when $H_k$ has order $p^k$. \nNow let's define the projection homomorphism $\\pi:G\\to G/\\langle x \\rangle$ by $\\pi(g)=g\\langle x\\rangle$. For each $0\\leq k\\leq {n-1}$ we have $\\pi^{-1}(H_k)\\leq G$. What is the order of $\\pi^{-1}(H_k)$? Well, by our assumption $H_k$ has $p^k$ left cosets of $\\langle x \\rangle$, and we know that each coset contains $p$ elements. So there are $p^{k+1}$ elements $g\\in G$ that satisfy $g\\langle x\\rangle\\in H_k$. Hence $|\\pi^{-1}(H_k)|=p^{k+1}$. So that way we found subgroups of $G$ of orders $p,p^2,p^3,...,p^n$ and of course there is also the trivial group of order $p^0$. \n",
    "tags": [
      "abstract-algebra",
      "group-theory",
      "proof-writing",
      "finite-groups"
    ],
    "score": 4,
    "answer_score": 11,
    "is_accepted": true,
    "question_id": 2936407,
    "answer_id": 2936422
  },
  {
    "theorem": "Generalized Pigeonhole Principle Proof",
    "context": "From my book Discrete Mathematics by Rosen, I can't understand the conclusion of the proof.\nTHE GENERALIZED PIGEONHOLE PRINCIPLE: If N objects are placed into k boxes, then there is at least one box containing at least ⌈N/k⌉ objects.\nProof by contradiction:\nSuppose that none of the boxes contains more than ⌈N/k⌉ objects. Then, the total number of objects is at most ⌈N/k⌉-1 objects. \n$$⌈N/k⌉ < (N/k) + 1$$\n$$k(⌈N/k⌉ - 1) < k[(⌈N/k⌉+1)-1] = N$$\nThis is a contradiction because there are a total of N objects.\nI don't understand how that inequality shows it's a contradiction, how did they get that the inequality shows less than N objects?\n",
    "proof": "The claim is that at least one of the $k$ boxes contain at least $\\lceil N/k\\rceil$ objects.\nThe proof goes by contradiction: Suppose the claim is false, then each box must have strictly less than $\\lceil N/k\\rceil$ objects, i.e., at most $\\lceil N/k\\rceil-1$ objects (the greatest integer strictly less than $n$ is $n-1$)\nNow, then since there are $k$ boxes and each box has objects $\\leq\\lceil N/k\\rceil-1$, the total number of objects from all the boxes is $\\leq k(\\lceil N/k\\rceil-1)\\lt k\\cdot N/k=N$ (we use $\\lceil x\\rceil-1\\lt x$) which gives us a contradiction since the total number of objects from all the boxes cannot be strictly less than $N$ (since $N$ is the total number of objects from all the boxes).\nNotice the one single $\\lt$ in the chain of inequalities in the penultimate step which makes the overall inequality strict, i.e, gives us $N\\lt N$\n\nHere's an informal (intuitive) way to interpret the theorem:\nWe usually look at the worst case scenario. If we want to keep the number of objects in each box as less as possible when putting the objects in the boxes, we can do this by putting one object in each box and then going over to the next box and not fill a previous box unless all the boxes have the same number of objects. We put an object in the 1st box, then one in the 2nd,.. and so on till the $k$th box until all the boxes have one object and then we repeat the above.\nIf $N$ is a multiple of $k$, then we'd have $N/k$ objects in each box at the end.\nOtherwise, we'll have the first $x$ boxes with $\\lceil N/k\\rceil$ objects where $x$ is the remainder when $N$ is divided by $k$ and the rest of the boxes will have exactly one less object than the first $x$ boxes.\n",
    "tags": [
      "discrete-mathematics",
      "proof-writing",
      "proof-explanation"
    ],
    "score": 4,
    "answer_score": 8,
    "is_accepted": true,
    "question_id": 2525067,
    "answer_id": 2525133
  },
  {
    "theorem": "Prove that $f(x) =\\sqrt{x}$ is uniformly continuous on $[0, \\infty)$",
    "context": "My professor did this problem in our Q&A session last night and his method was long and very involved.  I said I would email him my proof and ask for feedback, but when I did I got an out of office reply.  Our assignment is due before he returns.  So I'm asking here.\nDoes this proof work?  It's much simpler than what he did online and that left me feeling unsure about this.  If it doesn't work, can you tell me why not?  Thank you!\n\nMy new proof:\n\n",
    "proof": "You are correct. Here is the proof written in a different way.\nLet $x,y \\in [0,+\\infty)$. Notice that $\\left|\\sqrt{x}-\\sqrt{y} \\right|\\leq \\left|\\sqrt{x}+\\sqrt{y}\\right|$. Choosing $\\delta:=\\varepsilon^2$ in the definition, we get that if $|x-y|<\\delta$ then $$\\left|\\sqrt{x}-\\sqrt{y} \\right|^2\\leq \\left|\\sqrt{x}-\\sqrt{y} \\right|\\cdot \\left|\\sqrt{x}+\\sqrt{y} \\right|=|x-y|<\\varepsilon^2 \\implies \\left|\\sqrt{x}-\\sqrt{y}\\right|<\\varepsilon,$$ hence $x \\mapsto \\sqrt{x}$ is uniformely continious in $[0,+\\infty)$.\n",
    "tags": [
      "real-analysis",
      "proof-verification",
      "proof-writing",
      "uniform-continuity"
    ],
    "score": 4,
    "answer_score": 9,
    "is_accepted": true,
    "question_id": 2492762,
    "answer_id": 2492775
  },
  {
    "theorem": "Proving that a solution to the &quot;Cat behind 7 doors&quot; puzzle is optimal",
    "context": "\nThe \"Cat behind 7 Doors\" Puzzle. \nThere are 7 doors on a corridor, and a cat is behind one of them. We are trying to find the cat. Interesting thing is that, whenever the door we open is empty, we must close it after, and the cat must move to the door adjacent to him (1 right or 1 left). The cat can move to the door we have just closed.\n\nI was able to find that the solution for the number of trials for $n$ number of doors (when $n$ is larger than 3) is $2(n-2)$. And the order in which we do the trials is starting with 2nd door and going one by one until the $(n-1)$th door, repeating $(n-1)$ and going back to the 2nd door. For example for $5$ doors, the trials are $2$, $3$, $4$, $4$, $3$, $2$. \nI understand why this solution is correct. However, I am having a hard time proving if/why this solution is optimal, and I'm not even sure how to make a start on it. Do I use proof by induction? Do I use the formula? Any answers/help will be very much appreciated! \n",
    "proof": "We need to go through everything and see where it leads us, I will try to use as little mathematical notation as I possibly can get away with. Describing these problems with words are more fun in my opinion. \nFirst off we realize that the cat always moves from an odd numbered door to an even numbered door or vice versa. \nSo let's assume that the cat starts off in an even numbered door, i.e. any of the doors {$2$ or $4$ or $6$}. \n\nCheck door $2$, if the cat is there you win, else it was in {$4$ or $6$} and will move to {$3$ or $5$ or $7$}\nCheck door $3$, if the cat is there you win, else it was in {$5$ or $7$} and will move to {$4$ or $6$}\nCheck door $4$, if the cat is there you win, else it was in door $6$ and will move to {$5$ or $7$}\nCheck door $5$, if the cat is there you win, else it was in door $7$ and will move to $6$\nCheck door $6$, find the cat\n\nSo these steps will always work IF the cat started off in an even numbered door. Let's go through everything again and see what happens if it actually started off in an odd numbered door instead, i.e. door {$1$ or $3$ or $5$ or $7$}:\n\nWe checked door $2$, we didn't find it. The cat move from {$1$ or $3$ or $5$ or $7$} to {$2$ or $4$ or $6$}\nWe checked door $3$, we didn't find it. The cat move from {$2$ or $4$ or $6$} to {$1$ or $3$ or $5$ or $7$}\nWe checked door $4$, we didn't find it. The cat move from {$1$ or $3$ or $5$ or $7$} to {$2$ or $4$ or $6$}\nWe checked door $5$, we didn't find it. The cat move from {$2$ or $4$ or $6$} to {$1$ or $3$ or $5$ or $7$}\nWe checked door $6$, we didn't find it. The cat move from {$1$ or $3$ or $5$ or $7$} to {$2$ or $4$ or $6$}\n\nIf the cat starts off in an odd numbered door, after we've done the first round, the cat will just be in an even numbered door! This means we can just go back to the first round, and that second time we are guaranteed to find the cat.\nSo compiling this information for an odd number of doors:\nStart with door $2$, increment with $1$ until you reach door $n-1$. If the cat started off in an even numbered door, you will have found it by now; else it started off in an odd numbered door and you just have to repeat the process one more time, which will make you find it.\nThis will indeed give a worst-case scenario of $\\mathcal{O}(n)$, that is if you have to check all doors from $2 \\Rightarrow (n-1)$ twice, which is $n-2$ doors twice, which is $2(n-2)$ or  $\\mathcal{O}(n)$.\nAlso notice that when we go through the first round, we always pick the door that will minimize the number of doors that the cat can move to; giving the cat as few doors as possible that it can move to. This will yield us the optimal solution.\n",
    "tags": [
      "optimization",
      "proof-writing",
      "puzzle"
    ],
    "score": 4,
    "answer_score": 1,
    "is_accepted": true,
    "question_id": 2435154,
    "answer_id": 2435207
  },
  {
    "theorem": "Prove $A \\subset \\emptyset \\iff A = \\emptyset$",
    "context": "How does one prove this? Can one prove by contradiction by saying:\nLet $A$ be any set such that $A$ contains at least one element. Now assume $A \\subset \\emptyset$. This is clearly absurd by the definition of $\\emptyset$, so $A$ is not a subset of the empty set, and the only subset of $\\emptyset$ is $A$ such that $A = \\emptyset$.\nDoes this reasoning make sense? Help is much appreciated, this is my first time writing proofs.\n",
    "proof": "The definition of $\\varnothing$ refers to its lack of elements, it says nothing about its subsets, so although it is clearly absurd that a nonempty set should be a subset of $\\varnothing$, it's not enough to say \"it is clearly absurd\"... at least, not for a proof of such a low-level statement.\nSomething along the following lines would be better: suppose $A \\ne \\varnothing$. Then there exists $x \\in A$. But since $A \\subseteq \\varnothing$, it follows that $x \\in \\varnothing$, which is absurd by definition of $\\varnothing$.\nNotice that this proof differs from yours in that the contradiction ('absurdity') follows from statements about elements instead of about subsets.\n(Of course, a proof that doesn't use contradiction follows from the easy-to-prove fact that $\\varnothing \\subseteq A$ and by definition of set equality.)\n",
    "tags": [
      "elementary-set-theory",
      "proof-writing",
      "proof-verification"
    ],
    "score": 4,
    "answer_score": 8,
    "is_accepted": true,
    "question_id": 870344,
    "answer_id": 870353
  },
  {
    "theorem": "Prove determinant of $n \\times n$ matrix is $(a+(n-1)b)(a-b)^{n-1}$?",
    "context": "Prove $\\det(A)$ is $(a+(n-1)b)(a-b)^{n-1}$ where $A$ is $n \\times n$ matrix with $a$'s on diagonal and all other elements $b$, off diagonal.\n",
    "proof": "Note that for $\\lambda=a-b$ we have\n$$\nA-\\lambda I\n=\n\\begin{bmatrix}\nb      & \\cdots & b \\\\\n\\vdots & \\ddots & \\vdots \\\\\nb      & \\cdots & b\n\\end{bmatrix}\\tag{1}\n$$\nThe matrix in $(1)$ has rank $1$ so its nullspace has dimension $n-1$. Hence $\\lambda_1=a-b$ is an eigenvalue of $A$ whose geometric multiplicity is $n-1$.\nNow, note that for $\\lambda=a+(n-1)b$ we have\n$$\nA-\\lambda I=\n\\begin{bmatrix}\n(1-n)b & \\cdots & b \\\\\n\\vdots & \\ddots & \\vdots \\\\\nb      & \\cdots & (1-n)b\n\\end{bmatrix}\\tag{2}\n$$\nThe columns of the matrix in $(2)$ sum to $0$ so it has a nontrivial nullspace.\nThat is, $\\lambda_2=a-(n-1)b$ is an eigenvalue for $A$. Because $\\lambda_1$ has geometric multiplicity $n-1$, the geometric multiplicity of $\\lambda_2$ must be $1$. Hence the characteristic polynomial of $A$ is\n$$\n\\det(A-\\lambda I)=(\\lambda_1-\\lambda)^{n-1}(\\lambda_2-\\lambda)\\tag{3}\n$$\nFinally, plugging in $\\lambda=0$ into $(3)$ gives the result.\n",
    "tags": [
      "linear-algebra",
      "matrices",
      "proof-writing",
      "determinant"
    ],
    "score": 4,
    "answer_score": 5,
    "is_accepted": false,
    "question_id": 757320,
    "answer_id": 757342
  },
  {
    "theorem": "Do you need to find the domain of a function to prove injectivity?",
    "context": "I teach math and have had this debate with a fellow teacher. I believe the domain of a function is really not important to determine if a function is or is not injective if there is no \"real life\" context.\nWhat do you think? \nwould you find the domain of a function like\n$f(x)=1-\\ln(2-e^{2x})$ \nto show if the function is injective? If so, why?\n",
    "proof": "By definition functions have domains, proving injectivity without considering the domain is something devoided of sense, the domain of a function is part of its essence.\nIn practice there can be times in which whatever the domain is, the proofs will look the same (if the functions are in fact injective), but they are necessarily different because the 'first' step in proving injectivy of a function is taking 'two' arbitrary elements on its domain. You can't do this if you don't focus on the domain.\nThis doesn't mean that you have to 'find' the domain of a function to prove it is injective. Taking your example and letting $D_f$ denote the domain of $f$, the aforementioned first step would just be: let $x,y\\in D_f\\,\\ldots$\n",
    "tags": [
      "functions",
      "proof-writing"
    ],
    "score": 4,
    "answer_score": 7,
    "is_accepted": true,
    "question_id": 694953,
    "answer_id": 694960
  },
  {
    "theorem": "Complex numbers - proof",
    "context": "Let z and w be complex numbers such that $|z| = |w| = 1$ and $zw \\neq -1$. Prove that $\\frac{z + w}{zw + 1}$ is a real number.\nI let z = a + bi and w = c+ di so we have that $\\sqrt{a^2+b^2} = \\sqrt{c^2+d^2} = 1$ and $a^2+b^2 = c^2 + d^2 = 1$. I plugged it into the equation but I didn't really get anything worth noting. Does anyone know how to solve this? Thanks\n",
    "proof": "Since $|z|=|w|=1$, \n$$z\\bar z=w\\bar w=1.$$\nLetting $u$ be the given number, \n$$u=\\frac{z+w}{zw+1}=\\frac{(1/\\bar z)+(1/\\bar w)}{zw+1}=\\frac{\\bar w+\\bar z}{\\bar z\\bar w(zw+1)}=\\frac{\\bar z+\\bar w}{\\bar z\\bar w+1}=\\bar u.$$\n",
    "tags": [
      "complex-numbers",
      "proof-writing"
    ],
    "score": 4,
    "answer_score": 6,
    "is_accepted": true,
    "question_id": 632765,
    "answer_id": 632780
  },
  {
    "theorem": "Are there axioms in a natural deduction system?",
    "context": "In the Hilbert system, a proof may include some axioms. In a natural deduction system, it seems no axiom is involved, at least from the examples I read in logic books. So, I wonder how axioms such as those for ZFC play a role in a natural deduction system. Thanks.\n",
    "proof": "Hilbert system and natural deduction can be seen as two \"dual\" approaches to writing formal proofs. Roughly, when it comes to deriving formulas that are valid in a specific logical system (classical logic, intuitionistic logic, and so on), the Hilbert system has many logical axioms and minimizes the number of inference rules (in the propositional case, the only inference rule is modus ponens), while natural deduction only uses inference rules and has no axioms.\nIn natural deduction, a logical derivation can be represented as a tree-like structure whose leaves $A_1, \\dots, A_n$ (except the discharged ones, but let us abstract from that for the moment) are the assumptions or hypotheses, and whose root $B$ is the conclusion or thesis.  It means that \"from $A_1, \\dots, A_n$ one can derive $B$\", often noted $A_1, \\dots, A_n \\vdash B$. If you look at your derivation top-down, you start from the hypotheses $A_1, \\dots, A_n$ and you conclude $B$ by only repeatedly using the inference rules of natural deduction (which are syntactical manipulations of the logic symbols$-$connectives and quantifiers$-$occurring in a formula), without using any axioms.\nThe special case $n = 0$ means that $B$ is a logically valid formula (also known as a tautology in propositional classical logic), which is derived without any assumptions (as well as without any axioms): indeed, in the derivation proving $B$, the leaves are not \"active\" anymore, since all of them are been discharged by inference rules such as $\\to_\\text{intro}$ occurring in the derivation.\nBy the way, note that proving $A_1, \\dots, A_n \\vdash B$ is equivalent to prove $\\vdash (A_1 \\land \\dots \\land A_n) \\to B$.\nWhat happens when you want to use a formal system to write proofs in a specific mathematical theory (for instance, set theory as axiomatized in ZFC) and not in a purely logical framework? The point is that to formalize a mathematical theory, you have to manipulate non-logical symbols (such as $\\in$ in ZFC), and you need to \"teach\" your formal system$-$which is purely syntactical$-$how to manipulate them. The most natural and common way to do that in a formal system such as Hilbert calculus or natural deduction is to add non-logical axioms that are specific to that mathematical theory (for instance, the axioms of ZFC).\nBy \"non-logical\" I mean that the axioms of the mathematical theory are not logically valid formulas (otherwise there would be no need to add them) and they talk about basic properties that hold only in that mathematical theory.\nIn natural deduction, adding axioms means that you add $0$-ary inference rules, that is, inference rules with no premises and whose conclusion is one of the non-logical axioms. This way, a natural deduction derivation within a mathematical theory $\\mathcal{T}$ (such as ZFC), where $\\mathcal{T}$ is the set of axioms formally defining the mathematical theory, is a tree-like structure whose leaves $A_1, \\dots, A_n, A_{n+1}, \\dots, A_m$ are either assumptions $A_1, \\dots, A_n$ or axioms $A_{n+1}, \\dots, A_m \\in \\mathcal{T}$, and whose root $B$ is the conclusion.  It means that \"from $A_1, \\dots, A_n$ one can derive $B$, through the axioms $A_{n+1}, \\dots, A_m \\in \\mathcal{T}$\", often noted $A_1, \\dots, A_n \\vdash_\\mathcal{T} B$.\nOf course, the case $m = n$ means that your derivation never used any axioms in $\\mathcal{T}$, and so $B$ is a logical consequence of the assumptions $A_1, \\dots, A_n$, independently of the theory $\\mathcal{T}$.\nThere is an important technical difference between the assumptions $A_1, \\dots A_n$ and the axioms $A_{n+1}, \\dots, A_m$: the former can be discharged by logical inference rules such as $\\to_\\text{intro}$, the latter cannot.\nThere is an alternative view to deal with formal proofs of a mathematical theory $\\mathcal{T}$ in natural deduction. You do not distinguish between axioms in $\\mathcal{T}$ and assumptions, all the leaves of the derivation are considered as assumptions (which can be discharged by logical inference rules such as $\\to_\\text{intro}$). Technically, the tree-like structure of a natural deduction derivation is the same as in the previous viewpoint, what changes is the way you look at it. In particular, with this viewpoint, you lose the distinction between the hypotheses that are used to logically derive the thesis, and the non-logical axioms of the mathematical theory.\nThanks to this lack of distinction, it still holds that proving $A_1, \\dots, A_n, A_{n+1}, \\dots, A_m \\vdash B$ is equivalent to prove $\\vdash (A_1 \\land \\dots \\land A_n \\land A_{n+1} \\land \\dots \\land A_m) \\to B$.\n\nIn my opinion, using natural deduction or other formal proof systems to prove theorems in a mathematical theory such as ZFC can be of interest in only two cases:\n\nas a theoretical inquiry as a matter of principle;\n\nto understand how proof assistants (such as Coq, HOL/Isabelle, Agda, and Lean) work to get verified proofs.\n\n\nFor practical purposes, using natural deduction or any other formal systems to prove theorems in any mathematical theory such as ZFC is a tedious operation that yields very long and almost incomprehensible proofs (even when the theorem to prove is a basic property) where is hard to grasp the mathematical ideas behind a stream of a myriad of syntactical manipulations.\nAs an example, consider the (almost unintelligible!) natural deduction derivation below that proves the basic fact\n$$\\forall 𝑥\\forall a \\forall b \\forall c \\, \\big(𝑥 \\in a \\cap (b \\cup c) \\to x \\in (a \\cap b) \\cup (a \\cap c) \\big)$$\nin ZFC, starting from the following \"axioms\" (see also a comment at the end of this section):\n\n$\\text{ax}_1$: $\\ \\forall x \\forall y \\forall z \\, (x \\in y \\cap z \\to (x \\in y \\land x \\in z) )$,\n\n$\\text{ax}_2$: $\\ \\forall x \\forall y \\forall z \\, ((x \\in y \\land x \\in z) \\to x \\in y \\cap z )$,\n\n$\\text{ax}_3$: $\\ \\forall x \\forall y \\forall z \\, (x \\in y \\cup z \\to (x \\in y \\lor x \\in z) )$.\n\n\nSo, a natural deduction derivation proving $\\vdash_{\\text{ax}_1, \\text{ax}_2, \\text{ax}_3} \\forall 𝑥\\forall a \\forall b \\forall c \\, \\big(𝑥 \\in a \\cap (b \\cup c) \\to x \\in (a \\cap b) \\cup (a \\cap c) \\big)$ is the following\n$$\\small\n\\dfrac\n{\n {\\displaystyle [x \\in a \\cap (b \\cup c)]^\\circ \\atop {\\displaystyle{{\\vdots \\pi} \\atop x \\in b \\lor x \\in c}}}\n \\quad\n \\displaystyle{{ [x \\in a \\cap (b \\cup c)]^\\circ, [x \\in b]^*} \\atop\n \\displaystyle{\\vdots \\pi_1 \\atop x \\in (a \\cap b) \\cup (a \\cap c)}}\n \\quad\n \\displaystyle{{ [x \\in a \\cap (b \\cup c)]^\\circ, [x \\in c]^*} \\atop\n \\displaystyle{\\vdots \\pi_2 \\atop x \\in (a \\cap b) \\cup (a \\cap c)}}\n}\n{\\dfrac\n {x \\in (a \\cap b) \\cup (a \\cap c)}\n {\\dfrac{x \\in a \\cap (b \\cup c) \\to x \\in (a \\cap b) \\cup (a \\cap c) }{\\forall x \\forall a \\forall b \\forall c \\big(x \\in a \\cap (b \\cup c) \\to x \\in (a \\cap b) \\cup (a \\cap c) \\big)}\\forall_i \\times 4}\n \\!\\to_i^\\circ\n}\n\\!\\lor_e^*\n$$\nwhere the derivation $\\pi$ proving $ \\\nx \\in a \\cap (b \\cup c) \\vdash_{\\text{ax}_1, \\, \\text{ax}_3} x \\in b \\lor x \\in c$ is the following\n$$\\small\n \\dfrac\n {\n  \\dfrac\n  {\\dfrac{}{\\text{ax}_3}}\n  {x \\in b \\cup c \\to (x \\!\\in\\! b \\lor x \\!\\in\\! c)}\n  \\!{\\scriptstyle \\forall_e \\times 3}\n  \\quad \n   \\dfrac\n   {\n    \\dfrac\n    {\\dfrac{}{\\text{ax}_1}}\n    {x \\in a \\cap (b \\cup c) \\to (x \\!\\in\\! a \\land x \\in b \\cup c)}\n    {\\scriptstyle\\forall_e \\times 3}\n    \\quad { \\atop \\displaystyle{x \\in a \\cap (b \\cup c)}}\n   }\n   {\\dfrac{x \\in a \\land x \\in b \\cup c}{x \\in b \\cup c}\\land_{e_2}}\n   \\!\\!\\to_e\n }\n {x \\in b \\lor x \\in c}\n \\!\\!\\to_e\n$$\nand the derivation $\\pi_1$ proving $ \\\n x \\in a \\cap (b \\cup c), \\, x \\in b \\vdash_{\\text{ax}_1, \\, \\text{ax}_2} x \\in (a \\cap b) \\cup (a \\cap c)$ is the following (with $B = (x \\!\\in\\!a \\land x \\!\\in\\! b) \\to 𝑥 \\in a \\cap b$)\n$$\n\\dfrac\n{\n \\dfrac\n {\\dfrac{}{\\text{ax}_2}}\n {B}\n {\\scriptstyle\\forall_e \\times 3}\n \\quad\n \\dfrac\n   {\n    \\dfrac\n    {\n     \\dfrac\n     {\\dfrac{}{\\text{ax}_1}}\n     {x \\in a \\cap (b \\cup c) \\to (x \\!\\in\\! a \\land x \\in b \\cup c)}\n     {\\scriptstyle\\forall_e \\times 3}\n     \\quad { \\atop \\displaystyle{x \\in a \\cap (b \\cup c)}}\n    }\n    {\\dfrac\n     {x \\in a \\land x \\in b \\cup c}\n     {x \\in a}\n     \\land_{e_1}\n    }\n    \\!\\to_e\n    { \\atop {\\atop \\displaystyle{\\atop \\displaystyle{x \\in b}}}}\n   }\n   {x \\in a \\land x \\in b}\n   \\land_i\n}\n{\\dfrac{x \\in a \\cap b}{x \\in (a \\cap b) \\cup (a \\cap c)}\\lor_{i_1}}\n\\!\\to_e\n$$\nand the derivation $\\pi_2$ proving $ \\\nx \\in a \\cap (b \\cup c),\\, x \\in c \\vdash_{\\text{ax}_1, \\, \\text{ax}_2} x \\in (a \\cap b) \\cup (a \\cap c)$ is the following (with $C = (x \\!\\in\\!a \\land x \\!\\in\\! c) \\to 𝑥 \\in a \\cap c$)\n$$\n\\dfrac\n{\n \\dfrac\n {\\dfrac{}{\\text{ax}_2}}\n {C}\n {\\scriptstyle\\forall_e \\times 3}\n \\quad\n \\dfrac\n   {\n    \\dfrac\n    {\n     \\dfrac\n     {\\dfrac{}{\\text{ax}_1}}\n     {x \\in a \\cap (b \\cup c) \\to (x \\!\\in\\! a \\land x \\in b \\cup c)}\n     {\\scriptstyle\\forall_e \\times 3}\n     \\quad { \\atop \\displaystyle{x \\in a \\cap (b \\cup c)}}\n    }\n    {\\dfrac\n     {x \\in a \\land x \\in b \\cup c}\n     {x \\in a}\n     \\land_{e_1}\n    }\n    \\!\\to_e\n    { \\atop {\\atop \\displaystyle{\\atop \\displaystyle{x \\in c}}}}\n   }\n   {x \\in a \\land x \\in c}\n   \\land_i\n}\n{\\dfrac{x \\in a \\cap c}{x \\in (a \\cap b) \\cup (a \\cap c)}\\lor_{i_2}}\n\\!\\to_e\n$$\nNote that in the usual presentation of ZFC (see for instance here), the formulas I called $\\text{ax}_1$, $\\text{ax}_2$ and $\\text{ax}_3$ are not axioms but immediate consequences of more basic axioms. Therefore, the derivations above are actually bigger (and impossible to represent in one page), because they hide the derivations of $\\text{ax}_1$, $\\text{ax}_2$ and $\\text{ax}_3$ from the real axioms of ZFC.\n\nFor further and more sophisticated details about axioms in formal proof systems (with a slightly different point of view), see here and here.\n",
    "tags": [
      "logic",
      "proof-writing",
      "proof-theory",
      "natural-deduction",
      "formal-proofs"
    ],
    "score": 4,
    "answer_score": 10,
    "is_accepted": true,
    "question_id": 4836916,
    "answer_id": 4836937
  },
  {
    "theorem": "How to know when I&#39;ve proved a claim (with example)",
    "context": "I have a homework problem and want to try to understand the process without following steps from other outside sources. I understand basic logic and have done a handful of introductory proofs to this point. My problem is, when I am doing my scratchwork and gathering evidence, sometimes I can feel like I've just gone full circle and not addressed the claim or that I've completed the proof but am unaware of it.\nFor this problem, it asked me to prove that: For all positive real numbers $x$, the sum of $x$ and its reciprocal is greater than or equal to $2$.\nThis is my work currently:\nSuppose $x + \\frac{1}{x} \\geq 2$ where $x \\in\\mathbb{R}$ and $x > 0$.\nMultiplying by $x$, we obtain $x(x+\\frac{1}{x})\\geq2x$\nAnd $x(x+\\frac{1}{x})=x^{2}+1\\geq2x$\nSubtracting $2x$ from both sides gives $x^{2}-2x+1\\geq0$\nAnd further we see $x^{2}-2x+1=(x-1)^{2}\\geq0$ which is true for any real number $x$.\nThus, it has been shown. $\\square$\nDoes this count as a proof? I'm not sure if my assumption is the correct way to go about it. I understood that you assume $P$ and show that it means $Q$ is true. But in this case it feels like I'm assuming $P\\rightarrow Q$ which doesn't make sense to me.\nFor a clear question, could someone explain to me the logical groundwork  behind a proof like this? What constitutes proven and why does whatever is shown justify the claim to be proven?\n",
    "proof": "No, this is not a proof. You started with the claim $x + 1/x \\geq 2$ and derived a true statement. To provide a proof, you must start with known true statements and derive the fact that $x + 1/x \\geq 2$.\nEssentially, you did the proof backwards.\nThe correct proof would go as follows:\nNote that $x^2 - 2x + 1 = (x - 1)^2 \\geq 0$. Adding $2x$ to both sides, we see that $x^2 + 1 \\geq 2x$. Now since $x$ is positive, we can divide both sides by $x$ to see that $x + 1/x \\geq 2$, as required.\nIf you really want to know that you have provided a complete proof, you would translate this into a formal proof and provide a rigorous justification for every step.\n",
    "tags": [
      "logic",
      "proof-writing",
      "solution-verification",
      "proof-explanation"
    ],
    "score": 4,
    "answer_score": 7,
    "is_accepted": false,
    "question_id": 4367132,
    "answer_id": 4367137
  },
  {
    "theorem": "Proof that the roots of $\\mathrm e^{-πx}=\\sin πx$ approach integers as $x\\to \\infty$",
    "context": "This question is inspired by @gt6989b’s comment here.\nNumerical analysis suggests that the roots of the equation $\\newcommand{\\e}{\\mathrm{e}} \\e^{-πx} = \\sin πx$ rapidly and closely approach integers as $x\\to\\infty$. Here’s a quick list of the first nine solutions:\n$$\\begin{array}{l}\n0.18733579075230\\dots \\\\\n0.98560325090923\\dots \\\\\n2.00059331886993\\dots \\\\\n2.99997431047250\\dots \\\\\n4.00000111005168\\dots \\\\\n4.99999995203014\\dots \\\\\n6.00000000207297\\dots \\\\\n6.99999999991042\\dots \\\\\n8.00000000000387\\dots \\\\\n\\end{array}$$\nHow can I prove (or disprove) that these values will get closer and closer to integers?\nWolfie notes that the system has the alternate form $$\\newcommand{\\i}{\\mathrm{i}} \\e^{-πx} = \\frac{\\i\\e^{-\\i πx} - \\i\\e^{\\i πx}}2$$\n",
    "proof": "Since for a large positive integer $n$, $e^{-\\pi n} \\approx 0$ and $\\sin(\\pi x)$ has roots at the integers, we expect that the equation has roots close to positive integers. To gain a better approximation, we choose the approximate value $n \\in \\mathbb{N}$ for the $n$th root $x_n$ and apply Newton's method once. This yields the better approximation:\n$$\nx_n  \\approx n + \\frac{1}{\\pi }\\frac{1}{{( - 1)^n e^{\\pi n}  + 1}}.\n$$\nThis shows that $x_n$ converges to $n$ exponentially fast.\n",
    "tags": [
      "sequences-and-series",
      "limits",
      "proof-writing",
      "numerical-calculus"
    ],
    "score": 4,
    "answer_score": 5,
    "is_accepted": true,
    "question_id": 3602833,
    "answer_id": 3602864
  },
  {
    "theorem": "How to keep quantifiers organised in a proof?",
    "context": "I’m self-studying Tao’s Analysis I. There are instances when trying to write (or even read) a proof, it becomes difficult to keep track of quantification of variables—both universal and existential. I agree that these are most explicitly visible in a formal proof (with proper syntactical and logical rules), but most of us write proofs in English.\nThe trouble with this style, at least for me, arises when we try to extend the quantification of variables beyond a single sentence. For instance, “For all $n\\ge 1, P(n)$ holds. $P(n)$ implies $Q(n)$. Hence $Q(n)$ holds for all $n\\ge 1$.” In this example it was implicitly quite clear that $n$ was a bound variable in the second sentence too. But in quite convoluted arguments, keeping track of this kinda stuff can be daunting.\nBut even then, I have seen some instances where, even in English, this can made more explicit. For instance, in “There exists an $n_0$ such that $P(n_0)$ holds. Fix this $n_0$. ...”, it is quite clear that $n_0$ is existentially instantiated in what follows.\nHow to be explicit and clear about this? I’m looking for some suggestions on this from you mathematicians. How do you take care of this?\n",
    "proof": "I also had this issue when learning. The short answer is you just develop a system which works for you, and you just get used to things by reading/practicing more.\nI'll just illustrate with a simple example of what helped me. For instance, if I'm writing an $\\epsilon, \\delta$ proof for proving a certain function is uniformly continuous, I would first write down/think in my mind what the statement to be shown is, and I would write all the quantifiers explicitly. So if I want to show $f:X \\to Z$ is uniformly continuous ($X$, $Z$ being metric spaces say), then I would first write down for myself:\n\\begin{align}\n\\forall \\epsilon > 0, \\exists \\delta > 0: \\, \\forall x,y \\in X, \\, \\, \\text{if $d_X(x,y)< \\delta$ then $d_Z(f(x), f(y)) < \\epsilon$}.\n\\end{align}\nSomething which helped me was to introduce variables in the same order as they appear in the statement to be proven. For example, in the above statement, $\\epsilon$ is the first thing to appear, and it appears with a universal quantifier. Hence, the very first sentence of my proof would always start \n\n\"Let $\\epsilon > 0$ be an arbitrary number.\" \n\nAnd I would add in the adjective \"arbitrary\", purely for emphasis, because for some reason it just helped me to register that it is a universally quantified object.\nThen, the next thing which appears in the statement to be proven is $\\delta$, and it appears with an existential quantifier. So, after some relevant steps, I would say\n\n\"Choose $\\delta = ...$\"\n\nor \n\n\"Define $\\delta = ...$\"\n\nOnce again, I add in an adjective in front of the $\\delta$ to make it clear (atleast to myself) that it is an existential quantifier. Then the next step of the proof requires the universally quantified $x,y$. So, I would say something like\n\nTake any $x,y \\in X$ such that $d_X(x,y) < \\delta$.\n\nHopefully you get the gist; I basically introduce variables in the exact same order as they appear in a mathematical statement, and when I introduce them in my proof, I add adjectives in front to emphasize to myself what kind of quantifier it is.\n\nSo, for example, if I have to prove that $f: \\Bbb{R} \\to \\Bbb{R}$ defined by $f(x) = \\sin(2x)$ is uniformly continuous on $\\Bbb{R}$, here's how I'd structure the argument:\n\nLet $\\epsilon > 0$ be an arbitrary number. Now, choose $\\delta := \\dfrac{\\epsilon}{2}$. Take any $x,y \\in \\Bbb{R}$ such that $|x-y| < \\delta$. Then, we have\n  \\begin{align}\n|\\sin(2x) - \\sin(2y)| & \\leq \\sup_{\\xi \\in \\Bbb{R}}\\left|\\dfrac{d}{d \\xi} \\sin(2\\xi) \\right| \\cdot |x-y| \\tag{Mean-value theorem} \\\\\n& \\leq 2 \\cdot \\delta \\\\\n&< \\epsilon \\tag{by definition of $\\delta$}\n\\end{align}\n  Since $\\epsilon > 0$ was arbitrary, this proves $f$ is uniformly continuous on $\\Bbb{R}$.\n\nHopefully that's helpful.\n\nEdit: Your purpose:\nOf course, the manner in which you structure an argument heavily depends on who you're writing for, and also how comfortable you are with subject material. For example, are you in an introductory proofs course writing an assignment? Are you in final year of a math specialist program? Basically, you need to know your audience and the level of detail necessary for a certain argument.\nThe argument I gave above for uniform continuity of $f(x) = \\sin(2x)$ is something I would have written in my first year calculus course, if $\\epsilon, \\delta$ arguments are new to me, and if I was writing for an assignment. As mentioned in the comments, sometimes, certain adjectives are unnecessary, like the use of \"arbitrary\" in my proof above. Because that is already implied by the word \"let\", and this is common writing practice in math texts. Depending on the circumstances, I think the following one-liner would even suffice:\n\nSince $f$ has a bounded derivative on $\\Bbb{R}$, it is Lipschitz continuous on $\\Bbb{R}$, and hence uniformly continuous.\n\nBut of course, if its your first introduction to a subject, it is always best to be very explicit about your proof strategy and your subsequent reasoning.\n",
    "tags": [
      "proof-writing",
      "soft-question",
      "formal-proofs"
    ],
    "score": 4,
    "answer_score": 4,
    "is_accepted": false,
    "question_id": 3599073,
    "answer_id": 3599112
  },
  {
    "theorem": "is this true for an induction proof",
    "context": "If I prove $P(0),$ \"for all $n: P(n) \\Rightarrow P(n+2)$\", and \"for all $n: P(n) \\Rightarrow P(n+3)$\", it still may not be true that $P(n)$ is true for all naturals $n.$\nIf so how can we be sure about $n+1.$\n",
    "proof": "If you prove $P(0)$ and $\\forall n(P(n)\\to P(n+2))$ then you have proven $P$ is true of any even natural number. (Since we have $P(0)$ and $P(0)\\to P(2),$ and thus $P(2),$ and we have $P(2)\\to P(4)$ and thus $P(4)$ and so on.) \nSimilarly, if you prove $P(0)$ and $\\forall n(P(n)\\to P(n+3))$ then you have proven $P$ is true of any natural number that is a multiple of $3.$ \nGeneralizing, if you prove $P(0)$ and $\\forall n(P(n)\\to P(n+k))$ then you have proven that $P$ is true of every multiple of $k.$ \nWhen you do so for $k=1,$ you show it for every multiple of $1,$ i.e. every natural number.\n",
    "tags": [
      "proof-writing",
      "induction",
      "solution-verification"
    ],
    "score": 4,
    "answer_score": 4,
    "is_accepted": true,
    "question_id": 3406607,
    "answer_id": 3406629
  },
  {
    "theorem": "Proof that if $n^2-1$ is divisible by $8$ then $n$ is odd",
    "context": "I am still very new to proofs. I am just getting some practice, as I am still struggling with the concepts. Any feedback would be greatly appreciated. Thank you in advance.\nClaim: If $n^2-1$ is divisible by $8$, then $n$ is odd.\nProof: Suppose $n$ is even; that is, $n=2k$, for some $k \\in \\mathbb{Z}$. Then, $n^2-1 = (2k)^2-1$, which is odd. Therefore, $n$ must be odd and thus, contradicts our assumption that $n$ is even.\nThis is my second practice proof I am working on so please be gentle! I am still learning the logic and working through the proofs slowly. Any feedback, tips, or hints/tricks would be greatly appreciated! Thanks.\n",
    "proof": "In your problem, proof-by-contradiction works as follows: \n(Hypothesis) Suppose 8 divides $n^2-1$. \n(Assumption) Let us assume that $n$ is even. \n$n$ is even $\\implies$ $n=2k$ for some $k \\in \\mathbb{Z}$\nThen, $n^2-1 = (2k)^2-1$ \n$n^2-1 = 2(2k^2)-1 \\implies n^2-1 $ is an odd number.\n$n^2-1$ is odd $\\implies$ 8 does not divide $n^2-1$. \nContradiction! That is, assuming n is even contradicts our hypothesis (i.e., $n^2-1$ is divisible by 8).\nSo our assumption is wrong. This means $n$ has to be odd.\nAlternatively, we can prove the contrapositive (i.e., Proving $A\\implies B $ is equivalent to proving $ \\neg B \\implies \\neg A $ ). This works as follows in your case:\nSuppose $n$ is even.\nThen, $n = 2k$  for some $k \\in \\mathbb{Z}$\n$\\implies$ $n^2-1 = (2k)^2-1$\n$\\implies$ $n^2-1 = 2(2k^2)-1$\n$\\implies$ $n^2-1$ is odd.\n$\\implies$ $n^2-1$ is not divisible by 8.\nThus, we have proved this:  $n$ is not odd $\\implies$ $n^2-1$ is not divisible by 8, which is equivalent to proving this: $n^2-1$ is divisible 8 $\\implies n$ is odd.  \n",
    "tags": [
      "elementary-number-theory",
      "proof-verification",
      "proof-writing"
    ],
    "score": 4,
    "answer_score": 5,
    "is_accepted": false,
    "question_id": 2864035,
    "answer_id": 2864066
  },
  {
    "theorem": "Transitions from $\\frac{1}{n}$ to $\\epsilon$ in real analysis proofs",
    "context": "I would like to prove the following statement: \n\nLet $A\\subseteq\\mathbb{R}$ be nonempty and bounded above, and let $s\\in\\mathbb{R}$ have the property that for all $n\\in\\mathbb{N}$, $s+\\frac{1}{n}$ is an upper bound for $A$ and $s-\\frac{1}{n}$ is not an upper bound for $A$. Prove that $s=\\sup A$. \n\nWhat I would like to know is if I can use the Archimedean property to \"exchange\" $\\frac{1}{n}$ with some $\\epsilon >0$. If I can accomplish this exchange successfully, the rest of the proof is trivial. \nHere is my attempt: By the Archimedean property, for any $y>0$ there exists a number $n\\in\\mathbb{N}$ such that $y>\\frac{1}{n}>0$. Therefore, for any $\\epsilon_0>0$ we can choose $\\epsilon=\\frac{1}{n}$ such that $\\epsilon_0>\\epsilon>0$. \nI'm not too sure how I can word this to make sense. Basically, how do I convey that the proof using $\\frac{1}{n}$ is equivalent to the proof using $\\epsilon$? \n",
    "proof": "Yes you can do that.\nLet $n > \\frac 1{\\epsilon}$.  Then $\\frac 1n < \\epsilon$.\nSo for any $p < s$, let $\\epsilon= s-p > 0$ then if $n > \\frac 1 {\\epsilon}$ then $p = s-\\epsilon < s-\\frac 1n$ so $p$ is not upper bound so $\\sup A \\ge s$.\nFor and $r >s$, let $\\epsilon = r -s > 0$ and $n > \\frac 1{\\epsilon}$ so $s + \\frac 1n < r$ so $s + \\frac 1n$ is an upper bound so $r$ is an upper bound but not the least upper bound.  \nSo $\\sup A \\not > s$ so $\\sup A \\le s$.\nSo $\\sup A = s$.\n",
    "tags": [
      "real-analysis",
      "proof-verification",
      "proof-writing"
    ],
    "score": 4,
    "answer_score": 3,
    "is_accepted": true,
    "question_id": 2678686,
    "answer_id": 2678719
  },
  {
    "theorem": "How does one know if $A \\implies B$ (an implication) is true without knowing if $B$ (the consequent) is true?",
    "context": "This might be a weird question but I was trying to distinguish the difference between an implication and modus ponens. I think the distinction is clear in my head (but I have something missing), modus ponens is just a rule of inference that produces more true statement based on $A$ and $A \\implies B$. An implication is a truth function that when given statements with truth values, i.e. the truth values of $A \\implies B$ can be known only from the truth values of $A$ and $B$ (no other info is needed). It can be easily evaluated by looking up the truth table. These make sense. However, I reached a confusion/contradiction in the model of how I thought logic (and mathematics!) worked. It seems to me that according to modus ponens we need to know already the value of the implication $A \\implies B$ before we can known if $B$ is true. However, according to the table to evaluate the truth function we need the values of both $A$ and $B$. So it seems like a chicken and egg problem. How is it actually possible to know $A \\implies B$ without knowing $B$? It seems odd to me. I do understand however, how the inference is suppose to work. Since we know the output of the function (=implication) and one of its inputs, then it should be trivial, to know the other input because of the way the truth table for implication is defined. I think that part makes sense. However, what I don't understand is how in practice we are able to know $A \\implies B$ is true in the first. \nI think the main issue I had is that in my head what I thought is that for $A \\implies B$ to be known to be true, we actually proceeded to apply rules of inference to our statements and then reached $B$. Thats I think how I thought I did maths in practice. I started with $A$ and applied valid maths rules and inference rules until I reached $B$. Thus, it seems that I never actually used the truth functional implication to do any maths, only maths facts that produced step by step another maths step until the final $B$ was produced. I assume there must be some confusion in how I thought mathematics worked, so I wanted to clarify, does someone know where my confusion is? \nWhat does a \"maths step\" even mean? I thought it was modus ponens but now I realized I need to know $A \\implies B$ for that to be true but that can't be true because that's what I am trying to figure out how to get the truth value of in the first place to be able to even use modus ponens.\nIn the end it all seems to boil down to, how do we actually conclude $A \\implies B$ is true in a proof?\nI always thought that we started with $A$, the just mechanically moved from $A$ to the next step and the next step until $A$ arrived at $B$ and then at that point we'd know $B$ was true. Is that not correct? I'm not sure if what I am asking is what a \"step\" means in mathematics. It seems like it because I would have thought intuitively that a series of steps like that must be a set of implications OR alternatively a series of steps of Modus Ponens. Regardless, it seems to me I understand what the difference between a modus ponens and implications are but I can't seem how to figure out how an implication is even known to be true in the first place without resulting in circular logic. \nHow does one know if $A \\implies B$ (an implication) is true without knowing if $B$ (the consequent is true) is true?\n\nmy apologies I wasn't quite sure how to compress my question.\n",
    "proof": "I assume you would agree with a statement like \"If it rains, the streets get wet', right?\nNow, before you agreed to the truth of that conditional, did you look out the window to see if right now it is raining or not, and whether right now the streets are getting wet or not? No, clearly not.\nYes, it is true that we often come to the truth of conditionals based on empirical evidence, i.e. presumably on a bunch of cases where we observe the antecedent is true and the consequent also true ... and also not seeing cases where the antecedent is true and the consequent false. Such conditionals are established by non-deductive reasoning though, but inductive generalizations or other such reasoning. So, there is no circularity of reasoning here.\nAlso, some conditionals are simply asserted as part of some definition or axiom, so in that case any observations of ay statements actually being true or false aren't used at all.\nFurthermore, conditionals are often part of universal statements. The 'if it rains, the streets get wet' can in fact be seen as such as well: it is really a universal claim about any place and any time.  So it is really the universal we are either establishing on the basis of many observations, or are simply asserting as an axiom, and so there we are certainly not establishing the truth of that universal conditional on a single observation as to whether the antecedent and consequent are true or false.\nFinally, once conditionals are established, we can just use them, without having to worry whether the antecedent is actually true or false, or whether the consequent is true or false. So, in practice, there is really no chicken-and-egg problem here.\n",
    "tags": [
      "logic",
      "proof-writing",
      "propositional-calculus",
      "first-order-logic"
    ],
    "score": 4,
    "answer_score": 3,
    "is_accepted": false,
    "question_id": 2576180,
    "answer_id": 2576198
  },
  {
    "theorem": "Need help proving there is not a surjective function from A to the set of all functions from A to A",
    "context": "We have $F_A = \\{ f|f:A\\rightarrow A \\}$ is the set of all functions from A to A, where A is a nonempty set that could be infinite. I'm trying to show that if $|A|>1$ then $|A| < | F_A |$. I have shown that $|A| \\leq | F_A |$ by finding a one to one function from A to $F_A $, but I don't know how to go about showing there isn't an onto function.\n",
    "proof": "Suppose we have a surjective map $m: A \\rightarrow F_A$, consider the image of an element $a \\in A$ under this map; it will be a mapping $f_a : A \\rightarrow A$, now consider the image of $a$ under this map, $ f_a(a)$ will be an element of  $ A$. Define a mapping $x:A \\rightarrow A $ by requiring $x(a)$ is an element of $A$ that is different from $f_a(a)$; this function will not be in the image of $m$ and thus $m$ is not surjective.\n",
    "tags": [
      "proof-writing"
    ],
    "score": 4,
    "answer_score": 7,
    "is_accepted": true,
    "question_id": 2513231,
    "answer_id": 2513246
  },
  {
    "theorem": "Is is true that $p_{n+1}p_n \\mod p_{n+1}+p_n$ is prime?",
    "context": "I noticed a pattern when messing around with prime numbers, and I conjectured that, if $p_n$ is the nth prime number, then\n$$(p_{n+1}p_n)\\mod(p_{n+1}+p_n)$$\nis also prime. I have very little experience in dealing with prime numbers, so I'm not even sure how to start this proof. Can someone please either show me how to start this or disprove it? It would also be helpful if you know of any online resources that have any kind of practice proofs regarding prime numbers.\nThanks!\n",
    "proof": "$$17 \\times 19 = 323 \\equiv 35  \\pmod { 36}$$\n",
    "tags": [
      "proof-writing",
      "prime-numbers"
    ],
    "score": 4,
    "answer_score": 5,
    "is_accepted": true,
    "question_id": 2278414,
    "answer_id": 2278424
  },
  {
    "theorem": "Prove that $(a^2+2)(b^2+2)(c^2+2)\\geq 3(a+b+c)^2$",
    "context": "\nFor the non-negative real numbers $a, b, c$ prove that $$(a^2+2)(b^2+2)(c^2+2)\\geq 3(a+b+c)^2$$\n\nWhat I did is applying Holder's inequality in LHS:$$(a^2+(\\sqrt{2})^2)(b^2+(\\sqrt{2})^2)(c^2+(\\sqrt{2})^2) \\geq (abc + 2\\sqrt{2})^2$$ \nThen it suffices to prove that $$(abc+2\\sqrt2)^2 \\geq 3(a+b+c)^2 \\\\ \\Rightarrow abc+2\\sqrt2 \\geq \\sqrt3(a+b+c)$$ \nBut I don't know how to proceed. I also think I applied Holder's Inequality incorrectly.\n",
    "proof": "By C-S $ (a^{2}+2)\\left(1+\\frac{(b+c)^{2}}{2}\\right)\\geq(a+b+c)^{2}$.\nHence, it remains to prove that $ (b^{2}+2)(c^{2}+2)\\geq3\\left(1+\\frac{(b+c)^{2}}{2}\\right)$,\nwhich is equivalent to $ (b-c)^{2}+2(bc-1)^{2}\\geq0$.\n",
    "tags": [
      "inequality",
      "polynomials",
      "proof-writing",
      "contest-math",
      "cauchy-schwarz-inequality"
    ],
    "score": 4,
    "answer_score": 7,
    "is_accepted": true,
    "question_id": 1602810,
    "answer_id": 1602823
  },
  {
    "theorem": "A quadratic polynomial is nonnegative if and only if the discriminant is nonpositive",
    "context": "I want to show that if $a>0$ the inequality $ax^2+2bx+c\\ge 0 $ for all values of $x$ if and only if $b^2-ac\\le 0$.\nI tried to prove it by: $ax^2+2bx+c≥ b^2-ac$. Used partial derivatives with respect to $a$. Sorry, I am new to proofs. Can someone help me out?\n",
    "proof": "The quadratic formula is all you need to prove this. \nIf you have a quadratic polynomial: \n$$ ax^2+bx+c=0$$\nThen, to find the zeroes, you rearrange the above expression into the quadratic formula: \n$$ x=\\frac{-b\\pm \\sqrt{b^2-4ac}}{2a}$$\nFrom here, you can tell the discriminant, $\\Delta =b^2-4ac$ and from $\\Delta $ you can tell how many zeroes (where the graph touches the x-axis) the function will have. If $\\Delta =0$ the polynomial function will have 1 zero, since the plus or minus term becomes zero and there is only one value for x. If $\\Delta\\gt0 $ then the function will have two zeroes, since the plus or minus term afects the negative b term. If $\\Delta\\lt0 $ then the function will not have any real zeroes because the term inside the root will be negative and the square root of a negative number is an imaginary number. \nTherefore, when $b^2-4ac\\le0$ the polynomial equation can only have a maximum of one zero, its square root term (the root of the determinant $\\Delta $) will only either yield a zero or an imaginary number. If we also know that $a\\gt0$, then we know that the parabola defined by the quadratic will open upward like a happy face. We also know that it will only have one zero, maximum. Thus, the parabola will always be above the x-axis, or it will only touch it once, making the above statement $a>0$,$ax2+2bx+c≥0$ for all values of x if,and only if $b^2−ac≤0$\n",
    "tags": [
      "calculus",
      "polynomials",
      "proof-writing",
      "quadratics"
    ],
    "score": 4,
    "answer_score": 7,
    "is_accepted": false,
    "question_id": 1076225,
    "answer_id": 1076262
  },
  {
    "theorem": "Prove that $m+\\frac{4}{m^2}\\geq3$ for every $m &gt; 0$",
    "context": "How to deal with $m+\\frac{4}{m^2}\\geq3$ for every $m > 0$ ? I multiplied both sides by $m^2$ and got $m^3+4-3m^2\\geq0$ and have no idea how to continue with this\n",
    "proof": "$$\\frac{m}{2} + \\frac{m}{2}+ \\frac{4}{m^2} \\geq 3(\\frac{m \\cdot m \\cdot 4}{2\\cdot 2 \\cdot m^2})^{1/3} = 3$$\n",
    "tags": [
      "algebra-precalculus",
      "inequality",
      "proof-writing"
    ],
    "score": 4,
    "answer_score": 7,
    "is_accepted": false,
    "question_id": 1001834,
    "answer_id": 1001841
  },
  {
    "theorem": "Prove a=v*dv/dx",
    "context": "Using calculus, and assuming a particle moving along the x-axis is concerned, prove that $a=v*dv/dx$\n~~~~~~~~~~~~\nthis is what I did, but im not sure it's rigorous enough:\n$a=dv/dt$\n$t=x/v$\n$a=dv/d(x/v)$\n(read all x1 as x subscript 1)\n$a=\\lim_{(x_1 \\rightarrow x_0)}\\frac {(v_1-v_0)}{(x_1/v_1 - x_0/v_0)}$\nConsider the denominator; as $x_1\\rightarrow x_0$, \n$v_1\\rightarrow v_0$;\nso we can rewrite the equation as\n$a= \\lim_{(x_1 \\rightarrow x_0)}\\frac {(v_1-v_0)}{((x_1-x_0)/v))}$\n(rest of proof flows easily from here)\nNow the last line is what im not so sure about; I believe my reasoning is correct, but the proof doesnt seem rigorous to me, at least not in the way it's notated; since we took the limit of denominator v shouldnt be included in the limit, but I dont know how to show it, can I say that\n $\\lim_{(x1 \\rightarrow x0)} x_1/v_1 = \\frac {\\lim_{(x_1 \\rightarrow x_0)} x_1)}{(\\lim_{ (x_1 \\rightarrow x_0)} v_1)} $\nwould that solve my problem?\n",
    "proof": "Not sure if this is \"calculusy\" enough for you: \n$$a = \\dfrac{dv}{dt} = v \\cdot \\dfrac{dv}{dt} \\cdot \\dfrac{1}{v} = v  \\cdot \\dfrac{dv}{dt} \\cdot \\dfrac{dt}{dx} = v\\cdot \\dfrac{dv}{dx}$$\n",
    "tags": [
      "calculus",
      "proof-verification",
      "proof-writing",
      "physics"
    ],
    "score": 4,
    "answer_score": 9,
    "is_accepted": false,
    "question_id": 951637,
    "answer_id": 951655
  },
  {
    "theorem": "H0w t0 prove that periodic decimal numbers are rational? $a_1...a_k(b_1b_2..b_l)={m \\over n}$",
    "context": "Given $a_1...a_k(b_1b_2..b_l)={m \\over n}$ how can I prove that periodic decimal numbers are rational?\nWhere do I even begin?\n",
    "proof": "Consider the decimal $r=0.b_1b_2\\ldots b_nb_1b_2\\ldots$, that is, a simple decimal of period $n$.  Multiply $r$ by $10^n$:\n$$10^n r = b_1b_2\\ldots b_n.b_1b_2\\ldots b_n \\ldots$$\nor\n$$(10^n-1) r =  b_1b_2\\ldots b_n$$\nRemember that $b_1b_2\\ldots b_n$ is an integer, so that $r$ is a rational number after reduction to simplest form.\nFor a more general decimal $0.a_1a_2 \\ldots a_m b_1b_2\\ldots b_nb_1b_2 \\ldots$, note that $0.a_1a_2 \\ldots a_m$ is a rational number in and of itself, leaving the remainder as a rational number ($10^{-m}$) times the repeating decimal, so that the more general case also leaves a rational number.\n",
    "tags": [
      "proof-writing",
      "rational-numbers"
    ],
    "score": 4,
    "answer_score": 3,
    "is_accepted": true,
    "question_id": 295195,
    "answer_id": 295202
  },
  {
    "theorem": "Proving an inequality",
    "context": "Suppose $a$ and $b$ are real numbers. Prove that if $a<b$ then $\\frac{a+b}{2}<b$.\nThe 'solution' hints at adding $b$ to both sides of the inequality $a<b$, and $a+b<2b$ is as far as I've got (I don't know the reason for adding $b$ to both sides, either - or rather, why you'd think to add the $b$ to each side to help finish the proof, instead of doing something else) - I'd like to know what to try next.\nSource of exercise: How To Prove it: A Structured Approach, Second Edition - Daniel J. Velleman.\n\nThank-you for the responses. Is the following solution sufficient proof, or is there something I'm missing?\n\nProof. Suppose $a$ and $b$ are real numbers, and $a<b$. Adding $b$ to both sides of the inequality $a<b$, we get $a+b<2b$. Subsequently,\n  we can divide both sides by 2, to get $\\frac{a+b}{2}<b$. Therefore, if\n  $a<b$, it follows then that $\\frac{a+b}{2}<b$.\n\n",
    "proof": "The reason for adding $b$ to both sides is that you want to get $a+b$.  Once you've got $a+b < 2b$, you can divide both sides by $2$.  The reason for doing that is that you want $(a+b)/2$.  Here it matters that $2$ is positive: if you divide both sides by a negative number, then you'd have to change $<$ to $>$, but since $2$ is positive, $<$ remains $<$.\n",
    "tags": [
      "inequality",
      "proof-writing"
    ],
    "score": 4,
    "answer_score": 4,
    "is_accepted": true,
    "question_id": 51435,
    "answer_id": 51436
  },
  {
    "theorem": "Is a monotone bijection on $\\mathbb N$ necessarily the identity map?",
    "context": "A junior fellow gave me this problem as a doubt.\n\nLet $f\\colon\\mathbb N\\to\\mathbb N$ be increasing and a bijection then show that $f(n)=n$ for all $n\\in\\mathbb N$.\n\nI could prove this easily using induction. The base case is like this... Suppose $f(1)\\neq 1$ then $f(1)\\geq 2$ and since $f$ is monotone, we miss $1$ from the image of $f$ (contradiction). Thus, $f(1)=1$.\nBut they are not satisfied with the solution. I would love to see some alternative and a generalisation of this result. For instance, if we replace $\\mathbb N$ by $\\mathbb Q$, the result does not hold since $x\\mapsto 2x$ is a counter example. What is the property of $\\mathbb N$ that made the difference?\n",
    "proof": "Well, your induction proof.... doesn't have an inductive step!\nLet $P(n)$ be:  $f(n) = n$\nBase case (can be done for $0$ or $1$ depending on you definition of $\\mathbb N$.  That is not relevant.  I'll do it for $0\\not \\in \\mathbb N$)\n$f(1) =1$.  If $f(1)\\ne 1$ then $f(1)=c$ for some natural number $c\\ne 1$.  As $1$ is the smallest natural number $c > 1$.  As $f$ is strictly increasing for any $n \\ge 1$ we have $f(n) \\ge f(1)= c> 1$.  So there is not $n$ where $f(n)=1$.  So the function is not surjective. That contradicts its a bijection so $f(1) = 1$.\nInductive step:\nAssume $f(n) = n$. As $f$ is increasing $f(n+1) > f(n) = n$.  In other words, $f(n+1) \\ge n+1$.  Suppose $f(n+1) \\ne n+1$.  As $f(n+1) \\ge n+1$,  $f(n+1) \\ne n+1 \\implies f(n+1)> n+1$.  Let $f(n+1)=c > n+1$.\nWe claim there is no $m$ so that $f(m) = n+1$.  If $m \\le n$ then as $f$ is increasing $f(m) \\le n < n+1$.  If $m > n$ then $m \\ge n+1$ and so $f(m) \\ge f(n+1)=c > n+1$.  Thus we have a contradiction that $f$ is not surjective.  so $f(m+1) = m+1$.\n\nBut are you familiar with the well ordering principal.\n\nEvery non-empty subset of $\\mathbb N$ has a least element.\n\nSuppose $K = \\{n\\in \\mathbb N| f(n)\\ne n\\}$.\nOutline of proof:  If $K$ is empty then $f$ is identity. (Because if $K$ is empty, there are not $n$ so that $f(n)\\ne n$ so for all $n$ we have $f(n)=n$.)\nWe are going to prove $K$ is empty.\nTo prove $K$ is empty we are going to prove that $K$ does not have a least element.  (By well-ordering principal, every non-empty subset has a least element. So if $K$ does not, it must be empty.)\nTo prove $K$ does not have a least element we will show that for any arbitrary $k\\in K$ that $k$ is not the least element of $K$.  If no arbitrary $k\\in K$ can be the least element, then $K$ doesn't have a least element.\nSo we are going to assume $k \\in K$ and we will prove that $k$ is not the least element of $K$.  That will be a sufficient proof.  Got it? Let's go:\nLet $k \\in K$ so $f(k) \\ne k$.  Let $f(k)=m$.  As $f$ is surjective there must be a $w$ so that $f(w)=k$.  As $f(w)=k\\ne f(k)$ we have $w\\ne k=f(w)$ so $w\\in K$.  And as $f$ is injective and $k\\ne m$ we must have $m=f(k)\\ne f(m)$.  So $m \\in K$.\nSo recap:  We have $w,k,m \\in K$ and $f(w)=k$ and $f(k)=m$.\nIf $m < k$ then $k$ is not the least element of $K$.\nIf $k < m$ then $f(w) = k < m = f(k)$.  As $f$ is strictly increasing, $w < k$.  So $k$ is not the least element of $K$.\n(And, of course, we've already established $k \\ne m$.)\nThat's it.  We are done.\nTo recap.  If $k \\in K$ then either $f(k)=m < k$ or $w=f^{-1}(k) < k$.  So $k$ can't be the least element of $K$.  As $k$ was completely arbitrary. $K$ can not have a least element. All non-empty subsets of the natural numbers have a least element, so $K$ can't be non-empty.  So $K$ is empty and so for all $n\\in \\mathbb N; n\\not \\in K$ so $f(n) = n$ and so $f$ is the identity.\n",
    "tags": [
      "proof-writing",
      "monotone-functions"
    ],
    "score": 4,
    "answer_score": 1,
    "is_accepted": true,
    "question_id": 4985421,
    "answer_id": 4985505
  },
  {
    "theorem": "What is the proof of this binomial coefficient lemma?",
    "context": "I have this lemma in my course material, but it came with no proof:\n$$ {n\\choose k}+{n\\choose k-1} = {n+1\\choose k}$$\nI tried to prove it myself and got quite far I think, but I have no idea how to continue, so I am here asking for help to finish it.\nThis is how far I got:\n$$ \\frac{n!}{k!(n-k)!}+\\frac{n!}{(k-1)!(n-k+1)!}=\\frac{n!}{k!(n-k)!}+\\frac{n!(n-k)}{(k-1)!(n-k)!} = \\frac{n!}{k!(n-k)!}+\\frac{n!(n-k)k}{k!(n-k)!} = \\frac{n!+n!(n-k)k}{k!(n-k)!} $$\n",
    "proof": "$$\\frac{n!}{k!(n-k)!}+\\frac{n!}{(k-1)!(n-k+1)!}$$\n$$=\\frac{n!(n-k+1)}{k!(n-k+1)!}+\\frac{k\\cdot n!}{k!(n-k+1)!}$$\n$$=\\frac{n!(n-k+1+k)}{k!(n-k+1)!}=\\frac{(n+1)!}{k!(n-k+1)!}$$\n",
    "tags": [
      "proof-writing",
      "binomial-coefficients"
    ],
    "score": 4,
    "answer_score": 6,
    "is_accepted": false,
    "question_id": 3822487,
    "answer_id": 3822493
  },
  {
    "theorem": "Natural deduction proof for $(a\\to(b\\to c))\\to((a\\to b)\\to(a\\to c))$",
    "context": "I am trying to prove that \n$(a\\to(b\\to c))\\to((a\\to b)\\to(a\\to c))$\nholds in natural deduction, in particular when I work backwards from a Fitch style proof I can only get so far:\n\nHow can I prove it?\n",
    "proof": "You have only to remove the unnecessary LEM application : lines 3,9,10,11, and you will get the correct derivation :\n1) $a \\to (b \\to c)$ --- assumed [a]\n2) $a \\to b$ --- assumed [b]\n3) $a$ --- assumed [c]\n4) $b$ --- from 2) and 3), by $\\to$-elim\n5) $b \\to c$ --- from 1) and 3), by $\\to$-elim\n6) $c$ --- from 5) and 4), by $\\to$-elim\n7) $a \\to c$ --- from 3) and 6), by $\\to$-intro, discharging [c]\n8) $(a \\to b) \\to (a \\to c)$ --- from 2) and 7) by $\\to$-intro, discharging [b]\n\n9) $\\vdash (a \\to (b \\to c)) \\to ((a \\to b) \\to (a \\to c))$ --- from 1) and 8) by $\\to$-intro, discharging [a].\n\n",
    "tags": [
      "logic",
      "proof-writing",
      "propositional-calculus",
      "natural-deduction"
    ],
    "score": 4,
    "answer_score": 6,
    "is_accepted": true,
    "question_id": 2935693,
    "answer_id": 2935718
  },
  {
    "theorem": "Proof Verification: If $A \\subset B$ and $B \\subset C$, then $ A \\cup B \\subset C$",
    "context": "I am trying to prove that:\n\nIf $A \\subset B$ and $B \\subset C$, then $ A \\cup B \\subset C$\n\nMy proof is :\nGiven some $x \\in A \\cup B$, it is true that either $x \\in A$ and/or $x \\in B$. IN the case that $x \\in A$  it is true that $x \\in B$, as $A \\subset B$, and that $x \\in C$ , as $B \\subset C$. In the case that $x \\in B$ it is true that $x \\in C$, as $B \\subset C$. Therefore, $A\\cup B \\subset C$\nIs this correct?. Any tips to improve this would be appreciated as I am self taught and new to proof writing.\n",
    "proof": "Just to answer  : yes, the approach is correct.\nYou could also prove that $A∪B=B$, and then the fact follows from $B=A∪B\\subset C$. \nFor example, if $x \\in B$ is true, then of course $x$ is in $A$ or $x$ is in $B$ is true, so $B \\subset A \\cup B$. If $x \\in A$, then $x \\in B$ because $A \\subset B$, and if $x \\in B$, then of course $x \\in B$, so $A \\cup B \\subset B$, hence $A \\cup B = B$.\n",
    "tags": [
      "elementary-set-theory",
      "proof-verification",
      "proof-writing"
    ],
    "score": 4,
    "answer_score": 6,
    "is_accepted": true,
    "question_id": 2876194,
    "answer_id": 2876199
  },
  {
    "theorem": "Prove that $\\lim\\limits_{n \\rightarrow \\infty}\\frac{x^n}{n!} = 0$",
    "context": "So I have to prove 2 things:\n\nThat $\\lim\\limits_{n \\rightarrow \\infty}\\frac{x^n}{n!} = 0$ where $n \\in \\mathbb N$ and $x \\in \\mathbb R, x>0$. \nThat $\\lim\\limits_{n \\rightarrow \\infty}\\frac{x^n}{n!} = 0$ where $n \\in \\mathbb N$ and $x \\in \\mathbb R$. \n\nFor #1, I know that $\\frac{x^n}{n!} >0$, which means that I can find an upper bound and use squeeze theorem. For #2, I have no idea where to start.\n",
    "proof": "I think I've answered this question before, but note that $e^x=\\sum_{n=0}^{\\infty} \\frac{x^n}{n!}$, so for every fixed $x\\in \\mathbb{R}$ the series converges, thus the sequence of the terms $\\{\\frac{x^n}{n!}\\}_{n\\in \\mathbb{N}}$ must converge to zero as $n$ goes to infinity, otherwise, the sum would not converge.\n",
    "tags": [
      "sequences-and-series",
      "limits",
      "proof-writing"
    ],
    "score": 4,
    "answer_score": 2,
    "is_accepted": false,
    "question_id": 2447850,
    "answer_id": 2447856
  },
  {
    "theorem": "Prove that $A^T$ and $A^T A$ have the same column space",
    "context": "Or more abstractly, let $T \\in \\mathcal{L}(U,V)$ be a linear map over finite dimensional vector spaces, I need to prove that $T^*$ and $T^* T$ have the same range.\nThe direction $v \\in range(T^*T) \\rightarrow v \\in range(T^*)$ is obvious. I'm stuck on the other direction. \nSuppose $u\\in range(T^*)$, then there exists $v \\in V$ such that $u = T^*v$. Now how do I show $u \\in range(T^*T)$? (I've proved that $T$ and $T^*T$ have the same nullspace, but that doesn't seem helpful here)\n",
    "proof": "$A$ and $A^TA$ have the same null space. Therefore, the orthogonal complements of their null spaces are the same. It is well-known that the orthogonal complement of any matrix $M$ is the column space of $M^T$, so $A^T$ and $(A^TA)^T=A^TA$ have the same column space.\n",
    "tags": [
      "linear-algebra",
      "proof-writing",
      "operator-theory",
      "linear-transformations"
    ],
    "score": 4,
    "answer_score": 7,
    "is_accepted": true,
    "question_id": 2077958,
    "answer_id": 2077977
  }
]